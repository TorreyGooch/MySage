{
  "video_id": "TCH_1BHY58I",
  "sentences": [
    {
      "id": 1,
      "text": "hi everyone hi everyone today we are continuing our today we are continuing our today we are continuing our implementation of makemore implementation of makemore implementation of makemore now in the last lecture we implemented now in the last lecture we implemented now in the last lecture we implemented the bigram language model and we the bigram language model and we the bigram language model and we implemented it both using counts and implemented it both using counts and implemented it both using counts and also using a super simple neural network also using a super simple neural network also using a super simple neural network that had a single linear layer that had a single linear layer that had a single linear layer now this is the now this is the now this is the jupyter notebook that we built out last jupyter notebook that we built out last jupyter notebook that we built out last lecture lecture lecture and we saw that the way we approached and we saw that the way we approached and we saw that the way we approached this is that we looked at only the this is that we looked at only the this is that we looked at only the single previous character and we single previous character and we single previous character and we predicted the distribution for the predicted the distribution for the predicted the distribution for the character that would go next in the character that would go next in the character that would go next in the sequence and we did that by taking sequence and we did that by taking sequence and we did that by taking counts and normalizing them into counts and normalizing them into counts and normalizing them into probabilities probabilities probabilities so that each row here sums to one so that each row here sums to one so that each row here sums to one now this is all well and good if you now this is all well and good if you now this is all well and good if you only have one character of previous only have one character of previous only have one character of previous context context context and this works and it's approachable the and this works",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 2,
      "text": "and it's approachable the and this works",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 3,
      "text": "and it's approachable the problem with this model of course is problem with this model of course is problem with this model of course is that that that the predictions from this model are not the predictions from this model are not the predictions from this model are not very good because you only take one very good because you only take one very good because you only take one character of context so the model didn't character of context so the model didn't character of context so the model didn't produce very name like sounding things produce very name like sounding things produce very name like sounding things now the problem with this approach now the problem with this approach now the problem with this approach though is that if we are to take more though is that if we are to take more though is that if we are to take more context into account when predicting the context into account when predicting the context into account when predicting the next character in a sequence things next character in a sequence things next character in a sequence things quickly blow up and this table the size quickly blow up and this table the size quickly blow up and this table the size of this table grows and in fact it grows of this table grows and in fact it grows of this table grows and in fact it grows exponentially with the length of the exponentially with the length of the exponentially with the length of the context context because if we only take a single because if we only take a single because if we only take a single character at a time that's 27 character at a time that's 27 character at a time that's 27 possibilities of context possibilities of context possibilities of context but if we take two characters in the but if we take two characters in the but if we take two characters in the past and try to predict the third one past and try to predict the third one past and try to predict the third one suddenly the number of rows in this suddenly the number of rows in this suddenly the number of rows in this matrix you can look at it that way matrix you can look at it that way matrix you can look at it that way is 27 times 27 so there's 729 is 27 times 27 so there's 729 is 27 times 27 so there's 729 possibilities for what could have come possibilities for what could have come possibilities for what could have come in the context in the context in the context if we take three characters as the if we take three characters as the if we take three characters as the context suddenly we have context suddenly we have context suddenly we have 20 000 possibilities of context 20 000 possibilities of context 20 000 possibilities of context and so there's just way too many rows of and so there's just way too many rows of and so there's just way too many rows of this matrix it's way too few counts this matrix",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 4,
      "text": "it's way too few counts this matrix",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 5,
      "text": "it's way too few counts for each possibility and the whole thing for each possibility and the whole thing for each possibility and the whole thing just kind of explodes and doesn't work just kind of explodes and doesn't work just kind of explodes and doesn't work very well very well very well",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 6,
      "text": "so that's why today we're going to move so that's why today we're going to move so that's why today we're going to move on to this bullet point here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 7,
      "text": "and we're on to this bullet point here and we're on to this bullet point here and we're going to implement a multi-layer going to implement a multi-layer going to implement a multi-layer perceptron model to predict the next uh perceptron model to predict the next uh perceptron model to predict the next uh character in a sequence character in a sequence character in a sequence and this modeling approach that we're and this modeling approach that we're and this modeling approach that we're going to adopt follows this paper going to adopt follows this paper going to adopt follows this paper benguetal 2003 benguetal 2003 benguetal 2003 so i have the paper pulled up here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 8,
      "text": "so i have the paper pulled up here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 9,
      "text": "so i have the paper pulled up here now this isn't the very first paper that now this isn't the very first paper that now this isn't the very first paper that proposed the use of multiglio proposed the use of multiglio proposed the use of multiglio perceptrons or neural networks to perceptrons or neural networks to perceptrons or neural networks to predict the next character or token in a predict the next character or token in a predict the next character or token in a sequence but it's definitely one that is sequence but it's definitely one that is sequence but it's definitely one that is uh was very influential around that time uh was very influential around that time uh was very influential around that time it is very often cited to stand in for it is very often cited to stand in for it is very often cited to stand in for this idea",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 10,
      "text": "and i think it's a very nice this idea",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 11,
      "text": "and i think it's a very nice this idea",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 12,
      "text": "and i think it's a very nice write-up",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 13,
      "text": "and so this is the paper that write-up and so this is the paper that write-up",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 14,
      "text": "and so this is the paper that we're going to first look at",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 15,
      "text": "and then we're going to first look at",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 16,
      "text": "and then we're going to first look at and then implement now this paper has 19 pages so implement now this paper has 19 pages so implement now this paper has 19 pages",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 17,
      "text": "so we don't have time to go into we don't have time to go into we don't have time to go into the full detail of this paper but i the full detail of this paper",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 18,
      "text": "but i the full detail of this paper",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 19,
      "text": "but i invite you to read it invite you to read it invite you to read it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 20,
      "text": "it's very readable interesting and has a it's very readable interesting and has a it's very readable interesting and has a lot of interesting ideas in it as well lot of interesting ideas in it as well lot of interesting ideas in it as well in the introduction they describe the in the introduction they describe the in the introduction they describe the exact same problem i just described and exact same problem",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 21,
      "text": "i just described and exact same problem i just described and then to address it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 22,
      "text": "they propose the then to address it they propose the then to address it they propose the following model following model following model now keep in mind that we are building a now keep in mind that we are building a now keep in mind that we are building a character level language model",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 23,
      "text": "so we're character level language model",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 24,
      "text": "so we're character level language model",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 25,
      "text": "so we're working on the level of characters in working on the level of characters in working on the level of characters in this paper they have a vocabulary of 17 this paper they have a vocabulary of 17 this paper they have a vocabulary of 17 000 possible words and they instead 000 possible words and they instead 000 possible words and they instead build a word level language model but build a word level language model but build a word level language model but we're going to still stick with the we're going to still stick with the we're going to still stick with the characters but we'll take the same characters but we'll take the same characters but we'll take the same modeling approach modeling approach modeling approach now what they do is basically they now what they do is basically they now what they do is basically they propose to take every one of these words propose to take every one of these words propose to take every one of these words seventeen thousand words and they're seventeen thousand words and they're seventeen thousand words and they're going to associate to each word a say going to associate to each word a say going to associate to each word a say thirty dimensional feature vector thirty dimensional feature vector thirty dimensional feature vector so every word is now so every word is now so every word is now embedded into a thirty dimensional space embedded into a thirty dimensional space embedded into a thirty dimensional space you can think of it that way",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 26,
      "text": "so we have you can think of it that way",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 27,
      "text": "so we have you can think of it that way",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 28,
      "text": "so we have 17 000 points or vectors in a 30 17 000 points or vectors in a 30 17 000 points or vectors in a 30 dimensional space dimensional space dimensional space and that's um you might imagine that's",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 29,
      "text": "and that's um you might imagine that's",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 30,
      "text": "and that's um you might imagine that's very crowded that's a lot of points for very crowded that's a lot of points for very crowded that's a lot of points for a very small space a very small space a very small space now now now in the beginning these words are in the beginning these words are in the beginning these words are initialized completely randomly so initialized completely randomly so initialized completely randomly so they're spread out at random they're spread out at random they're spread out at random",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 31,
      "text": "but then we're going to tune these",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 32,
      "text": "but then we're going to tune these",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 33,
      "text": "but then we're going to tune these embeddings of these words using back embeddings of these words using back embeddings of these words using back propagation propagation propagation so during the course of training of this so during the course of training of this so during the course of training of this neural network these points or vectors neural network these points or vectors neural network these points or vectors are going to basically move around in are going to basically move around in are going to basically move around in this space",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 34,
      "text": "and you might imagine that this space and you might imagine that this space and you might imagine that for example words that have very similar for example words that have very similar for example words that have very similar meanings or that are indeed synonyms of meanings or that are indeed synonyms of meanings or that are indeed synonyms of each other might end up in a very each other might end up in a very each other might end up in a very similar part of the space and conversely similar part of the space and conversely similar part of the space and conversely words that mean very different things words that mean very different things words that mean very different things would go somewhere else in a space would go somewhere else in a space would go somewhere else in a space now their modeling approach otherwise is now their modeling approach otherwise is now their modeling approach otherwise is identical to ours they are using a identical to ours they are using a identical to ours they are using a multi-layer neural network to predict multi-layer neural network to predict multi-layer neural network to predict the next word given the previous words the next word given the previous words the next word given the previous words and to train the neural network they are and to train the neural network they are and to train the neural network they are maximizing the log likelihood of the maximizing the log likelihood of the maximizing the log likelihood of the training data just like we did training data just like we did training data just like we did so the modeling approach itself is so the modeling approach itself is so the modeling approach itself is identical now here they have a concrete identical now here they have a concrete identical now here they have a concrete example of this intuition example of this intuition example of this intuition why does it work why does it work why does it work basically suppose that for example you basically suppose that for example you basically suppose that for example you are trying to predict a dog was running are trying to predict a dog was running are trying to predict a dog was running in a blank in a blank in a blank now suppose that the exact phrase a dog now suppose that the exact phrase a dog now suppose that the exact phrase a dog was running in a was running in a was running in a has never occurred in a training data has never occurred in a training data has never occurred in a training data and here you are at sort of test time and here you are at sort of test time and here you are at sort of test time later when the model is deployed later when the model is deployed later when the model is deployed somewhere somewhere somewhere",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 35,
      "text": "and it's trying to make a sentence",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 36,
      "text": "and and it's trying to make a sentence",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 37,
      "text": "and and it's trying to make a sentence",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 38,
      "text": "and it's saying a dog was running in a blank it's saying a dog was running in a blank it's saying a dog was running in a blank and because it's never encountered this and because it's never encountered this and because it's never encountered this exact phrase in the training set you're exact phrase in the training set you're exact phrase in the training set you're out of distribution as we say like you out of distribution as we say like you out of distribution as we say like you don't have fundamentally any don't have fundamentally any don't have fundamentally any reason to suspect reason to suspect reason to suspect what might come next what might come next what might come next",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 39,
      "text": "but this approach actually allows you to but this approach actually allows you to",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 40,
      "text": "but this approach actually allows you to get around that because maybe you didn't get around that because maybe you didn't get around that because maybe you didn't see the exact phrase a dog was running see the exact phrase a dog was running see the exact phrase a dog was running in a something",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 41,
      "text": "but maybe you've seen in a something",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 42,
      "text": "but maybe you've seen in a something",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 43,
      "text": "but maybe you've seen similar phrases maybe you've seen the similar phrases maybe you've seen the similar phrases maybe you've seen the phrase the dog was running in a blank phrase the dog was running in a blank phrase the dog was running in a blank",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 44,
      "text": "and maybe your network has learned that and maybe your network has learned that and maybe your network has learned that a and the a and the a and the are like frequently are interchangeable are like frequently are interchangeable are like frequently are interchangeable with each other",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 45,
      "text": "and so maybe it took the with each other",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 46,
      "text": "and so maybe it took the with each other",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 47,
      "text": "and so maybe it took the embedding for a and the embedding for embedding for a and the embedding for embedding for a and the embedding for the",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 48,
      "text": "and it actually put them like nearby the",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 49,
      "text": "and it actually put them like nearby the",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 50,
      "text": "and it actually put them like nearby each other in the space",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 51,
      "text": "and so you can each other in the space and so you can each other in the space",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 52,
      "text": "and so you can transfer knowledge through that transfer knowledge through that transfer knowledge through that embedding and you can generalize in that embedding and you can generalize in that embedding and you can generalize in that way way way similarly the network could know that similarly the network could know that similarly the network could know that cats and dogs are animals and they cats and dogs are animals and they cats and dogs are animals and they co-occur in lots of very similar co-occur in lots of very similar co-occur in lots of very similar contexts and so even though you haven't contexts and so even though you haven't contexts and so even though you haven't seen this exact phrase seen this exact phrase seen this exact phrase or if you haven't seen exactly walking or if you haven't seen exactly walking or if you haven't seen exactly walking or running or running or running you can through the embedding space you can through the embedding space you can through the embedding space transfer knowledge and you can transfer knowledge and you can transfer knowledge and you can generalize to novel scenarios generalize to novel scenarios generalize to novel scenarios so let's now scroll down to the diagram",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 53,
      "text": "so let's now scroll down to the diagram",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 54,
      "text": "so let's now scroll down to the diagram of the neural network of the neural network of the neural network they have a nice diagram here they have a nice diagram here they have a nice diagram here and in this example we are taking three and in this example we are taking three and in this example we are taking three previous words previous words previous words and we are trying to predict the fourth and we are trying to predict the fourth",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 55,
      "text": "and we are trying to predict the fourth word word word in a sequence in a sequence in a sequence now these three previous words as i now these three previous words as i now these three previous words as i mentioned uh we have a vocabulary of 17 mentioned uh we have a vocabulary of 17 mentioned uh we have a vocabulary of 17 000 um possible words 000 um possible words 000 um possible words so every one of these",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 56,
      "text": "so every one of these so every one of these basically basically basically basically basically basically are the index of the incoming word are the index of the incoming word are the index of the incoming word and because there are 17 000 words this and because there are 17 000 words this and because there are 17 000 words this is an integer between 0 and 16999 now there's also a lookup table that now there's also a lookup table that they call c they call c they call c this lookup table is a matrix that is 17 this lookup table is a matrix that is 17 this lookup table is a matrix that is 17 000 by say 30. 000",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 57,
      "text": "by say 30. 000 by say 30.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 58,
      "text": "and basically what we're doing here is and basically what we're doing here is and basically what we're doing here is we're treating this as a lookup table we're treating this as a lookup table we're treating this as a lookup table",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 59,
      "text": "and so every index is and so every index is and so every index is plucking out a row of this embedding plucking out a row of this embedding plucking out a row of this embedding matrix matrix matrix so that each index is converted to the so that each index is converted to the so that each index is converted to the 30 dimensional vector that corresponds 30 dimensional vector that corresponds 30 dimensional vector that corresponds to the embedding vector for that word to the embedding vector for that word to the embedding vector for that word so here we have the input layer of 30 so here we have the input layer of 30 so here we have the input layer of 30 neurons for three words making up 90 neurons for three words making up 90 neurons for three words making up 90 neurons in total neurons in total neurons in total",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 60,
      "text": "and here they're saying that this matrix and here they're saying that this matrix and here they're saying that this matrix c is shared across all the words so c is shared across all the words so c is shared across all the words so we're always indexing into the same we're always indexing into the same we're always indexing into the same matrix c over and over um matrix c over and over um matrix c over and over um for each one of these for each one of these for each one of these words next up is the hidden layer of words next up is the hidden layer of words next up is the hidden layer of this neural network the size of this this neural network the size of this this neural network the size of this hidden neural layer of this neural net hidden neural layer of this neural net hidden neural layer of this neural net is a hoppy parameter",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 61,
      "text": "so we use the word is a hoppy parameter",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 62,
      "text": "so we use the word is a hoppy parameter",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 63,
      "text": "so we use the word hyperparameter when it's kind of like a hyperparameter when it's kind of like a hyperparameter when it's kind of like a design choice up to the designer of the design choice up to the designer of the design choice up to the designer of the neural net and this can be as large as neural net and this can be as large as neural net and this can be as large as you'd like or as small as you'd like so you'd like or as small as you'd like",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 64,
      "text": "so you'd like or as small as you'd like so for example the size could be a hundred for example the size could be a hundred for example the size could be a hundred and we are going to go over multiple",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 65,
      "text": "and we are going to go over multiple",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 66,
      "text": "and we are going to go over multiple choices of the size of this hidden layer choices of the size of this hidden layer choices of the size of this hidden layer and we're going to evaluate how well",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 67,
      "text": "and we're going to evaluate how well",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 68,
      "text": "and we're going to evaluate how well they work they work they work so say there were 100 neurons here all so say there were 100 neurons here all so say there were 100 neurons here all of them would be fully connected to the of them would be fully connected to the of them would be fully connected to the 90 words or 90 um 90 words or 90 um 90 words or 90 um numbers that make up these three words numbers that make up these three words numbers that make up these three words so this is a fully connected layer",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 69,
      "text": "so this is a fully connected layer so this is a fully connected layer then there's a 10 inch long linearity then there's a 10 inch long linearity then there's a 10 inch long linearity and then there's this output layer and and then there's this output layer and and then there's this output layer and because there are 17 000 possible words because there are 17 000 possible words because there are 17 000 possible words that could come next that could come next that could come next this layer has 17 000 neurons this layer has 17 000 neurons this layer has 17 000 neurons and all of them are fully connected to and all of them are fully connected to and all of them are fully connected to all of these neurons in the hidden layer all of these neurons in the hidden layer all of these neurons in the hidden layer so there's a lot of parameters here so there's a lot of parameters here so there's a lot of parameters here because there's a lot of words so most because there's a lot of words so most because there's a lot of words so most computation is here this is the computation is here this is the computation is here this is the expensive layer expensive layer expensive layer now there are 17 000 logits here so on now there are 17 000 logits here so on now there are 17 000 logits here so on top of there",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 70,
      "text": "we have the softmax layer top of there we have the softmax layer top of there",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 71,
      "text": "we have the softmax layer which we've seen in our previous video which we've seen in our previous video which we've seen in our previous video as well so every one of these logits is as well so every one of these logits is as well so every one of these logits is exponentiated and then everything is exponentiated and then everything is exponentiated and then everything is normalized to sum to 1 so that we have a normalized to sum to 1 so that we have a normalized to sum to 1 so that we have a nice probability distribution for the nice probability distribution for the nice probability distribution for the next word in the sequence next word in the sequence next word in the sequence now of course during training we now of course during training we now of course during training we actually have the label we have the actually have the label we have the actually have the label we have the identity of the next word in a sequence identity of the next word in a sequence identity of the next word in a sequence that word that word that word or its index is used to pluck out the or its index is used to pluck out the or its index is used to pluck out the probability of that word probability of that word probability of that word",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 72,
      "text": "and then we are maximizing the",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 73,
      "text": "and then we are maximizing the",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 74,
      "text": "and then we are maximizing the probability of that word probability of that word with respect to the parameters of this with respect to the parameters of this with respect to the parameters of this neural net neural net neural net so the parameters are the weights and so the parameters are the weights and so the parameters are the weights and biases of this output layer biases of this output layer biases of this output layer the weights and biases of the hidden the weights and biases of the hidden the weights and biases of the hidden layer and the embedding lookup table c layer and the embedding lookup table c layer and the embedding lookup table c and all of that is optimized using back and all of that is optimized using back and all of that is optimized using back propagation propagation and these uh dashed arrows ignore those and these uh dashed arrows ignore those and these uh dashed arrows ignore those uh that represents a variation of a uh that represents a variation of a uh that represents a variation of a neural nut that we are not going to neural nut that we are not going to neural nut that we are not going to explore in this video explore in this video explore in this video",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 75,
      "text": "so that's the setup and now let's so that's the setup and now let's so that's the setup and now let's implement it implement it implement it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 76,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 77,
      "text": "so i started a brand new notebook",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 78,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 79,
      "text": "so i started a brand new notebook",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 80,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 81,
      "text": "so i started a brand new notebook for this lecture for this lecture for this lecture we are importing pytorch and we are we are importing pytorch",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 82,
      "text": "and we are we are importing pytorch and we are importing matplotlib",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 83,
      "text": "so we can create importing matplotlib",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 84,
      "text": "so we can create importing matplotlib",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 85,
      "text": "so we can create figures figures figures then i am reading all the names into a then i am reading all the names into a then i am reading all the names into a list of words like i did before and i'm list of words like i did before and i'm list of words like i did before and i'm showing the first eight right here showing the first eight right here showing the first eight right here keep in mind that we have a 32 000 in keep in mind that we have a 32 000 in keep in mind that we have a 32 000 in total these are just the first eight total these are just the first eight total these are just the first eight and then here i'm building out the",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 86,
      "text": "and then here i'm building out the",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 87,
      "text": "and then here i'm building out the vocabulary of characters and all the vocabulary of characters and all the vocabulary of characters and all the mappings from the characters as strings mappings from the characters as strings mappings from the characters as strings to integers and vice versa to integers and vice versa to integers and vice versa now the first thing we want to do is we now the first thing we want to do is we now the first thing we want to do is we want to compile the data set for the want to compile the data set for the want to compile the data set for the neural network neural network neural network and i had to rewrite this code",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 88,
      "text": "um i'll and i had to rewrite this code",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 89,
      "text": "um i'll and i had to rewrite this code",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 90,
      "text": "um i'll show you in a second what it looks like show you in a second what it looks like show you in a second what it looks like so this is the code that i created for so this is the code that i created for so this is the code that i created for the dataset creation so let me first run the dataset creation so let me first run the dataset creation so let me first run it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 91,
      "text": "and then i'll briefly explain how it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 92,
      "text": "and then i'll briefly explain how it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 93,
      "text": "and then i'll briefly explain how this works this works this works so first we're going to define something so first we're going to define something so first we're going to define something called block size and this is basically called block size and this is basically called block size and this is basically the context length of how many the context length of how many the context length of how many characters do we take to predict the characters do we take to predict the characters do we take to predict the next one so here in this example we're next one",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 94,
      "text": "so here in this example we're next one so here in this example we're taking three characters to predict the taking three characters to predict the taking three characters to predict the fourth one so we have a block size of fourth one so we have a block size of fourth one",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 95,
      "text": "so we have a block size of three that's the size of the block",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 96,
      "text": "that three",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 97,
      "text": "that's the size of the block that three",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 98,
      "text": "that's the size of the block that supports the prediction supports the prediction supports the prediction then here i'm building out the x",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 99,
      "text": "and y then here i'm building out the x",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 100,
      "text": "and y then here i'm building out the x and y the x are the input to the neural net the x are the input to the neural net the x are the input to the neural net and the y are the labels for each and the y are the labels for each and the y are the labels for each example inside x example inside x example inside x then i'm airing over the first five then i'm airing over the first five then i'm airing over the first five words i'm doing first five just for words i'm doing first five just for words i'm doing first five just for efficiency while we are developing all efficiency while we are developing all efficiency while we are developing all the code but then later we're going to the code but then later we're going to the code but then later we're going to come here and erase this so that we use come here and erase this so that we use come here and erase this so that we use the entire training set the entire training set the entire training set so here i'm printing the word so here i'm printing the word so here i'm printing the word emma",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 101,
      "text": "and here i'm basically showing the emma and here i'm basically showing the emma and here i'm basically showing the examples that we can generate the five examples that we can generate the five examples that we can generate the five examples that we can generate out of the examples that we can generate out of the examples that we can generate out of the single single single um sort of word emma um sort of word emma um sort of word emma",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 102,
      "text": "so so so when we are given the context of just uh when we are given the context of just uh when we are given the context of just uh dot dot the first character in a dot dot the first character in a dot dot the first character in a sequence is e sequence is e sequence is e in this context the label is m in this context the label is m in this context the label is m when the context is this the label is m when the context is this the label is m when the context is this the label is m and",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 103,
      "text": "so forth",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 104,
      "text": "and so the way i build this and",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 105,
      "text": "so forth and so the way i build this and so forth and so the way i build this out is first",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 106,
      "text": "i start with a padded out is first",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 107,
      "text": "i start with a padded out is first",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 108,
      "text": "i start with a padded context of just zero tokens context of just zero tokens context of just zero tokens then i iterate over all the characters",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 109,
      "text": "i then i iterate over all the characters",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 110,
      "text": "i then i iterate over all the characters i get the character in the sequence",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 111,
      "text": "and i get the character in the sequence and i get the character in the sequence",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 112,
      "text": "and i basically build out the array y of this basically build out the array y of this basically build out the array y of this current character and the array x which current character and the array x which current character and the array x which stores the current running context stores the current running context stores the current running context and then here see i print everything and and then here see i print everything and and then here see i print everything",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 113,
      "text": "and here i um crop the context and enter the here i um crop the context and enter the here i um crop the context and enter the new character in a sequence",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 114,
      "text": "so this is new character in a sequence so this is new character in a sequence",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 115,
      "text": "so this is kind of like a rolling window of context kind of like a rolling window of context kind of like a rolling window of context now we can change the block size here to now we can change the block size here to now we can change the block size here to for example four for example four for example four and in that case we'll be predicting the and in that case we'll be predicting the and in that case we'll be predicting the fifth character given the previous four fifth character given the previous four fifth character given the previous four or it can be five or it can be five or it can be five and then it would look like this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 116,
      "text": "and then it would look like this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 117,
      "text": "and then it would look like this or it can be say ten or it can be say ten or it can be say ten",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 118,
      "text": "and then it would look something like and then it would look something like and then it would look something like this we're taking ten characters to this we're taking ten characters to this we're taking ten characters to predict the eleventh one predict the eleventh one predict the eleventh one",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 119,
      "text": "and we're always padding with dots and we're always padding with dots and we're always padding with dots so let me bring this back to three so let me bring this back to three so let me bring this back to three just so that we have what we have here just so that we have what we have here just so that we have what we have here in the paper in the paper in the paper and finally the data set right now looks and finally the data set right now looks and finally the data set right now looks as follows as follows as follows from these five words we have created a from these five words we have created a from these five words we have created a data set of 32 examples data set of 32 examples data set of 32 examples and each input of the neural net is and each input of the neural net is and each input of the neural net is three integers and we have a label that three integers and we have a label that three integers and we have a label that is also an integer is also an integer is also an integer y y y",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 120,
      "text": "so x looks like this so x looks like this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 121,
      "text": "so x looks like this these are the individual examples these are the individual examples these are the individual examples and then y are the labels and then y are the labels and then y are the labels so so given this given this given this let's now write a neural network that let's now write a neural network that let's now write a neural network that takes these axes and predicts the y's takes these axes and predicts the y's takes these axes and predicts the y's first let's build the embedding lookup first let's build the embedding lookup first let's build the embedding lookup table c table c table",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 122,
      "text": "c",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 123,
      "text": "so we have 27 possible characters and so we have 27 possible characters",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 124,
      "text": "and so we have 27 possible characters and we're going to embed them in a lower we're going to embed them in a lower we're going to embed them in a lower dimensional space dimensional space in the paper they have 17 000 words and in the paper they have 17 000 words and in the paper they have 17 000 words",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 125,
      "text": "and they bet them in uh spaces as small they bet them in uh spaces as small they bet them in uh spaces as small dimensional as 30.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 126,
      "text": "so they cram 17 000 dimensional as 30.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 127,
      "text": "so they cram 17 000 dimensional as 30.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 128,
      "text": "so they cram 17 000 words into 30 dimensional space in our words into 30 dimensional space in our words into 30 dimensional space in our case we have only 27 possible characters case we have only 27 possible characters case we have only 27 possible characters so let's grab them in something as small so let's grab them in something as small so let's grab them in something as small as to start with for example a as to start with for example a as to start with for example a two-dimensional space two-dimensional space two-dimensional space so this lookup table will be random so this lookup table will be random so this lookup table will be random numbers numbers numbers and we'll have 27 rows and we'll have and we'll have 27 rows and we'll have and we'll have 27 rows and we'll have two columns two columns two columns",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 129,
      "text": "right",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 130,
      "text": "so each 20 each one of 27 right",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 131,
      "text": "so each 20 each one of 27 right",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 132,
      "text": "so each 20 each one of 27 characters will have a two-dimensional characters will have a two-dimensional characters will have a two-dimensional embedding embedding embedding so that's our matrix c of embeddings in so that's our matrix c of embeddings in so that's our matrix c of embeddings in the beginning initialized randomly the beginning initialized randomly the beginning initialized randomly now before we embed all of the integers now before we embed all of the integers now before we embed all of the integers inside the input x using this lookup inside the input x using this lookup inside the input x using this lookup table c table",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 133,
      "text": "c let me actually just try to embed a let me actually just try to embed a let me actually just try to embed a single individual integer like say five single individual integer like say five single individual integer like say five so we get a sense of how this works so we get a sense of how this works so we get a sense of how this works now now one way this works of course is we can one way this works of course is we can one way this works of course is we can just take the c and we can index into just take the c and we can index into just take the c and we can index into row five row five row five and that gives us a vector the fifth row and that gives us a vector the fifth row and that gives us a vector the fifth row of c of c of c and um and um and um this is one way to do it this is one way to do it this is one way to do it the other way that i presented in the the other way that i presented in the the other way that i presented in the previous lecture is actually seemingly previous lecture is actually seemingly previous lecture is actually seemingly different but actually identical so in different",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 134,
      "text": "but actually identical so in different",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 135,
      "text": "but actually identical so in the previous lecture what we did is we the previous lecture what we did is we the previous lecture what we did is we took these integers and we used the one took these integers and we used the one took these integers and we used the one hot encoding to first encode them so f.1 hot encoding to first encode them so f.1 hot encoding to first encode them so f.1 hot hot hot we want to encode integer 5 and we want we want to encode integer 5",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 136,
      "text": "and we want we want to encode integer 5 and we want to tell it that the number of classes is to tell it that the number of classes is to tell it that the number of classes is 27 so that's the 26 dimensional vector 27 so that's the 26 dimensional vector 27",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 137,
      "text": "so that's the 26 dimensional vector of all zeros except the fifth bit is of all zeros except the fifth bit is of all zeros except the fifth bit is turned on turned on turned on now this actually doesn't work now this actually doesn't work now this actually doesn't work the reason is that the reason is that the reason is that this input actually must be a doorstop this input actually must be a doorstop this input actually must be a doorstop tensor tensor tensor and i'm making some of these errors and i'm making some of these errors and i'm making some of these errors intentionally just so you get to see intentionally just so you get to see intentionally just so you get to see some errors and how to fix them some errors and how to fix them some errors and how to fix them",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 138,
      "text": "so this must be a tester not an int so this must be a tester not an int",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 139,
      "text": "so this must be a tester not an int fairly straightforward to fix we get a fairly straightforward to fix we get a fairly straightforward to fix we get a one hot vector the fifth dimension is one hot vector the fifth dimension is one hot vector the fifth dimension is one and the shape of this is 27.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 140,
      "text": "one and the shape of this is 27.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 141,
      "text": "one and the shape of this is 27.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 142,
      "text": "and now notice that just as i briefly and now notice that just as i briefly and now notice that just as i briefly alluded to in the previous video if we alluded to in the previous video if we alluded to in the previous video if we take this one hot vector and we multiply take this one hot vector and we multiply take this one hot vector and we multiply it by c then then what would you expect what would you expect what would you expect",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 143,
      "text": "well number one",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 144,
      "text": "well number one",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 145,
      "text": "well number one first you'd expect an error first you'd expect an error first you'd expect an error because because because expected scalar type long but found expected scalar type long but found expected scalar type long but found float float float so a little bit confusing",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 146,
      "text": "but so a little bit confusing",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 147,
      "text": "but so a little bit confusing",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 148,
      "text": "but the problem here is that one hot the the problem here is that one hot the the problem here is that one hot the data type of it data type of it data type of it is is is long it's a 64-bit integer",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 149,
      "text": "but this is a long it's a 64-bit integer but this is a long it's a 64-bit integer but this is a float tensor and so pytorch doesn't know float tensor and so pytorch doesn't know float tensor and so pytorch doesn't know how to multiply an int with a float and how to multiply an int with a float and how to multiply an int with a float and that's why we had to explicitly cast that's why we had to explicitly cast that's why we had to explicitly cast this to a float so that we can multiply this to a float so that we can multiply this to a float so that we can multiply now the output actually here now the output actually here now the output actually here is identical is identical is identical and that it's identical because of the and that it's identical because of the and that it's identical because of the way the matrix multiplication here works way the matrix multiplication here works way the matrix multiplication here works we have the one hot um vector we have the one hot um vector we have the one hot um vector multiplying columns of c multiplying columns of c multiplying columns of c and because of all the zeros they and because of all the zeros they and because of all the zeros they actually end up masking out everything actually end up masking out everything actually end up masking out everything in c except for the fifth row which is in c except for the fifth row which is in c except for the fifth row which is plucked out plucked out plucked out and so we actually arrive at the same",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 150,
      "text": "and so we actually arrive at the same",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 151,
      "text": "and so we actually arrive at the same result result result and that tells you that and that tells you that and that tells you that here we can interpret this first here we can interpret this first here we can interpret this first piece here this embedding of the integer piece here this embedding of the integer piece here this embedding of the integer we can either think of it as the integer we can either think of it as the integer we can either think of it as the integer indexing into a lookup table c",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 152,
      "text": "but indexing into a lookup table c",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 153,
      "text": "but indexing into a lookup table c",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 154,
      "text": "but equivalently we can also think of this equivalently we can also think of this equivalently we can also think of this little piece here as a first layer of little piece here as a first layer of little piece here as a first layer of this bigger neural net this bigger neural net this bigger neural net this layer here has neurons that have no this layer here has neurons that have no this layer here has neurons that have no non-linearity there's no 10h they're non-linearity there's no 10h they're non-linearity there's no 10h they're just linear neurons and their weight just linear neurons and their weight just linear neurons and their weight matrix is c matrix is c matrix is c",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 155,
      "text": "and then we are encoding integers into and then we are encoding integers into and then we are encoding integers into one hot and feeding those into a neural one hot and feeding those into a neural one hot and feeding those into a neural net and this first layer basically net and this first layer basically net and this first layer basically embeds them embeds them embeds them so those are two equivalent ways of so those are two equivalent ways of so those are two equivalent ways of doing the same thing we're just going to doing the same thing we're just going to doing the same thing we're just going to index because it's much much faster and index because it's much much faster and index because it's much much faster and we're going to discard this we're going to discard this we're going to discard this interpretation of one hot inputs into interpretation of one hot inputs into interpretation of one hot inputs into neural nets",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 156,
      "text": "and we're just going to neural nets",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 157,
      "text": "and we're just going to neural nets",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 158,
      "text": "and we're just going to index integers and create and use index integers and create and use index integers and create and use embedding tables now embedding a single embedding tables now embedding a single embedding tables now embedding a single integer like 5 is easy enough we can integer like 5 is easy enough we can integer like 5 is easy enough we can simply ask pytorch to retrieve the fifth simply ask pytorch to retrieve the fifth simply ask pytorch to retrieve the fifth row of c row of c row of c or the row index five of c or the row index five of c or the row index five of c",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 159,
      "text": "but how do we simultaneously embed all",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 160,
      "text": "but how do we simultaneously embed all",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 161,
      "text": "but how do we simultaneously embed all of these 32 by three integers stored in of these 32 by three integers stored in of these 32 by three integers stored in array x array x array x luckily pytorch indexing is fairly luckily pytorch indexing is fairly luckily pytorch indexing is fairly flexible and quite powerful flexible and quite powerful flexible and quite powerful so it doesn't just work to so it doesn't just work to so it doesn't just work to ask for a single element five like this ask for a single element five like this ask for a single element five like this you can actually index using lists so you can actually index using lists so you can actually index using lists so for example we can get the rows five six for example we can get the rows five six for example we can get the rows five six and seven and seven and seven and this will just work like this we can and this will just work like this we can and this will just work like this we can index with a list index with a list index with a list it doesn't just have to be a list it can it doesn't just have to be a list it can it doesn't just have to be a list it can also be a actually a tensor of integers also be a actually a tensor of integers also be a actually a tensor of integers and we can index with that and we can index with that and we can index with that",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 162,
      "text": "so this is a integer tensor 567 and this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 163,
      "text": "so this is a integer tensor 567 and this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 164,
      "text": "so this is a integer tensor 567 and this will just work as well will just work as well will just work as well in fact we can also for example repeat in fact we can also for example repeat in fact we can also for example repeat row 7 and retrieve it multiple times row 7 and retrieve it multiple times row 7 and retrieve it multiple times and and and that same index will just get embedded that same index will just get embedded that same index will just get embedded multiple times here multiple times here multiple times here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 165,
      "text": "so here we are indexing with a so here we are indexing with a so here we are indexing with a one-dimensional one-dimensional one-dimensional tensor of integers but it turns out that tensor of integers but it turns out that tensor of integers but it turns out that you can also index with you can also index with you can also index with multi-dimensional tensors of integers multi-dimensional tensors of integers multi-dimensional tensors of integers here we have a two-dimensional in tensor here we have a two-dimensional in tensor here we have a two-dimensional in tensor of integers so we can simply just do c of integers so we can simply just do c of integers so we can simply just do c at x and this just works at x and this just works at x and this just works and the shape of this and the shape of this and the shape of this is is 32 by 3 which is the original shape and 32 by 3 which is the original shape and 32 by 3 which is the original shape and now for every one of those 32 by 3 now for every one of those 32 by 3 now for every one of those 32 by 3 integers we've retrieved the embedding integers we've retrieved the embedding integers we've retrieved the embedding vector vector vector here so basically we have that as an here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 166,
      "text": "so basically we have that as an here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 167,
      "text": "so basically we have that as an example example example the 13th or example index 13 the 13th or example index 13 the 13th or example index 13 the second dimension is the integer 1 as the second dimension is the integer 1 as the second dimension is the integer 1 as an example an example an example",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 168,
      "text": "and so and so",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 169,
      "text": "and so here if we do c of x which gives us that here if we do c of x which gives us that here if we do c of x which gives us that array",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 170,
      "text": "and then we index into 13 by two array",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 171,
      "text": "and then we index into 13 by two array",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 172,
      "text": "and then we index into 13 by two of that array of that array of that array",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 173,
      "text": "then we we get the embedding then we we get the embedding then we we get the embedding here here here and you can verify that and you can verify that and you can verify that c c c at one which is the integer at that at one which is the integer at that at one which is the integer at that location is indeed equal to this location is indeed equal to this location is indeed equal to this you see they're equal you see they're equal you see they're equal so basically long story short pytorch so basically long story short pytorch so basically long story short pytorch indexing is awesome and to embed indexing is awesome and to embed indexing is awesome and to embed simultaneously all of the integers in x simultaneously all of the integers in x simultaneously all of the integers in x we can simply do c of x we can simply do c of x we can simply do c of x",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 174,
      "text": "and that is our embedding and that is our embedding and that is our embedding and that just works and that just works and that just works now let's construct this layer here the now let's construct this layer here the now let's construct this layer here the hidden layer hidden layer hidden layer",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 175,
      "text": "so we have that w1 as i'll call it are so we have that w1 as i'll call it are so we have that w1 as i'll call it are these weights which we will initialize these weights which we will initialize these weights which we will initialize randomly randomly randomly now the number of inputs to this layer now the number of inputs to this layer now the number of inputs to this layer is going to be is going to be is going to be three times two right because we have three times two right because we have three times two right because we have two dimensional embeddings",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 176,
      "text": "and we have two dimensional embeddings and we have two dimensional embeddings and we have three of them three of them three of them",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 177,
      "text": "so the number of inputs is 6 so the number of inputs is 6 so the number of inputs is 6 and the number of neurons in this layer and the number of neurons in this layer and the number of neurons in this layer is a variable up to us is a variable up to us is a variable up to us",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 178,
      "text": "let's use 100 neurons as an example let's use 100 neurons as an example let's use 100 neurons as an example and then biases and then biases and then biases will be also initialized randomly as an will be also initialized randomly as an will be also initialized randomly as an example example and let's",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 179,
      "text": "and we just need 100 of them and let's and we just need 100 of them and let's and we just need 100 of them now the problem with this is we can't now the problem with this is we can't now the problem with this is we can't simply normally we would take the input simply normally we would take the input simply normally we would take the input in this case that's embedding and we'd in this case that's embedding",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 180,
      "text": "and we'd in this case that's embedding",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 181,
      "text": "and we'd like to multiply it with these weights like to multiply it with these weights like to multiply it with these weights and then we would like to add the bias",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 182,
      "text": "and then we would like to add the bias",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 183,
      "text": "and then we would like to add the bias this is roughly what we want to do this is roughly what we want to do this is roughly what we want to do",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 184,
      "text": "but the problem here is that these but the problem here is that these but the problem here is that these embeddings are stacked up in the embeddings are stacked up in the embeddings are stacked up in the dimensions of this input tensor dimensions of this input tensor dimensions of this input tensor",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 185,
      "text": "so this will not work this matrix so this will not work this matrix",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 186,
      "text": "so this will not work this matrix multiplication because this is a shape multiplication because this is a shape multiplication because this is a shape 32 by 3 by 2",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 187,
      "text": "and i can't multiply that 32 by 3 by 2",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 188,
      "text": "and i can't multiply that 32 by 3 by 2",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 189,
      "text": "and i can't multiply that by 6 by 100 by 6 by 100 by 6 by 100 so somehow we need to concatenate these",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 190,
      "text": "so somehow we need to concatenate these",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 191,
      "text": "so somehow we need to concatenate these inputs here together so that we can do inputs here together so that we can do inputs here together so that we can do something along these lines which something along these lines which something along these lines which currently does not work currently does not work currently does not work so how do we transform this 32 by 3 by 2 so how do we transform this 32 by 3 by 2 so how do we transform this 32 by 3 by 2 into a 32 by 6 so that we can actually into a 32 by 6 so that we can actually into a 32 by 6 so that we can actually perform perform perform this this this multiplication over here i'd like to multiplication over here i'd like to multiplication over here i'd like to show you that there are usually many show you that there are usually many show you that there are usually many ways of ways of ways of implementing what you'd like to do in implementing what you'd like to do in implementing what you'd like to do in torch and some of them will be faster torch and some of them will be faster torch and some of them will be faster better shorter etc better shorter etc better shorter etc and that's because torch is a very large and that's because torch is a very large",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 192,
      "text": "and that's because torch is a very large library",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 193,
      "text": "and it's got lots and lots of library and it's got lots and lots of library and it's got lots and lots of functions",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 194,
      "text": "so if you just go to the functions so if you just go to the functions so if you just go to the documentation and click on torch you'll documentation and click on torch you'll documentation and click on torch you'll see that my slider here is very tiny and see that my slider here is very tiny and see that my slider here is very tiny and that's because there are so many that's because there are so many that's because there are so many functions that you can call on these functions that you can call on these functions that you can call on these tensors tensors tensors to transform them create them multiply to transform them create them multiply to transform them create them multiply them add them perform all kinds of them add them perform all kinds of them add them perform all kinds of different operations on them different operations on them different operations on them and so this is kind of like",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 195,
      "text": "and so this is kind of like",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 196,
      "text": "and so this is kind of like the space of possibility if you will the space of possibility if you will the space of possibility if you will now one of the things that you can do is now one of the things that you can do is now one of the things that you can do is if we can control here ctrl f for if we can control here ctrl f for if we can control here ctrl f for concatenate and we see that there's a concatenate and we see that there's a concatenate and we see that there's a function function function torque.cat short for concatenate torque.cat short for concatenate torque.cat short for concatenate and this concatenates the given sequence and this concatenates the given sequence and this concatenates the given sequence of tensors in a given dimension of tensors in a given dimension of tensors in a given dimension and these sensors must have the same and these sensors must have the same and these sensors must have the same shape etc so we can use the concatenate shape etc so we can use the concatenate shape etc so we can use the concatenate operation to in a naive way concatenate operation to in a naive way concatenate operation to in a naive way concatenate these three embeddings for each input these three embeddings for each input these three embeddings for each input so in this case we have m of so in this case we have m of so in this case we have m of amp of the shape and really what we want amp of the shape and really what we want amp of the shape and really what we want to do is we want to retrieve these three to do is we want to retrieve these three to do is we want to retrieve these three parts and concatenate them parts and concatenate them parts and concatenate them",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 197,
      "text": "so we want to grab all the examples so we want to grab all the examples so we want to grab all the examples we want to grab we want to grab we want to grab first the zeroth first the zeroth first the zeroth index index index and then all of and then all of and then all of this this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 198,
      "text": "so this plucks out so this plucks out so this plucks out the 32 by 2 embeddings of just the first the 32 by 2 embeddings of just the first the 32 by 2 embeddings of just the first word here word here word here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 199,
      "text": "and so basically we want this guy",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 200,
      "text": "and so basically we want this guy",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 201,
      "text": "and so basically we want this guy we want the first dimension and we want we want the first dimension",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 202,
      "text": "and we want we want the first dimension",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 203,
      "text": "and we want the second dimension the second dimension the second dimension",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 204,
      "text": "and these are the three pieces and these are the three pieces and these are the three pieces individually individually individually",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 205,
      "text": "and then we want to treat this as a",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 206,
      "text": "and then we want to treat this as a",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 207,
      "text": "and then we want to treat this as a sequence and we want to torch that cat sequence",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 208,
      "text": "and we want to torch that cat sequence",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 209,
      "text": "and we want to torch that cat on that sequence",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 210,
      "text": "so this is the list on that sequence so this is the list on that sequence",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 211,
      "text": "so this is the list tor.cat takes a tor.cat takes a tor.cat takes a sequence sequence sequence of tensors and then we have to tell it of tensors",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 212,
      "text": "and then we have to tell it of tensors",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 213,
      "text": "and then we have to tell it along which dimension to concatenate along which dimension to concatenate along which dimension to concatenate so in this case all these are 32 by 2 so in this case all these are 32 by 2 so in this case all these are 32 by 2",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 214,
      "text": "and we want to concatenate not across and we want to concatenate not across and we want to concatenate not across dimension 0 by the cross dimension one dimension 0 by the cross dimension one dimension 0 by the cross dimension one so passing in one so passing in one so passing in one gives us a result gives us a result gives us a result the shape of this is 32 by 6 exactly as the shape of this is 32 by 6 exactly as the shape of this is 32 by 6 exactly as we'd like we'd like we'd like so that basically took 32 and squashed so that basically took 32 and squashed so that basically took 32 and squashed these by concatenating them into 32 by these by concatenating them into 32 by these by concatenating them into 32 by 6. 6. 6.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 215,
      "text": "now this is kind of ugly because this now this is kind of ugly because this now this is kind of ugly because this code would not generalize if we want to code would not generalize if we want to code would not generalize if we want to later change the block size right now we later change the block size right now we later change the block size right now we have three inputs have three inputs have three inputs three words but what if we had five three words but what if we had five three words but what if we had five then here we would have to change the then here we would have to change the then here we would have to change the code because i'm indexing directly well code because i'm indexing directly well code because i'm indexing directly well torch comes to rescue again because that torch comes to rescue again because that torch comes to rescue again because that turns out to be a function called unbind turns out to be a function called unbind turns out to be a function called unbind and it removes a tensor dimension so it removes the tensor dimension so it removes the tensor dimension returns a tuple of all slices along a returns a tuple of all slices along a returns a tuple of all slices along a given dimension given dimension given dimension without it without it without it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 216,
      "text": "so this is exactly what we need so this is exactly what we need so this is exactly what we need and basically when we call torch dot and basically when we call torch dot and basically when we call torch dot unbind unbind unbind torch dot unbind torch dot unbind torch dot unbind of m of m of m and pass in dimension and pass in dimension and pass in dimension 1 index 1 1 index 1 1 index 1 this gives us a list of this gives us a list of this gives us a list of a list of tensors exactly equivalent to a list of tensors exactly equivalent to a list of tensors exactly equivalent to this this so running this so running this so running this gives us a line gives us a line gives us a line 3 3 3",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 217,
      "text": "and it's exactly this list",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 218,
      "text": "and so we can and it's exactly this list",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 219,
      "text": "and so we can and it's exactly this list",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 220,
      "text": "and so we can call torch.cat on it call torch.cat on it call torch.cat on it and along the first dimension and along the first dimension and along the first dimension and this works and this works and this works and this shape is the same and this shape is the same and this shape is the same",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 221,
      "text": "but now this is uh it doesn't matter if",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 222,
      "text": "but now this is uh it doesn't matter if",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 223,
      "text": "but now this is uh it doesn't matter if we have block size 3 or 5 or 10 this we have block size 3 or 5 or 10 this we have block size 3 or 5 or 10 this will just work will just work will just work so this is one way to do it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 224,
      "text": "but it turns so this is one way to do it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 225,
      "text": "but it turns so this is one way to do it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 226,
      "text": "but it turns out that in this case there's actually a out that in this case there's actually a out that in this case there's actually a significantly better and more efficient significantly better and more efficient significantly better and more efficient way and this gives me an opportunity to way and this gives me an opportunity to way and this gives me an opportunity to hint at some of the internals of hint at some of the internals of hint at some of the internals of torch.tensor torch.tensor torch.tensor so let's create so let's create so let's create an array here an array here an array here of elements from 0 to 17 of elements from 0 to 17 of elements from 0 to 17 and the shape of this and the shape of this is just 18.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 227,
      "text": "it's a single picture of 18 is just 18.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 228,
      "text": "it's a single picture of 18 is just 18.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 229,
      "text": "it's a single picture of 18 numbers numbers it turns out that we can very quickly it turns out that we can very quickly it turns out that we can very quickly re-represent this as different sized and re-represent this as different sized and re-represent this as different sized and dimensional tensors dimensional tensors dimensional tensors we do this by calling a view we do this by calling a view we do this by calling a view and we can say that actually this is not and we can say that actually this is not and we can say that actually this is not a single vector of 18 this is a two by a single vector of 18 this is a two by a single vector of 18 this is a two by nine tensor or alternatively this is a nine tensor or alternatively this is a nine tensor or alternatively this is a nine by two tensor nine by two tensor nine by two tensor or this is actually a three by three by or this is actually a three by three by or this is actually a three by three by two tensor two tensor two tensor as long as the total number of elements as long as the total number of elements as long as the total number of elements here multiply to be the same here multiply to be the same here multiply to be the same this will just work and this will just work and this will just work and in pytorch this operation calling that in pytorch this operation calling that in pytorch this operation calling that view is extremely efficient view is extremely efficient view is extremely efficient and the reason for that is that and the reason for that is that and the reason for that is that in each tensor there's something called in each tensor there's something called in each tensor there's something called the underlying storage the underlying storage the underlying storage and the storage is just the numbers and the storage is just the numbers and the storage is just the numbers always as a one-dimensional vector and always as a one-dimensional vector and always as a one-dimensional vector and this is how this tensor is represented this is how this tensor is represented this is how this tensor is represented in the computer memory it's always a in the computer memory it's always a in the computer memory it's always a one-dimensional vector one-dimensional vector one-dimensional vector but when we call that view we are but when we call that view we are but when we call that view we are manipulating some of attributes of that manipulating some of attributes of that manipulating some of attributes of that tensor that dictate how this tensor that dictate how this tensor that dictate how this one-dimensional sequence is interpreted one-dimensional sequence is interpreted one-dimensional sequence is interpreted to be an n-dimensional tensor to be an n-dimensional tensor to be an n-dimensional tensor",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 230,
      "text": "and so what's happening here is that",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 231,
      "text": "no",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 232,
      "text": "and so what's happening here is that",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 233,
      "text": "no",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 234,
      "text": "and so what's happening here is that no memory is being changed copied moved or memory is being changed copied moved or memory is being changed copied moved or created when we call that view the created when we call that view the created when we call that view the storage storage storage is identical but when you call that view is identical but when you call that view is identical but when you call that view some of the internal some of the internal some of the internal attributes of the view of the sensor are attributes of the view of the sensor are attributes of the view of the sensor are being manipulated and changed in being manipulated and changed in being manipulated and changed in particular that's something there's particular that's something there's particular that's something there's something called a storage offset something called a storage offset something called a storage offset strides and shapes and those are strides and shapes and those are strides and shapes and those are manipulated so that this one-dimensional manipulated so that this one-dimensional manipulated so that this one-dimensional sequence of bytes is seen as different sequence of bytes is seen as different sequence of bytes is seen as different and dimensional arrays and dimensional arrays and dimensional arrays there's a blog post here from eric there's a blog post here from eric there's a blog post here from eric called pi torch internals where he goes called pi torch internals where he goes called pi torch internals where he goes into some of this with respect to tensor into some of this with respect to tensor into some of this with respect to tensor and how the view of the tensor is and how the view of the tensor is and how the view of the tensor is represented represented represented",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 235,
      "text": "and this is really just like a logical and this is really just like a logical and this is really just like a logical construct of representing the physical construct of representing the physical construct of representing the physical memory memory memory and so this is a pretty good um blog",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 236,
      "text": "and so this is a pretty good um blog",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 237,
      "text": "and so this is a pretty good um blog post that you can go into i might also post that you can go into i might also post that you can go into i might also create an entire video on the internals create an entire video on the internals create an entire video on the internals of torch tensor and how this works of torch tensor and how this works of torch tensor and how this works for here we just note that this is an for here we just note that this is an for here we just note that this is an extremely efficient operation extremely efficient operation extremely efficient operation and if i delete this and come back to and if i delete this and come back to and if i delete this and come back to our end our end our end we see that the shape of our end is 32 we see that the shape of our end is 32 we see that the shape of our end is 32 by three by two",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 238,
      "text": "but we can simply by three by two",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 239,
      "text": "but we can simply by three by two",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 240,
      "text": "but we can simply ask for pytorch to view this instead as ask for pytorch to view this instead as ask for pytorch to view this instead as a 32 by six a 32 by six a 32 by six and the way this gets flattened into a and the way this gets flattened into a and the way this gets flattened into a 32 by six array 32 by six array 32 by six array just happens that just happens that just happens that these two these two these two get stacked up get stacked up get stacked up in a single row",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 241,
      "text": "and so that's basically in a single row",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 242,
      "text": "and so that's basically in a single row",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 243,
      "text": "and so that's basically the concatenation operation that we're the concatenation operation that we're the concatenation operation that we're after after after and you can verify that this actually and you can verify that this actually and you can verify that this actually gives the exact same result as what we gives the exact same result as what we gives the exact same result as what we had before had before had before so this is an element y equals and",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 244,
      "text": "you so this is an element y equals and you so this is an element y equals and you can see that all the elements of these can see that all the elements of these can see that all the elements of these two tensors are the same two tensors are the same two tensors are the same",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 245,
      "text": "and so we get the exact same result",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 246,
      "text": "and so we get the exact same result and so we get the exact same result so long story short we can actually just so long story short we can actually just so long story short we can actually just come here come here come here and if we just view this as a 32x6 instead then this multiplication will instead then this multiplication will work and give us the hidden states that work and give us the hidden states that work and give us the hidden states that we're after we're after we're after so if this is h",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 247,
      "text": "so if this is h",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 248,
      "text": "so if this is h then h shape is now then h shape is now then h shape is now the 100 dimensional activations for the 100 dimensional activations for the 100 dimensional activations for every one of our 32 examples every one of our 32 examples every one of our 32 examples and this gives the desired result let me and this gives the desired result let me and this gives the desired result let me do two things here number one let's not do two things here number one let's not do two things here number one let's not use 32 we can for example do something use 32 we can for example do something use 32 we can for example do something like m.shape at 0 m.shape at 0 so that we don't hard code these numbers so that we don't hard code these numbers so that we don't hard code these numbers and this would work for any size of this and this would work for any size of this and this would work for any size of this amp amp amp or alternatively we can also do negative or alternatively we can also do negative or alternatively we can also do negative one when we do negative one pi torch one when we do negative one pi torch one when we do negative one pi torch will infer what this should be will infer what this should be will infer what this should be because the number of elements must be because the number of elements must be because the number of elements must be the same and we're saying that this is 6 the same",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 249,
      "text": "and we're saying that this is 6 the same",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 250,
      "text": "and we're saying that this is 6 by church will derive that this must be by church will derive that this must be by church will derive that this must be 32 or whatever else it is if m is of 32 or whatever else it is if m is of 32 or whatever else it is if m is of different size different size different size the other thing is here um the other thing is here um the other thing is here um one more thing i'd like to point out is one more thing i'd like to point out is one more thing i'd like to point out is here when we do the concatenation here when we do the concatenation here when we do the concatenation this actually is much less efficient this actually is much less efficient this actually is much less efficient because um this concatenation would because um this concatenation would because um this concatenation would create a whole new tensor with a whole create a whole new tensor with a whole create a whole new tensor with a whole new storage so new memory is being new storage so new memory is being new storage so new memory is being created because there's no way to created because there's no way to created because there's no way to concatenate tensors just by manipulating concatenate tensors just by manipulating concatenate tensors just by manipulating the view attributes the view attributes the view attributes so this is inefficient and creates all so this is inefficient and creates all so this is inefficient and creates all kinds of new memory kinds of new memory kinds of new memory uh so let me delete this now",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 251,
      "text": "uh so let me delete this now",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 252,
      "text": "uh so let me delete this now we don't need this we don't need this we don't need this and here to calculate h we want to also and here to calculate h we want to also and here to calculate h we want to also dot 10h dot 10h dot 10h of this of this of this to get our to get our to get our oops to get our h oops to get our h oops to get our h",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 253,
      "text": "so these are now numbers between so these are now numbers between so these are now numbers between negative one and one because of the 10h negative one and one because of the 10h negative one and one because of the 10h and we have",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 254,
      "text": "and we have",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 255,
      "text": "and we have that the shape is 32 by 100 that the shape is 32 by 100 that the shape is 32 by 100 and that is basically this hidden layer and that is basically this hidden layer and that is basically this hidden layer of activations here of activations here of activations here for every one of our 32 examples for every one of our 32 examples for every one of our 32 examples now there's one more thing i've lost now there's one more thing i've lost now there's one more thing i've lost over that we have to be very careful over that we have to be very careful over that we have to be very careful with and that this with and that this with and that this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 256,
      "text": "and that's this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 257,
      "text": "plus here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 258,
      "text": "and that's this plus here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 259,
      "text": "and that's this plus here in particular we want to make sure that in particular we want to make sure that in particular we want to make sure that the broadcasting will do what we like the broadcasting will do what we like the broadcasting will do what we like the shape of this is 32 by 100 and the the shape of this is 32 by 100 and the the shape of this is 32 by 100 and the ones shape is 100.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 260,
      "text": "ones shape is 100.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 261,
      "text": "ones shape is 100.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 262,
      "text": "so we see that the addition here will so we see that the addition here will so we see that the addition here will broadcast these two and in particular we broadcast these two and in particular we broadcast these two and in particular we have 32 by 100 broadcasting to 100.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 263,
      "text": "have 32 by 100 broadcasting to 100.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 264,
      "text": "have 32 by 100 broadcasting to 100.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 265,
      "text": "so broadcasting will align on the right",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 266,
      "text": "so broadcasting will align on the right",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 267,
      "text": "so broadcasting will align on the right create a fake dimension here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 268,
      "text": "so this create a fake dimension here so this create a fake dimension here so this will become a 1 by 100 row vector and will become a 1 by 100 row vector and will become a 1 by 100 row vector and then it will copy vertically then it will copy vertically then it will copy vertically for every one of these rows of 32 and do for every one of these rows of 32 and do for every one of these rows of 32 and do an element wise addition an element wise addition an element wise addition so in this case the correct thing will so in this case the correct thing will so in this case the correct thing will be happening because the same bias be happening because the same bias be happening because the same bias vector will be added to all the rows vector will be added to all the rows vector will be added to all the rows of of of this matrix so that is correct that's this matrix so that is correct that's this matrix so that is correct that's what we'd like and it's always good what we'd like",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 269,
      "text": "and it's always good what we'd like",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 270,
      "text": "and it's always good practice you just make sure practice you just make sure practice you just make sure so that you don't shoot yourself in the so that you don't shoot yourself in the so that you don't shoot yourself in the foot and finally let's create the final foot and finally let's create the final foot and finally let's create the final layer here layer here layer here so let's create so let's create w2 and v2 w2 and v2 w2 and v2 the input now is 100 the input now is 100 the input now is 100 and the output number of neurons will be and the output number of neurons will be and the output number of neurons will be for us 27 because we have 27 possible for us 27 because we have 27 possible for us 27 because we have 27 possible characters that come next characters that come next characters that come next so the biases will be 27 as well so the biases will be 27 as well so the biases will be 27 as well so therefore the logits which are the so therefore the logits which are the so therefore the logits which are the outputs of this neural net outputs of this neural net outputs of this neural net are going to be um are going to be um are going to be um h h h multiplied by w2 plus b2 multiplied by w2 plus b2 multiplied by w2 plus b2 logistic shape is 32 by 27 logistic shape is 32 by 27 logistic shape is 32 by 27 and the logits look and the logits look and the logits look good now exactly as we saw in the good now exactly as we saw in the good now exactly as we saw in the previous video we want to take these previous video we want to take these previous video we want to take these logits and we want to first exponentiate logits",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 271,
      "text": "and we want to first exponentiate logits",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 272,
      "text": "and we want to first exponentiate them to get our fake counts them to get our fake counts them to get our fake counts",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 273,
      "text": "and then we want to normalize them into",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 274,
      "text": "and then we want to normalize them into",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 275,
      "text": "and then we want to normalize them into a probability a probability a probability so prob is counts divide",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 276,
      "text": "so prob is counts divide",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 277,
      "text": "so prob is counts divide and now and now and now counts dot sum along the first dimension counts dot sum along the first dimension counts dot sum along the first dimension and keep them as true exactly as in the and keep them as true exactly as in the and keep them as true exactly as in the previous video previous video previous video and so and so prob that shape now",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 278,
      "text": "is 32 by 27 prob that shape now is 32 by 27 prob that shape now is 32 by 27 and you'll see that every row of prob and you'll see that every row of prob and you'll see that every row of prob sums to one",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 279,
      "text": "so it's normalized sums to one",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 280,
      "text": "so it's normalized sums to one",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 281,
      "text": "so it's normalized so that gives us the probabilities now so that gives us the probabilities now so that gives us the probabilities now of course we have the actual letter that of course we have the actual letter that of course we have the actual letter that comes next and that comes from this comes next and that comes from this comes next and that comes from this array y array y array y which we which we created during the which we which we created during the which we which we created during the dataset creation",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 282,
      "text": "so why is this last dataset creation so why is this last dataset creation so why is this last piece here which is the identity of the piece here which is the identity of the piece here which is the identity of the next character in the sequence that we'd next character in the sequence that we'd next character in the sequence that we'd like to now predict like to now predict like to now predict so what we'd like to do now is just as so what we'd like to do now is just as so what we'd like to do now is just as in the previous video we'd like to index in the previous video we'd like to index in the previous video we'd like to index into the rows of prob and in each row into the rows of prob and in each row into the rows of prob and in each row we'd like to pluck out the probability we'd like to pluck out the probability we'd like to pluck out the probability assigned to the correct character assigned to the correct character assigned to the correct character as given here as given here as given here so first we have torch.range of 32 which so first we have torch.range of 32 which so first we have torch.range of 32 which is kind of like a iterator over is kind of like a iterator over is kind of like a iterator over numbers from 0 to 31 numbers from 0 to 31 numbers from 0 to 31",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 283,
      "text": "and then we can index into prob in the",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 284,
      "text": "and then we can index into prob in the",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 285,
      "text": "and then we can index into prob in the following way following way following way prop in prop in prop in torch.range of 32 which iterates the torch.range of 32 which iterates the torch.range of 32 which iterates the roads and in each row we'd like to grab roads and in each row we'd like to grab roads and in each row we'd like to grab this column as given by y this column as given by y this column as given by y",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 286,
      "text": "so this gives the current probabilities so this gives the current probabilities so this gives the current probabilities as assigned by this neural network with as assigned by this neural network with as assigned by this neural network with this setting of its weights this setting of its weights this setting of its weights to the correct character in the sequence to the correct character in the sequence to the correct character in the sequence and you can see here that this looks and you can see here that this looks and you can see here that this looks okay for some of these characters like okay for some of these characters like okay for some of these characters like this is basically 0.2 this is basically 0.2 this is basically 0.2",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 287,
      "text": "but it doesn't look very good at all for but it doesn't look very good at all for but it doesn't look very good at all for many other characters like this is many other characters like this is many other characters like this is 0.0701 probability",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 288,
      "text": "and so the network 0.0701 probability",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 289,
      "text": "and so the network 0.0701 probability",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 290,
      "text": "and so the network thinks that some of these are extremely thinks that some of these are extremely thinks that some of these are extremely unlikely",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 291,
      "text": "but of course we haven't unlikely but of course we haven't unlikely",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 292,
      "text": "but of course we haven't trained the neural network yet so trained the neural network yet so trained the neural network yet",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 293,
      "text": "so this will improve and ideally all of this will improve and ideally all of this will improve and ideally all of these numbers here of course are one these numbers here of course are one these numbers here of course are one because then we are correctly predicting because then we are correctly predicting because then we are correctly predicting the next character the next character the next character now just as in the previous video we now just as in the previous video we now just as in the previous video we want to take these probabilities we want want to take these probabilities we want want to take these probabilities we want to look at the lock probability to look at the lock probability to look at the lock probability",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 294,
      "text": "and then we want to look at the average",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 295,
      "text": "and then we want to look at the average",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 296,
      "text": "and then we want to look at the average probability probability probability and the negative of it to create the and the negative of it to create the and the negative of it to create the negative log likelihood loss negative log likelihood loss negative log likelihood loss so the loss here is 17 so the loss here is 17 so the loss here is 17 and this is the loss that we'd like to and this is the loss that we'd like to and this is the loss that we'd like to minimize to get the network to predict minimize to get the network to predict minimize to get the network to predict the correct character in the sequence the correct character in the sequence the correct character in the sequence",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 297,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 298,
      "text": "so i rewrote everything here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 299,
      "text": "and okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 300,
      "text": "so i rewrote everything here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 301,
      "text": "and okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 302,
      "text": "so i rewrote everything here and made it a bit more respectable so here's made it a bit more respectable so here's made it a bit more respectable so here's our data set here's all the parameters our data set here's all the parameters our data set here's all the parameters that we defined that we defined that we defined i'm now using a generator to make it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 303,
      "text": "i'm now using a generator to make it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 304,
      "text": "i'm now using a generator to make it reproducible",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 305,
      "text": "reproducible reproducible",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 306,
      "text": "i clustered all the parameters into a i clustered all the parameters into a i clustered all the parameters into a single list of parameters so that for single list of parameters so that for single list of parameters so that for example it's easy to count them and see example it's easy to count them and see example it's easy to count them and see that in total we currently have about that in total we currently have about that in total we currently have about 3400 parameters 3400 parameters 3400 parameters and this is the forward pass as we and this is the forward pass as we and this is the forward pass as we developed it developed it developed it and we arrive at a single number here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 307,
      "text": "and we arrive at a single number here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 308,
      "text": "and we arrive at a single number here the loss that is currently expressing the loss that is currently expressing the loss that is currently expressing how well how well how well this neural network works with the this neural network works with the this neural network works with the current setting of parameters current setting of parameters current setting of parameters now i would like to make it even more now i would like to make it even more now i would like to make it even more respectable so in particular see these respectable so in particular see these respectable so in particular see these lines here where we take the logits and lines here where we take the logits and lines here where we take the logits and we calculate the loss we calculate the loss we calculate the loss we're not actually reinventing the wheel we're not actually reinventing the wheel we're not actually reinventing the wheel here this is just um here this is just um here this is just um classification and many people use classification and many people use classification and many people use classification and that's why there is a classification and that's why there is a classification and that's why there is a functional.cross entropy function in functional.cross entropy function in functional.cross entropy function in pytorch to calculate this much more pytorch to calculate this much more pytorch to calculate this much more efficiently efficiently efficiently so we can just simply call f.cross so we can just simply call f.cross so we can just simply call f.cross entropy entropy entropy and we can pass in the logits",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 309,
      "text": "and we can and we can pass in the logits",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 310,
      "text": "and we can and we can pass in the logits and we can pass in the pass in the pass in the array of targets y array of targets y array of targets",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 311,
      "text": "y and this calculates the exact same loss so in fact we can simply put this here so in fact we can simply put this here and erase these three lines and we're and erase these three lines and we're and erase these three lines and we're going to get the exact same result now going to get the exact same result now going to get the exact same result now there are actually many good reasons to there are actually many good reasons to there are actually many good reasons to prefer f.cross entropy over rolling your prefer f.cross entropy over rolling your prefer f.cross entropy over rolling your own implementation like this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 312,
      "text": "i did this own implementation like this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 313,
      "text": "i did this own implementation like this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 314,
      "text": "i did this for educational reasons but you'd never for educational reasons but you'd never for educational reasons but you'd never use this in practice why is that use this in practice why is that use this in practice why is that number one when you use f.cross entropy number one when you use f.cross entropy number one when you use f.cross entropy by torch will not actually create all by torch will not actually create all by torch will not actually create all these intermediate tensors because these these intermediate tensors because these these intermediate tensors because these are all new tensors in memory and all are all new tensors in memory and all are all new tensors in memory and all this is fairly inefficient to run like this is fairly inefficient to run like this is fairly inefficient to run like this instead pytorch will cluster up all this instead pytorch will cluster up all this instead pytorch will cluster up all these operations and very often create these operations and very often create these operations and very often create have fused kernels that very efficiently have fused kernels that very efficiently have fused kernels that very efficiently evaluate these expressions that are sort evaluate these expressions that are sort evaluate these expressions that are sort of like clustered mathematical of like clustered mathematical of like clustered mathematical operations operations operations number two the backward pass can be made",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 315,
      "text": "number two the backward pass can be made",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 316,
      "text": "number two the backward pass can be made much more efficient and not just because much more efficient and not just because much more efficient and not just because it's a fused kernel but also it's a fused kernel but also it's a fused kernel but also analytically and mathematically it's analytically and mathematically it's analytically and mathematically it's much",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 317,
      "text": "it's often a very much simpler much it's often a very much simpler much it's often a very much simpler backward pass to implement backward pass to implement backward pass to implement we actually sell this with micrograd we actually sell this with micrograd we actually sell this with micrograd you see here when we implemented 10h the you see here when we implemented 10h the you see here when we implemented 10h the forward pass of this operation to forward pass of this operation to forward pass of this operation to calculate the 10h was actually a fairly calculate the 10h was actually a fairly calculate the 10h was actually a fairly complicated mathematical expression complicated mathematical expression complicated mathematical expression",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 318,
      "text": "but because it's a clustered but because it's a clustered",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 319,
      "text": "but because it's a clustered mathematical expression when we did the mathematical expression when we did the mathematical expression when we did the backward pass we didn't individually backward pass we didn't individually backward pass we didn't individually backward through the x",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 320,
      "text": "and the two times backward through the x and the two times backward through the x and the two times and the minus one in division etc we and the minus one in division etc we and the minus one in division etc we just said it's one minus t squared and just said it's one minus t squared and just said it's one minus t squared and that's a much simpler mathematical that's a much simpler mathematical that's a much simpler mathematical expression expression expression and we were able to do this because and we were able to do this because and we were able to do this because we're able to reuse calculations and we're able to reuse calculations and we're able to reuse calculations and because we are able to mathematically because we are able to mathematically because we are able to mathematically and analytically derive the derivative and analytically derive the derivative and analytically derive the derivative and often that expression simplifies and often that expression simplifies and often that expression simplifies mathematically and so there's much less mathematically and so there's much less mathematically and so there's much less to implement to implement to implement so not only can can it be made more so not only can can it be made more so not only can can it be made more efficient because it runs in a fused efficient because it runs in a fused efficient because it runs in a fused kernel but also because the expressions kernel",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 321,
      "text": "but also because the expressions kernel but also because the expressions can take a much simpler form can take a much simpler form can take a much simpler form mathematically mathematically mathematically so that's number one number two",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 322,
      "text": "so that's number one number two",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 323,
      "text": "so that's number one number two under the hood f that cross entropy can under the hood f that cross entropy can under the hood f that cross entropy can also be significantly more um also be significantly more um also be significantly more um numerically well behaved let me show you numerically well behaved let me show you numerically well behaved let me show you an example of how this works suppose we have a logits of negative 2 3 suppose we have a logits of negative 2 3 negative 3 0 and 5 negative 3 0 and 5 negative 3 0 and 5 and then we are taking the exponent of and then we are taking the exponent of and then we are taking the exponent of it and normalizing it to sum to 1. it and normalizing it to sum to 1. it and normalizing it to sum to 1.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 324,
      "text": "so so when logits take on this values when logits take on this values when logits take on this values everything is well and good",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 325,
      "text": "and we get a everything is well and good",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 326,
      "text": "and we get a everything is well and good",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 327,
      "text": "and we get a nice probability distribution nice probability distribution nice probability distribution now consider what happens when some of now consider what happens when some of now consider what happens when some of these logits take on more extreme values these logits take on more extreme values these logits take on more extreme values and that can happen during optimization and that can happen during optimization and that can happen during optimization of the neural network of the neural network suppose that some of these numbers grow suppose that some of these numbers grow suppose that some of these numbers grow very negative like say negative 100 very negative like say negative 100 very negative like say negative 100 then actually everything will come out then actually everything will come out then actually everything will come out fine",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 328,
      "text": "we still get the probabilities that fine we still get the probabilities that fine we still get the probabilities that um um um you know are well behaved and they sum you know are well behaved and they sum you know are well behaved and they sum to one and everything is great to one and everything is great to one and everything is great but because of the way the x works if but because of the way the x works if but because of the way the x works if you have very positive logits let's say you have very positive logits let's say you have very positive logits let's say positive 100 in here positive 100 in here positive 100 in here you actually start to run into trouble you actually start to run into trouble you actually start to run into trouble and we get not a number here and we get not a number here and we get not a number here and the reason for that is that these and the reason for that is that these and the reason for that is that these counts have an if here have an if here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 329,
      "text": "so if you pass in a very negative number",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 330,
      "text": "so if you pass in a very negative number so if you pass in a very negative number to x you just get a very negative sorry to x you just get a very negative sorry to x you just get a very negative sorry not negative but very small number very not negative but very small number very not negative but very small number very very near zero",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 331,
      "text": "and that's fine very near zero",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 332,
      "text": "and that's fine very near zero",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 333,
      "text": "and that's fine",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 334,
      "text": "but if you pass in a very positive but if you pass in a very positive",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 335,
      "text": "but if you pass in a very positive number suddenly we run out of range in number suddenly we run out of range in number suddenly we run out of range in our floating point number that our floating point number that our floating point number that represents these counts represents these counts represents these counts",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 336,
      "text": "so basically we're taking e",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 337,
      "text": "and we're so basically we're taking e",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 338,
      "text": "and we're so basically we're taking e",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 339,
      "text": "and we're raising it to the power of 100 and that raising it to the power of 100 and that raising it to the power of 100 and that gives us if because we run out of gives us if because we run out of gives us if because we run out of dynamic range on this floating point dynamic range on this floating point dynamic range on this floating point number that is count number that is count number that is count",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 340,
      "text": "and so we cannot pass very large logits",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 341,
      "text": "and so we cannot pass very large logits",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 342,
      "text": "and so we cannot pass very large logits through this expression through this expression through this expression now let me reset these numbers to now let me reset these numbers to now let me reset these numbers to something reasonable something reasonable something reasonable the way pi torch solved this the way pi torch solved this the way pi torch solved",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 343,
      "text": "this is that is that is that you see how we have a well-behaved you see how we have a well-behaved you see how we have a well-behaved result here result here result here it turns out that because of the it turns out that because of the it turns out that because of the normalization here you can actually normalization here you can actually normalization here you can actually offset logits by any arbitrary constant offset logits by any arbitrary constant offset logits by any arbitrary constant value that you want so if i add 1 here value that you want so if i add 1 here value that you want so if i add 1 here you actually get the exact same result you actually get the exact same result you actually get the exact same result or if i add 2 or if i add 2 or if i add 2 or if i subtract three or if i subtract three or if i subtract three any offset will produce the exact same any offset will produce the exact same any offset will produce the exact same probabilities probabilities so because negative numbers are okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 344,
      "text": "but so because negative numbers are okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 345,
      "text": "but so because negative numbers are okay but positive numbers can actually overflow positive numbers can actually overflow positive numbers can actually overflow this x what patrick does is it this x what patrick does is it this x what patrick does is it internally calculates the maximum value internally calculates the maximum value internally calculates the maximum value that occurs in the logits and it that occurs in the logits and it that occurs in the logits and it subtracts it so in this case it would subtracts it so in this case it would subtracts it so in this case it would subtract five subtract five subtract five and so therefore the greatest number in and so therefore the greatest number in and so therefore the greatest number in logits will become zero and all the logits will become zero and all the logits will become zero and all the other numbers will become some negative other numbers will become some negative other numbers will become some negative numbers numbers and then the result of this is always and then the result of this is always and then the result of this is always well behaved",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 346,
      "text": "so even if we have 100 here well behaved so even if we have 100 here well behaved so even if we have 100 here previously previously previously not good but because pytorch will not good but because pytorch will not good but because pytorch will subtract 100 this will work subtract 100 this will work subtract 100 this will work and so there's many good reasons to call and so there's many good reasons to call and so there's many good reasons to call cross-entropy number one the forward cross-entropy number one the forward cross-entropy number one the forward pass can be much more efficient the pass can be much more efficient the pass can be much more efficient the backward pass can be much more efficient backward pass can be much more efficient backward pass can be much more efficient and also things can be much more and also things can be much more and also things can be much more numerically well behaved",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 347,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 348,
      "text": "so let's numerically well behaved",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 349,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 350,
      "text": "so let's numerically well behaved",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 351,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 352,
      "text": "so let's now set up the training of this neural now set up the training of this neural now set up the training of this neural net net net we have the forward pass uh we don't need these uh we don't need these is that we have the losses equal to the is that we have the losses equal to the is that we have the losses equal to the f.cross entropy that's the forward pass f.cross entropy that's the forward pass f.cross entropy that's the forward pass then we need the backward pass first we then we need the backward pass first we then we need the backward pass first we want to set the gradients to be zero so want to set the gradients to be zero so want to set the gradients to be zero so for p in parameters for p in parameters for p in parameters we want to make sure that p dot grad is we want to make sure that p dot grad is we want to make sure that p dot grad is none which is the same as setting it to none which is the same as setting it to none which is the same as setting it to zero in pi torch zero in pi torch zero in pi torch and then lost that backward to populate and then lost that backward to populate and then lost that backward to populate those gradients those gradients those gradients once we have the gradients we can do the once we have the gradients we can do the once we have the gradients we can do the parameter update so for p in parameters parameter update so for p in parameters parameter update so for p in parameters we want to take all the we want to take all the we want to take all the data",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 353,
      "text": "and we want to nudge it data",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 354,
      "text": "and we want to nudge it data",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 355,
      "text": "and we want to nudge it learning rate times p dot grad",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 356,
      "text": "and then we want to repeat this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 357,
      "text": "and then we want to repeat this a few times now this won't suffice and it will now this won't suffice and it will create an error because we also have to create an error because we also have to create an error because we also have to go for pn parameters go for pn parameters go for pn parameters",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 358,
      "text": "and we have to make sure that p dot and we have to make sure that p dot and we have to make sure that p dot requires grad is set to true in pi torch requires grad is set to true in pi torch requires grad is set to true in pi torch and this should just work and this should just work and this should just work",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 359,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 360,
      "text": "so we started off with loss of 17",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 361,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 362,
      "text": "so we started off with loss of 17",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 363,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 364,
      "text": "so we started off with loss of 17 and we're decreasing it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 365,
      "text": "and we're decreasing it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 366,
      "text": "and we're decreasing it let's run longer let's run longer let's run longer",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 367,
      "text": "and you see how the loss decreases and you see how the loss decreases and you see how the loss decreases a lot here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 368,
      "text": "so if we just run for a thousand times if we just run for a thousand times we get a very very low loss and that we get a very very low loss and that we get a very very low loss and that means that we're making very good means that we're making very good means that we're making very good predictions now the reason that this is predictions now the reason that this is predictions now the reason that this is so straightforward right now so straightforward right now so straightforward right now is because we're only um is because we're only um is because we're only um overfitting 32 examples overfitting 32 examples overfitting 32 examples so we only have 32 examples uh of",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 369,
      "text": "the so we only have 32 examples uh of the so we only have 32 examples uh of the first five words first five words first five words and therefore it's very easy to make and therefore it's very easy to make and therefore it's very easy to make this neural net fit only these two 32 this neural net fit only these two 32 this neural net fit only these two 32 examples because we have 3 400 examples because we have 3 400 examples because we have 3 400 parameters and only 32 examples so we're parameters and only 32 examples so we're parameters and only 32 examples so we're doing what's called overfitting a single doing what's called overfitting a single doing what's called overfitting a single batch of the data batch of the data batch of the data and getting a very low loss and good and getting a very low loss and good and getting a very low loss and good predictions predictions predictions um",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 370,
      "text": "but that's just because we have so",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 371,
      "text": "um but that's just because we have so um",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 372,
      "text": "but that's just because we have so many parameters for so few examples so many parameters for so few examples so many parameters for so few examples",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 373,
      "text": "so it's easy to it's easy to it's easy to uh make this be very low uh make this be very low uh make this be very low now we're not able to achieve exactly now we're not able to achieve exactly now we're not able to achieve exactly zero zero zero and the reason for that is we can for and the reason for that is we can for and the reason for that is we can for example look at logits which are being example look at logits which are being example look at logits which are being predicted predicted predicted and we can look at the max along the and we can look at the max along the and we can look at the max along the first dimension first dimension first dimension and in pi torch and in pi torch and in pi torch max reports both the actual values that max reports both the actual values that max reports both the actual values that take on the maximum number but also the take on the maximum number but also the take on the maximum number but also the indices of piece indices of piece indices of piece and you'll see that the indices are very and you'll see that the indices are very and you'll see that the indices are very close to the labels close to the labels close to the labels but in some cases they differ but in some cases they differ but in some cases they differ for example in this very first example for example in this very first example for example in this very first example the predicted index is 19 but the label the predicted index is 19 but the label the predicted index is 19 but the label is five is five is five and we're not able to make loss be zero and we're not able to make loss be zero and we're not able to make loss be zero and fundamentally that's because here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 374,
      "text": "and fundamentally that's because here and fundamentally that's because here the very first or the zeroth index is the very first or the zeroth index is the very first or the zeroth index is the example where dot dot dot is the example where dot dot dot is the example where dot dot dot is supposed to predict e",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 375,
      "text": "but you see how supposed to predict e",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 376,
      "text": "but you see how supposed to predict e",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 377,
      "text": "but you see how dot dot dot is also supposed to predict dot dot dot is also supposed to predict dot dot dot is also supposed to predict an o and dot dot is also supposed to an o and dot dot is also supposed to an o and dot dot is also supposed to predict an i",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 378,
      "text": "and then s as well and so predict an i",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 379,
      "text": "and then s as well and so predict an i",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 380,
      "text": "and then s as well and so basically e o a or s are all possible basically e o a or s are all possible basically e o a or s are all possible outcomes in a training set for the exact outcomes in a training set for the exact outcomes in a training set for the exact same input",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 381,
      "text": "so we're not able to same input",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 382,
      "text": "so we're not able to same input",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 383,
      "text": "so we're not able to completely over fit and um completely over fit and um completely over fit and um and make the loss be exactly zero so but and make the loss be exactly zero so",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 384,
      "text": "but and make the loss be exactly zero",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 385,
      "text": "so",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 386,
      "text": "but we're getting very close in the cases we're getting very close in the cases we're getting very close in the cases where where where there's a unique input for a unique there's a unique input for a unique there's a unique input for a unique output in those cases we do what's output in those cases we do what's output in those cases we do what's called overfit and we basically get the called overfit",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 387,
      "text": "and we basically get the called overfit and we basically get the exact same and the exact correct result exact same and the exact correct result exact same and the exact correct result so now all we have to do so now all we have to do so now all we have to do is we just need to make sure that we is we just need to make sure that we is we just need to make sure that we read in the full data set and optimize read in the full data set and optimize read in the full data set and optimize the neural net the neural net the neural net",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 388,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 389,
      "text": "so let's swing back up",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 390,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 391,
      "text": "so let's swing back up",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 392,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 393,
      "text": "so let's swing back up where we created the dataset where we created the dataset where we created the dataset",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 394,
      "text": "and we see that here we only use the and we see that here we only use the and we see that here we only use the first five words so let me now erase first five words so let me now erase first five words so let me now erase this this and let me erase the print statements and let me erase the print statements and let me erase the print statements",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 395,
      "text": "otherwise we'd be printing way too much otherwise we'd be printing way too much",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 396,
      "text": "otherwise we'd be printing way too much",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 397,
      "text": "and so when we processed the full data",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 398,
      "text": "and so when we processed the full data",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 399,
      "text": "and so when we processed the full data set of all the words we now had 228 000 set of all the words we now had 228 000 set of all the words we now had 228 000 examples instead of just 32.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 400,
      "text": "examples instead of just 32.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 401,
      "text": "examples instead of just 32.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 402,
      "text": "so let's now scroll back down so let's now scroll back down so let's now scroll back down to this is much larger reinitialize the to this is much larger reinitialize the to this is much larger reinitialize the weights the same number of parameters weights the same number of parameters weights the same number of parameters they all require gradients they all require gradients they all require gradients and then let's push this print out and then let's push this print out and then let's push this print out lost.item to be here lost.item to be here lost.item to be here and let's just see how the optimization and let's just see how the optimization and let's just see how the optimization goes if we run this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 403,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 404,
      "text": "so we started with a fairly high",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 405,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 406,
      "text": "so we started with a fairly high loss",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 407,
      "text": "and then as we're optimizing the loss and then as we're optimizing the loss and then as we're optimizing the loss is coming down",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 408,
      "text": "but you'll notice that it takes quite a",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 409,
      "text": "but you'll notice that it takes quite a bit of time for every single iteration bit of time for every single iteration bit of time for every single iteration so let's actually address that because so let's actually address that because so let's actually address that because we're doing way too much work forwarding we're doing way too much work forwarding we're doing way too much work forwarding and backwarding 220 000 examples and backwarding 220 000 examples and backwarding 220 000 examples in practice what people usually do is in practice what people usually do is in practice what people usually do is they perform forward and backward pass they perform forward and backward pass they perform forward and backward pass and update on many batches of the data and update on many batches of the data and update on many batches of the data",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 410,
      "text": "so what we will want to do is we want to so what we will want to do is we want to so what we will want to do is we want to randomly select some portion of the data randomly select some portion of the data randomly select some portion of the data set and that's a mini batch and then set and that's a mini batch and then set and that's a mini batch",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 411,
      "text": "and then only forward backward and update on that only forward backward and update on that only forward backward and update on that little mini batch and then little mini batch",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 412,
      "text": "and then little mini batch",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 413,
      "text": "and then we iterate on those many batches we iterate on those many batches we iterate on those many batches so in pytorch we can for example use so in pytorch we can for example use",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 414,
      "text": "so in pytorch we can for example use storage.randint storage.randint storage.randint we can generate numbers between 0 and 5 we can generate numbers between 0 and 5 we can generate numbers between 0 and 5 and make 32 of them",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 415,
      "text": "i believe the size has to be a i believe the size has to be a tuple tuple tuple in my torch in my torch in my torch so we can have a tuple 32 of numbers",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 416,
      "text": "so we can have a tuple 32 of numbers so we can have a tuple 32 of numbers between zero and five but actually we between zero and five but actually we between zero and five",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 417,
      "text": "but actually we want x dot shape of zero here want x dot shape of zero here want x dot shape of zero here and so this creates uh integers that",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 418,
      "text": "and so this creates uh integers that",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 419,
      "text": "and so this creates uh integers that index into our data set and there's 32 index into our data set and there's 32 index into our data set and there's 32 of them",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 420,
      "text": "so if our mini batch size is 32 of them",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 421,
      "text": "so if our mini batch size is 32 of them",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 422,
      "text": "so if our mini batch size is 32 then we can come here and we can first then we can come here and we can first then we can come here and we can first do a mini batch do a mini batch do a mini batch construct construct construct so in the integers that we want to so in the integers that we want to so in the integers that we want to optimize in this optimize in this optimize in this single iteration single iteration single iteration are in the ix are in the ix are in the ix",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 423,
      "text": "and then we want to index into and then we want to index into",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 424,
      "text": "and then we want to index into x x x with ix to only grab those rows with ix to only grab those rows with ix to only grab those rows so we're only getting 32 rows of x",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 425,
      "text": "so we're only getting 32 rows of x",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 426,
      "text": "so we're only getting 32 rows of x and therefore embeddings will again be and therefore embeddings will again be and therefore embeddings will again be 32 by three by two not two hundred 32 by three by two not two hundred 32 by three by two not two hundred thousand by three by two thousand by three by two thousand by three by two",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 427,
      "text": "and then this ix has to be used not just",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 428,
      "text": "and then this ix has to be used not just",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 429,
      "text": "and then this ix has to be used not just to index into x to index into x to index into x but also to index into y but also to index into y but also to index into y and now this should be many batches",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 430,
      "text": "and and now this should be many batches",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 431,
      "text": "and and now this should be many batches and this should be much much faster so this should be much much faster so this should be much much faster",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 432,
      "text": "so okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 433,
      "text": "so it's instant almost okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 434,
      "text": "so it's instant almost okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 435,
      "text": "so it's instant almost so this way we can run many many so this way we can run many many",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 436,
      "text": "so this way we can run many many examples examples examples nearly instantly and decrease the loss nearly instantly and decrease the loss nearly instantly and decrease the loss much much faster much much faster much much faster now because we're only dealing with mini now because we're only dealing with mini now because we're only dealing with mini batches the quality of our gradient is batches the quality of our gradient is batches the quality of our gradient is lower so the direction is not as lower so the direction is not as lower so the direction is not as reliable it's not the actual gradient reliable it's not the actual gradient reliable it's not the actual gradient direction direction direction but the gradient direction is good but the gradient direction is good but the gradient direction is good enough even when it's estimating on only enough even when it's estimating on only enough even when it's estimating on only 32 examples that it is useful and so 32 examples that it is useful and so 32 examples that it is useful",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 437,
      "text": "and so it's much better to have an approximate it's much better to have an approximate it's much better to have an approximate gradient and just make more steps than gradient and just make more steps than gradient and just make more steps than it is to evaluate the exact gradient and it is to evaluate the exact gradient and it is to evaluate the exact gradient and take fewer steps so that's why in take fewer steps so that's why in take fewer steps so that's why in practice uh this works quite well practice uh this works quite well practice uh this works quite well so let's now continue the optimization let me take out this lost item from here let me take out this lost item from here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 438,
      "text": "and uh and uh and uh place it over here at the end place it over here at the end place it over here at the end",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 439,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 440,
      "text": "so we're hovering around 2.5 or so",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 441,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 442,
      "text": "so we're hovering around 2.5 or so",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 443,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 444,
      "text": "so we're hovering around 2.5 or so however this is only the loss for that however this is only the loss for that however this is only the loss for that mini batch",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 445,
      "text": "so let's actually evaluate mini batch so let's actually evaluate mini batch so let's actually evaluate the loss the loss the loss here here for all of x for all of x for all of x and for all of y just so we have a and for all of y",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 446,
      "text": "just so we have a and for all of y just so we have a full sense of exactly how all the model full sense of exactly how all the model full sense of exactly how all the model is doing right now is doing right now is doing right now",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 447,
      "text": "so right now we're at about 2.7 on the so",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 448,
      "text": "right now we're at about 2.7 on the so",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 449,
      "text": "right now we're at about 2.7 on the entire training set entire training set entire training set so let's so let's so let's run the optimization for a while run the optimization for a while run the optimization for a while",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 450,
      "text": "okay right 2.6",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 451,
      "text": "okay right 2.6",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 452,
      "text": "okay right 2.6 2.57 2.57 2.57 2.53",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 453,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 454,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 455,
      "text": "so one issue of course is we don't know so one issue of course is we don't know so one issue of course is we don't know if we're stepping too slow or too fast if we're stepping too slow or too fast if we're stepping too slow or too fast so this point one i just guessed it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 456,
      "text": "so this point one i just guessed it so this point one i just guessed it so one question is how do you determine so one question is how do you determine so one question is how do you determine this learning rate this learning rate this learning rate",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 457,
      "text": "and how do we gain confidence that we're and how do we gain confidence that we're and how do we gain confidence that we're stepping in the right stepping in the right stepping in the right sort of speed so i'll show you one way sort of speed",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 458,
      "text": "so i'll show you one way sort of speed",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 459,
      "text": "so i'll show you one way to determine a reasonable learning rate to determine a reasonable learning rate to determine a reasonable learning rate it works as follows let's reset our it works as follows let's reset our it works as follows let's reset our parameters parameters parameters to the initial to the initial to the initial settings settings settings and now let's and now let's and now let's print in every step print in every step print in every step but let's only do 10 steps or so",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 460,
      "text": "but let's only do 10 steps or so but let's only do 10 steps or so or maybe maybe 100 steps or maybe maybe 100 steps or maybe maybe 100 steps we want to find like a very reasonable we want to find like a very reasonable we want to find like a very reasonable set set set search range if you will so for example search range if you will so for example search range if you will so for example if this is like very low if this is like very low if this is like very low then then we see that the loss is barely we see that the loss is barely we see that the loss is barely decreasing so that's not decreasing so that's not decreasing so that's not that's like too low basically so let's that's like too low basically so let's that's like too low basically so let's try try try this one this one this one",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 461,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 462,
      "text": "so we're decreasing the loss",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 463,
      "text": "but okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 464,
      "text": "so we're decreasing the loss",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 465,
      "text": "but okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 466,
      "text": "so we're decreasing the loss but like not very quickly so that's a pretty like not very quickly",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 467,
      "text": "so that's a pretty like not very quickly so that's a pretty good low range good low range good low range now let's reset it again now let's reset it again now let's reset it again and now let's try to find the place at and now let's try to find the place at and now let's try to find the place at which the loss kind of explodes which the loss kind of explodes",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 468,
      "text": "which the loss kind of explodes uh so maybe at negative one",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 469,
      "text": "okay we see that we're minimizing the okay we see that we're minimizing the loss but you see how uh it's kind of loss but you see how uh it's kind of loss but you see how uh it's kind of unstable it goes up and down quite a bit unstable it goes up and down quite a bit unstable it goes up and down quite a bit um so negative one is probably like a um so negative one is probably like a um so negative one is probably like a fast learning rate let's try negative fast learning rate let's try negative fast learning rate let's try negative 10.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 470,
      "text": "10.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 471,
      "text": "10.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 472,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 473,
      "text": "so this okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 474,
      "text": "so this okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 475,
      "text": "so this isn't optimizing this is not working isn't optimizing this is not working isn't optimizing this is not working very well so negative 10 is way too big very well so negative 10 is way too big very well so negative 10 is way too big negative one was already kind of big negative one was already kind of big negative one was already kind of big um um so therefore so therefore so therefore negative one was like somewhat negative one was like somewhat negative one was like somewhat reasonable if i reset reasonable if i reset reasonable if i reset so i'm thinking that the right learning",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 476,
      "text": "so i'm thinking that the right learning",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 477,
      "text": "so i'm thinking that the right learning rate is somewhere between rate is somewhere between rate is somewhere between uh negative zero point zero zero one and uh negative zero point zero zero one and uh negative zero point zero zero one and um um negative one negative one negative one so the way we can do this here is we can so the way we can do this here is we can so the way we can do this here is we can use uh torch shot lens space use uh torch shot lens space use uh torch shot lens space and we want to basically do something",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 478,
      "text": "and we want to basically do something",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 479,
      "text": "and we want to basically do something like this between zero and one",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 480,
      "text": "but like this between zero and one but like this between zero and one",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 481,
      "text": "but um um those number of steps is one more those number of steps is one more those number of steps is one more parameter that's required let's do a parameter that's required let's do a parameter that's required let's do a thousand steps this creates 1000 thousand steps this creates 1000 thousand steps this creates 1000 numbers between 0.01 and 1 numbers between 0.01 and 1 numbers between 0.01 and 1",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 482,
      "text": "but it doesn't really make sense to step",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 483,
      "text": "but it doesn't really make sense to step",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 484,
      "text": "but it doesn't really make sense to step between these linearly so instead let me between these linearly so instead let me between these linearly so instead let me create learning rate exponent create learning rate exponent create learning rate exponent and instead of 0.001 this will be a and instead of 0.001 this will be a and instead of 0.001 this will be a negative 3 and this will be a zero negative 3 and this will be a zero negative 3 and this will be a zero and then the actual lrs that we want to and then the actual lrs that we want to and then the actual lrs that we want to search over are going to be 10 to the search over are going to be 10 to the search over are going to be 10 to the power of lre power of lre power of lre so now what we're doing is we're so now what we're doing is we're so now what we're doing is we're stepping linearly between the exponents stepping linearly between the exponents stepping linearly between the exponents of these learning rates this is 0.001 of these learning rates this is 0.001 of these learning rates this is 0.001 and this is 1 because 10 to the power of and this is 1 because 10 to the power of and this is 1 because 10 to the power of 0 is 1. 0 is 1. 0 is 1.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 485,
      "text": "and therefore we are spaced and therefore we are spaced and therefore we are spaced exponentially in this interval exponentially in this interval exponentially in this interval",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 486,
      "text": "so these are the candidate learning so these are the candidate learning so these are the candidate learning rates rates rates that we want to sort of like search over that we want to sort of like search over that we want to sort of like search over roughly roughly roughly so now what we're going to do is so now what we're going to do is so now what we're going to do is here we are going to run the here we are going to run the here we are going to run the optimization for 1000 steps optimization for 1000 steps optimization for 1000 steps and instead of using a fixed number and instead of using a fixed number and instead of using a fixed number we are going to use learning rate we are going to use learning rate we are going to use learning rate indexing into here lrs of i indexing into here lrs of i indexing into here lrs of i and make this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 487,
      "text": "i so basically let me reset this to be so basically let me reset this to be again starting from random again starting from random again starting from random creating these learning rates between creating these learning rates between creating these learning rates between negative negative negative zero points between 0.001 and um zero points between 0.001 and um zero points between 0.001 and um one but exponentially stopped one but exponentially stopped one but exponentially stopped and here what we're doing is we're and here what we're doing is we're",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 488,
      "text": "and here what we're doing is we're iterating a thousand times iterating a thousand times iterating a thousand times we're going to use the learning rate we're going to use the learning rate we're going to use the learning rate um that's in the beginning very very low um that's in the beginning very very low",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 489,
      "text": "um",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 490,
      "text": "that's in the beginning very very low in the beginning is going to be 0.001 in the beginning is going to be 0.001 in the beginning is going to be 0.001 but by the end it's going to be but by the end it's going to be but by the end it's going to be 1. 1.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 491,
      "text": "1.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 492,
      "text": "and then we're going to step with that",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 493,
      "text": "and then we're going to step with that",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 494,
      "text": "and then we're going to step with that learning rate learning rate learning rate and now what we want to do is we want to and now what we want to do is we want to and now what we want to do is we want to keep track of the uh learning rates that we used and we want learning rates that we used",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 495,
      "text": "and we want to look at the losses to look at the losses to look at the losses that resulted that resulted that resulted and so here let me and so here let me and so here let me track stats track stats track stats so lri.append lr so lri.append lr so lri.append lr and um lost side that append",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 496,
      "text": "and um lost side that append",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 497,
      "text": "and um lost side that append loss that item loss that item loss that item",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 498,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 499,
      "text": "okay so again reset everything so again reset everything so again reset everything and then run and then run and then run",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 500,
      "text": "and so basically we started with a very and so basically we started with a very",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 501,
      "text": "and so basically we started with a very low learning rate and we went all the low learning rate and we went all the low learning rate and we went all the way up to a learning rate of negative way up to a learning rate of negative way up to a learning rate of negative one one one and now what we can do is we can plt",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 502,
      "text": "and now what we can do is we can plt",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 503,
      "text": "and now what we can do is we can plt that plot that plot that plot",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 504,
      "text": "and we can plot the two so we can plot and we can plot the two so we can plot and we can plot the two so we can plot the learning rates on the x-axis and the the learning rates on the x-axis and the the learning rates on the x-axis and the losses we saw on the y-axis losses we saw on the y-axis losses we saw on the y-axis and often you're going to find that your and often you're going to find that your and often you're going to find that your plot looks something like this plot looks something like this plot looks something like this where in the beginning where in the beginning where in the beginning you had very low learning rates so you had very low learning rates so you had very low learning rates so basically anything basically anything basically anything barely anything happened barely anything happened barely anything happened then we got to like a nice spot here then we got to like a nice spot here then we got to like a nice spot here and then as we increase the learning",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 505,
      "text": "and then as we increase the learning and then as we increase the learning rate enough rate enough rate enough we basically started to be kind of we basically started to be kind of we basically started to be kind of unstable here unstable here unstable here so a good learning rate turns out to be so a good learning rate turns out to be so a good learning rate turns out to be somewhere around here somewhere around here somewhere around here um and because we have lri here um and because we have lri here um and because we have lri here um um we actually may want to we actually may want to we actually may want to um do not lr do not lr not the learning rate but the exponent not the learning rate but the exponent not the learning rate but the exponent so that would be the lre at i is maybe so that would be the lre at i is maybe so that would be the lre at i is maybe what we want to log so let me reset this what we want to log so let me reset this what we want to log so let me reset this and redo that calculation and redo that calculation and redo that calculation but now on the x axis we have the but now on the x axis we have the but now on the x axis we have the [Music]",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 506,
      "text": "[Music] [Music] exponent of the learning rate and so we exponent of the learning rate and so we exponent of the learning rate and so we can see the exponent of the learning can see the exponent of the learning can see the exponent of the learning rate that is good to use it would be rate that is good to use it would be rate that is good to use it would be sort of like roughly in the valley here sort of like roughly in the valley here sort of like roughly in the valley here because here the learning rates are just because here the learning rates are just because here the learning rates are just way too low and then here where we way too low and then here where we way too low and then here where we expect relatively good learning rates expect relatively good learning rates expect relatively good learning rates somewhere here and then here things are somewhere here and then here things are somewhere here and then here things are starting to explode so somewhere around starting to explode so somewhere around starting to explode so somewhere around negative one x the exponent of the negative one x the exponent of the negative one x",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 507,
      "text": "the exponent of the learning rate is a pretty good setting learning rate is a pretty good setting learning rate is a pretty good setting and 10 to the negative one is 0.1 so 0.1 and 10 to the negative one is 0.1 so 0.1 and 10 to the negative one is 0.1 so 0.1 is actually 0.1 was actually a fairly is actually 0.1 was actually a fairly is actually 0.1 was actually a fairly good learning rate around here good learning rate around here good learning rate around here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 508,
      "text": "and that's what we had in the initial",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 509,
      "text": "and that's what we had in the initial",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 510,
      "text": "and that's what we had in the initial setting setting setting",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 511,
      "text": "but that's roughly how you would",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 512,
      "text": "but that's roughly how you would",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 513,
      "text": "but that's roughly how you would determine it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 514,
      "text": "and so here now we can take determine it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 515,
      "text": "and so here now we can take determine it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 516,
      "text": "and so here now we can take out the tracking of these out the tracking of these out the tracking of these and we can just simply set lr to be 10 and we can just simply set lr to be 10 and we can just simply set lr to be 10 to the negative one or to the negative one or to the negative one or basically otherwise 0.1 as it was before basically otherwise 0.1 as it was before basically otherwise 0.1 as it was before and now we have some confidence that",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 517,
      "text": "and now we have some confidence that",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 518,
      "text": "and now we have some confidence that this is actually a fairly good learning this is actually a fairly good learning this is actually a fairly good learning rate rate rate",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 519,
      "text": "and so now we can do is we can crank up",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 520,
      "text": "and so now we can do is we can crank up",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 521,
      "text": "and so now we can do is we can crank up the iterations the iterations the iterations we can reset our optimization we can reset our optimization we can reset our optimization and and we can run for a pretty long time using we can run for a pretty long time using we can run for a pretty long time using this learning rate this learning rate oops",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 522,
      "text": "and we don't want to print that's oops",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 523,
      "text": "and we don't want to print that's oops",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 524,
      "text": "and we don't want to print that's way too much printing way too much printing way too much printing",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 525,
      "text": "so let me again reset so let me again reset so let me again reset and run ten thousand stops",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 526,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 527,
      "text": "so we're 0.2 2.48 roughly let's run",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 528,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 529,
      "text": "so we're 0.2 2.48 roughly let's run another 10 000 steps 2.46 2.46 and now let's do one learning rate decay and now let's do one learning rate decay and now let's do one learning rate decay what this means is we're going to take what this means is we're going to take what this means is we're going to take our learning rate and we're going to 10x our learning rate",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 530,
      "text": "and we're going to 10x",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 531,
      "text": "our learning rate and we're going to 10x lower it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 532,
      "text": "and so we're at the late stages lower it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 533,
      "text": "and so we're at the late stages lower it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 534,
      "text": "and so we're at the late stages of training potentially and we may want of training potentially and we may want of training potentially and we may want to go a bit slower let's do one more to go a bit slower let's do one more to go a bit slower let's do one more actually at 0.1 just to see if we're making a dent here we're making a dent here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 535,
      "text": "okay we're still making dent and by the okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 536,
      "text": "we're still making dent and by the okay we're still making dent and by the way the way the way the bi-gram loss that we achieved last video bi-gram loss that we achieved last video bi-gram loss that we achieved last video was 2.45",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 537,
      "text": "so we've already surpassed the was 2.45",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 538,
      "text": "so we've already surpassed the was 2.45",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 539,
      "text": "so we've already surpassed the bi-gram model bi-gram model bi-gram model and once i get a sense that this is and once i get a sense that this is and once i get a sense that this is actually kind of starting to plateau off actually kind of starting to plateau off actually kind of starting to plateau off people like to do as i mentioned this people like to do as i mentioned this people like to do as i mentioned this learning rate decay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 540,
      "text": "so let's try to learning rate decay so let's try to learning rate decay so let's try to decay the loss decay the loss decay the loss the learning rate",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 541,
      "text": "i mean and we achieve it about 2.3 now and we achieve it about 2.3 now obviously this is janky and not exactly obviously this is janky and not exactly obviously this is janky and not exactly how you would train it in production but how you would train it in production but how you would train it in production but this is roughly what you're going this is roughly what you're going this is roughly what you're going through you first find a decent learning through you first find a decent learning through you first find a decent learning rate using the approach that i showed rate using the approach that i showed rate using the approach that i showed you",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 542,
      "text": "you",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 543,
      "text": "you",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 544,
      "text": "then you start with that learning rate then you start with that learning rate then you start with that learning rate and you train for a while and you train for a while and you train for a while and then at the end people like to do a and then at the end people like to do a and then at the end people like to do a learning rate decay where you decay the learning rate decay where you decay the learning rate decay where you decay the learning rate by say a factor of 10 and learning rate by say a factor of 10 and learning rate by say a factor of 10 and you do a few more steps",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 545,
      "text": "and then you get you do a few more steps",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 546,
      "text": "and then you get you do a few more steps",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 547,
      "text": "and then you get a trained network roughly speaking a trained network roughly speaking a trained network roughly speaking so we've achieved 2.3 and dramatically so we've achieved 2.3 and dramatically so we've achieved 2.3 and dramatically improved on the bi-gram language model improved on the bi-gram language model improved on the bi-gram language model using this simple neural net as using this simple neural net as using this simple neural net as described here described here described here using these 3 400 parameters now there's using these 3 400 parameters now there's using these 3 400 parameters now there's something we have to be careful with something we have to be careful with something we have to be careful with i said that we have a better model i said that we have a better model i said that we have a better model because we are achieving a lower loss because we are achieving a lower loss because we are achieving a lower loss 2.3 much lower than 2.45 with the 2.3 much lower than 2.45 with the 2.3 much lower than 2.45 with the bi-gram model previously bi-gram model previously bi-gram model previously now that's not exactly true and the now that's not exactly true and the now that's not exactly true and the reason that's not true is that this is actually fairly small model",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 548,
      "text": "but this is actually fairly small model but these models can get larger and larger these models can get larger and larger these models can get larger and larger if you keep adding neurons and if you keep adding neurons and if you keep adding neurons and parameters so you can imagine that we parameters so you can imagine that we parameters so you can imagine that we don't potentially have a thousand don't potentially have a thousand don't potentially have a thousand parameters we could have 10 000 or 100 parameters we could have 10 000 or 100 parameters we could have 10 000 or 100 000 or millions of parameters 000 or millions of parameters 000 or millions of parameters and as the capacity of the neural and as the capacity of the neural and as the capacity of the neural network grows network grows network grows it becomes more and more capable of it becomes more and more capable of it becomes more and more capable of overfitting your training set overfitting your training set overfitting your training set what that means is that the loss on the what that means is that the loss on the what that means is that the loss on the training set on the data that you're training set on the data that you're training set on the data that you're training on will become very very low as training on will become very very low as training on will become very very low as low as zero low as zero low as zero but all that the model is doing is but all that the model is doing is but all that the model is doing is memorizing your training set verbatim so memorizing your training set verbatim so memorizing your training set verbatim",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 549,
      "text": "so if you take that model and it looks like if you take that model and it looks like if you take that model and it looks like it's working",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 550,
      "text": "really well",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 551,
      "text": "but you try to it's working",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 552,
      "text": "really well",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 553,
      "text": "but you try to it's working",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 554,
      "text": "really well",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 555,
      "text": "but you try to sample from it you will basically only sample from it you will basically only sample from it you will basically only get examples exactly as they are in the get examples exactly as they are in the get examples exactly as they are in the training set you won't get any new data training set you won't get any new data training set you won't get any new data in addition to that if you try to in addition to that if you try to in addition to that if you try to evaluate the loss on some withheld names evaluate the loss on some withheld names evaluate the loss on some withheld names or other words or other words or other words you will actually see that the loss on you will actually see that the loss on you will actually see that the loss on those can be very high and so basically those can be very high and so basically those can be very high",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 556,
      "text": "and so basically it's not a good model it's not a good model it's not a good model so the standard in the field is to split so the standard in the field is to split so the standard in the field is to split up your data set into three splits as we up your data set into three splits as we up your data set into three splits as we call them we have the training split the call them we have the training split the call them we have the training split the dev split or the validation split dev split or the validation split dev split or the validation split and the test split and the test split and the test split so so training split training split training split test or um sorry dev or validation split test or um sorry dev or validation split test or um sorry dev or validation split and test split and typically this would and test split and typically this would and test split and typically this would be say eighty percent of your data set be say eighty percent of your data set be say eighty percent of your data set this could be ten percent",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 557,
      "text": "and this ten this could be ten percent and this ten this could be ten percent and this ten percent roughly percent roughly percent roughly so you have these three splits of the",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 558,
      "text": "so you have these three splits of the so you have these three splits of the data data data now these eighty percent of your now these eighty percent of your now these eighty percent of your trainings of the data set the training trainings of the data set the training trainings of the data set the training set is used to optimize the parameters set is used to optimize the parameters set is used to optimize the parameters of the model just like we're doing here of the model just like we're doing here of the model just like we're doing here using gradient descent using gradient descent using gradient descent these 10 percent of the these 10 percent of the these 10 percent of the examples the dev or validation split examples the dev or validation split examples the dev or validation split they're used for development over all they're used for development over all they're used for development over all the hyper parameters of your model so the hyper parameters of your model so the hyper parameters of your model so hyper parameters are for example the hyper parameters are for example the hyper parameters are for example the size of this hidden layer size of this hidden layer size of this hidden layer the size of the embedding so this is a the size of the embedding",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 559,
      "text": "so this is a the size of the embedding so this is a hundred or a two for us but we could try hundred or a two for us but we could try hundred or a two for us but we could try different things different things different things the strength of the regularization which the strength of the regularization which the strength of the regularization which we aren't using yet so far we aren't using yet so far we aren't using yet so far so there's lots of different hybrid so there's lots of different hybrid so there's lots of different hybrid parameters and settings that go into parameters and settings that go into parameters and settings that go into defining your neural net and you can try defining your neural net and you can try defining your neural net and you can try many different variations of them and many different variations of them and many different variations of them and see whichever one works best on your see whichever one works best on your see whichever one works best on your validation split validation split validation split so this is used to train the parameters so this is used to train the parameters so this is used to train the parameters this is used to train the hyperprimers this is used to train the hyperprimers this is used to train the hyperprimers and test split is used to evaluate and test split is used to evaluate and test split is used to evaluate basically the performance of the model basically the performance of the model basically the performance of the model at the end at the end at the end",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 560,
      "text": "so we're only evaluating the loss on the so we're only evaluating the loss on the so we're only evaluating the loss on the test plate very very sparingly and very test plate very very sparingly and very test plate very very sparingly and very few times because every single time you few times because every single time you few times because every single time you evaluate your test loss and you learn evaluate your test loss and you learn evaluate your test loss and you learn something from it something from it something from it you are basically starting to also train you are basically starting to also train you are basically starting to also train on the test split on the test split on the test split so you are only allowed to test the loss so you are only allowed to test the loss so you are only allowed to test the loss on a test on a test on a test set set very very few times otherwise you risk very very few times otherwise you risk very very few times otherwise you risk overfitting to it as well as you overfitting to it as well as you overfitting to it as well as you experiment on your model experiment on your model experiment on your model so let's also split up our training data so let's also split up our training data so let's also split up our training data into train dev and test",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 561,
      "text": "and then we are into train dev and test and then we are into train dev and test and then we are going to train on train going to train on train going to train on train and only evaluate on tests very very and only evaluate on tests very very and only evaluate on tests very very sparingly",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 562,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 563,
      "text": "so here we go sparingly",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 564,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 565,
      "text": "so here we go sparingly",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 566,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 567,
      "text": "so here we go here is where we took all the words and here is where we took all the words and here is where we took all the words and put them into x and y tensors put them into x and y tensors put them into x and y tensors so instead let me create a new cell here so instead let me create a new cell here so instead let me create a new cell here and let me just copy paste some code and let me just copy paste some code and let me just copy paste some code here here because i don't think it's that because i don't think it's that because i don't think it's that complex but complex but complex but we're going to try to save a little bit we're going to try to save a little bit we're going to try to save a little bit of time of time of time i'm converting this to be a function now i'm converting this to be a function now i'm converting this to be a function now and this function takes some list of and this function takes some list of and this function takes some list of words and builds the arrays x and y for words and builds the arrays x and y for words and builds the arrays x and y for those words only those words only those words only",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 568,
      "text": "and then here i am shuffling up all the and then here i am shuffling up all the and then here i am shuffling up all the words so these are the input words that words so these are the input words that words so these are the input words that we get we get we get we are randomly shuffling them all up we are randomly shuffling them all up we are randomly shuffling them all up and then um and then um and then um we're going to we're going to we're going to set n1 to be set n1 to be set n1 to be the number of examples that there's 80 the number of examples that there's 80 the number of examples that there's 80 of the words and n2 to be of the words and n2 to be of the words and n2 to be 90 90 90 of the way of the words so basically if of the way of the words so basically if of the way of the words so basically if len of words is 32 000 n1 is len of words is 32 000 n1 is len of words is 32 000 n1 is well sorry",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 569,
      "text": "i should probably run this well",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 570,
      "text": "sorry i should probably run this well",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 571,
      "text": "sorry i should probably run this n1 is 25 000 and n2 is 28 000.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 572,
      "text": "n1 is 25 000 and n2 is 28 000.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 573,
      "text": "n1 is 25 000 and n2 is 28 000.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 574,
      "text": "and so here we see that",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 575,
      "text": "and so here we see that",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 576,
      "text": "and so here we see that i'm calling build data set to build the i'm calling build data set to build the i'm calling build data set to build the training set x and y training set x and y training set x and y by indexing into up to and one so we're by indexing into up to and one",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 577,
      "text": "so we're by indexing into up to and one",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 578,
      "text": "so we're going to have only 25 000 training words going to have only 25 000 training words going to have only 25 000 training words",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 579,
      "text": "and then we're going to have",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 580,
      "text": "and then we're going to have",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 581,
      "text": "and then we're going to have roughly roughly n2 minus n1 n2 minus n1 n2 minus",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 582,
      "text": "n1 3 3 000 validation examples or dev 3 3 000 validation examples or dev 3 3 000 validation examples or dev examples and we're going to have when of words basically minus and two when of words basically minus and two or or or 3 204 examples 3 204 examples 3 204 examples here for the test set here for the test set here for the test set so so now we have x's and y's now we have x's and y's now we have x's and y's for all those three splits",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 583,
      "text": "oh yeah",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 584,
      "text": "i'm printing their size here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 585,
      "text": "oh yeah",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 586,
      "text": "i'm printing their size here inside the function as well",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 587,
      "text": "but here we don't have words but these",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 588,
      "text": "but here we don't have words but these are already the individual examples made are already the individual examples made are already the individual examples made from those words from those words from those words so let's now scroll down here so let's now scroll down here so let's now scroll down here and the data set now for training is and the data set now for training is and the data set now for training is more like this more like this more like this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 589,
      "text": "and then when we reset the network when we're training we're only going to when we're training we're only going to be training using x train",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 590,
      "text": "be training using x train be training using x train x train and y train x train and y train x train and y train",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 591,
      "text": "so that's the only thing we're training so that's the only thing we're training so that's the only thing we're training on let's see where we are on the let's see where we are on the single batch single batch single batch let's now train maybe a few more steps training neural networks can take a training neural networks can take a while usually you don't do it inline you while usually you don't do it inline you while usually you don't do it inline you launch a bunch of jobs and you wait for launch a bunch of jobs",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 592,
      "text": "and you wait for launch a bunch of jobs and you wait for them to finish um can take in multiple them to finish um can take in multiple them to finish um can take in multiple days",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 593,
      "text": "and so on days and so on days and so on luckily this is a very small network",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 594,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 595,
      "text": "so the loss is pretty good",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 596,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 597,
      "text": "so the loss is pretty good",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 598,
      "text": "oh we accidentally used a learning rate",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 599,
      "text": "oh we accidentally used a learning rate",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 600,
      "text": "oh we accidentally used a learning rate that is way too low that is way too low that is way too low",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 601,
      "text": "so let me actually come back so let me actually come back so let me actually come back we use the decay learning rate of 0.01",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 602,
      "text": "so this will train much faster so this will train much faster and then here when we evaluate and then here when we evaluate and then here when we evaluate let's use the dep set here let's use the dep set here let's use the dep set here xdev xdev xdev and ydev to evaluate the loss and ydev to evaluate the loss and ydev to evaluate the loss",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 603,
      "text": "okay okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 604,
      "text": "and let's now decay the learning rate and let's now decay the learning rate and let's now decay the learning rate and only do say 10 000 examples and let's evaluate the dev loss and let's evaluate the dev loss ones here ones here ones here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 605,
      "text": "okay so we're getting about 2.3 on dev",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 606,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 607,
      "text": "so we're getting about 2.3 on dev",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 608,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 609,
      "text": "so we're getting about 2.3 on dev",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 610,
      "text": "and so the neural network when it was and so the neural network when it was and so the neural network when it was training did not see these dev examples training did not see these dev examples training did not see these dev examples it hasn't optimized on them and yet it hasn't optimized on them and yet it hasn't optimized on them and yet when we evaluate the loss on these dev when we evaluate the loss on these dev when we evaluate the loss on these dev we actually get a pretty decent loss we actually get a pretty decent loss we actually get a pretty decent loss and so we can also look at what the",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 611,
      "text": "and so we can also look at what the",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 612,
      "text": "and so we can also look at what the loss is on all of training set loss is on all of training set loss is on all of training set oops oops oops",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 613,
      "text": "and so we see that the training and the",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 614,
      "text": "and so we see that the training and the",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 615,
      "text": "and so we see that the training and the dev loss are about equal so we're not dev loss are about equal",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 616,
      "text": "so we're not dev loss are about equal",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 617,
      "text": "so we're not over fitting over fitting over fitting um this model is not powerful enough to um this model is not powerful enough to um this model is not powerful enough to just be purely memorizing the data and just be purely memorizing the data and just be purely memorizing the data and so far we are what's called underfitting so far we are what's called underfitting so far we are what's called underfitting because the training loss and the dev or because the training loss and the dev or because the training loss and the dev or test losses are roughly equal so what test losses are roughly equal so what test losses are roughly equal",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 618,
      "text": "so what that typically means is that our network that typically means is that our network that typically means is that our network is very tiny very small",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 619,
      "text": "and we expect to is very tiny very small",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 620,
      "text": "and we expect to is very tiny very small",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 621,
      "text": "and we expect to make performance improvements by scaling make performance improvements by scaling make performance improvements by scaling up the size of this neural net so let's up the size of this neural net",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 622,
      "text": "so let's up the size of this neural net so let's do that now",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 623,
      "text": "so let's come over here do that now",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 624,
      "text": "so let's come over here do that now",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 625,
      "text": "so let's come over here and let's increase the size of the and let's increase the size of the and let's increase the size of the neural net the easiest way to do this is neural net the easiest way to do this is neural net the easiest way to do this is we can come here to the hidden layer we can come here to the hidden layer we can come here to the hidden layer which currently has 100 neurons and which currently has 100 neurons and which currently has 100 neurons and let's just bump this up so let's do 300 let's just bump this up so let's do 300 let's just bump this up so let's do 300 neurons neurons neurons and then this is also 300 biases and and then this is also 300 biases and and then this is also 300 biases and here we have 300 inputs into the final here we have 300 inputs into the final here we have 300 inputs into the final layer layer layer",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 626,
      "text": "so so let's initialize our neural net we now let's initialize our neural net we now let's initialize our neural net we now have ten thousand ex ten thousand have ten thousand ex ten thousand have ten thousand ex ten thousand parameters instead of three thousand parameters instead of three thousand parameters instead of",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 627,
      "text": "three thousand parameters parameters",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 628,
      "text": "and then we're not using this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 629,
      "text": "and then we're not using this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 630,
      "text": "and then we're not using this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 631,
      "text": "and then here what i'd like to do is i'd",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 632,
      "text": "and then here what i'd like to do is i'd",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 633,
      "text": "and then here what i'd like to do is i'd like to actually uh keep track of uh like to actually uh keep track of uh like to actually uh keep track of uh tap tap tap",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 634,
      "text": "um okay let's just do this let's keep stats",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 635,
      "text": "okay let's just do this let's keep stats again again again and here when we're keeping track of the loss let's just also keep track of the loss let's just also keep track of the steps and let's just have i here steps and let's just have i here steps and let's just have i here and let's train on thirty thousand and let's train on thirty thousand and let's train on thirty thousand or rather say or rather say or rather say okay let's try thirty thousand",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 636,
      "text": "okay let's try thirty thousand",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 637,
      "text": "okay let's try thirty thousand",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 638,
      "text": "and we are at point one",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 639,
      "text": "and we are at point one and we are at point one and and we should be able to run this we should be able to run this we should be able to run this and optimize the neural net and optimize the neural net and optimize the neural net and then here basically i want to and then here basically i want to and then here basically i want to plt.plot plt.plot plt.plot",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 640,
      "text": "the steps the steps the steps against the loss so these are the x's and y's so these are the x's and y's and this is and this is and this is the loss function and how it's being the loss function and how it's being the loss function and how it's being optimized optimized optimized now you see that there's quite a bit of now you see that there's quite a bit of now you see that there's quite a bit of thickness to this and that's because we thickness to this and that's because we thickness to this and that's because we are optimizing over these mini batches are optimizing over these mini batches are optimizing over these mini batches and the mini batches create a little bit and the mini batches create a little bit and the mini batches create a little bit of noise of noise of noise in this in this in this uh where are we in the def set we are at uh where are we in the def set we are at uh where are we in the def set we are at 2.5 so we still haven't optimized this 2.5",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 641,
      "text": "so we still haven't optimized this 2.5",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 642,
      "text": "so we still haven't optimized this neural net very well neural net very well neural net very well",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 643,
      "text": "and that's probably because we made it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 644,
      "text": "and that's probably because we made it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 645,
      "text": "and that's probably because we made it bigger it might take longer for this bigger it might take longer for this bigger it might take longer for this neural net to converge neural net to converge neural net to converge um um and so let's continue training and so let's continue training and so let's continue training um um",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 646,
      "text": "yeah let's just continue training one possibility is that the batch size one possibility is that the batch size is so low is so low is so low that uh we just have way too much noise that uh we just have way too much noise that uh we just have way too much noise in the training and we may want to in the training and we may want to in the training and we may want to increase the batch size so that we have increase the batch size so that we have increase the batch size so that we have a bit more um correct gradient and we're a bit more um correct gradient and we're a bit more um correct gradient and we're not thrashing too much and we can not thrashing too much and we can not thrashing too much and we can actually like optimize more properly",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 647,
      "text": "okay this will now become meaningless because this will now become meaningless because this will now become meaningless because we've reinitialized these so we've reinitialized these so we've reinitialized these",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 648,
      "text": "so",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 649,
      "text": "yeah this looks not",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 650,
      "text": "yeah this looks not",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 651,
      "text": "yeah this looks not pleasing right now",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 652,
      "text": "but there probably is pleasing right now",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 653,
      "text": "but there probably is pleasing right now",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 654,
      "text": "but there probably is like a tiny improvement",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 655,
      "text": "but it's so hard like a tiny improvement",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 656,
      "text": "but it's so hard like a tiny improvement",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 657,
      "text": "but it's so hard to tell to tell to tell let's go again let's go again let's go again 2.52",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 658,
      "text": "let's try to decrease the learning rate let's try to decrease the learning rate by factor",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 659,
      "text": "two okay we're at 2.32",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 660,
      "text": "okay we're at 2.32 let's continue training we basically expect to see a lower loss we basically expect to see a lower loss than what we had before because now we than what we had before because now we than what we had before because now we have a much much bigger model and we have a much much bigger model and we have a much much bigger model and we were under fitting so we'd expect that were under fitting so we'd expect that were under fitting so we'd expect that increasing the size of the model should increasing the size of the model should increasing the size of the model should help the neural net help the neural net help the neural net 2.32",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 661,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 662,
      "text": "so that's not happening too 2.32 okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 663,
      "text": "so that's not happening too 2.32 okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 664,
      "text": "so that's not happening too well",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 665,
      "text": "well well now one other concern is that even now one other concern is that even now one other concern is that even though we've made the 10h layer here or though we've made the 10h layer here or though we've made the 10h layer here or the hidden layer much much bigger it the hidden layer much much bigger it the hidden layer much much bigger it could be that the bottleneck of the could be that the bottleneck of the could be that the bottleneck of the network right now are these embeddings network right now are these embeddings network right now are these embeddings that are two dimensional it can be that that are two dimensional it can be that that are two dimensional it can be that we're just cramming way too many we're just cramming way too many we're just cramming way too many characters into just two dimensions and characters into just two dimensions and characters into just two dimensions and the neural net is not able to really use the neural net is not able to really use the neural net is not able to really use that space effectively and that that is that space effectively and that that is that space effectively and that that is sort of like the bottleneck to our sort of like the bottleneck to our sort of like the bottleneck to our network's performance network's performance network's performance okay 2.23 so just by decreasing the okay 2.23 so just by decreasing the okay 2.23 so just by decreasing the learning rate i was able to make quite a learning rate i was able to make quite a learning rate i was able to make quite a bit of progress",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 666,
      "text": "let's run this one more bit of progress let's run this one more bit of progress let's run this one more time and then evaluate the training and the and then evaluate the training and the dev loss now one more thing after training that now one more thing after training that i'd like to do is i'd like to visualize i'd like to do is i'd like to visualize i'd like to do is i'd like to visualize the um the um the um embedding vectors for these embedding vectors for these embedding vectors for these characters before we scale up the characters before we scale up the characters before we scale up the embedding size from two embedding size from two embedding size from two because we'd like to make uh this because we'd like to make uh this because we'd like to make uh this bottleneck potentially go away bottleneck potentially go away bottleneck potentially go away",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 667,
      "text": "but once i make this greater than two we but once i make this greater than two we",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 668,
      "text": "but once i make this greater than two we won't be able to visualize them won't be able to visualize them won't be able to visualize them so here so here so here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 669,
      "text": "okay we're at 2.23 and 2.24",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 670,
      "text": "okay we're at 2.23 and 2.24",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 671,
      "text": "okay we're at 2.23 and 2.24",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 672,
      "text": "so um we're not improving much more and so um we're not improving much more and so um we're not improving much more and maybe the bottleneck now is the maybe the bottleneck now is the maybe the bottleneck now is the character embedding size which is two character embedding size which is two character embedding size which is two so here i have a bunch of code that will so here i have a bunch of code that will so here i have a bunch of code that will create a figure create a figure create a figure",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 673,
      "text": "and then we're going to visualize",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 674,
      "text": "and then we're going to visualize",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 675,
      "text": "and then we're going to visualize the embeddings that were trained by the the embeddings that were trained by the the embeddings that were trained by the neural net neural net on these characters because right now on these characters because right now on these characters because right now the embedding has just two",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 676,
      "text": "so",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 677,
      "text": "we can the embedding has just two",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 678,
      "text": "so",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 679,
      "text": "we can the embedding has just two so we can visualize all the characters with the x visualize all the characters with the x visualize all the characters with the x and the y coordinates as the two and the y coordinates as the two and the y coordinates as the two embedding locations for each of these embedding locations for each of these embedding locations for each of these characters characters characters and so here are the x coordinates and and so here are the x coordinates and and so here are the x coordinates and the y coordinates which are the columns the y coordinates which are the columns the y coordinates which are the columns of c of c and then for each one i also include the and then for each one i also include the and then for each one i also include the text of the little character text of the little character text of the little character so here what we see is actually kind of so here what we see is actually kind of so here what we see is actually kind of interesting the network has basically learned to the network has basically learned to separate out the characters and cluster separate out the characters and cluster separate out the characters and cluster them a little bit uh",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 680,
      "text": "so for example you them a little bit uh",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 681,
      "text": "so for example you them a little bit uh so for example you see how the vowels see how the vowels see how the vowels a e i o u are clustered up here a e",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 682,
      "text": "i o u are clustered up here a e",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 683,
      "text": "i o u are clustered up here so that's telling us that is that the so that's telling us that is that the so that's telling us that is that the neural net treats these is very similar neural net treats these is very similar neural net treats these is very similar right because when they feed into the right because when they feed into the right because when they feed into the neural net neural net the embedding uh for all these the embedding uh for all these the embedding uh for all these characters is very similar and so the characters is very similar and so the characters is very similar and so the neural net thinks that they're very neural net thinks that they're very neural net thinks that they're very similar and kind of like interchangeable similar and kind of like interchangeable similar and kind of like interchangeable if that makes sense if that makes sense if that makes sense um um then the the points that are like really then the the points that are like really then the the points that are like really far away are for example q q is kind of far away are for example q q is kind of far away are for example q q is kind of treated as an exception and q has a very treated as an exception and q has a very treated as an exception and q has a very special special special embedding vector so to speak embedding vector so to speak embedding vector so to speak similarly dot which is a special similarly dot which is a special similarly dot which is a special character is all the way out here character is all the way out here character is all the way out here and a lot of the other letters are sort and a lot of the other letters are sort and a lot of the other letters are sort of like clustered up here and so it's of like clustered up here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 684,
      "text": "and so it's of like clustered up here",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 685,
      "text": "and so it's kind of interesting that there's a kind of interesting that there's a kind of interesting that there's a little bit of structure here little bit of structure here little bit of structure here after the training after the training after the training and it's not definitely not random",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 686,
      "text": "and and it's not definitely not random",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 687,
      "text": "and and it's not definitely not random and these embeddings make sense these embeddings make sense these embeddings make sense",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 688,
      "text": "so we're now going to scale up the so we're now going to scale up the so we're now going to scale up the embedding size and won't be able to embedding size and won't be able to embedding size and won't be able to visualize it directly",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 689,
      "text": "but we expect that visualize it directly",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 690,
      "text": "but we expect that visualize it directly",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 691,
      "text": "but we expect that because we're under fitting because we're under fitting because we're under fitting",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 692,
      "text": "and we made this layer much bigger and and we made this layer much bigger and and we made this layer much bigger and did not sufficiently improve the loss did not sufficiently improve the loss did not sufficiently improve the loss we're thinking that the um we're thinking that the um we're thinking that the um constraint to better performance right constraint to better performance right constraint to better performance right now could be these embedding pictures so now could be these embedding pictures so now could be these embedding pictures so let's make them bigger",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 693,
      "text": "okay so let's let's make them bigger",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 694,
      "text": "okay so let's let's make them bigger",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 695,
      "text": "okay so let's scroll up here scroll up here scroll up here and now we don't have two dimensional and now we don't have two dimensional and now we don't have two dimensional embeddings we are going to have embeddings we are going to have embeddings we are going to have say 10 dimensional embeddings for each say 10 dimensional embeddings for each say 10 dimensional embeddings for each word word then then this layer will receive 3 times 10 so 30 this layer will receive 3 times 10 so 30 this layer will receive 3 times 10 so 30 inputs inputs inputs will go into will go into will go into the hidden layer the hidden layer the hidden layer let's also make the hidden layer a bit let's also make the hidden layer a bit let's also make the hidden layer a bit smaller so instead of 300 let's just do smaller so instead of 300 let's just do smaller so instead of 300 let's just do 200 neurons in that hidden layer 200 neurons in that hidden layer 200 neurons in that hidden layer so now the total number of elements will so now the total number of elements will so now the total number of elements will be slightly bigger at 11 000 be slightly bigger at 11 000 be slightly bigger at 11 000 and then here we have to be a bit",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 696,
      "text": "and then here we have to be a bit",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 697,
      "text": "and then here we have to be a bit careful",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 698,
      "text": "because um careful because um careful",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 699,
      "text": "because um okay the learning rate we set to 0.1 okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 700,
      "text": "the learning rate we set to 0.1 okay the learning rate we set to 0.1 here we are hardcoded in six and here we are hardcoded in six and here we are hardcoded in six and obviously if you're working in obviously if you're working in obviously if you're working in production you don't wanna be production you don't wanna be production you don't wanna be hard-coding magic numbers but instead of hard-coding magic numbers but instead of hard-coding magic numbers but instead of six this should now be thirty six this should now be thirty six this should now be thirty",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 701,
      "text": "um um and let's run for fifty thousand and let's run for fifty thousand and let's run for fifty thousand iterations and let me split out the iterations and let me split out the iterations and let me split out the initialization here outside initialization here outside initialization here outside so that when we run this cell multiple so that when we run this cell multiple so that when we run this cell multiple times it's not going to wipe out times it's not going to wipe out times it's not going to wipe out our loss in addition to that in addition to that here here let's instead of logging lost.item let's let's instead of logging lost.item let's let's instead of logging lost.item let's actually actually actually log the log the log the let's let's let's do log 10 do log 10 do log 10",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 702,
      "text": "i believe that's a function of the loss i believe that's a function of the loss i believe that's a function of the loss and i'll show you why in a second let's and i'll show you why in a second let's and i'll show you why in a second let's optimize this",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 703,
      "text": "basically i'd like to plot the log loss basically i'd like to plot the log loss instead of the loss because when you instead of the loss because when you instead of the loss because when you plot the loss many times it can have plot the loss many times it can have plot the loss many times it can have this hockey stick appearance and log this hockey stick appearance and log this hockey stick appearance and log squashes it in squashes it in squashes it in uh so it just kind of like looks nicer uh so it just kind of like looks nicer uh so it just kind of like looks nicer so the x-axis is step",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 704,
      "text": "i so the x-axis is step",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 705,
      "text": "i so the x-axis is step i and the y-axis will be the loss i",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 706,
      "text": "and then here this is 30.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 707,
      "text": "and then here this is 30.",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 708,
      "text": "ideally we wouldn't be hard-coding these",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 709,
      "text": "okay so let's look at the loss",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 710,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 711,
      "text": "so let's look at the loss",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 712,
      "text": "okay it's again very thick because the okay it's again very thick because the okay it's again very thick because the mini batch size is very small",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 713,
      "text": "but the mini batch size is very small but the mini batch size is very small but the total loss over the training set is 2.3 total loss over the training set is 2.3 total loss over the training set is 2.3 and the the tests or the def set is 2.38 and the the tests or the def set is 2.38 and the the tests or the def set is 2.38 as well as well as",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 714,
      "text": "well so so far so good uh let's try to now so so far so good uh let's try to now so so far so good uh let's try to now decrease the learning rate decrease the learning rate decrease the learning rate by a factor of 10 we'd hope that we would be able to beat we'd hope that we would be able to beat uh 2.32 but again we're just kind of like doing",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 715,
      "text": "but again we're just kind of like doing this very haphazardly",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 716,
      "text": "so i don't this very haphazardly",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 717,
      "text": "so i don't this very haphazardly",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 718,
      "text": "so i don't actually have confidence that our actually have confidence that our actually have confidence that our learning rate is set very well that our learning rate is set very well that our learning rate is set very well that our learning rate decay which we just do learning rate decay which we just do learning rate decay which we just do at random is set very well at random is set very well at random is set very well and um so the optimization here is kind and um so the optimization here is kind and um so the optimization here is kind of suspect to be honest and this is not of suspect to be honest and this is not of suspect to be honest and this is not how you would do it typically in how you would do it typically in how you would do it typically in production in production you would production in production you would production in production you would create parameters or hyper parameters create parameters or hyper parameters create parameters or hyper parameters out of all these settings and then you out of all these settings and then you out of all these settings",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 719,
      "text": "and then you would run lots of experiments and see would run lots of experiments and see would run lots of experiments and see whichever ones are working well for you whichever ones are working well for you whichever ones are working well for you",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 720,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 721,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 722,
      "text": "so we have 2.17 now and 2.2",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 723,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 724,
      "text": "so you so we have 2.17 now and 2.2",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 725,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 726,
      "text": "so you so we have 2.17 now and 2.2",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 727,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 728,
      "text": "so you see how the training and the validation see how the training and the validation see how the training and the validation performance are starting to slightly performance are starting to slightly performance are starting to slightly slowly depart slowly depart slowly depart",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 729,
      "text": "so maybe we're getting the sense that so maybe we're getting the sense that so maybe we're getting the sense that the neural net the neural net is getting good enough or is getting good enough or is getting good enough or that number of parameters is large that number of parameters is large that number of parameters is large enough that we are slowly starting to enough that we are slowly starting to enough that we are slowly starting to overfit overfit overfit let's maybe run one more iteration of let's maybe run one more iteration of let's maybe run one more iteration of this this and see where we get",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 730,
      "text": "but yeah basically you would be running",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 731,
      "text": "but",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 732,
      "text": "yeah basically you would be running lots of experiments",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 733,
      "text": "and then you are lots of experiments and then you are lots of experiments and then you are slowly scrutinizing whichever ones give slowly scrutinizing whichever ones give slowly scrutinizing whichever ones give you the best depth performance",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 734,
      "text": "and then you the best depth performance and then you the best depth performance",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 735,
      "text": "and then once you find all the once you find all the once you find all the hyper parameters that make your dev hyper parameters that make your dev hyper parameters that make your dev performance good you take that model and performance good you take that model and performance good you take that model and you evaluate the test set performance a you evaluate the test set performance a you evaluate the test set performance a single time and that's the number that single time",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 736,
      "text": "and that's the number that single time",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 737,
      "text": "and that's the number that you report in your paper or wherever you report in your paper or wherever you report in your paper or wherever else you want to talk about and brag else you want to talk about and brag else you want to talk about and brag about your model",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 738,
      "text": "so let's then rerun the plot and rerun so let's then rerun the plot and rerun the train and death and because we're getting lower loss now and because we're getting lower loss now it is the case that the embedding size it is the case that the embedding size it is the case that the embedding size of these was holding us back very likely",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 739,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 740,
      "text": "so 2.162.19 is what we're roughly okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 741,
      "text": "so 2.162.19 is what we're roughly getting getting getting so there's many ways to go from many so there's many ways to go from many so there's many ways to go from many ways to go from here we can continue ways to go from here we can continue ways to go from here we can continue tuning the optimization tuning the optimization tuning the optimization we can continue for example playing with we can continue for example playing with we can continue for example playing with the sizes of the neural net or we can the sizes of the neural net or we can the sizes of the neural net or we can increase the number of uh increase the number of uh increase the number of uh words or characters in our case that we words or characters in our case that we words or characters in our case that we are taking as an input so instead of are taking as an input so instead of are taking as an input so instead of just three characters we could be taking just three characters we could be taking just three characters we could be taking more characters as an input and that more characters as an input and that more characters as an input and that could further improve the loss could further improve the loss could further improve the loss",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 742,
      "text": "okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 743,
      "text": "so i changed the code slightly",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 744,
      "text": "so okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 745,
      "text": "so i changed the code slightly",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 746,
      "text": "so okay",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 747,
      "text": "so i changed the code slightly",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 748,
      "text": "so we have here 200 000 steps of the we have here 200 000 steps of the we have here 200 000 steps of the optimization and in the first 100 000 optimization and in the first 100 000 optimization and in the first 100 000 we're using a learning rate of 0.1 and we're using a learning rate of 0.1 and we're using a learning rate of 0.1 and then in the next 100 000 we're using a then in the next 100 000 we're using a then in the next 100 000 we're using a learning rate of 0.01 learning rate of 0.01 learning rate of 0.01 this is the loss that i achieve this is the loss that i achieve this is the loss that i achieve and these are the performance on the and these are the performance on the and these are the performance on the training and validation loss training and validation loss training and validation loss and in particular the best validation and in particular the best validation and in particular the best validation loss i've been able to obtain in the loss i've been able to obtain in the loss i've been able to obtain in the last 30 minutes or so is 2.17 last 30 minutes or so is 2.17 last 30 minutes",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 749,
      "text": "or so is 2.17 so now i invite you to beat this number",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 750,
      "text": "so now i invite you to beat this number",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 751,
      "text": "so now i invite you to beat this number and you have quite a few knobs available and you have quite a few knobs available and you have quite a few knobs available to you to i think surpass this number to you to i think surpass this number to you to i think surpass this number",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 752,
      "text": "so number one you can of course change so number one you can of course change so number one you can of course change the number of neurons in the hidden the number of neurons in the hidden the number of neurons in the hidden layer of this model you can change the layer of this model you can change the layer of this model you can change the dimensionality of the embedding dimensionality of the embedding dimensionality of the embedding lookup table lookup table lookup table you can change the number of characters you can change the number of characters you can change the number of characters that are feeding in as an input that are feeding in as an input that are feeding in as an input as the context into this model as the context into this model as the context into this model and then of course you can change the and then of course you can change the and then of course you can change the details of the optimization how long are details of the optimization how long are details of the optimization how long are we running what is the learning rate how we running what is the learning rate how we running what is the learning rate how does it change over time does it change over time does it change over time how does it decay how does it decay how does it decay you can change the batch size and you you can change the batch size and you you can change the batch size and you may be able to actually achieve a much may be able to actually achieve a much may be able to actually achieve a much better convergence speed better convergence speed better convergence speed in terms of in terms of in terms of how many seconds or minutes it takes to how many seconds or minutes it takes to how many seconds or minutes it takes to train the model and get train the model and get train the model and get your result in terms of really good your result in terms of really good your result in terms of really good loss loss loss and then of course i actually invite you and then of course i actually invite you and then of course i actually invite you to read this paper it is 19 pages but at to read this paper it is 19 pages but at to read this paper it is 19 pages but at this point you should actually be able this point you should actually be able this point you should actually be able to read a good chunk of this paper and to read a good chunk of this paper and to read a good chunk of this paper and understand understand understand pretty good chunks of it pretty good chunks of it pretty good chunks of it and this paper also has quite a few and this paper also has quite a few and this paper also has quite a few ideas for improvements that you can play ideas for improvements that you can play ideas for improvements that you can play with with with so all of those are not available to you so all of those are not available to you so all of those are not available to you and you should be able to beat this and you should be able to beat this and you should be able to beat this number i'm leaving that as an exercise number i'm leaving that as an exercise number i'm leaving that as an exercise to the reader and that's it for now and to the reader and that's it for now and to the reader and that's it for now",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 753,
      "text": "and i'll see you next time before we wrap up i also wanted to show before we wrap up i also wanted to show how you would sample from the model how you would sample from the model how you would sample from the model so we're going to generate 20 samples so we're going to generate 20 samples so we're going to generate 20 samples at first we begin with all dots so at first we begin with all dots so at first we begin with all dots so that's the context that's the context that's the context",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 754,
      "text": "and then until we generate and then until we generate and then until we generate the zeroth character again the zeroth character again the zeroth character again we're going to embed the current context we're going to embed the current context we're going to embed the current context using the embedding table c now usually using the embedding table c now usually using the embedding table c now usually uh here the first dimension was the size uh here the first dimension was the size uh here the first dimension was the size of the training set",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 755,
      "text": "but here we're only of the training set",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 756,
      "text": "but here we're only of the training set",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 757,
      "text": "but here we're only working with a single example that we're working with a single example that we're working with a single example that we're generating so this is just the mission generating so this is just the mission generating so this is just the mission one just for simplicity and so this embedding then gets",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 758,
      "text": "and so this embedding then gets projected into the end state you get the projected into the end state you get the projected into the end state you get the logits logits logits now we calculate the probabilities for now we calculate the probabilities for now we calculate the probabilities for that you can use f.softmax that you can use f.softmax that you can use f.softmax of logits and that just basically of logits and that just basically of logits and that just basically exponentiates the logits and makes them exponentiates the logits and makes them exponentiates the logits and makes them sum to one and similar to cross entropy sum to one and similar to cross entropy sum to one and similar to cross entropy it is careful that there's no overflows it is careful that there's no overflows it is careful that there's no overflows once we have the probabilities we sample once we have the probabilities we sample once we have the probabilities we sample from them using torture multinomial to from them using torture multinomial to from them using torture multinomial to get our next index and then we shift the get our next index",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 759,
      "text": "and then we shift the get our next index",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 760,
      "text": "and then we shift the context window to append the index and context window to append the index and context window to append the index and record it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 761,
      "text": "and then we can just record it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 762,
      "text": "and then we can just record it",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 763,
      "text": "and then we can just decode all the integers to strings decode all the integers to strings decode all the integers to strings and print them out and print them out and print them out and so these are some example samples and so these are some example samples",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 764,
      "text": "and so these are some example samples and you can see that the model now works and you can see that the model now works and you can see that the model now works much better so the words here are much much better so the words here are much much better so the words here are much more word like or name like so we have more word like or name like so we have more word like or name like so we have things like ham things like ham things like ham joes you know it's starting to sound a little",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 765,
      "text": "you know it's starting to sound a little bit more name-like so we're definitely bit more name-like",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 766,
      "text": "so we're definitely bit more name-like",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 767,
      "text": "so we're definitely making progress but we can still improve making progress but we can still improve making progress but we can still improve on this model quite a lot on this model quite a lot on this model quite a lot",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 768,
      "text": "okay sorry there's some bonus content",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 769,
      "text": "i okay sorry there's some bonus content",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 770,
      "text": "i okay sorry there's some bonus content i wanted to mention that i want to make wanted to mention that i want to make wanted to mention that i want to make these notebooks more accessible",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 771,
      "text": "and so i these notebooks more accessible",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 772,
      "text": "and so i these notebooks more accessible",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 773,
      "text": "and so i don't want you to have to like install don't want you to have to like install don't want you to have to like install jupyter notebooks and torch and jupyter notebooks and torch and jupyter notebooks and torch and everything else so i will be sharing a everything else so i will be sharing a everything else",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 774,
      "text": "so i will be sharing a link to a google colab link to a google colab link to a google colab and google collab will look like a and google collab will look like a and google collab will look like a notebook in your browser and you can notebook in your browser and you can notebook in your browser and you can just go to the url and you'll be able to just go to the url and you'll be able to just go to the url and you'll be able to execute all of the code that you saw in execute all of the code that you saw in execute all of the code that you saw in the google collab and so this is me the google collab and so this is me the google collab and so this is me executing the code in this lecture and i executing the code in this lecture and i executing the code in this lecture",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 775,
      "text": "and i shortened it a little bit but basically shortened it a little bit but basically shortened it a little bit",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 776,
      "text": "but basically you're able to train the exact same you're able to train the exact same you're able to train the exact same network and then plot and sample from network and then plot and sample from network and then plot and sample from the model and everything is ready for the model and everything is ready for the model and everything is ready for you to like tinker with the numbers you to like tinker with the numbers you to like tinker with the numbers right there in your browser",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 777,
      "text": "no right there in your browser",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 778,
      "text": "no right there in your browser",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 779,
      "text": "no installation necessary installation necessary installation necessary",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 780,
      "text": "so i just wanted to point that out",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 781,
      "text": "and so i just wanted to point that out",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    },
    {
      "id": 782,
      "text": "and so i just wanted to point that out and the link to this will be in the video the link to this will be in the video the link to this will be in the video description",
      "start_time": "00:00:01.910",
      "end_time": "01:15:40.800"
    }
  ]
}