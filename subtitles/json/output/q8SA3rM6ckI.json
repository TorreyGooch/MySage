{
  "video_id": "q8SA3rM6ckI",
  "sentences": [
    {
      "id": 1,
      "text": "hi everyone so today we are once again hi everyone so today we are once again continuing our implementation of make continuing our implementation of make continuing our implementation of make more now so far we've come up to here more now so far we've come up to here more now so far we've come up to here montalia perceptrons and our neural net montalia perceptrons and our neural net montalia perceptrons and our neural net looked like this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 2,
      "text": "and we were looked like this and we were looked like this and we were implementing this over the last few implementing this over the last few implementing this over the last few lectures lectures lectures now I'm sure everyone is very excited to now I'm sure everyone is very excited to now I'm sure everyone is very excited to go into recurring neural networks and go into recurring neural networks and go into recurring neural networks and all of their variants and how they work all of their variants and how they work all of their variants and how they work and the diagrams look cool",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 3,
      "text": "and it's very and the diagrams look cool and it's very and the diagrams look cool",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 4,
      "text": "and it's very exciting and interesting",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 5,
      "text": "and we're going exciting and interesting",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 6,
      "text": "and we're going exciting and interesting",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 7,
      "text": "and we're going to get a better result but unfortunately to get a better result but unfortunately to get a better result",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 8,
      "text": "but unfortunately I think we have to remain here for one I think we have to remain here for one I think we have to remain here for one more lecture and the reason for that is more lecture and the reason for that is more lecture and the reason for that is we've already trained this multilio we've already trained this multilio we've already trained",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 9,
      "text": "this multilio perceptron right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 10,
      "text": "and we are getting perceptron right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 11,
      "text": "and we are getting perceptron right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 12,
      "text": "and we are getting pretty good loss",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 13,
      "text": "and I think we have a pretty good loss",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 14,
      "text": "and I think we have a pretty good loss",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 15,
      "text": "and I think we have a pretty decent understanding of the pretty decent understanding of the pretty decent understanding of the architecture and how it works but the architecture and how it works but the architecture and how it works but the line of code here that I take an issue line of code here that I take an issue line of code here that I take an issue with is here lost up backward that is we with is here lost up backward that is we with is here lost up backward that is we are taking a pytorch auto grad and using are taking a pytorch auto grad and using are taking a pytorch auto grad and using it to calculate all of our gradients it to calculate all of our gradients it to calculate all of our gradients along the way",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 16,
      "text": "and I would like to remove along the way",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 17,
      "text": "and I would like to remove along the way",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 18,
      "text": "and I would like to remove the use of lost at backward",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 19,
      "text": "and I would the use of lost at backward",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 20,
      "text": "and I would the use of lost at backward",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 21,
      "text": "and I would like us to write our backward pass like us to write our backward pass like us to write our backward pass manually on the level of tensors",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 22,
      "text": "and I manually on the level of tensors and I manually on the level of tensors and I think that this is a very useful think that this is a very useful think that this is a very useful exercise for the following reasons exercise for the following reasons exercise for the following reasons I actually have an entire blog post on I actually have an entire blog post on I actually have an entire blog post on this topic",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 23,
      "text": "but I'd like to call back this topic",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 24,
      "text": "but I'd like to call back this topic",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 25,
      "text": "but I'd like to call back propagation a leaky abstraction propagation a leaky abstraction propagation a leaky abstraction and what I mean by that is back and what I mean by that is back and what I mean by that is back propagation does doesn't just make your propagation does doesn't just make your propagation does doesn't just make your neural networks just work magically it's neural networks just work magically it's neural networks just work magically it's not the case they can just Stack Up not the case they can just Stack Up not the case they can just Stack Up arbitrary Lego blocks of differentiable arbitrary Lego blocks of differentiable arbitrary Lego blocks of differentiable functions and just cross your fingers functions and just cross your fingers functions and just cross your fingers and back propagate and everything is and back propagate and everything is and back propagate and everything is great things don't just work great things don't just work great things don't just work automatically it is a leaky abstraction automatically it is a leaky abstraction automatically it is a leaky abstraction in the sense that you can shoot yourself in the sense that you can shoot yourself in the sense that you can shoot yourself in the foot if you do not understanding in the foot if you do not understanding in the foot if you do not understanding its internals it will magically not work its internals it will magically not work its internals it will magically not work or not work optimally and you will need or not work optimally and you will need or not work optimally and you will need to understand how it works under the to understand how it works under the to understand how it works under the hood if you're hoping to debug it and if hood if you're hoping to debug it and if hood if you're hoping to debug it and if you are hoping to address it in your you are hoping to address it in your you are hoping to address it in your neural nut neural nut neural",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 26,
      "text": "nut um so this blog post here from a while um so this blog post here from a while um so this blog post here from a while ago goes into some of those examples so ago goes into some of those examples so ago goes into some of those examples so for example we've already covered them for example we've already covered them for example we've already covered them some of them already for example the some of them already for example the some of them already for example the flat tails of these functions and how flat tails of these functions and how flat tails of these functions and how you do not want to saturate them too you do not want to saturate them too you do not want to saturate them too much because your gradients will die the much because your gradients will die the much because your gradients will die the case of dead neurons which I've already case of dead neurons which I've already case of dead neurons which I've already covered as well covered as well covered as well the case of exploding or Vanishing the case of exploding or Vanishing the case of exploding or Vanishing gradients in the case of repair neural gradients in the case of repair neural gradients in the case of repair neural networks which we are about to cover networks which we are about to cover networks which we are about to cover and then also you will often come across and then also you will often come across and then also you will often come across some examples in the wild some examples in the wild some examples in the wild this is a snippet that I found uh in a this is a snippet that I found uh in a this is a snippet that I found uh in a random code base on the internet where random code base on the internet where random code base on the internet where they actually have like a very subtle they actually have like a very subtle they actually have like a very subtle but pretty major bug in their but pretty major bug in their but pretty major bug in their implementation and the bug points at the implementation and the bug points at the implementation and the bug points at the fact that the author of this code does fact that the author of this code does fact that the author of this code does not actually understand by propagation not actually understand by propagation not actually understand by propagation so they're trying to do here is they're so they're trying to do here is they're so they're trying to do here is they're trying to clip the loss at a certain trying to clip the loss at a certain trying to clip the loss at a certain maximum value",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 27,
      "text": "but actually what they're maximum value",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 28,
      "text": "but actually what they're maximum value",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 29,
      "text": "but actually what they're trying to do is they're trying to trying to do is they're trying to trying to do is they're trying to collect the gradients to have a maximum collect the gradients to have a maximum collect the gradients to have a maximum value instead of trying to clip the loss value instead of trying to clip the loss value instead of trying to clip the loss at a maximum value and at a maximum value and at a maximum value and um indirectly they're basically causing um indirectly they're basically causing um indirectly they're basically causing some of the outliers to be actually some of the outliers to be actually some of the outliers to be actually ignored because when you clip a loss of ignored because when you clip a loss of ignored because when you clip a loss of an outlier you are setting its gradient an outlier you are setting its gradient an outlier you are setting its gradient to zero and so have a look through this to zero and so have a look through this to zero and so have a look through this and read through it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 30,
      "text": "but there's and read through it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 31,
      "text": "but there's and read through it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 32,
      "text": "but there's basically a bunch of subtle issues that basically a bunch of subtle issues that basically a bunch of subtle issues that you're going to avoid if you actually you're going to avoid if you actually you're going to avoid if you actually know what you're doing and that's why I know what you're doing",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 33,
      "text": "and that's why I know what you're doing",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 34,
      "text": "and that's why I don't think it's the case that because don't think it's the case that because don't think it's the case that because pytorch or other Frameworks offer pytorch or other Frameworks offer pytorch or other Frameworks offer autograd",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 35,
      "text": "it is okay for us to ignore how autograd it is okay for us to ignore how autograd it is okay for us to ignore how it works it works it works now we've actually already covered now we've actually already covered now we've actually already covered covered autograd and we wrote micrograd covered autograd",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 36,
      "text": "and we wrote micrograd covered autograd",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 37,
      "text": "and we wrote micrograd but micrograd was an autograd engine but micrograd was an autograd engine but micrograd was an autograd engine only on the level of individual scalars only on the level of individual scalars only on the level of individual scalars so the atoms were single individual so the atoms were single individual so the atoms were single individual numbers and uh you know I don't think numbers and uh you know I don't think numbers and uh you know I don't think it's enough",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 38,
      "text": "and I'd like us to basically it's enough",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 39,
      "text": "and I'd like us to basically it's enough",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 40,
      "text": "and I'd like us to basically think about back propagation on level of think about back propagation on level of think about back propagation on level of tensors as well and so in a summary I tensors as well and so in a summary I tensors as well and so in a summary I think it's a good exercise I think it is think it's a good exercise I think it is think it's a good exercise I think it is very very valuable you're going to very very valuable you're going to very very valuable you're going to become better at debugging neural become better at debugging neural become better at debugging neural networks and making sure that you networks and making sure that you networks and making sure that you understand what you're doing it is going understand what you're doing it is going understand what you're doing it is going to make everything fully explicit so to make everything fully explicit so to make everything fully explicit so you're not going to be nervous about you're not going to be nervous about you're not going to be nervous about what is hidden away from you and what is hidden away from you and what is hidden away from you and basically in general we're going to basically in general we're going to basically in general we're going to emerge stronger and so let's get into it emerge stronger and so let's get into it emerge stronger and so let's get into it a bit of a fun historical note here is a bit of a fun historical note here is a bit of a fun historical note here is that today writing your backward pass by that today writing your backward pass by that today writing your backward pass by hand and manually is not recommended and hand and manually is not recommended and hand and manually is not recommended and no one does it except for the purposes no one does it except for the purposes no one does it except for the purposes of exercise but about 10 years ago in of exercise but about 10 years ago in of exercise but about 10 years ago in deep learning this was fairly standard deep learning this was fairly standard deep learning this was fairly standard and in fact pervasive so at the time and in fact pervasive so at the time and in fact pervasive",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 41,
      "text": "so at the time everyone used to write their own everyone used to write their own everyone used to write their own backward pass by hand manually including backward pass by hand manually including backward pass by hand manually including myself",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 42,
      "text": "and it's just what you would do myself",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 43,
      "text": "and it's just what you would do myself",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 44,
      "text": "and it's just what you would do so we used to ride backward pass by hand so we used to ride backward pass by hand so we used to ride backward pass by hand and now everyone just calls lost that and now everyone just calls lost that and now everyone just calls lost that backward uh we've lost something I want backward uh we've lost something I want backward",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 45,
      "text": "uh we've lost something I want to give you a few examples of this so to give you a few examples of this so to give you a few examples of this so here's a 2006 paper from Jeff Hinton and here's a 2006 paper from Jeff Hinton and here's a 2006 paper from Jeff Hinton and Russell selectinov in science that was Russell selectinov in science that was Russell selectinov in science that was influential at the time and this was influential at the time and this was influential at the time and this was training some architectures called training some architectures called training some architectures called restricted bolstery machines and restricted bolstery machines and restricted bolstery machines and basically it's an auto encoder trained basically it's an auto encoder trained basically it's an auto encoder trained here and this is from roughly 2010 I had here and this is from roughly 2010 I had here and this is from roughly 2010 I had a library for training researchable a library for training researchable a library for training researchable machines and this was at the time machines and this was at the time machines and this was at the time written in Matlab so python was not used written in Matlab so python was not used written in Matlab so python was not used for deep learning pervasively it was all for deep learning pervasively it was all for deep learning pervasively it was all Matlab and Matlab was this a scientific Matlab and Matlab was this a scientific Matlab and Matlab was this a scientific Computing package that everyone would Computing package that everyone would Computing package that everyone would use so we would write Matlab which is use so we would write Matlab which is use so we would write Matlab which is barely a programming language as well barely a programming language as well barely a programming language as well but I've had a very convenient tensor but I've had a very convenient tensor but I've had a very convenient tensor class and was this a Computing class and was this a Computing class and was this a Computing environment and you would run here it environment",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 46,
      "text": "and you would run here it environment",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 47,
      "text": "and you would run here it would all run on a CPU of course",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 48,
      "text": "but you would all run on a CPU of course",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 49,
      "text": "but you would all run on a CPU of course",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 50,
      "text": "but you would have very nice plots to go with it would have very nice plots to go with it would have very nice plots to go with it and a built-in debugger and it was and a built-in debugger",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 51,
      "text": "and it was and a built-in debugger",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 52,
      "text": "and it was pretty nice now the code in this package pretty nice now the code in this package pretty nice now the code in this package in 2010 that I wrote for fitting in 2010 that I wrote for fitting in 2010 that I wrote for fitting research multiple machines to a large research multiple machines to a large research multiple machines to a large extent is recognizable",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 53,
      "text": "but I wanted to extent is recognizable",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 54,
      "text": "but I wanted to extent is recognizable",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 55,
      "text": "but I wanted to show you how you would",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 56,
      "text": "well I'm creating show you how you would",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 57,
      "text": "well I'm creating show you how you would",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 58,
      "text": "well I'm creating the data in the XY batches",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 59,
      "text": "I'm the data in the XY batches",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 60,
      "text": "I'm the data in the XY batches",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 61,
      "text": "I'm initializing the neural nut",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 62,
      "text": "so it's got initializing the neural nut",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 63,
      "text": "so it's got initializing the neural nut",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 64,
      "text": "so it's got weights and biases just like we're used weights and biases just like we're used weights and biases just like we're used to and then this is the training Loop to and then this is the training Loop to and then this is the training Loop where we actually do the forward pass where we actually do the forward pass where we actually do the forward pass and",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 65,
      "text": "then here at this time they didn't and then here at this time they didn't and then here at this time they didn't even necessarily use back propagation to even necessarily use back propagation to even necessarily use back propagation to train neural networks",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 66,
      "text": "so this in train neural networks so this in train neural networks so this in particular implements contrastive particular implements contrastive particular implements contrastive Divergence which estimates a gradient Divergence which estimates a gradient Divergence which estimates a gradient and then here we take that gradient and and then here we take that gradient and and then here we take that gradient and use it for a parameter update along the use it for a parameter update along the use it for a parameter update along the lines that we're used to lines that we're used to lines that we're used to um",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 67,
      "text": "yeah here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 68,
      "text": "um yeah",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 69,
      "text": "here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 70,
      "text": "um yeah here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 71,
      "text": "but you can see that basically people but you can see that basically people but you can see that basically people are meddling with these gradients uh are meddling with these gradients uh are meddling with these gradients uh directly and inline and themselves uh it directly and inline and themselves uh it directly and inline and themselves uh it wasn't that common to use an auto grad wasn't that common to use an auto grad wasn't that common to use an auto grad engine here's one more example from a engine here's one more example from a engine here's one more example from a paper of mine from 2014 paper of mine from 2014 paper of mine from 2014 um called the fragmented embeddings um called the fragmented embeddings um called the fragmented embeddings and here what I was doing is I was",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 72,
      "text": "and here what I was doing is I was",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 73,
      "text": "and here what I was doing is I was aligning images and text aligning images and text aligning images and text um",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 74,
      "text": "and so it's kind of like a clip if um",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 75,
      "text": "and so it's kind of like a clip if um",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 76,
      "text": "and so it's kind of like a clip if you're familiar with it but instead of you're familiar with it but instead of you're familiar with it but instead of working on the level of entire images working on the level of entire images working on the level of entire images and entire sentences it was working on and entire sentences it was working on and entire sentences it was working on the level of individual objects and the level of individual objects and the level of individual objects and little pieces of sentences and I was little pieces of sentences",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 77,
      "text": "and I was little pieces of sentences and I was embedding them and then calculating very embedding them and then calculating very embedding them and then calculating very much like a clip-like loss",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 78,
      "text": "and I dig up much like a clip-like loss",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 79,
      "text": "and I dig up much like a clip-like loss",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 80,
      "text": "and I dig up the code from 2014 of how I implemented the code from 2014 of how I implemented the code from 2014 of how I implemented this and it was already in numpy and this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 81,
      "text": "and it was already in numpy and this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 82,
      "text": "and it was already in numpy and python python python",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 83,
      "text": "and here I'm planting the cost function",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 84,
      "text": "and here I'm planting the cost function",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 85,
      "text": "and here I'm planting the cost function and it was standard to implement not",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 86,
      "text": "and it was standard to implement not",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 87,
      "text": "and it was standard to implement not just the cost but also the backward pass just the cost but also the backward pass just the cost but also the backward pass manually",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 88,
      "text": "so here I'm calculating the manually",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 89,
      "text": "so here I'm calculating the manually",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 90,
      "text": "so here I'm calculating the image embeddings sentence embeddings the image embeddings sentence embeddings the image embeddings sentence embeddings the loss function I calculate this course loss function I calculate this course loss function I calculate this course this is the loss function and then once this is the loss function and then once this is the loss function and then once I have the loss function I do the I have the loss function I do the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 91,
      "text": "I have the loss function I do the backward pass right here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 92,
      "text": "so I backward backward pass right here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 93,
      "text": "so I backward backward pass right here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 94,
      "text": "so I backward through the loss function and through through the loss function and through through the loss function and through the neural nut",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 95,
      "text": "and I append the neural nut",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 96,
      "text": "and I append the neural nut",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 97,
      "text": "and I append regularization so everything was done by regularization so everything was done by regularization so everything was done by hand manually",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 98,
      "text": "and you were just right hand manually",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 99,
      "text": "and you were just right hand manually",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 100,
      "text": "and you were just right out the backward pass",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 101,
      "text": "and then you would out the backward pass",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 102,
      "text": "and then you would out the backward pass",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 103,
      "text": "and then you would use a gradient Checker to make sure that use a gradient Checker to make sure that use a gradient Checker to make sure that your numerical estimate of the gradient your numerical estimate of the gradient your numerical estimate of the gradient agrees with the one you calculated agrees with the one you calculated agrees with the one you calculated during back propagation",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 104,
      "text": "so this was very during back propagation so this was very during back propagation",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 105,
      "text": "so this was very standard for a long time but today of standard for a long time",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 106,
      "text": "but today of standard for a long time",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 107,
      "text": "but today of course it is standard to use an auto course it is standard to use an auto course it is standard to use an auto grad engine grad engine grad engine",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 108,
      "text": "um but it was definitely useful",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 109,
      "text": "and I um",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 110,
      "text": "but it was definitely useful",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 111,
      "text": "and I um",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 112,
      "text": "but it was definitely useful",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 113,
      "text": "and I think people sort of understood how think people sort of understood how think people sort of understood how these neural networks work on a very these neural networks work on a very these neural networks work on a very intuitive level",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 114,
      "text": "and so I think it's a intuitive level",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 115,
      "text": "and so I think it's a intuitive level",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 116,
      "text": "and so I think it's a good exercise again",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 117,
      "text": "and this is where we good exercise again",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 118,
      "text": "and this is where we good exercise again",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 119,
      "text": "and this is where we want to be",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 120,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 121,
      "text": "so just as a reminder want to be okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 122,
      "text": "so just as a reminder want to be okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 123,
      "text": "so just as a reminder from our previous lecture this is The from our previous lecture this is The from our previous lecture this is The jupyter Notebook that we implemented at jupyter Notebook that we implemented at jupyter Notebook that we implemented at the time and the time and the time",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 124,
      "text": "and we're going to keep everything the same we're going to keep everything the same we're going to keep everything the same",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 125,
      "text": "so we're still going to have a two layer",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 126,
      "text": "so we're still going to have a two layer",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 127,
      "text": "so we're still going to have a two layer multiplayer perceptron with a batch multiplayer perceptron with a batch multiplayer perceptron with a batch normalization layer",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 128,
      "text": "so the forward pass normalization layer",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 129,
      "text": "so the forward pass normalization layer so the forward pass will be basically identical to this will be basically identical to this will be basically identical to this lecture",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 130,
      "text": "but here we're going to get rid lecture",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 131,
      "text": "but here we're going to get rid lecture",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 132,
      "text": "but here we're going to get rid of lost and backward and instead we're of lost and backward and instead we're of lost and backward and instead we're going to write the backward pass going to write the backward pass going to write the backward pass manually manually manually now here's the starter code for this now here's the starter code for this now here's the starter code for this lecture we are becoming a back prop lecture we are becoming a back prop lecture we are becoming a back prop ninja in this notebook ninja in this notebook ninja in this notebook and the first few cells here are and the first few cells here are and the first few cells here are identical to what we are used to so we identical to what we are used to so we identical to what we are used to so we are doing some imports loading the data are doing some imports loading the data are doing some imports loading the data set and processing the data set none of set and processing the data set none of set and processing the data set none of this changed this changed this changed now here I'm introducing a utility now here I'm introducing a utility now here I'm introducing a utility function that we're going to use later function that we're going to use later function that we're going to use later to compare the gradients so in to compare the gradients so in to compare the gradients so in particular we are going to have the particular we are going to have the particular we are going to have the gradients that we estimate manually gradients that we estimate manually gradients that we estimate manually ourselves",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 133,
      "text": "and we're going to have ourselves and we're going to have ourselves",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 134,
      "text": "and we're going to have gradients that Pi torch calculates and gradients that Pi torch calculates and gradients that Pi torch calculates and we're going to be checking for we're going to be checking for we're going to be checking for correctness assuming of course that correctness assuming of course that correctness assuming of course that pytorch is correct pytorch is correct pytorch is correct",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 135,
      "text": "um then here we have the initialization um then here we have the initialization um then here we have the initialization that we are quite used to so we have our that we are quite used to so we have our that we are quite used to so we have our embedding table for the characters the embedding table for the characters the embedding table for the characters the first layer second layer and the batch first layer second layer and the batch first layer second layer and the batch normalization in between normalization in between normalization in between and here's where we create all the and here's where we create all the and here's where we create all the parameters now you will note that I parameters now you will note that I parameters now you will note that I changed the initialization a little bit changed the initialization a little bit changed the initialization a little bit uh to be small numbers so normally you uh to be small numbers so normally you uh to be small numbers so normally you would set the biases to be all zero here would set the biases to be all zero here would set the biases to be all zero here I am setting them to be small random I am setting them to be small random I am setting them to be small random numbers and I'm doing this because numbers and I'm doing this because numbers and I'm doing this because if your variables are initialized to if your variables are initialized to if your variables are initialized to exactly zero sometimes what can happen exactly zero sometimes what can happen exactly zero sometimes what can happen is that can mask an incorrect is that can mask an incorrect is that can mask an incorrect implementation of a gradient implementation of a gradient implementation of a gradient um because uh when everything is zero it um because uh when everything is zero it um because uh when everything is zero it sort of like simplifies and gives you a sort of like simplifies and gives you a sort of like simplifies and gives you a much simpler expression of the gradient much simpler expression of the gradient much simpler expression of the gradient than you would otherwise get and so by than you would otherwise get and so by than you would otherwise get and so by making it small numbers I'm trying to making it small numbers I'm trying to making it small numbers I'm trying to unmask those potential errors in these unmask those potential errors in these unmask those potential errors in these calculations calculations calculations you also notice that I'm using uh B1 in you also notice that I'm using uh B1 in you also notice that I'm using uh B1 in the first layer I'm using a bias despite the first layer I'm using a bias despite the first layer I'm using a bias despite batch normalization right afterwards batch normalization right afterwards batch normalization right afterwards",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 136,
      "text": "um so this would typically not be what um so this would typically not be what um so this would typically not be what you do because we talked about the fact you do because we talked about the fact you do because we talked about the fact that you don't need the bias",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 137,
      "text": "but I'm that you don't need the bias",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 138,
      "text": "but I'm that you don't need the bias",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 139,
      "text": "but I'm doing this here just for fun doing this here just for fun doing this here just for fun um because we're going to have a um because we're going to have a um because we're going to have a gradient with respect to it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 140,
      "text": "and we can gradient with respect to it and we can gradient with respect to it and we can check that we are still calculating it check that we are still calculating it check that we are still calculating it correctly even though this bias is correctly even though this bias is correctly even though this bias is asparious asparious asparious",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 141,
      "text": "so here I'm calculating a single batch",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 142,
      "text": "so here I'm calculating a single batch",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 143,
      "text": "so here I'm calculating a single batch",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 144,
      "text": "and then here I'm doing a forward pass and then here I'm doing a forward pass and then here I'm doing a forward pass now you'll notice that the forward pass now you'll notice that the forward pass now you'll notice that the forward pass is significantly expanded from what we is significantly expanded from what we is significantly expanded from what we are used to here the forward pass was are used to here the forward pass was are used to here the forward pass was just just just um here um here um here now the reason that the forward pass is now the reason that the forward pass is now the reason that the forward pass is longer is for two reasons number one longer is for two reasons number one longer is for two reasons number one here we just had an F dot cross entropy here we just had an F dot cross entropy here we just had an F dot cross entropy but here I am bringing back a explicit but here I am bringing back a explicit but here I am bringing back a explicit implementation of the loss function implementation of the loss function implementation of the loss function and number two and number two and number two I've broken up the implementation into I've broken up the implementation into I've broken up the implementation into manageable chunks so we have a lot a lot manageable chunks so we have a lot a lot manageable chunks",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 145,
      "text": "so we have a lot a lot more intermediate tensors along the way more intermediate tensors along the way more intermediate tensors along the way in the forward pass and that's because in the forward pass and that's because in the forward pass and that's because we are about to go backwards and we are about to go backwards and we are about to go backwards and calculate the gradients in this back calculate the gradients in this back calculate the gradients in this back propagation from the bottom to the top propagation from the bottom to the top propagation from the bottom to the top so we're going to go upwards and just so we're going to go upwards and just so we're going to go upwards and just like we have for example the lock props like we have for example the lock props like we have for example the lock props tensor in a forward pass in the backward tensor in a forward pass in the backward tensor in a forward pass in the backward pass we're going to have a d-lock probes pass we're going to have a d-lock probes pass we're going to have a d-lock probes which is going to store the derivative which is going to store the derivative which is going to store the derivative of the loss with respect to the lock of the loss with respect to the lock of the loss with respect to the lock props tensor",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 146,
      "text": "and so we're going to be props tensor",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 147,
      "text": "and so we're going to be props tensor",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 148,
      "text": "and so we're going to be prepending D to every one of these prepending D to every one of these prepending D to every one of these tensors and calculating it along the way tensors and calculating it along the way tensors and calculating it along the way of this back propagation of this back propagation of this back propagation so as an example we have a b and raw so as an example we have a b and raw so as an example we have a b and raw here we're going to be calculating a DB here we're going to be calculating a DB here we're going to be calculating a DB in raw so here I'm telling pytorch that in raw so here I'm telling pytorch that in raw so here I'm telling pytorch that we want to retain the grad of all these we want to retain the grad of all these we want to retain the grad of all these intermediate values because here in intermediate values because here in intermediate values because here in exercise one we're going to calculate exercise one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 149,
      "text": "we're going to calculate exercise one we're going to calculate the backward pass so we're going to the backward pass so we're going to the backward pass so we're going to calculate all these D values D variables calculate all these D values D variables calculate all these D values D variables and use the CNP function I've introduced and use the CNP function I've introduced and use the CNP function I've introduced above to check our correctness with above to check our correctness with above to check our correctness with respect to what pi torch is telling us respect to what pi torch is telling us respect to what pi torch is telling us this is going to be exercise one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 150,
      "text": "uh this is going to be exercise one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 151,
      "text": "uh this is going to be exercise one uh where we sort of back propagate through where we sort of back propagate through where we sort of back propagate through this entire graph this entire graph this entire graph now just to give you a very quick now just to give you a very quick now just to give you a very quick preview of what's going to happen in preview of what's going to happen in preview of what's going to happen in exercise two and below here we have exercise two and below here we have exercise two and below here we have fully broken up the loss and back fully broken up the loss and back fully broken up the loss and back propagated through it manually in all propagated through it manually in all propagated through it manually in all the little Atomic pieces that make it up the little Atomic pieces that make it up the little Atomic pieces that make it up",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 152,
      "text": "but here we're going to collapse the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 153,
      "text": "but here we're going to collapse the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 154,
      "text": "but here we're going to collapse the laws into a single cross-entropy call laws into a single cross-entropy call laws into a single cross-entropy call and instead we're going to analytically",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 155,
      "text": "and instead we're going to analytically",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 156,
      "text": "and instead we're going to analytically derive using math and paper and pencil derive using math and paper and pencil derive using math and paper and pencil the gradient of the loss with respect to the gradient of the loss with respect to the gradient of the loss with respect to the logits and instead of back the logits and instead of back the logits and instead of back propagating through all of its little propagating through all of its little propagating through all of its little chunks one at a time we're just going to chunks one at a time we're just going to chunks one at a time we're just going to analytically derive what that gradient analytically derive what that gradient analytically derive what that gradient is",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 157,
      "text": "and we're going to implement that is",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 158,
      "text": "and we're going to implement that is",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 159,
      "text": "and we're going to implement that which is much more efficient as we'll which is much more efficient as we'll which is much more efficient as we'll see in the in a bit see in the in a bit see in the in a bit",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 160,
      "text": "then we're going to do the exact same then we're going to do the exact same then we're going to do the exact same thing for patch normalization so instead thing for patch normalization so instead thing for patch normalization so instead of breaking up bass drum into all the of breaking up bass drum into all the of breaking up bass drum into all the old tiny components we're going to use old tiny components we're going to use old tiny components we're going to use uh pen and paper and Mathematics and uh pen and paper and Mathematics and uh pen and paper and Mathematics and calculus to derive the gradient through calculus to derive the gradient through calculus to derive the gradient through the bachelor Bachelor layer",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 161,
      "text": "so we're the bachelor Bachelor layer",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 162,
      "text": "so we're the bachelor Bachelor layer",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 163,
      "text": "so we're going to calculate the backward going to calculate the backward going to calculate the backward passthrough bathroom layer in a much passthrough bathroom layer in a much passthrough bathroom layer in a much more efficient expression instead of more efficient expression instead of more efficient expression instead of backward propagating through all of its backward propagating through all of its backward propagating through all of its little pieces independently little pieces independently little pieces independently so there's going to be exercise three",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 164,
      "text": "so there's going to be exercise three",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 165,
      "text": "so there's going to be exercise three and then in exercise four we're going to and then in exercise four we're going to and then in exercise four we're going to put it all together",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 166,
      "text": "and this is the full put it all together",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 167,
      "text": "and this is the full put it all together",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 168,
      "text": "and this is the full code of training this two layer MLP and code of training this two layer MLP and code of training this two layer MLP and we're going to basically insert our we're going to basically insert our we're going to basically insert our manual back prop",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 169,
      "text": "and we're going to take manual back prop and we're going to take manual back prop",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 170,
      "text": "and we're going to take out lost it backward",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 171,
      "text": "and you will out lost it backward",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 172,
      "text": "and you will out lost it backward",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 173,
      "text": "and you will basically see that you can get all the basically see that you can get all the basically see that you can get all the same results using fully your own code same results using fully your own code same results using fully your own code and the only thing we're using from and the only thing we're using from and the only thing we're using from pytorch is the torch.tensor to make the pytorch is the torch.tensor to make the pytorch is the torch.tensor to make the calculations efficient",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 174,
      "text": "but otherwise you calculations efficient",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 175,
      "text": "but otherwise you calculations efficient",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 176,
      "text": "but otherwise you will understand fully what it means to will understand fully what it means to will understand fully what it means to forward and backward and neural net and forward and backward and neural net and forward and backward and neural net and train it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 177,
      "text": "and I think that'll be awesome train it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 178,
      "text": "and I think that'll be awesome train it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 179,
      "text": "and I think that'll be awesome",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 180,
      "text": "so let's get to it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 181,
      "text": "so let's get to it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 182,
      "text": "so let's get to it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 183,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 184,
      "text": "so I read all the cells of this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 185,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 186,
      "text": "so I read all the cells of this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 187,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 188,
      "text": "so I read all the cells of this notebook all the way up to here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 189,
      "text": "and I'm notebook all the way up to here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 190,
      "text": "and I'm notebook all the way up to here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 191,
      "text": "and I'm going to erase this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 192,
      "text": "and I'm going to going to erase this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 193,
      "text": "and I'm going to going to erase this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 194,
      "text": "and I'm going to start implementing backward pass start implementing backward pass start implementing backward pass starting with d lock problems so we want starting with d lock problems",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 195,
      "text": "so we want starting with d lock problems",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 196,
      "text": "so we want to understand what should go here to to understand what should go here to to understand what should go here to calculate the gradient of the loss with calculate the gradient of the loss with calculate the gradient of the loss with respect to all the elements of the log respect to all the elements of the log respect to all the elements of the log props tensor props tensor props tensor",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 197,
      "text": "now I'm going to give away the answer now I'm going to give away the answer now I'm going to give away the answer here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 198,
      "text": "but I wanted to put a quick note here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 199,
      "text": "but I wanted to put a quick note here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 200,
      "text": "but I wanted to put a quick note here that I think would be most here that I think would be most here that I think would be most pedagogically useful for you is to pedagogically useful for you is to pedagogically useful for you is to actually go into the description of this actually go into the description of this actually go into the description of this video and find the link to this Jupiter video and find the link to this Jupiter video and find the link to this Jupiter notebook you can find it both on GitHub notebook you can find it both on GitHub notebook you can find it both on GitHub",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 201,
      "text": "but you can also find Google collab with but you can also find Google collab with but you can also find Google collab with it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 202,
      "text": "so you don't have to install anything it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 203,
      "text": "so you don't have to install anything it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 204,
      "text": "so you don't have to install anything you'll just go to a website on Google",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 205,
      "text": "you'll just go to a website on Google you'll just go to a website on Google collab and you can try to implement collab and you can try to implement collab and you can try to implement these derivatives or gradients yourself these derivatives or gradients yourself these derivatives or gradients yourself and then if you are not able to come to and then if you are not able to come to and then if you are not able to come to my video and see me do it and so work in my video and see me do it and so work in my video and see me do it and so work in Tandem and try it first yourself and Tandem and try it first yourself and Tandem and try it first yourself and then see me give away the answer and I then see me give away the answer and I then see me give away the answer",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 206,
      "text": "and I think that'll be most valuable to you think that'll be most valuable to you think that'll be most valuable to you and that's how I recommend you go",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 207,
      "text": "and that's how I recommend you go",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 208,
      "text": "and that's how I recommend you go through this lecture through this lecture through this lecture so we are starting here with d-log props so we are starting here with d-log props so we are starting here with d-log props now d-lock props will hold the now d-lock props will hold the now d-lock props will hold the derivative of the loss with respect to derivative of the loss with respect to derivative of the loss with respect to all the elements of log props all the elements of log props all the elements of log props what is inside log blobs the shape of what is inside log blobs the shape of what is inside log blobs the shape of this is 32 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 209,
      "text": "so it's not going to this is 32 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 210,
      "text": "so it's not going to this is 32 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 211,
      "text": "so it's not going to surprise you that D log props should surprise you that D log props should surprise you that D log props should also be an array of size 32 by 27 also be an array of size 32 by 27 also be an array of size 32 by 27 because we want the derivative loss with because we want the derivative loss with because we want the derivative loss with respect to all of its elements so the respect to all of its elements so the respect to all of its elements so the sizes of those are always going to be sizes of those are always going to be sizes of those are always going to be equal equal equal now how how does log props influence the now how how does log props influence the now how how does log props influence the loss okay loss is negative block probes loss okay loss is negative block probes loss okay loss is negative block probes indexed with range of N and YB and then indexed with range of N and YB and then indexed with range of N and YB and then the mean of that now just as a reminder the mean of that now just as a reminder the mean of that now just as a reminder YB is just a basically an array of all YB is just a basically an array of all YB is just a basically an array of all the correct indices the correct indices the correct indices um so what we're doing here is we're um",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 212,
      "text": "so what we're doing here is we're um",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 213,
      "text": "so what we're doing here is we're taking the lock props array of size 32 taking the lock props array of size 32 taking the lock props array of size 32 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 214,
      "text": "by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 215,
      "text": "by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 216,
      "text": "right right right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 217,
      "text": "and then we are going in every single",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 218,
      "text": "and then we are going in every single",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 219,
      "text": "and then we are going in every single row and in each row we are plugging row and in each row we are plugging row and in each row we are plugging plucking out the index eight and then 14 plucking out the index eight and then 14 plucking out the index eight and",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 220,
      "text": "then 14 and 15 and so on",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 221,
      "text": "so we're going down the and 15 and so on so we're going down the and 15 and so on so we're going down the rows that's the iterator range of N and rows that's the iterator range of N and rows that's the iterator range of N",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 222,
      "text": "and then we are always plucking out the then we are always plucking out the then we are always plucking out the index of the column specified by this index of the column specified by this index of the column specified by this tensor YB",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 223,
      "text": "so in the zeroth row we are tensor YB",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 224,
      "text": "so in the zeroth row we are tensor YB",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 225,
      "text": "so in the zeroth row we are taking the eighth column in the first taking the eighth column in the first taking the eighth column in the first row we're taking the 14th column Etc and row we're taking the 14th column Etc and row we're taking the 14th column Etc and so log props at this plugs out so log props at this plugs out so log props at this plugs out all those all those all those log probabilities of the correct next log probabilities of the correct next log probabilities of the correct next character in a sequence character in a sequence character in a sequence so that's what that does and the shape",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 226,
      "text": "so that's what that does and the shape",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 227,
      "text": "so that's what that does and the shape of this or the size of it is of course of this or the size of it is of course of this or the size of it is of course 32 because our batch size is 32. 32 because our batch size is 32. 32 because our batch size is 32.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 228,
      "text": "so these elements get plugged out and so these elements get plugged out and so these elements get plugged out and then their mean and the negative of that then their mean and the negative of that then their mean and the negative of that becomes loss becomes loss becomes loss",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 229,
      "text": "so I always like to work with simpler",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 230,
      "text": "so I always like to work with simpler",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 231,
      "text": "so I always like to work with simpler examples to understand the numerical examples to understand the numerical examples to understand the numerical form of derivative",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 232,
      "text": "what's going on here form of derivative",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 233,
      "text": "what's going on here form of derivative",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 234,
      "text": "what's going on here is once we've plucked out these examples is once we've plucked out these examples is once we've plucked out these examples um we're taking the mean",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 235,
      "text": "and then the um we're taking the mean",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 236,
      "text": "and then the um we're taking the mean",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 237,
      "text": "and then the negative so the loss basically negative so the loss basically negative so the loss basically I can write it this way is the negative I can write it this way is the negative I can write it this way is the negative of say a plus b plus c of say a plus b plus c of say a plus b plus c and the mean of those three numbers and the mean of those three numbers and the mean of those three numbers would be say negative would divide three would be say negative would divide three would be say negative would divide three that would be how we achieve the mean of that would be how we achieve the mean of that would be how we achieve the mean of three numbers ABC although we actually three numbers ABC although we actually three numbers ABC although we actually have 32 numbers here have 32 numbers here have 32 numbers here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 238,
      "text": "and so what is basically the loss by say",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 239,
      "text": "and so what is basically the loss by say",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 240,
      "text": "and so what is basically the loss by say like d a right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 241,
      "text": "like d a right like d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 242,
      "text": "a right well if we simplify this expression well if we simplify this expression well if we simplify this expression mathematically this is negative one over mathematically this is negative one over mathematically this is negative one over three of A and negative plus negative three of A and negative plus negative three of A and negative plus negative one over three of B one over three of B one over three of B plus negative 1 over 3 of c",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 243,
      "text": "and so what plus negative 1 over 3 of c",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 244,
      "text": "and so what plus negative 1 over 3 of c",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 245,
      "text": "and so what is D loss by D A",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 246,
      "text": "it's just negative one is D loss by D A",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 247,
      "text": "it's just negative one is D loss by D A",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 248,
      "text": "it's just negative one over three over three over three",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 249,
      "text": "and so you can see that if we don't just",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 250,
      "text": "and so you can see that if we don't just",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 251,
      "text": "and so you can see that if we don't just have a b and c",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 252,
      "text": "but we have 32 numbers have a b and c",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 253,
      "text": "but we have 32 numbers have a b and c",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 254,
      "text": "but we have 32 numbers then D loss by D then D loss by D then D loss by D um you know every one of those numbers",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 255,
      "text": "um you know every one of those numbers um you know every one of those numbers is going to be one over N More generally is going to be one over N More generally is going to be one over N More generally because n is the um the size of the because n is the um the size of the because n is the um the size of the batch 32 in this case batch 32 in this case batch 32 in this case so D loss by so D loss by so D loss by um D Lock probs is negative 1 over n um D Lock probs is negative 1 over n um D Lock probs is negative 1 over n in all these places in all these places in all these places now what about the other elements inside now what about the other elements inside now what about the other elements inside lock problems because lock props is lock problems because lock props is lock problems because lock props is large array you see that lock problems large array you see that lock problems large array you see that lock problems at shape is 32 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 256,
      "text": "but only 32 of at shape is 32 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 257,
      "text": "but only 32 of at shape is 32 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 258,
      "text": "but only 32 of them participate in the loss calculation them participate in the loss calculation them participate in the loss calculation so what's the derivative of all the so what's the derivative of all the so what's the derivative of all the other most of the elements that do not other most of the elements that do not other most of the elements that do not get plucked out here get plucked out here get plucked out here while their loss intuitively is zero while their loss intuitively is zero while their loss intuitively is zero sorry they're gradient intuitively is sorry they're gradient intuitively is sorry they're gradient intuitively is zero and that's because they did not zero and that's because they did not zero and that's because they did not participate in the loss participate in the loss participate in the loss so most of these numbers inside this so most of these numbers inside this so most of these numbers inside this tensor does not feed into the loss and tensor does not feed into the loss and tensor does not feed into the loss",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 259,
      "text": "and so if we were to change these numbers",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 260,
      "text": "so if we were to change these numbers so if we were to change these numbers then the loss doesn't change which is then the loss doesn't change which is then the loss doesn't change which is the equivalent of way of saying that the the equivalent of way of saying that the the equivalent of way of saying that the derivative of the loss with respect to derivative of the loss with respect to them is zero they don't impact it them is zero they don't impact it them is zero they don't impact it so here's a way to implement this so here's a way to implement this so here's a way to implement this derivative then we start out with derivative then we start out with derivative then we start out with torch.zeros of shape 32 by 27 or let's torch.zeros of shape 32 by 27 or let's torch.zeros of shape 32 by 27 or let's just say instead of doing this because just say instead of doing this because just say instead of doing this because we don't want to hard code numbers let's we don't want to hard code numbers let's we don't want to hard code numbers let's do torch.zeros like do torch.zeros like do torch.zeros like block probs so basically this is going block probs so basically this is going block probs so basically this is going to create an array of zeros exactly in to create an array of zeros exactly in to create an array of zeros exactly in the shape of log probs the shape of log probs the shape of log probs and then we need to set the derivative and then we need to set the derivative and then we need to set the derivative of negative 1 over n inside exactly of negative 1 over n inside exactly of negative 1 over n inside exactly these locations so here's what we can do these locations so here's what we can do these locations so here's what we can do the lock props indexed in The Identical",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 261,
      "text": "the lock props indexed in The Identical the lock props indexed in The Identical way way way will be just set to negative one over will be just set to negative one over will be just set to negative one over zero divide n zero divide n zero divide n right just like we derived here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 262,
      "text": "right just like we derived here right just like we derived here so now let me erase all this reasoning so now let me erase all this reasoning so now let me erase all this reasoning",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 263,
      "text": "and then this is the candidate",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 264,
      "text": "and then this is the candidate",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 265,
      "text": "and then this is the candidate derivative for D log props let's derivative for D log props let's derivative for D log props let's uncomment the first line and check that uncomment the first line and check that uncomment the first line and check that this is correct this is correct this is correct",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 266,
      "text": "okay so CMP ran and let's go back to CMP",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 267,
      "text": "okay so CMP ran and let's go back to CMP",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 268,
      "text": "okay so CMP ran and let's go back to CMP and you see that what it's doing is it's",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 269,
      "text": "and you see that what it's doing is it's",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 270,
      "text": "and you see that what it's doing is it's calculating if calculating if calculating if the calculated value by us which is DT the calculated value by us which is DT the calculated value by us which is DT is exactly equal to T dot grad as is exactly equal to T dot grad as is exactly equal to T dot grad as calculated by pi torch",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 271,
      "text": "and then this is calculated by pi torch and then this is calculated by pi torch and then this is making sure that all the elements are making sure that all the elements are making sure that all the elements are exactly equal and then converting this exactly equal and then converting this exactly equal and then converting this to a single Boolean value because we to a single Boolean value because we to a single Boolean value because we don't want the Boolean tensor we just don't want the Boolean tensor we just don't want the Boolean tensor we just want to Boolean value want to Boolean value want to Boolean value",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 272,
      "text": "and then here we are making sure that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 273,
      "text": "and then here we are making sure that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 274,
      "text": "and then here we are making sure",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 275,
      "text": "that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 276,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 277,
      "text": "if they're not exactly equal",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 278,
      "text": "maybe okay if they're not exactly equal",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 279,
      "text": "maybe okay if they're not exactly equal maybe they are approximately equal because of they are approximately equal because of they are approximately equal because of some floating Point issues",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 280,
      "text": "but they're some floating Point issues but they're some floating Point issues but they're very very close very very close very very close",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 281,
      "text": "so here we are using torch.allclose so here we are using torch.allclose so here we are using torch.allclose which has a little bit of a wiggle which has a little bit of a wiggle which has a little bit of a wiggle available because sometimes you can get available because sometimes you can get available because sometimes you can get very very close",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 282,
      "text": "but if you use a very very close but if you use a very very close but if you use a slightly different calculation because a slightly different calculation because a slightly different calculation because a floating Point arithmetic you can get a floating Point arithmetic you can get a floating Point arithmetic you can get a slightly different result so this is slightly different result so this is slightly different result so this is checking if you get an approximately checking if you get an approximately checking if you get an approximately close result close result close result and then here we are checking the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 283,
      "text": "and then here we are checking the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 284,
      "text": "and then here we are checking the maximum uh basically the value that has maximum uh basically the value that has maximum uh basically the value that has the highest difference and what is the the highest difference and what is the the highest difference and what is the difference in the absolute value difference in the absolute value difference in the absolute value difference between those two and so we difference between those two and so we difference between those two and so we are printing whether we have an exact are printing whether we have an exact are printing whether we have an exact equality an approximate equality and equality an approximate equality and equality an approximate equality and what is the largest difference what is the largest difference what is the largest difference and",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 285,
      "text": "so here and so here and so here we see that we actually have exact we see that we actually have exact we see that we actually have exact equality and so therefore of course we equality and so therefore of course we equality and so therefore of course we also have an approximate equality and also have an approximate equality and also have an approximate equality and the maximum difference is exactly zero the maximum difference is exactly zero the maximum difference is exactly zero so basically our d-log props is exactly so basically our d-log props is exactly so basically our d-log props is exactly equal to what pytors calculated to be equal to what pytors calculated to be equal to what pytors calculated to be lockprops.grad in its back propagation lockprops.grad in its back propagation lockprops.grad in its back propagation",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 286,
      "text": "so so far we're working pretty well",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 287,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 288,
      "text": "so so far we're working pretty well",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 289,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 290,
      "text": "so so far we're working pretty well",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 291,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 292,
      "text": "so let's now continue our back so let's now continue our back",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 293,
      "text": "so let's now continue our back propagation propagation propagation we have that lock props depends on we have that lock props depends on we have that lock props depends on probes through a log probes through a log probes through a log so all the elements of probes are being so all the elements of probes are being so all the elements of probes are being element wise applied log to element wise applied log to element wise applied log to now if we want deep props then then now if we want deep props then then now if we want deep props then then remember your micrograph training remember your micrograph training remember your micrograph training we have like a log node it takes in we have like a log node it takes in we have like a log node it takes in probs and creates log probs and the probs and creates log probs and the probs and creates log probs and the props will be the local derivative of props will be the local derivative of props will be the local derivative of that individual Operation Log times the that individual Operation Log times the that individual Operation Log times the derivative loss with respect to its derivative loss with respect to its derivative loss with respect to its output which in this case is D log props output which in this case is D log props output which in this case is D log props",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 294,
      "text": "so what is the local derivative of this so what is the local derivative of this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 295,
      "text": "so what is the local derivative of this operation",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 296,
      "text": "well we are taking log element operation",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 297,
      "text": "well we are taking log element operation",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 298,
      "text": "well we are taking log element wise",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 299,
      "text": "and we can come here and we can see wise",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 300,
      "text": "and we can come here and we can see wise",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 301,
      "text": "and we can come here and we can see well from alpha is your friend that d by well from alpha is your friend that d by well from alpha is your friend that d by DX of log of x is just simply one of our DX of log of x is just simply one of our DX of log of x is just simply one of our X X X",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 302,
      "text": "so therefore in this case X is problems so therefore in this case X is problems so therefore in this case X is problems",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 303,
      "text": "so we have d by DX is one over X which so we have d by DX is one over X which so we have d by DX is one over X which is one of our probes and then this is is one of our probes and then this is is one of our probes and then this is the local derivative and then times we the local derivative and then times we the local derivative and then times we want to chain it want to chain it want to chain it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 304,
      "text": "so this is chain rule so this is chain rule",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 305,
      "text": "so this is chain rule times do log props times do log props times do log props let me uncomment this and let me run the let me uncomment this and let me run the let me uncomment this and let me run the cell in place and we see that the cell in place and we see that the cell in place and we see that the derivative of props as we calculated derivative of props as we calculated derivative of props as we calculated here is exactly correct here is exactly correct here is exactly correct and so notice here how this works probes and so notice here how this works probes and so notice here how this works probes that are props is going to be inverted that are props is going to be inverted that are props is going to be inverted and then element was multiplied here and then element was multiplied here and then element was multiplied here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 306,
      "text": "so if your probes is very very close to so if your probes is very very close to so if your probes is very very close to one that means you are your network is one that means you are your network is one that means you are your network is currently predicting the character currently predicting the character currently predicting the character correctly then this will become one over correctly then this will become one over correctly then this will become one over one and D log probes just gets passed one and D log probes just gets passed one and D log probes just gets passed through through through",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 307,
      "text": "but if your probabilities are but if your probabilities are but if your probabilities are incorrectly assigned so if the correct incorrectly assigned so if the correct incorrectly assigned so if the correct character here is getting a very low character here is getting a very low character here is getting a very low probability then 1.0 dividing by it will probability then 1.0 dividing by it will probability then 1.0 dividing by it will boost this boost this boost",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 308,
      "text": "this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 309,
      "text": "and then multiply by the log props",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 310,
      "text": "so and then multiply by the log props",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 311,
      "text": "so and then multiply by the log props so basically what this line is doing basically what this line is doing basically what this line is doing intuitively is it's taking the examples intuitively is it's taking the examples intuitively is it's taking the examples that have a very low probability that have a very low probability that have a very low probability currently assigned and it's boosting currently assigned and it's boosting currently assigned",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 312,
      "text": "and it's boosting their gradient uh you can you can look their gradient",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 313,
      "text": "uh you can you can look their gradient uh you can you can look at it that way next up is Count some imp at it that way next up is Count some imp at it that way next up is Count some imp",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 314,
      "text": "so we want the river of this now let me",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 315,
      "text": "so we want the river of this now let me",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 316,
      "text": "so we want the river of this now let me just pause here and kind of introduce just pause here and kind of introduce just pause here and kind of introduce What's Happening Here in general because What's Happening Here in general because What's Happening Here in general because I know it's a little bit confusing we I know it's a little bit confusing we I know it's a little bit confusing we have the locusts that come out of the have the locusts that come out of the have the locusts that come out of the neural nut here what I'm doing is I'm neural nut here what I'm doing is I'm neural nut here what I'm doing is I'm finding the maximum in each row",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 317,
      "text": "and I'm finding the maximum in each row",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 318,
      "text": "and I'm finding the maximum in each row",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 319,
      "text": "and I'm subtracting it for the purposes of subtracting it for the purposes of subtracting it for the purposes of numerical stability and we talked about numerical stability and we talked about numerical stability and we talked about how if you do not do this you run how if you do not do this you run how if you do not do this you run numerical issues if some of the logits numerical issues if some of the logits numerical issues if some of the logits take on two large values because we end take on two large values because we end take on two large values because we end up exponentiating them up exponentiating them up exponentiating them",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 320,
      "text": "so this is done just for safety so this is done just for safety",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 321,
      "text": "so this is done just for safety numerically then here's the numerically then here's the numerically then here's the exponentiation of all the sort of like exponentiation of all the sort of like exponentiation of all the sort of like logits to create our accounts and then logits to create our accounts and then logits to create our accounts",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 322,
      "text": "and then we want to take the some of these counts we want to take the some of these counts we want to take the some of these counts and normalize so that all of the probes and normalize so that all of the probes and normalize so that all of the probes sum to one sum to one sum to one now here instead of using one over count now here instead of using one over count now here instead of using one over count sum I use uh raised to the power of sum I use uh raised to the power of sum I use uh raised to the power of negative one mathematically they are negative one mathematically they are negative one mathematically they are identical",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 323,
      "text": "I just found that there's identical I just found that there's identical I just found that there's something wrong with the pytorch something wrong with the pytorch something wrong with the pytorch implementation of the backward pass of implementation of the backward pass of implementation of the backward pass of division division division",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 324,
      "text": "um and it gives like a real result",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 325,
      "text": "but um and it gives like a real result",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 326,
      "text": "but um and it gives like a real result but that doesn't happen for star star native that doesn't happen for star star native that doesn't happen for star star native one that's why I'm using this formula one that's why I'm using this formula one that's why I'm using this formula instead but basically all that's instead but basically all that's instead but basically all that's happening here is we got the logits happening here is we got the logits happening here is we got the logits we're going to exponentiate all of them we're going to exponentiate all of them we're going to exponentiate all of them and want to normalize the counts to and want to normalize the counts to and want to normalize the counts to create our probabilities it's just that create our probabilities it's just that create our probabilities it's just that it's happening across multiple lines it's happening across multiple lines it's happening across multiple lines so now",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 327,
      "text": "so now so now here we want to First Take the derivative we we want",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 328,
      "text": "to First Take the derivative we want to back propagate into account want to back propagate into account want to back propagate into account sumiv and then into counts as well sumiv and then into counts as well sumiv and then into counts as well so what should be the count sum M now we so what should be the count sum M now we so what should be the count sum M",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 329,
      "text": "now we actually have to be careful here because actually have to be careful here because actually have to be careful here because we have to scrutinize and be careful we have to scrutinize and be careful we have to scrutinize and be careful with the shapes so counts that shape and with the shapes so counts that shape and with the shapes so counts that shape and then count some inverse shape then count some inverse shape then count some inverse shape are different are different are different so in particular counts as 32 by 27 but so in particular counts as 32 by 27 but so in particular counts as 32 by 27 but this count sum m is 32 by 1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 330,
      "text": "and so in this count sum m is 32 by 1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 331,
      "text": "and so in this count sum m is 32 by 1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 332,
      "text": "and so in this multiplication here we also have an this multiplication here we also have an this multiplication here we also have an implicit broadcasting that pytorch will implicit broadcasting that pytorch will implicit broadcasting that pytorch will do because it needs to take this column do because it needs to take this column do because it needs to take this column tensor of 32 numbers and replicate it tensor of 32 numbers and replicate it tensor of 32 numbers and replicate it horizontally 27 times to align these two horizontally 27 times to align these two horizontally 27 times to align these two tensors so it can do an element twice tensors so it can do an element twice tensors so it can do an element twice multiply multiply multiply",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 333,
      "text": "so really what this looks like is the so really what this looks like is the so really what this looks like is the following using a toy example again following using a toy example again following using a toy example again what we really have here is just props what we really have here is just props what we really have here is just props is counts times conservative",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 334,
      "text": "so it's a C is counts times conservative",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 335,
      "text": "so it's a C is counts times conservative",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 336,
      "text": "so it's a C equals a times B equals a times B equals a times B but a is 3 by 3 and b is just three by but a is 3 by 3 and b is just three by but a is 3 by 3 and b is just three by one a column tensor and so pytorch one a column tensor and so pytorch one a column tensor and so pytorch internally replicated this elements of B internally replicated this elements of B internally replicated this elements of B",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 337,
      "text": "and it did that across all the columns and it did that across all the columns and it did that across all the columns so for example B1 which is the first so for example B1 which is the first so for example B1 which is the first element of B would be replicated here element of B would be replicated here element of B would be replicated here across all the columns in this across all the columns in this across all the columns in this multiplication multiplication multiplication and now we're trying to back propagate",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 338,
      "text": "and now we're trying to back propagate",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 339,
      "text": "and now we're trying to back propagate through this operation to count some m through this operation to count some m through this operation to count some m so when we're calculating this so when we're calculating this so when we're calculating this derivative derivative derivative it's important to realize that these two it's important to realize that these two it's important to realize that these two this looks like a single operation",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 340,
      "text": "but this looks like a single operation but this looks like a single operation but actually is two operations applied actually is two operations applied actually is two operations applied sequentially the first operation that sequentially the first operation that sequentially the first operation that pytorch did is it took this column pytorch did is it took this column pytorch did is it took this column tensor and replicated it across all the tensor and replicated it across all the tensor and replicated it across all the um across all the columns basically 27 um across all the columns basically 27 um across all the columns basically 27 times so that's the first operation it's times so that's the first operation it's times so that's the first operation it's a replication",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 341,
      "text": "and then the second a replication",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 342,
      "text": "and then the second a replication",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 343,
      "text": "and then the second operation is the multiplication so let's operation is the multiplication so let's operation is the multiplication so let's first background through the first background through the first background through the multiplication multiplication if these two arrays are of the same size if these two arrays are of the same size if these two arrays are of the same size",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 344,
      "text": "and we just have a and b of both of them",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 345,
      "text": "and we just have a and b of both of them",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 346,
      "text": "and we just have a and b of both of them three by three then how do we mult how three by three then how do we mult how three by three then how do we mult how do we back propagate through a do we back propagate through a do we back propagate through a multiplication",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 347,
      "text": "so if we just have multiplication so if we just have multiplication so if we just have scalars and not tensors then if you have scalars and not tensors then if you have scalars and not tensors then if you have C equals a times B then what is uh the C equals a times B then what is uh the C equals a times B then what is uh the order of the of C with respect to B well order of the of C with respect to B well order of the of C with respect to B",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 348,
      "text": "well it's just a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 349,
      "text": "and so that's the local it's just a and so that's the local",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 350,
      "text": "it's just a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 351,
      "text": "and so that's the local derivative derivative so here in our case undoing the so here in our case undoing the so here in our case undoing the multiplication and back propagating multiplication and back propagating multiplication and back propagating through just the multiplication itself through just the multiplication itself through just the multiplication itself which is element wise is going to be the which is element wise is going to be the which is element wise is going to be the local derivative which in this case is local derivative which in this case is local derivative which in this case is simply counts because counts is the a simply counts because counts is the a simply counts because counts is the a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 352,
      "text": "so",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 353,
      "text": "this is the local derivative and then so this is the local derivative",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 354,
      "text": "and then so this is the local derivative and then times because the chain rule D props times because the chain rule D props times because the chain rule D props so this here is the derivative or the so this here is the derivative or the so this here is the derivative or the gradient but with respect to replicated gradient but with respect to replicated gradient but with respect to replicated B B B",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 355,
      "text": "but we don't have a replicated B",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 356,
      "text": "we just",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 357,
      "text": "but we don't have a replicated B",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 358,
      "text": "we just",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 359,
      "text": "but we don't have a replicated B we just have a single B column",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 360,
      "text": "so how do we now have a single B column",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 361,
      "text": "so how do we now have a single B column",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 362,
      "text": "so how do we now back propagate through the replication back propagate through the replication back propagate through the replication and intuitively this B1 is the same and intuitively this B1 is the same and intuitively this B1 is the same variable and it's just reused multiple variable and it's just reused multiple variable and it's just reused multiple times times times",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 363,
      "text": "and so you can look at it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 364,
      "text": "and so you can look at it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 365,
      "text": "and so you can look at it as being equivalent to a case we've as being equivalent to a case we've as being equivalent to a case we've encountered in micrograd encountered in micrograd encountered in micrograd",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 366,
      "text": "and so here I'm just pulling out a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 367,
      "text": "and so here I'm just pulling out a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 368,
      "text": "and so here I'm just pulling out a random graph we used in micrograd we had random graph we used in micrograd we had random graph we used in micrograd we had an example where a single node an example where a single node an example where a single node has its output feeding into two branches has its output feeding into two branches has its output feeding into two branches of basically the graph until the last of basically the graph until the last of basically the graph until the last function and we're talking about how the function and we're talking about how the function and we're talking about how the correct thing to do in the backward pass correct thing to do in the backward pass correct thing to do in the backward pass is we need to sum all the gradients that is we need to sum all the gradients that is we need to sum all the gradients that arrive at any one node so across these arrive at any one node so across these arrive at any one node so across these different branches the gradients would different branches the gradients would different branches the gradients would sum sum sum so if a node is used multiple times the so if a node is used multiple times the so if a node is used multiple times the gradients for all of its uses sum during gradients for all of its uses sum during gradients for all of its uses sum during back propagation back propagation back propagation so here B1 is used multiple times in all so here B1 is used multiple times in all so here B1 is used multiple times in all these columns and therefore the right these columns and therefore the right these columns and therefore the right thing to do here is to sum thing to do here is to sum thing to do here is to sum horizontally across all the rows",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 369,
      "text": "so I'm horizontally across all the rows",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 370,
      "text": "so I'm horizontally across all the rows",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 371,
      "text": "so I'm going to sum in going to sum in going to sum in Dimension one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 372,
      "text": "but we want to retain this Dimension one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 373,
      "text": "but we want to retain this Dimension one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 374,
      "text": "but we want to retain this Dimension so that the uh so that counts Dimension so that the uh so that counts Dimension so that the uh so that counts some end and its gradient are going to some end and its gradient are going to some end and its gradient are going to be exactly the same shape so we want to be exactly the same shape",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 375,
      "text": "so we want to be exactly the same shape",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 376,
      "text": "so we want to make sure that we keep them as true so make sure that we keep them as true so make sure that we keep them as true so we don't lose this dimension and this we don't lose this dimension and this we don't lose this dimension and this will make the count sum M be exactly will make the count sum M be exactly will make the count sum M be exactly shape 32 by 1. shape 32 by 1. shape 32 by 1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 377,
      "text": "so revealing this comparison as well and so revealing this comparison as well and so revealing this comparison as well and running this we see that we get an exact running this we see that we get an exact running this we see that we get an exact match match match",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 378,
      "text": "so this derivative is exactly correct so this derivative is exactly correct",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 379,
      "text": "so this derivative is exactly correct and let me erase and let me erase and let me erase this now let's also back propagate into this now let's also back propagate into this now let's also back propagate into counts which is the other variable here counts which is the other variable here counts which is the other variable here to create probes so from props to count to create probes so from props to count to create probes so from props to count some INF we just did that let's go into some INF we just did that let's go into some INF we just did that let's go into counts as well counts as well counts as well so decounts will be the chances are a so DC by d a is just B",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 380,
      "text": "the chances are a so DC by d a is just B",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 381,
      "text": "so therefore it's count summative",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 382,
      "text": "so therefore it's count summative",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 383,
      "text": "so therefore it's count summative um and then times chain rule the props um and then times chain rule the props um and then times chain rule the props now councilman is three two by One D now councilman is three two by One D now councilman is three two by One D probs is 32 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 384,
      "text": "probs is 32 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 385,
      "text": "probs is 32 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 386,
      "text": "so so so um those will broadcast fine and will um those will broadcast fine and will um those will broadcast fine and will give us decounts there's no additional give us decounts there's no additional give us decounts there's no additional summation required here summation required here summation required here um there will be a broadcasting that um there will be a broadcasting that um there will be a broadcasting that happens in this multiply here because happens in this multiply here because happens in this multiply here because count some M needs to be replicated count some M needs to be replicated count some M needs to be replicated again to correctly multiply D props but again to correctly multiply D props but again to correctly multiply D props but that's going to give the correct result that's going to give the correct result that's going to give the correct result so as far as the single operation is so as far as the single operation is so as far as the single operation is concerned so we back probably go from concerned so we back probably go from concerned",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 387,
      "text": "so we back probably go from props to counts but we can't actually props to counts",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 388,
      "text": "but we can't actually props to counts",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 389,
      "text": "but we can't actually check the derivative counts uh I have it check the derivative counts uh I have it check the derivative counts uh I have it much later on and the reason for that is much later on and the reason for that is much later on and the reason for that is because count sum in depends on counts because count sum in depends on counts because count sum in depends on counts and so there's a second Branch here that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 390,
      "text": "and so there's a second Branch here that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 391,
      "text": "and so there's a second Branch here that we have to finish because can't summon we have to finish because can't summon we have to finish because can't summon back propagates into account sum and back propagates into account sum and back propagates into account sum and count sum will buy properly into counts count sum will buy properly into counts count sum will buy properly into counts and so counts is a node that is being and so counts is a node that is being and so counts is a node that is being used twice it's used right here in two used twice it's used right here in two used twice it's used right here in two props and it goes through this other props and it goes through this other props and it goes through this other Branch through count summative Branch through count summative Branch through count summative",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 392,
      "text": "so even though we've calculated the so even though we've calculated the so even though we've calculated the first contribution of it we still have first contribution of it we still have first contribution of it we still have to calculate the second contribution of to calculate the second contribution of to calculate the second contribution of it later it later it later",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 393,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 394,
      "text": "so we're continuing with this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 395,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 396,
      "text": "so we're continuing with this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 397,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 398,
      "text": "so we're continuing with this Branch we have the derivative for count Branch we have the derivative for count Branch we have the derivative for count sum if now we want the derivative of sum if now we want the derivative of sum if now we want the derivative of count sum",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 399,
      "text": "so D count sum equals what is count sum",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 400,
      "text": "so D count sum equals what is count sum",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 401,
      "text": "so D count sum equals what is the local derivative of this operation the local derivative of this operation the local derivative of this operation so this is basically an element wise one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 402,
      "text": "so this is basically an element wise one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 403,
      "text": "so this is basically an element wise one over counts sum over counts sum over counts sum so count sum raised to the power of so count sum raised to the power of so count sum raised to the power of negative one is the same as one over negative one is the same as one over negative one is the same as one over count sum if we go to all from alpha we count sum if we go to all from alpha we count sum if we go to all from alpha we see that x to the negative one D by D by see that x to the negative one D by D by see that x to the negative one D by D by D by DX of it is basically Negative X to D by DX of it is basically Negative X to D by DX of it is basically Negative X to the negative 2.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 404,
      "text": "right one negative one the negative 2.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 405,
      "text": "right one negative one the negative 2.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 406,
      "text": "right one negative one over squared is the same as Negative X over squared is the same as Negative X over squared is the same as Negative X to the negative two to the negative two to the negative two",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 407,
      "text": "so D count sum here will be local so D count sum here will be local so D count sum here will be local derivative is going to be negative derivative is going to be negative derivative is going to be negative um um um counts sum to the negative two that's counts sum to the negative two that's counts sum to the negative two that's the local derivative times chain rule the local derivative times chain rule the local derivative times chain rule which is D count sum in so that's D count sum",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 408,
      "text": "so that's D count sum",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 409,
      "text": "let's uncomment this and check that I am let's uncomment this and check that I am let's uncomment this and check that I am correct",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 410,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 411,
      "text": "so we have perfect equality correct",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 412,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 413,
      "text": "so we have perfect equality correct",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 414,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 415,
      "text": "so we have perfect equality and there's no sketchiness going on here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 416,
      "text": "and there's no sketchiness going on here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 417,
      "text": "and there's no sketchiness going on here with any shapes because these are of the with any shapes because these are of the with any shapes because these are of the same shape",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 418,
      "text": "okay next up we want to back same shape",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 419,
      "text": "okay next up we want to back same shape",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 420,
      "text": "okay next up we want to back propagate through this line we have that propagate through this line we have that propagate through this line we have that count sum it's count.sum along the rows count sum it's count.sum along the rows count sum",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 421,
      "text": "it's count.sum along the rows",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 422,
      "text": "so I wrote out so I wrote out so I wrote out um some help here we have to keep in um some help here we have to keep in um some help here we have to keep in mind that counts of course is 32 by 27 mind that counts of course is 32 by 27 mind that counts of course is 32 by 27 and count sum is 32 by 1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 423,
      "text": "so in this and count sum is 32 by 1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 424,
      "text": "so in this and count sum is 32 by 1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 425,
      "text": "so in this back propagation we need to take this back propagation we need to take this back propagation we need to take this column of derivatives and transform it column of derivatives and transform it column of derivatives and transform it into a array of derivatives into a array of derivatives into a array of derivatives two-dimensional array two-dimensional array two-dimensional array",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 426,
      "text": "so what is this operation doing we're so what is this operation doing we're so what is this operation doing we're taking in some kind of an input like say taking in some kind of an input like say taking in some kind of an input like say a three by three Matrix a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 427,
      "text": "and we are a three by three Matrix a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 428,
      "text": "and we are a three by three Matrix a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 429,
      "text": "and we are summing up the rows into a column tells summing up the rows into a column tells summing up the rows into a column tells her B1 b2b3 that is basically this her B1 b2b3 that is basically this her B1 b2b3 that is basically this so now we have the derivatives of the so now we have the derivatives of the so now we have the derivatives of the loss with respect to B all the elements loss with respect to B all the elements loss with respect to B all the elements of B of B of B and now we want to derivative loss with and now we want to derivative loss with and now we want to derivative loss with respect to all these little A's respect to all these little A's respect to all these little A's",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 430,
      "text": "so how do the B's depend on the ace is so how do the B's depend on the ace is so how do the B's depend on the ace is basically what we're after what is the basically what we're after what is the basically what we're after what is the local derivative of this operation local derivative of this operation local derivative of this operation",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 431,
      "text": "well we can see here that B1 only well we can see here that B1 only well we can see here that B1 only depends on these elements here the depends on these elements here the depends on these elements here the derivative of B1 with respect to all of derivative of B1 with respect to all of derivative of B1 with respect to all of these elements down here is zero but for these elements down here is zero but for these elements down here is zero but for these elements here like a11 a12 Etc the these elements here like a11 a12 Etc the these elements here like a11 a12 Etc the local derivative is one right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 432,
      "text": "so DB 1 by local derivative is one right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 433,
      "text": "so DB 1 by local derivative is one right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 434,
      "text": "so DB 1 by D A 1 1 for example is one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 435,
      "text": "so it's one D",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 436,
      "text": "A 1 1 for example is one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 437,
      "text": "so it's one D",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 438,
      "text": "A 1 1 for example is one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 439,
      "text": "so it's one one and one one and one one and one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 440,
      "text": "so when we have the derivative of loss so when we have the derivative of loss so when we have the derivative of loss with respect to B1 with respect to B1 with respect to B1 did a local derivative of B1 with did a local derivative of B1 with did a local derivative of B1 with respect to these inputs is zeros here respect to these inputs is zeros here respect to these inputs is zeros here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 441,
      "text": "but it's one on these guys but it's one on these guys",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 442,
      "text": "but it's one on these guys so in the chain rule so in the chain rule so in the chain rule we have the local derivative uh times we have the local derivative uh times we have the local derivative uh times sort of the derivative of B1 and so sort of the derivative of B1 and so sort of the derivative of B1 and so because the local derivative is one on because the local derivative is one on because the local derivative is one on these three elements the look of them these three elements the look of them these three elements the look of them are multiplying the derivative of B1 are multiplying the derivative of B1 are multiplying the derivative of B1 will just be the derivative of B1 and so will just be the derivative of B1 and so will just be the derivative of B1 and so you can look at it as a router basically you can look at it as a router basically you can look at it as a router basically an addition is a router of gradient an addition is a router of gradient an addition is a router of gradient whatever gradient comes from above it whatever gradient comes from above it whatever gradient comes from above it just gets routed equally to all the just gets routed equally to all the just gets routed equally to all the elements that participate in that elements that participate in that elements that participate in that addition addition addition so in this case the derivative of B1 so in this case the derivative of B1 so in this case the derivative of B1 will just flow equally to the derivative will just flow equally to the derivative will just flow equally to the derivative of a11 a12 and a13 of a11 a12 and a13 of a11 a12 and a13 .",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 443,
      "text": "so if we have a derivative of all the .",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 444,
      "text": "so if we have a derivative of all the .",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 445,
      "text": "so if we have a derivative of all the elements of B and in this column tensor elements of B and in this column tensor elements of B and in this column tensor which is D counts sum that we've which is D counts sum that we've which is D counts sum that we've calculated just now calculated just now calculated just now we basically see that what that amounts we basically see that what that amounts we basically see that what that amounts to is all of these are now flowing to to is all of these are now flowing to to is all of these are now flowing to all these elements of a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 446,
      "text": "and they're all these elements of a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 447,
      "text": "and they're all these elements of a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 448,
      "text": "and they're doing that horizontally doing that horizontally doing that horizontally",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 449,
      "text": "so basically what we want is we want to so basically what we want is we want to so basically what we want is we want to take the decount sum of size 30 by 1 and take the decount sum of size 30 by 1 and take the decount sum of size 30 by 1",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 450,
      "text": "and we just want to replicate it 27 times we just want to replicate it 27 times we just want to replicate it 27 times horizontally to create 32 by 27 array horizontally to create 32 by 27 array horizontally to create 32 by 27 array so there's many ways to implement this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 451,
      "text": "so there's many ways to implement this so there's many ways to implement this operation you could of course just operation you could of course just operation you could of course just replicate the tensor",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 452,
      "text": "but I think maybe replicate the tensor",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 453,
      "text": "but I think maybe replicate the tensor",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 454,
      "text": "but I think maybe one clean one is that the counts is one clean one is that the counts is one clean one is that the counts is simply torch dot once like simply torch dot once like simply torch dot once like",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 455,
      "text": "so just an two-dimensional arrays of so just an two-dimensional arrays of so just an two-dimensional arrays of ones in the shape of counts so 32 by 27 ones in the shape of counts so 32 by 27 ones in the shape of counts so 32 by 27 times D counts sum",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 456,
      "text": "so this way we're times D counts sum",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 457,
      "text": "so this way we're times D counts sum",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 458,
      "text": "so this way we're letting the broadcasting here basically letting the broadcasting here basically letting the broadcasting here basically implement the replication you can look implement the replication you can look implement the replication you can look at it that way at it that way at it that way",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 459,
      "text": "but then we have to also be careful",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 460,
      "text": "but then we have to also be careful",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 461,
      "text": "but then we have to also be careful because decounts was already calculated because decounts was already calculated because decounts was already calculated we calculated earlier here and that was we calculated earlier here and that was we calculated earlier here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 462,
      "text": "and that was just the first branch",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 463,
      "text": "and we're now just the first branch and we're now just the first branch",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 464,
      "text": "and we're now finishing the second Branch",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 465,
      "text": "so we need finishing the second Branch",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 466,
      "text": "so we need finishing the second Branch",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 467,
      "text": "so we need to make sure that these gradients add so to make sure that these gradients add so to make sure that these gradients add so plus equals plus equals plus equals and",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 468,
      "text": "then here and then here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 469,
      "text": "and then here um let's comment out the comparison and um let's comment out the comparison and um let's comment out the comparison and let's make sure crossing fingers that we let's make sure crossing fingers that we let's make sure crossing fingers that we have the correct result so pytorch have the correct result so pytorch have the correct result so pytorch agrees with us on this gradient as well agrees with us on this gradient as well agrees with us on this gradient",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 470,
      "text": "as well okay hopefully we're getting a hang of",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 471,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 472,
      "text": "hopefully we're getting a hang of",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 473,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 474,
      "text": "hopefully we're getting a hang of this now counts as an element-wise X of this now counts as an element-wise X of this now counts as an element-wise X of Norm legits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 475,
      "text": "so now we want D Norm logits Norm legits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 476,
      "text": "so now we want D Norm logits Norm legits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 477,
      "text": "so now we want D Norm logits and because it's an element price and because it's an element price and because it's an element price operation everything is very simple what operation everything is very simple what operation everything is very simple what is the local derivative of e to the X is the local derivative of e to the X is the local derivative of e to the X",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 478,
      "text": "it's famously just e to the x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 479,
      "text": "so this is it's famously just e to the x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 480,
      "text": "so this is it's famously just e to the x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 481,
      "text": "so this is the local derivative that is the local derivative now we that is the local derivative now we already calculated it and it's inside already calculated it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 482,
      "text": "and it's inside already calculated it and it's inside counts",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 483,
      "text": "so we may as well potentially counts so we may as well potentially counts so we may as well potentially just reuse counts that is the local just reuse counts that is the local just reuse counts that is the local derivative derivative times uh D counts funny as that looks constant decount is funny as that looks constant decount is derivative on the normal objects and now derivative on the normal objects and now derivative on the normal objects and now let's erase this and let's verify and it let's erase this and let's verify and it let's erase this and let's verify and it looks good so that's uh normal agents so that's uh normal agents",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 484,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 485,
      "text": "so we are here on this line now",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 486,
      "text": "the okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 487,
      "text": "so we are here on this line now",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 488,
      "text": "the okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 489,
      "text": "so we are here on this line now the normal objects normal objects normal objects we have that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 490,
      "text": "and we're trying to we have that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 491,
      "text": "and we're trying to we have that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 492,
      "text": "and we're trying to calculate the logits and deloget Maxes calculate the logits and deloget Maxes calculate the logits and deloget Maxes so back propagating through this line so back propagating through this line so back propagating through this line now we have to be careful here because now we have to be careful here because now we have to be careful here because the shapes again are not the same",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 493,
      "text": "and so the shapes again are not the same",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 494,
      "text": "and so the shapes again are not the same",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 495,
      "text": "and so there's an implicit broadcasting there's an implicit broadcasting there's an implicit broadcasting Happening Here Happening Here Happening Here so normal jits has this shape 32 by 27 so normal jits has this shape 32 by 27 so normal jits has this shape 32 by 27 logist does as well but logit Maxis is logist does as well but logit Maxis is logist does as well but logit Maxis is only 32 by one so there's a broadcasting only 32 by one so there's a broadcasting only 32 by one so there's a broadcasting here in the minus here in the minus here in the minus now here I try to sort of write out a now here I try to sort of write out a now here I try to sort of write out a two example again we basically have that two example again we basically have that two example",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 496,
      "text": "again we basically have that this is our C equals a minus B this is our C equals a minus B this is our C equals a minus B and we see that because of the shape and we see that because of the shape and we see that because of the shape these are three by three but this one is these are three by three but this one is these are three by three but this one is just a column just a column just a column and so for example every element of C we",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 497,
      "text": "and so for example every element of C we and so for example every element of C we have to look at how it uh came to be and have to look at how it uh came to be and have to look at how it uh came to be and every element of C is just the every element of C is just the every element of C is just the corresponding element of a minus uh corresponding element of a minus uh corresponding element of a minus uh basically that associated b basically that associated b basically that associated b",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 498,
      "text": "so it's very clear now that the so it's very clear now that the so it's very clear now that the derivatives of every one of these c's derivatives of every one of these c's derivatives of every one of these c's with respect to their inputs are one for with respect to their inputs are one for with respect to their inputs are one for the corresponding a the corresponding a the corresponding a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 499,
      "text": "and it's a negative one for the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 500,
      "text": "and it's a negative one for the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 501,
      "text": "and it's a negative one for the corresponding B corresponding B corresponding B",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 502,
      "text": "and so therefore and so",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 503,
      "text": "therefore and so therefore um um the derivatives on the C will flow the derivatives on the C will flow the derivatives on the C will flow equally to the corresponding Ace and equally to the corresponding Ace and equally to the corresponding Ace and then also to the corresponding base but then also to the corresponding base but then also to the corresponding base but then in addition to that the B's are then in addition to that the B's are then in addition to that the B's are broadcast so we'll have to do the broadcast",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 504,
      "text": "so we'll have to do the broadcast",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 505,
      "text": "so we'll have to do the additional sum just like we did before additional sum just like we did before additional sum just like we did before",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 506,
      "text": "and of course the derivatives for B's and of course the derivatives for B's and of course the derivatives for B's will undergo a minus because the local will undergo a minus because the local will undergo a minus because the local derivative here is uh negative one derivative here is uh negative one derivative here is uh negative one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 507,
      "text": "so DC three two by D B3 is negative one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 508,
      "text": "so DC three two by D B3 is negative one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 509,
      "text": "so DC three two by D B3 is negative one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 510,
      "text": "so let's just Implement that basically so let's just Implement that basically so let's just Implement that basically delugits will be uh exactly copying the delugits will be uh exactly copying the delugits will be uh exactly copying the derivative on normal objects derivative on normal objects derivative on normal objects so so delugits equals the norm logits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 511,
      "text": "and I'll delugits equals the norm logits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 512,
      "text": "and I'll delugits equals the norm logits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 513,
      "text": "and I'll do a DOT clone for safety so we're just do a DOT clone for safety so we're just do a DOT clone for safety so we're just making a copy making a copy making a copy and then we have that the loaded Maxis",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 514,
      "text": "and then we have that the loaded Maxis",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 515,
      "text": "and then we have that the loaded Maxis will be the negative of the non-legits will be the negative of the non-legits will be the negative of the non-legits because of the negative sign because of the negative sign because of the negative sign",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 516,
      "text": "and then we have to be careful because and then we have to be careful because and then we have to be careful because logic Maxis is a column logic Maxis is a column logic Maxis is a column",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 517,
      "text": "and so just like we saw before because",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 518,
      "text": "and so just like we saw before because",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 519,
      "text": "and so just like we saw before because we keep replicating the same elements we keep replicating the same elements we keep replicating the same elements across all the columns across all the columns across all the columns then in the backward pass because we then in the backward pass because we then in the backward pass because we keep reusing this these are all just keep reusing this these are all just keep reusing this these are all just like separate branches of use of that like separate branches of use of that like separate branches of use of that one variable and so therefore we have to one variable",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 520,
      "text": "and so therefore we have to one variable",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 521,
      "text": "and so therefore we have to do a Sum along one would keep them do a Sum along one would keep them do a Sum along one would keep them equals true so that we don't destroy equals true so that we don't destroy equals true so that we don't destroy this dimension this dimension this dimension",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 522,
      "text": "and then the logic Maxes will be the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 523,
      "text": "and then the logic Maxes will be the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 524,
      "text": "and then the logic Maxes will be the same shape now we have to be careful same shape now we have to be careful same shape now we have to be careful because this deloaches is not the final because this deloaches is not the final because this deloaches is not the final deloaches and that's because not only do deloaches and that's because not only do deloaches and that's because not only do we get gradient signal into logits we get gradient signal into logits we get gradient signal into logits through here but the logic Maxes as a through here but the logic Maxes as a through here but the logic Maxes as a function of logits and that's a second function of logits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 525,
      "text": "and that's a second function of logits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 526,
      "text": "and that's a second Branch into logits so this is not yet Branch into logits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 527,
      "text": "so this is not yet Branch into logits so this is not yet our final derivative for logits we will our final derivative for logits we will our final derivative for logits we will come back later for the second branch come back later for the second branch come back later for the second branch for now the logic Maxis is the final for now the logic Maxis is the final for now the logic Maxis is the final derivative so let me uncomment this CMP derivative so let me uncomment this CMP derivative so let me uncomment this CMP here and let's just run this here and let's just run this here and let's just run this and logit Maxes hit by torch agrees with and logit Maxes hit by torch agrees with and logit Maxes hit by torch agrees with us us us so that was the derivative into through",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 528,
      "text": "so that was the derivative into through so that was the derivative into through this line this line this line now before we move on I want to pause now before we move on I want to pause now before we move on I want to pause here briefly",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 529,
      "text": "and I want to look at these here briefly",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 530,
      "text": "and I want to look at these here briefly",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 531,
      "text": "and I want to look at these logic Maxes and especially their logic Maxes and especially their logic Maxes and especially their gradients gradients gradients we've talked previously in the previous we've talked previously in the previous we've talked previously in the previous lecture that the only reason we're doing lecture that the only reason we're doing lecture that the only reason we're doing this is for the numerical stability of this is for the numerical stability of this is for the numerical stability of the softmax that we are implementing the softmax that we are implementing the softmax that we are implementing here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 532,
      "text": "and we talked about how if you take here and we talked about how if you take here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 533,
      "text": "and we talked about how if you take these logents for any one of these these logents for any one of these these logents for any one of these examples so one row of this logit's examples so one row of this logit's examples so one row of this logit's tensor if you add or subtract any value tensor if you add or subtract any value tensor if you add or subtract any value equally to all the elements then the equally to all the elements then the equally to all the elements then the value of the probes will be unchanged value of the probes will be unchanged value of the probes will be unchanged you're not changing soft Max",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 534,
      "text": "the only you're not changing soft Max the only you're not changing soft Max the only thing that this is doing is it's making thing that this is doing is it's making thing that this is doing is it's making sure that X doesn't overflow and the sure that X doesn't overflow and the sure that X doesn't overflow and the reason we're using a Max is because then reason we're using a Max is because then reason we're using a Max is because then we are guaranteed that each row of we are guaranteed that each row of we are guaranteed that each row of logits the highest number is zero and so logits the highest number is zero and so logits the highest number is zero and so this will be safe this will be safe this will be safe and",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 535,
      "text": "so",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 536,
      "text": "and so",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 537,
      "text": "and so um um basically what that has repercussions basically what that has repercussions basically what that has repercussions if it is the case that changing logit if it is the case that changing logit if it is the case that changing logit Maxis does not change the props and Maxis does not change the props and Maxis does not change the props and therefore there's not change the loss therefore there's not change the loss therefore there's not change the loss then the gradient on logic masses should then the gradient on logic masses should then the gradient on logic masses should be zero right because saying those two be zero right because saying those two be zero right because saying those two things is the same things is the same things is the same",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 538,
      "text": "so indeed we hope that this is very very so indeed we hope that this is very very so indeed we hope that this is very very small numbers so indeed we hope this is small numbers so indeed we hope this is small numbers so indeed we hope this is zero now because of floating Point uh zero now because of floating Point uh zero now because of floating Point uh sort of wonkiness sort of wonkiness sort of wonkiness um this doesn't come out exactly zero",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 539,
      "text": "um this doesn't come out exactly zero",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 540,
      "text": "um",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 541,
      "text": "this doesn't come out exactly zero only in some of the rows it does",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 542,
      "text": "but we only in some of the rows it does",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 543,
      "text": "but we only in some of the rows it does",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 544,
      "text": "but we get extremely small values like one e get extremely small values like one e get extremely small values like one e negative nine or ten and so this is negative nine or ten and so this is negative nine or ten and so this is telling us that the values of loaded telling us that the values of loaded telling us that the values of loaded Maxes are not impacting the loss as they Maxes are not impacting the loss as they Maxes are not impacting the loss as they shouldn't shouldn't shouldn't it feels kind of weird to back propagate it feels kind of weird to back propagate it feels kind of weird to back propagate through this branch honestly because through this branch honestly because through this branch honestly because if you have any implementation of like f if you have any implementation of like f if you have any implementation of like f dot cross entropy and pytorch and you dot cross entropy and pytorch and you dot cross entropy and pytorch and you you block together all these elements you block together all these elements you block together all these elements",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 545,
      "text": "and you're not doing the back and you're not doing the back",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 546,
      "text": "and you're not doing the back propagation piece by piece then you propagation piece by piece then you propagation piece by piece then you would probably assume that the would probably assume that the would probably assume that the derivative through here is exactly zero derivative through here is exactly zero derivative through here is exactly zero uh so you would be sort of uh so you would be sort of uh so you would be sort of um skipping this branch because it's um skipping this branch because it's um skipping this branch because it's only done for numerical stability but only done for numerical stability but only done for numerical stability but it's interesting to see that even if you it's interesting to see that even if you it's interesting to see that even if you break up everything into the full atoms break up everything into the full atoms break up everything into the full atoms and you still do the computation as and you still do the computation as and you still do the computation as you'd like with respect to numerical you'd like with respect to numerical you'd like with respect to numerical stability uh the correct thing happens stability uh the correct thing happens stability uh the correct thing happens and you still get a very very small",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 547,
      "text": "and you still get a very very small",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 548,
      "text": "and you still get a very very small gradients here gradients here gradients here um basically reflecting the fact that um basically reflecting the fact that um basically reflecting the fact that the values of these do not matter with the values of these do not matter with the values of these do not matter with respect to the final loss respect to the final loss respect to the final loss",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 549,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 550,
      "text": "so let's now continue back",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 551,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 552,
      "text": "so let's now continue back",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 553,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 554,
      "text": "so let's now continue back propagation through this line here we've propagation through this line here we've propagation through this line here we've just calculated the logit Maxis and now just calculated the logit Maxis and now just calculated the logit Maxis and now we want to back prop into logits through we want to back prop into logits through we want to back prop into logits through this second branch this second branch this second branch now here of course we took legits and we now here of course we took legits and we now here of course we took legits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 555,
      "text": "and we took the max along all the rows and then took the max along all the rows and then took the max along all the rows and then we looked at its values here now the way we looked at its values here now the way we looked at its values here now the way this works is that in pytorch this works is that in pytorch this works is that in pytorch this thing here this thing here this thing here the max returns both the values and it the max returns both the values and it the max returns both the values and it Returns the indices at which those Returns the indices at which those Returns the indices at which those values to count the maximum value values to count the maximum value values to count the maximum value now in the forward pass we only used now in the forward pass we only used now in the forward pass we only used values because that's all we needed but values because that's all we needed but values because that's all we needed but in the backward pass it's extremely in the backward pass it's extremely in the backward pass it's extremely useful to know about where those maximum useful to know about where those maximum useful to know about where those maximum values occurred and we have the indices values occurred and we have the indices values occurred and we have the indices at which they occurred and this will of at which they occurred and this will of at which they occurred and this will of course helps us to help us do the back course helps us to help us do the back course helps us to help us do the back propagation because what should the propagation because what should the propagation because what should the backward pass be here in this case we backward pass be here in this case we backward pass be here in this case we have the largest tensor which is 32 by have the largest tensor which is 32 by have the largest tensor which is 32 by 27 and in each row we find the maximum 27 and in each row we find the maximum 27 and in each row we find the maximum value and then that value gets plucked value and then that value gets plucked value and then that value gets plucked out into loaded Maxis and so intuitively out into loaded Maxis and so intuitively out into loaded Maxis and so intuitively um basically the derivative flowing um basically the derivative flowing um basically the derivative flowing through here then should be one through here then should be one through here then should be one times the look of derivatives is 1 for times the look of derivatives is 1 for times the look of derivatives is 1 for the appropriate entry that was plucked the appropriate entry that was plucked the appropriate entry that was plucked out out out and then times the global derivative of and then times the global derivative of and then times the global derivative of the logic axis the logic axis the logic axis",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 556,
      "text": "so really what we're doing here if you so really what we're doing here if you so really what we're doing here if you think through it is we need to take the think through it is we need to take the think through it is we need to take the deloachet Maxis and we need to scatter deloachet Maxis and we need to scatter deloachet Maxis and we need to scatter it to the correct positions in these it to the correct positions in these it to the correct positions in these logits from where the maximum values logits from where the maximum values logits from where the maximum values came came came",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 557,
      "text": "and so um I came up with one line of code sort of I came up with one line of code sort of I came up with one line of code sort of that does that let me just erase a bunch that does that let me just erase a bunch that does that let me just erase a bunch of stuff here so the line of uh you of stuff here so the line of uh you of stuff here so the line of uh you could do it kind of very similar to what could do it kind of very similar to what could do it kind of very similar to what we've done here where we create a zeros we've done here where we create a zeros we've done here where we create a zeros and then we populate uh the correct and then we populate uh the correct and then we populate uh the correct elements uh so we use the indices here elements uh so we use the indices here elements uh so we use the indices here and we would set them to be one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 558,
      "text": "but you and we would set them to be one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 559,
      "text": "but you and we would set them to be one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 560,
      "text": "but you can also use one hot can also use one hot can also use one hot so F dot one hot",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 561,
      "text": "and then I'm taking the so F dot one hot",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 562,
      "text": "and then I'm taking the so F dot one hot",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 563,
      "text": "and then I'm taking the lowest of Max over the First Dimension lowest of Max over the First Dimension lowest of Max over the First Dimension dot indices and I'm telling uh pytorch dot indices",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 564,
      "text": "and I'm telling uh pytorch dot indices",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 565,
      "text": "and I'm telling uh pytorch that the dimension of every one of these that the dimension of every one of these that the dimension of every one of these tensors should be tensors should be tensors should be um um 27",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 566,
      "text": "and so what this is going to do 27",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 567,
      "text": "and so what this is going to do 27",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 568,
      "text": "and so what this is going to do is okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 569,
      "text": "I apologize this is crazy filthy is",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 570,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 571,
      "text": "I apologize this is crazy filthy is",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 572,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 573,
      "text": "I apologize this is crazy filthy that I am sure of this that I am sure of this that I am sure of this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 574,
      "text": "it's really just a an array of where the it's really just a an array of where the it's really just a an array of where the Maxes came from in each row and that Maxes came from in each row and that Maxes came from in each row and that element is one and the all the other element is one and the all the other element is one and the all the other elements are zero",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 575,
      "text": "so it's a one-half elements are zero",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 576,
      "text": "so it's a one-half elements are zero",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 577,
      "text": "so it's a one-half Vector in each row and these indices are Vector in each row and these indices are Vector in each row and these indices are now populating a single one in the now populating a single one in the now populating a single one in the proper place proper place proper place",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 578,
      "text": "and then what I'm doing here is I'm",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 579,
      "text": "and then what I'm doing here is I'm",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 580,
      "text": "and then what I'm doing here is I'm multiplying by the logit Maxis and keep multiplying by the logit Maxis and keep multiplying by the logit Maxis and keep in mind that this is a column in mind that this is a column in mind that this is a column of 32 by 1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 581,
      "text": "and so when I'm doing this of 32 by 1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 582,
      "text": "and so when I'm doing this of 32 by 1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 583,
      "text": "and so when I'm doing this times the logic Maxis the logic Maxes times the logic Maxis the logic Maxes times the logic Maxis the logic Maxes will broadcast and that column will you will broadcast and that column will you will broadcast and that column will you know get replicated and in an element know get replicated and in an element know get replicated and in an element wise multiply will ensure that each of wise multiply will ensure that each of wise multiply will ensure that each of these just gets routed to whichever one these just gets routed to whichever one these just gets routed to whichever one of these bits is turned on of these bits is turned on of these bits is turned on and so that's another way to implement",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 584,
      "text": "and so that's another way to implement",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 585,
      "text": "and so that's another way to implement uh this kind of a this kind of a uh this kind of a this kind of a uh this kind of a this kind of a operation and both of these can be used operation and both of these can be used operation and both of these can be used I just thought I would show an I just thought I would show an I just thought I would show an equivalent way to do it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 586,
      "text": "and I'm using equivalent way to do it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 587,
      "text": "and I'm using equivalent way to do it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 588,
      "text": "and I'm using plus equals because we already plus equals because we already plus equals because we already calculated the logits here and this is calculated the logits here and this is calculated the logits here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 589,
      "text": "and this is not the second branch not the second branch not the second branch",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 590,
      "text": "so let's so let's so let's look at logits and make sure that this look at logits and make sure that this look at logits and make sure that this is correct is correct is correct",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 591,
      "text": "and we see that we have exactly the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 592,
      "text": "and we see that we have exactly the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 593,
      "text": "and we see that we have exactly the correct answer correct answer correct answer next up we want to continue with logits next up we want to continue with logits next up we want to continue with logits here that is an outcome of a matrix here that is an outcome of a matrix here that is an outcome of a matrix multiplication and a bias offset in this multiplication and a bias offset in this multiplication and a bias offset in this linear layer linear layer linear layer",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 594,
      "text": "so I've printed out the shapes of all so I've printed out the shapes of all so I've printed out the shapes of all these intermediate tensors we see that these intermediate tensors we see that these intermediate tensors we see that logits is of course 32 by 27 as we've logits is of course 32 by 27 as we've logits is of course 32 by 27 as we've just seen just seen just seen then the H here is 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 595,
      "text": "so these then the H here is 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 596,
      "text": "so these then the H here is 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 597,
      "text": "so these are 64 dimensional hidden States and are 64 dimensional hidden States and are 64 dimensional hidden States and then this W Matrix projects those 64 then this W Matrix projects those 64 then this W Matrix projects those 64 dimensional vectors into 27 dimensions dimensional vectors into 27 dimensions dimensional vectors into 27 dimensions and then there's a 27 dimensional offset",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 598,
      "text": "and then there's a 27 dimensional offset",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 599,
      "text": "and then there's a 27 dimensional offset which is a one-dimensional vector which is a one-dimensional vector which is a one-dimensional vector now we should note that this plus here now we should note that this plus here now we should note that this plus here actually broadcasts because H multiplied actually broadcasts because H multiplied actually broadcasts because H multiplied by by W2 will give us a 32 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 600,
      "text": "and so by by W2 will give us a 32 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 601,
      "text": "and so by by W2 will give us a 32 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 602,
      "text": "and so then this plus B2 is a 27 dimensional then this plus B2 is a 27 dimensional then this plus B2 is a 27 dimensional lecture here lecture here lecture here now in the rules of broadcasting what's now in the rules of broadcasting what's now in the rules of broadcasting what's going to happen with this bias Vector is going to happen with this bias Vector is going to happen with this bias Vector is that this one-dimensional Vector of 27 that this one-dimensional Vector of 27 that this one-dimensional Vector of 27 will get aligned with a padded dimension will get aligned with a padded dimension will get aligned with a padded dimension of one on the left and it will basically of one on the left",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 603,
      "text": "and it will basically of one on the left",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 604,
      "text": "and it will basically become a row vector and then it will get become a row vector and then it will get become a row vector and then it will get replicated vertically 32 times to make replicated vertically 32 times to make replicated vertically 32 times to make it 32 by 27 and then there's an it 32 by 27 and then there's an it 32 by 27 and then there's an element-wise multiply element-wise multiply element-wise multiply now now now the question is how do we back propagate the question is how do we back propagate the question is how do we back propagate from logits to the hidden States the from logits to the hidden States the from logits to the hidden States the weight Matrix W2 and the bias B2 weight Matrix W2 and the bias B2 weight Matrix W2 and the bias B2 and you might think that we need to go and you might think that we need to go and you might think that we need to go to some Matrix calculus",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 605,
      "text": "and then we have to some Matrix calculus",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 606,
      "text": "and then we have to some Matrix calculus",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 607,
      "text": "and then we have to look up the derivative for a matrix to look up the derivative for a matrix to look up the derivative for a matrix multiplication",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 608,
      "text": "but actually you don't multiplication",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 609,
      "text": "but actually you don't multiplication",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 610,
      "text": "but actually you don't have to do any of that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 611,
      "text": "and you can go have to do any of that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 612,
      "text": "and you can go have to do any of that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 613,
      "text": "and you can go back to First principles and derive this back to First principles and derive this back to First principles and derive this yourself on a piece of paper and yourself on a piece of paper and yourself on a piece of paper and specifically what I like to do",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 614,
      "text": "and I specifically what I like to do and I specifically what I like to do and I what I find works well for me is you what I find works well for me is you what I find works well for me is you find a specific small example that you find a specific small example that you find a specific small example that you then fully write out and then in the then fully write out and then in the then fully write out and then in the process of analyzing how that individual process of analyzing how that individual process of analyzing how that individual small example works you will understand small example works you will understand small example works you will understand the broader pattern and you'll be able the broader pattern",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 615,
      "text": "and you'll be able the broader pattern",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 616,
      "text": "and you'll be able to generalize and write out the full to generalize and write out the full to generalize and write out the full general formula for what how these general formula for what how these general formula for what how these derivatives flow in an expression like derivatives flow in an expression like derivatives flow in an expression like this so let's try that out this so let's try that out this so let's try that out so pardon the low budget production here so pardon the low budget production here so pardon the low budget production here but what I've done here is I'm writing but what I've done here is I'm writing but what I've done here is I'm writing it out on a piece of paper really what it out on a piece of paper really what it out on a piece of paper really what we are interested in is we have a we are interested in is we have a we are interested in is we have a multiply B plus C and that creates a d multiply B plus C and that creates a d multiply B plus C and that creates a d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 617,
      "text": "and we have the derivative of the loss and we have the derivative of the loss and we have the derivative of the loss with respect to D",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 618,
      "text": "and we'd like to know with respect to D",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 619,
      "text": "and we'd like to know with respect to D",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 620,
      "text": "and we'd like to know what the derivative of the losses with what the derivative of the losses with what the derivative of the losses with respect to a b and c respect to a b and c respect to a b and c now these here are little now these here are little now these here are little two-dimensional examples of a matrix two-dimensional examples of a matrix two-dimensional examples of a matrix multiplication",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 621,
      "text": "Two by Two Times a two by multiplication Two by Two Times a two by multiplication Two by Two Times a two by two two two plus a 2 a vector of just two elements plus a 2 a vector of just two elements plus a 2 a vector of just two elements C1 and C2 gives me a two by two C1 and C2 gives me a two by two C1 and C2 gives me a two by two now notice here that I have a bias now notice here that I have a bias now notice here that I have a bias Vector here called C and the bisex Vector here called C and the bisex Vector here called C and the bisex vector is C1 and C2 but as I described vector is C1 and C2 but as I described vector is C1 and C2 but as I described over here that bias Vector will become a over here that bias Vector will become a over here that bias Vector will become a row Vector in the broadcasting and will row Vector in the broadcasting and will row Vector in the broadcasting and will replicate vertically so that's what's replicate vertically",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 622,
      "text": "so that's what's replicate vertically so that's what's happening here as well C1 C2 is happening here as well C1 C2 is happening here as well C1 C2 is replicated vertically and we see how we replicated vertically",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 623,
      "text": "and we see how we replicated vertically and we see how we have two rows of C1 C2 as a result have two rows of C1 C2 as a result have two rows of C1 C2 as a result so now when I say write it out",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 624,
      "text": "I just so now when I say write it out",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 625,
      "text": "I just so now when I say write it out",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 626,
      "text": "I just mean like this basically break up this mean like this basically break up this mean like this basically break up this matrix multiplication into the actual matrix multiplication into the actual matrix multiplication into the actual thing that that's going on under the thing that that's going on under the thing that that's going on under the hood",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 627,
      "text": "so as a result of matrix hood",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 628,
      "text": "so as a result of matrix hood so as a result of matrix multiplication and how it works d11 is multiplication and how it works d11 is multiplication and how it works d11 is the result of a DOT product between the the result of a DOT product between the the result of a DOT product between the first row of a and the First Column of B first row of a and the First Column of B first row of a and the First Column of B so a11 b11 plus a12 B21 plus C1 so a11 b11 plus a12 B21 plus C1 so a11 b11 plus a12 B21 plus C1 and so on so forth for all the other and so on so forth for all the other and so on so forth for all the other elements of D and once you actually elements of D and once you actually elements of D and once you actually write it out it becomes obvious this is write it out it becomes obvious this is write it out it becomes obvious this is just a bunch of multipliers and just a bunch of multipliers and just a bunch of multipliers and um adds and we know from micrograd how um adds and we know from micrograd how um adds and we know from micrograd how to differentiate multiplies and adds and to differentiate multiplies and adds and to differentiate multiplies and adds and so this is not scary anymore it's not so this is not scary anymore it's not so this is not scary anymore it's not just matrix multiplication it's just uh just matrix multiplication it's just uh just matrix multiplication it's just uh tedious unfortunately but this is tedious unfortunately but this is tedious unfortunately but this is completely tractable we have DL by D for completely tractable we have DL by D for completely tractable we have DL by D for all of these and we want DL by uh all all of these and we want DL by uh all all of these and we want DL by uh all these little other variables",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 629,
      "text": "so how do these little other variables so how do these little other variables so how do we achieve that and how do we actually we achieve that and how do we actually we achieve that and how do we actually get the gradients",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 630,
      "text": "okay so the low budget get the gradients",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 631,
      "text": "okay so the low budget get the gradients",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 632,
      "text": "okay so the low budget production continues here production continues here production continues here so let's for example derive the so let's for example derive the so let's for example derive the derivative of the loss with respect to derivative of the loss with respect to a11 a11 a11 we see here that a11 occurs twice in our we see here that a11 occurs twice in our we see here that a11 occurs twice in our simple expression right here right here simple expression right here right here simple expression right here right here and influences d11 and D12 and influences d11 and D12 and influences d11 and D12 .",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 633,
      "text": "so this is so what is DL by d a one .",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 634,
      "text": "so this is so what is DL by d a one .",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 635,
      "text": "so this is so what is DL by d a one one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 636,
      "text": "well it's DL by d11 times the local one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 637,
      "text": "well it's DL by d11 times the local one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 638,
      "text": "well it's DL by d11 times the local derivative of d11 which in this case is derivative of d11 which in this case is derivative of d11 which in this case is just b11 because that's what's just b11 because that's what's just b11 because that's what's multiplying a11 here multiplying a11 here multiplying a11 here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 639,
      "text": "so uh and likewise here the local",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 640,
      "text": "so uh and likewise here the local",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 641,
      "text": "so uh and likewise here the local derivative of D12 with respect to a11 is derivative of D12 with respect to a11 is derivative of D12 with respect to a11 is just B12 and so B12 well in the chain just B12 and so B12 well in the chain just B12 and so B12 well in the chain rule therefore multiply the L by d 1 2. rule therefore multiply the L by d 1 2. rule therefore multiply the L by d 1 2.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 642,
      "text": "and then because a11 is used both to and then because a11 is used both to and then because a11 is used both to produce d11 and D12 we need to add up produce d11 and D12 we need to add up produce d11 and D12 we need to add up the contributions of both of those sort the contributions of both of those sort the contributions of both of those sort of chains that are running in parallel of chains that are running in parallel of chains that are running in parallel and that's why we get a plus just adding and that's why we get a plus just adding and that's why we get a plus just adding up those two up those two up those two um those two contributions and that um those two contributions and that um those two contributions and that gives us DL by d a one one we can do the gives us DL by d a one one we can do the gives us DL by d a one one we can do the exact same analysis for the other one exact same analysis for the other one exact same analysis for the other one for all the other elements of a and when for all the other elements of a and when for all the other elements of a and when you simply write it out it's just super you simply write it out",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 643,
      "text": "it's just super you simply write it out",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 644,
      "text": "it's just super simple simple simple um taking of gradients on you know um taking of gradients on you know um taking of gradients on you know expressions like this expressions like this expressions like this you find that you find that you find that this Matrix DL by D A that we're after this Matrix DL by D A that we're after this Matrix DL by D A that we're after right if we just arrange all the all of right if we just arrange all the all of right if we just arrange all the all of them in the same shape as a takes",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 645,
      "text": "so a them in the same shape as a takes so a them in the same shape as a takes so a is just too much Matrix",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 646,
      "text": "so d l by D A is just too much Matrix",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 647,
      "text": "so d l by D A is just too much Matrix",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 648,
      "text": "so d l by D A here will be also just the same shape here will be also just the same shape here will be also just the same shape tester with the derivatives now so deal tester with the derivatives now so deal tester with the derivatives now so deal by D a11 Etc by D a11 Etc by D a11 Etc and we see that actually we can express and we see that actually we can express",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 649,
      "text": "and we see that actually we can express what we've written out here as a matrix what we've written out here as a matrix what we've written out here as a matrix multiplied multiplied multiplied",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 650,
      "text": "and so it just so happens that D all by",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 651,
      "text": "and so it just so happens that D all by",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 652,
      "text": "and so it just so happens that D all by that all of these formulas that we've that all of these formulas that we've that all of these formulas that we've derived here by taking gradients can derived here by taking gradients can derived here by taking gradients can actually be expressed as a matrix actually be expressed as a matrix actually be expressed as a matrix multiplication and in particular we see multiplication and in particular we see multiplication and in particular we see that it is the matrix multiplication of that it is the matrix multiplication of that it is the matrix multiplication of these two array matrices these two array matrices these two array matrices",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 653,
      "text": "so it is the um DL by D and then Matrix",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 654,
      "text": "so it is the um DL by D and then Matrix",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 655,
      "text": "so it is the um DL by D and then Matrix multiplying B but B transpose actually multiplying B but B transpose actually multiplying B but B transpose",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 656,
      "text": "actually so you see that B21 and b12 have changed so you see that B21 and b12 have changed so you see that B21 and b12 have changed place place place whereas before we had of course b11 B12 whereas before we had of course b11 B12 whereas before we had of course b11 B12 B2 on B22 so you see that this other B2 on B22 so you see that this other B2 on B22 so you see that this other Matrix B is transposed Matrix B is transposed Matrix B is transposed and so basically what we have long story and so basically what we have long story and so basically what we have long story short just by doing very simple short just by doing very simple short just by doing very simple reasoning here by breaking up the reasoning here by breaking up the reasoning here by breaking up the expression in the case of a very simple expression in the case of a very simple expression in the case of a very simple example is that DL by d a is which is example is that DL by d a is which is example is that DL by d a is which is this is simply equal to DL by DD Matrix this is simply equal to DL by DD Matrix this is simply equal to DL by DD Matrix multiplied with B transpose so that is what we have so far now we so that is what we have so far now we also want the derivative with respect to also want the derivative with respect to also want the derivative with respect to um B and C now um B and C now um B and C now for B",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 657,
      "text": "I'm not actually doing the full for B",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 658,
      "text": "I'm not actually doing the full for B",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 659,
      "text": "I'm not actually doing the full derivation because honestly it's um it's derivation because honestly it's um it's derivation because honestly it's um",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 660,
      "text": "it's not deep",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 661,
      "text": "it's just uh annoying it's not deep",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 662,
      "text": "it's just uh annoying it's not deep",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 663,
      "text": "it's just uh annoying it's exhausting you can actually do this exhausting you can actually do this exhausting you can actually do this analysis yourself you'll also find that analysis yourself you'll also find that analysis yourself you'll also find that if you take this these expressions and if you take this these expressions and if you take this these expressions and you differentiate with respect to b you differentiate with respect to b you differentiate with respect to b instead of a you will find that DL by DB instead of a you will find that DL by DB instead of a you will find that DL by DB is also a matrix multiplication in this is also a matrix multiplication in this is also a matrix multiplication in this case you have to take the Matrix a and case you have to take the Matrix a and case you have to take the Matrix a and transpose it and Matrix multiply that transpose it and Matrix multiply that transpose it and Matrix multiply that with bl by DD with bl by DD with bl by DD and that's what gives you a deal by DB and that's what gives you a deal by DB and that's what gives you a deal by DB and then here for the offsets C1 and C2 and then here for the offsets C1 and C2 and then here for the offsets C1 and C2 if you again just differentiate with if you again just differentiate with if you again just differentiate with respect to C1 you will find an respect to C1 you will find an respect to C1 you will find an expression like this expression like this expression like this and C2 an expression like this and C2 an expression like this and C2 an expression like this and basically you'll find the DL by DC and basically you'll find the DL by DC and basically you'll find the DL by DC is simply because they're just is simply because they're just is simply because they're just offsetting these Expressions you just offsetting these Expressions you just offsetting these Expressions you just have to take the deal by DD Matrix have to take the deal by DD Matrix have to take the deal by DD Matrix of the derivatives of D and you just of the derivatives of D and you just of the derivatives of D and you just have to sum across the columns and that have to sum across the columns and that have to sum across the columns and that gives you the derivatives for C gives you the derivatives for C gives you the derivatives for C so long story short so long story short so long story short the backward Paths of a matrix multiply the backward Paths of a matrix multiply the backward Paths of a matrix multiply is a matrix multiply is a matrix multiply is a matrix multiply and instead of just like we had D equals and instead of just like we had D equals and instead of just like we had D equals a times B plus C in the scalar case uh a times B plus C in the scalar case uh a times B plus C in the scalar case uh we sort of like arrive at something very we sort of like arrive at something very we sort of like arrive at something very very similar but now uh with a matrix very similar but now uh with a matrix very similar but now uh with a matrix multiplication instead of a scalar multiplication instead of a scalar multiplication instead of a scalar multiplication multiplication so the derivative of D with respect to a so the derivative of D with respect to a so the derivative of D with respect to a is is is DL by DD Matrix multiplied B trespose DL by DD Matrix multiplied B trespose DL by DD Matrix multiplied B trespose and here it's a transpose multiply deal",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 664,
      "text": "and here it's a transpose multiply deal",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 665,
      "text": "and here it's a transpose multiply deal by DD",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 666,
      "text": "but in both cases it's a matrix by DD but in both cases it's a matrix by DD but in both cases it's a matrix multiplication with the derivative and multiplication with the derivative and multiplication with the derivative and the other term in the multiplication the other term in the multiplication the other term in the multiplication and for C it is a sum and for C",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 667,
      "text": "it is a sum and for C it is a sum now I'll tell you a secret I can never now I'll tell you a secret I can never now I'll tell you a secret I can never remember the formulas that we just remember the formulas that we just remember the formulas that we just arrived for back proper gain information arrived for back proper gain information arrived for back proper gain information multiplication and I can back propagate multiplication and I can back propagate multiplication and I can back propagate through these Expressions just fine and through these Expressions just fine and through these Expressions just fine and the reason this works is because the the reason this works is because the the reason this works is because the dimensions have to work out dimensions have to work out dimensions have to work out uh so let me give you an example say I uh so let me give you an example say I uh so let me give you an example",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 668,
      "text": "say I want to create DH want to create DH want to create DH then what should the H be number one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 669,
      "text": "I then what should the H be number one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 670,
      "text": "I then what should the H be number one I have to know that the shape of DH must have to know that the shape of DH must have to know that the shape of DH must be the same as the shape of H be the same as the shape of H be the same as the shape of H and the shape of H is 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 671,
      "text": "and then and the shape of H is 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 672,
      "text": "and then and the shape of H is 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 673,
      "text": "and then the other piece of information I know is the other piece of information I know is the other piece of information I know is that DH must be some kind of matrix that DH must be some kind of matrix that DH must be some kind of matrix multiplication of the logits with W2 multiplication of the logits with W2 multiplication of the logits with W2 and delojits is 32 by 27 and W2 is a 64 and delojits is 32 by 27 and W2 is a 64 and delojits is 32 by 27 and W2 is a 64 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 674,
      "text": "there is only a single way to by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 675,
      "text": "there is only a single way to by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 676,
      "text": "there is only a single way to make the shape work out in this case and make the shape work out in this case and make the shape work out in this case and it is indeed the correct result in it is indeed the correct result in it is indeed the correct result in particular here H needs to be 32 by 64. particular here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 677,
      "text": "H needs to be 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 678,
      "text": "particular here H needs to be 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 679,
      "text": "the only way to achieve that is to take the only way to achieve that is to take the only way to achieve that is to take a deluges a deluges a deluges and Matrix multiply it with you see how and Matrix multiply it with you see how and Matrix multiply it with you see how I have to take W2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 680,
      "text": "but I have to I have to take W2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 681,
      "text": "but I have to I have to take W2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 682,
      "text": "but I have to transpose it to make the dimensions work transpose it to make the dimensions work transpose it to make the dimensions work out out",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 683,
      "text": "so w to transpose",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 684,
      "text": "and it's the only way so w to transpose",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 685,
      "text": "and it's the only way so w to transpose",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 686,
      "text": "and it's the only way to make these to Matrix multiply those to make these to Matrix multiply those to make these to Matrix multiply those two pieces to make the shapes work out two pieces to make the shapes work out two pieces to make the shapes work out and that turns out to be the correct and that turns out to be the correct and that turns out to be the correct formula",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 687,
      "text": "so if we come here we want DH formula so if we come here we want DH formula",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 688,
      "text": "so if we come here we want DH which is d a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 689,
      "text": "and we see that d a is DL which is d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 690,
      "text": "a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 691,
      "text": "and we see that d a is DL which is d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 692,
      "text": "a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 693,
      "text": "and we see that d a is DL by DD Matrix multiply B transpose by DD Matrix multiply B transpose by DD Matrix multiply B transpose so that's Delo just multiply and B is W2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 694,
      "text": "so that's Delo just multiply and B is W2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 695,
      "text": "so that's Delo just multiply and B is W2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 696,
      "text": "so W2 transpose which is exactly what we so W2 transpose which is exactly what we so W2 transpose which is exactly what we have here so there's no need to remember have here so there's no need to remember have here so there's no need to remember these formulas similarly now if I want these formulas similarly now if I want these formulas similarly now if I want dw2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 697,
      "text": "well I know that it must be a matrix dw2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 698,
      "text": "well I know that it must be a matrix dw2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 699,
      "text": "well I know that it must be a matrix multiplication of D logits and H multiplication of D logits and H multiplication of D logits and H",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 700,
      "text": "and maybe there's a few transpose like and maybe there's a few transpose like and maybe there's a few transpose like there's one transpose in there as well there's one transpose in there as well there's one transpose in there as well",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 701,
      "text": "and I don't know which way it is so",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 702,
      "text": "I and I don't know which way it is so",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 703,
      "text": "I and I don't know which way it is so I have to come to W2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 704,
      "text": "and I see that its have to come to W2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 705,
      "text": "and I see that its have to come to W2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 706,
      "text": "and I see that its shape is 64 by 27 shape is 64 by 27 shape is 64 by 27 and that has to come from some interest and that has to come from some interest and that has to come from some interest multiplication of these two multiplication of these two multiplication of these two and so to get a 64 by 27 I need to take and so to get a 64 by 27 I need to take and so to get a 64 by 27 I need to take um um H I need to transpose it H I need to transpose it H I need to transpose it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 707,
      "text": "and then I need to Matrix multiply it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 708,
      "text": "and then I need to Matrix multiply it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 709,
      "text": "and then I need to Matrix multiply it um so that will become 64 by 32 and then um so that will become 64 by 32 and then um so that will become 64 by 32",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 710,
      "text": "and then I need to make sure to multiply with the I need to make sure to multiply with the I need to make sure to multiply with the 32 by 27 and that's going to give me a 32 by 27 and that's going to give me a 32 by 27 and that's going to give me a 64 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 711,
      "text": "so I need to make sure it's 64 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 712,
      "text": "so I need to make sure it's 64 by 27.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 713,
      "text": "so I need to make sure it's multiplied this with the logist that multiplied this with the logist that multiplied this with the logist that shape just like that that's the only way shape just like that that's the only way shape just like that that's the only way to make the dimensions work out and just to make the dimensions work out and just to make the dimensions work out and just use matrix multiplication and if we come use matrix multiplication and if we come use matrix multiplication and if we come here we see that that's exactly what's here we see that that's exactly what's here we see that that's exactly what's here so a transpose a for us is H here so a transpose a for us is H here so a transpose a for us is H multiplied with deloaches multiplied with deloaches multiplied with deloaches",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 714,
      "text": "so that's W2 and then db2 so that's W2 and then db2 so that's W2 and then db2 is just the um is just the um is just the um vertical sum and actually in the same vertical sum and actually in the same vertical sum and actually in the same way there's only one way to make the way there's only one way to make the way there's only one way to make the shapes work out I don't have to remember shapes work out I don't have to remember shapes work out I don't have to remember that it's a vertical Sum along the zero that it's a vertical Sum along the zero that it's a vertical Sum along the zero axis because that's the only way that axis because that's the only way that axis because that's the only way that this makes sense because B2 shape is 27 this makes sense because B2 shape is 27 this makes sense because B2 shape is 27 so in order to get a um delugits so in order to get a um delugits so in order to get a um delugits here is 30 by 27 so knowing that it's here is 30 by 27 so knowing that it's here is 30 by 27 so knowing that it's just sum over deloaches in some just sum over deloaches in some just sum over deloaches in some Direction that direction must be zero because I that direction must be zero because I need to eliminate this Dimension",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 715,
      "text": "so it's need to eliminate this Dimension",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 716,
      "text": "so it's need to eliminate this Dimension",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 717,
      "text": "so it's this this this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 718,
      "text": "so this is so let's kind of like the so this is so let's kind of like the so this is so let's kind of like the hacky way let me copy paste and delete hacky way let me copy paste and delete hacky way let me copy paste and delete that and let me swing over here and this that and let me swing over here and this that and let me swing over here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 719,
      "text": "and this is our backward pass for the linear is our backward pass for the linear is our backward pass for the linear layer uh hopefully layer uh hopefully layer",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 720,
      "text": "uh hopefully so now let's uncomment so now let's uncomment so now let's uncomment these three",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 721,
      "text": "and we're checking that we these three",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 722,
      "text": "and we're checking that we these three",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 723,
      "text": "and we're checking that we got all the three derivatives correct got all the three derivatives correct got all the three derivatives correct and run and run and run and we see that h wh and B2 are all",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 724,
      "text": "and we see that h wh and B2 are all",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 725,
      "text": "and we see that h wh and B2 are all exactly correct so we back propagated exactly correct",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 726,
      "text": "so we back propagated exactly correct so we back propagated through a linear layer now next up we have derivative for the h now next up we have derivative for the h already",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 727,
      "text": "and we need to back propagate already and we need to back propagate already",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 728,
      "text": "and we need to back propagate through 10h into h preact through 10h into h preact through 10h into h preact so we want to derive DH preact so we want to derive DH preact so we want to derive DH preact",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 729,
      "text": "and here we have to back propagate",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 730,
      "text": "and here we have to back propagate",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 731,
      "text": "and here we have to back propagate through a 10 H",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 732,
      "text": "and we've already done through a 10 H and we've already done through a 10 H and we've already done this in micrograd",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 733,
      "text": "and we remember that this in micrograd and we remember that this in micrograd and we remember that 10h has a very simple backward formula 10h has a very simple backward formula 10h has a very simple backward formula now unfortunately if I just put in D by now unfortunately if I just put in D by now unfortunately if I just put in D by DX of 10 h of X into both from alpha it DX of 10 h of X into both from alpha it DX of 10 h of X into both from alpha it lets us down it tells us that it's a lets us down it tells us that it's a lets us down it tells us that it's a hyperbolic secant function squared of X hyperbolic secant function squared of X hyperbolic secant function squared of X it's not exactly helpful",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 734,
      "text": "but luckily it's not exactly helpful",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 735,
      "text": "but luckily it's not exactly helpful but luckily Google image search does not let us down Google image search does not let us down Google image search does not let us down and it gives us the simpler formula and and it gives us the simpler formula and and it gives us the simpler formula and in particular if you have that a is in particular if you have that a is in particular if you have that a is equal to 10 h of Z then d a by DZ by equal to 10 h of Z then d a by DZ by equal to 10 h of Z then d a by DZ by propagating through 10 H is just one propagating through 10 H is just one propagating through 10 H is just one minus a square and take note that 1 minus a square and take note that 1 minus a square and take note that 1 minus a square a here is the output of minus a square a here is the output of minus a square a here is the output of the 10h not the input to the 10h Z",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 736,
      "text": "so the 10h not the input to the 10h Z",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 737,
      "text": "so the 10h not the input to the 10h Z",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 738,
      "text": "so the D A by DZ is here formulated in the D A by DZ is here formulated in the D A by DZ is here formulated in terms of the output of that 10h terms of the output of that 10h terms of the output of that 10h and here also in Google image search we and here also in Google image search we and here also in Google image search we have the full derivation if you want to have the full derivation if you want to have the full derivation if you want to actually take the actual definition of actually take the actual definition of actually take the actual definition of 10h and work through the math to figure 10h and work through the math to figure 10h and work through the math to figure out 1 minus standard square of Z out 1 minus standard square of Z out 1 minus standard square of Z so 1 minus a square is the local so 1 minus a square is the local so 1 minus a square is the local derivative in our case that is 1 minus derivative in our case that is 1 minus derivative in our case that is 1 minus uh the output of 10 H squared which here uh the output of 10 H squared which here uh the output of 10 H squared which here is H is H is H",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 739,
      "text": "so it's h squared and that is the local",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 740,
      "text": "so it's h squared and that is the local",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 741,
      "text": "so it's h squared and that is the local derivative and then times the chain rule derivative and then times the chain rule derivative and then times the chain rule DH DH DH",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 742,
      "text": "so that is going to be our candidate so that is going to be our candidate so that is going to be our candidate implementation",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 743,
      "text": "so if we come here implementation so if we come here implementation so if we come here and then uncomment this let's hope for and then uncomment this let's hope for and then uncomment this let's hope for the best the best the best and we have the right answer and we have the right answer and we have the right answer",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 744,
      "text": "okay next up we have DH preact",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 745,
      "text": "and we okay next up we have DH preact",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 746,
      "text": "and we okay next up we have DH preact",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 747,
      "text": "and we want to back propagate into the gain the want to back propagate into the gain the want to back propagate into the gain the B and raw and the B and bias B and raw and the B and bias B and raw and the B and bias so here this is the bathroom parameters so here this is the bathroom parameters so here this is the bathroom parameters being gained in bias inside the bash being gained in bias inside the bash being gained in bias inside the bash term that take the B and raw that is term that take the B and raw that is term that take the B and raw that is exact unit caution and then scale it and exact unit caution and then scale it and exact unit caution and then scale it and shift it shift it shift it and these are the parameters of The and these are the parameters of The and these are the parameters of The Bachelor now here we have a Bachelor now here we have a Bachelor now here we have a multiplication",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 748,
      "text": "but it's worth noting multiplication",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 749,
      "text": "but it's worth noting multiplication but it's worth noting that this multiply is very very that this multiply is very very that this multiply is very very different from this Matrix multiply here different from this Matrix multiply here different from this Matrix multiply here Matrix multiply are DOT products between Matrix multiply are DOT products between Matrix multiply are DOT products between rows and Columns of these matrices rows and Columns of these matrices rows and Columns of these matrices involved this is an element twice involved this is an element twice involved this is an element twice multiply",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 750,
      "text": "so things are quite a bit multiply so things are quite a bit multiply",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 751,
      "text": "so things are quite a bit simpler simpler simpler now we do have to be careful with some now we do have to be careful with some now we do have to be careful with some of the broadcasting happening in this of the broadcasting happening in this of the broadcasting happening in this line of code though",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 752,
      "text": "so you see how BN line of code though",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 753,
      "text": "so you see how BN line of code though",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 754,
      "text": "so you see how BN gain and B and bias are 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 755,
      "text": "but H gain and B and bias are 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 756,
      "text": "but H gain and B and bias are 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 757,
      "text": "but H preact and B and raw are 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 758,
      "text": "preact and B and raw are 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 759,
      "text": "preact and B and raw are 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 760,
      "text": "so we have to be careful with that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 761,
      "text": "and so we have to be careful with that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 762,
      "text": "and so we have to be careful with that and make sure that all the shapes work out make sure that all the shapes work out make sure that all the shapes work out fine and that the broadcasting is fine and that the broadcasting is fine and that the broadcasting is correctly back propagated correctly back propagated correctly back propagated so in particular let's start with the B so in particular let's start with the B so in particular let's start with the B and Gain so DB and gain should be and Gain so DB and gain should be and Gain so DB and gain should be and here this is again elementorized and here this is again elementorized and here this is again elementorized multiply",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 763,
      "text": "and whenever we have a times b multiply and whenever we have a times b multiply and whenever we have a times b equals c we saw that the local equals c we saw that the local equals c we saw that the local derivative here is just if this is a the derivative here is just if this is a the derivative here is just if this is a the local derivative is just the B the other local derivative is just the B the other local derivative is just the B the other one so the local derivative is just B one so the local derivative is just B one so the local derivative is just B and raw and then times chain rule and raw and then times chain rule and raw and then times chain rule so DH preact so DH preact so DH preact so this is the candidate gradient now",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 764,
      "text": "so this is the candidate gradient now",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 765,
      "text": "so this is the candidate gradient now",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 766,
      "text": "again we have to be careful because B",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 767,
      "text": "again we have to be careful because B",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 768,
      "text": "again we have to be careful because B and Gain Is of size 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 769,
      "text": "but this and Gain Is of size 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 770,
      "text": "but this and Gain Is of size 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 771,
      "text": "but this here would be 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 772,
      "text": "here would be 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 773,
      "text": "here would be 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 774,
      "text": "and so and so um the correct thing to do in this case um the correct thing to do in this case um the correct thing to do in this case of course is that b and gain here is a of course is that b and gain here is a of course is that b and gain here is a rule Vector of 64 numbers it gets rule Vector of 64 numbers it gets rule Vector of 64 numbers it gets replicated vertically in this operation replicated vertically in this operation replicated vertically in this operation",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 775,
      "text": "and so therefore the correct thing to do",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 776,
      "text": "and so therefore the correct thing to do",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 777,
      "text": "and so therefore the correct thing to do is to sum because it's being replicated is to sum because it's being replicated is to sum because it's being replicated and therefore all the gradients in each and therefore all the gradients in each and therefore all the gradients in each of the rows that are now flowing of the rows that are now flowing of the rows that are now flowing backwards need to sum up to that same backwards need to sum up to that same backwards need to sum up to that same tensor DB and Gain so we have to sum tensor DB and Gain",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 778,
      "text": "so we have to sum tensor DB and Gain so we have to sum across all the zero all the examples across all the zero all the examples across all the zero all the examples basically basically basically which is the direction in which this which is the direction in which this which is the direction in which this gets replicated gets replicated gets replicated and now we have to be also careful and now we have to be also careful and now we have to be also careful because we because we because we um being gain is of shape 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 779,
      "text": "so in um being gain is of shape 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 780,
      "text": "so in um being gain is of shape 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 781,
      "text": "so in fact I need to keep them as true fact I need to keep them as true fact I need to keep them as true",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 782,
      "text": "otherwise I would just get 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 783,
      "text": "otherwise I would just get 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 784,
      "text": "otherwise I would just get 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 785,
      "text": "now I don't actually really remember why now I don't actually really remember why now I don't actually really remember why the being gain and the BN bias I made the being gain and the BN bias I made the being gain and the BN bias I made them be 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 786,
      "text": "them be 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 787,
      "text": "them be 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 788,
      "text": "um um but the biases B1 and B2 I just made but the biases B1 and B2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 789,
      "text": "I just made but the biases B1 and B2 I just made them be one-dimensional vectors they're them be one-dimensional vectors they're them be one-dimensional vectors they're not two-dimensional tensors",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 790,
      "text": "so I can't not two-dimensional tensors",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 791,
      "text": "so I can't not two-dimensional tensors",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 792,
      "text": "so I can't recall exactly why I left the gain and recall exactly why I left the gain and recall exactly why I left the gain and the bias as two-dimensional",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 793,
      "text": "but it the bias as two-dimensional",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 794,
      "text": "but it the bias as two-dimensional",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 795,
      "text": "but it doesn't really matter as long as you are doesn't really matter as long as you are doesn't really matter as long as you are consistent and you're keeping it the consistent and you're keeping it the consistent and you're keeping it the same same same",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 796,
      "text": "so in this case we want to keep the so in this case we want to keep the so in this case we want to keep the dimension so that the tensor shapes work dimension so that the tensor shapes work dimension so that the tensor shapes work next up we have B and raw so DB and raw next up we have B and raw so DB and raw next up we have B and raw so DB and raw will be BN gain will be BN gain will be BN gain multiplying multiplying multiplying dhreact",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 797,
      "text": "that's our chain rule now what dhreact that's our chain rule now what dhreact that's our chain rule now what about the about the about the um um dimensions of this we have to be careful dimensions of this we have to be careful dimensions of this we have to be careful",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 798,
      "text": "right so DH preact is 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 799,
      "text": "B and right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 800,
      "text": "so DH preact is 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 801,
      "text": "B and right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 802,
      "text": "so DH preact is 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 803,
      "text": "B and gain is 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 804,
      "text": "so it will just get gain is 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 805,
      "text": "so it will just get gain is 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 806,
      "text": "so it will just get replicated and to create this replicated and to create this replicated and to create this multiplication which is the correct multiplication which is the correct multiplication which is the correct thing because in a forward pass it also thing because in a forward pass it also thing because in a forward pass it also gets replicated in just the same way gets replicated in just the same way gets replicated in just the same way so in fact we don't need the brackets so in fact we don't need the brackets so in fact we don't need the brackets here we're done here we're done here we're done and the shapes are already correct and the shapes are already correct and the shapes are already correct and finally for the bias and finally for the bias and finally for the bias very similar this bias here is very very very similar this bias here is very very very similar this bias here is very very similar to the bias we saw when you similar to the bias we saw when you similar to the bias we saw when you layer in the linear layer and we see layer in the linear layer and we see layer in the linear layer and we see that the gradients from each preact will that the gradients from each preact will that the gradients from each preact will simply flow into the biases and add up simply flow into the biases and add up simply flow into the biases and add up because these are just these are just because these are just these are just because these are just these are just offsets offsets offsets and so basically we want this to be DH",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 807,
      "text": "and so basically we want this to be DH",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 808,
      "text": "and so basically we want this to be DH preact but it needs to Sum along the preact but it needs to Sum along the preact",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 809,
      "text": "but it needs to Sum along the right Dimension and in this case similar right Dimension and in this case similar right Dimension and in this case similar to the gain we need to sum across the to the gain we need to sum across the to the gain we need to sum across the zeroth dimension the examples because of zeroth dimension the examples because of zeroth dimension the examples because of the way that the bias gets replicated the way that the bias gets replicated the way that the bias gets replicated vertically vertically vertically and we also want to have keep them as",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 810,
      "text": "and we also want to have keep them as and we also want to have keep them as true true true",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 811,
      "text": "and so this will basically take this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 812,
      "text": "and and so this will basically take this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 813,
      "text": "and and so this will basically take this and sum it up and give us a 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 814,
      "text": "sum it up and give us a 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 815,
      "text": "sum it up and give us a 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 816,
      "text": "so this is the candidate implementation so this is the candidate implementation so this is the candidate implementation it makes all the shapes work it makes all the shapes work it makes all the shapes work let me bring it up down here and then let me bring it up down here and then let me bring it up down here and then let me uncomment these three lines let me uncomment these three lines let me uncomment these three lines to check that we are getting the correct to check that we are getting the correct to check that we are getting the correct result for all the three tensors and result for all the three tensors and result for all the three tensors and indeed we see that all of that got back indeed we see that all of that got back indeed we see that all of that got back propagated correctly",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 817,
      "text": "so now we get to propagated correctly so now we get to propagated correctly so now we get to the batch Norm layer we see how here the batch Norm layer we see how here the batch Norm layer we see how here being gay and being bias are the being gay and being bias are the being gay and being bias are the parameters so the back propagation ends parameters so the back propagation ends parameters so the back propagation ends but B and raw now is the output of the but B and raw now is the output of the but B and raw now is the output of the standardization standardization standardization so here what I'm doing of course is I'm so here what I'm doing of course is I'm so here what I'm doing of course is I'm breaking up the batch form into breaking up the batch form into breaking up the batch form into manageable pieces so we can back manageable pieces so we can back manageable pieces so we can back propagate through each line individually propagate through each line individually propagate through each line individually but basically what's happening is BN",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 818,
      "text": "but basically what's happening is BN",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 819,
      "text": "but basically what's happening is BN mean",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 820,
      "text": "I is the sum mean I is the sum mean I is the sum",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 821,
      "text": "so this is the B and mean I I apologize so this is the B and mean I I apologize so this is the B and mean I I apologize for the variable naming B and diff is x for the variable naming B and diff is x for the variable naming B and diff is x minus mu minus mu minus mu B and div 2 is x minus mu squared here B and div 2 is x minus mu squared here B and div 2 is x minus mu squared here inside the variance inside the variance inside the variance B and VAR is the variance so uh Sigma B and VAR is the variance so uh Sigma B and VAR is the variance so uh Sigma Square this is B and bar",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 822,
      "text": "and it's Square this is B and bar",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 823,
      "text": "and it's Square this is B and bar",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 824,
      "text": "and it's basically the sum of squares basically the sum of squares basically the sum of squares so this is the x minus mu squared",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 825,
      "text": "and so this is the x minus mu squared and so this is the x minus mu squared and then the sum now you'll notice one then the sum",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 826,
      "text": "now you'll notice one then the sum",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 827,
      "text": "now you'll notice one departure here departure here departure here here it is normalized as 1 over m here it is normalized as 1 over m here it is normalized as 1 over m uh which is number of examples here I'm uh which is number of examples here I'm uh which is number of examples here I'm normalizing as one over n minus 1 normalizing as one over n minus 1 normalizing as one over n minus 1 instead of N and this is deliberate and instead of N and this is deliberate and instead of N and this is deliberate and I'll come back to that in a bit when we I'll come back to that in a bit when we I'll come back to that in a bit when we are at this line it is something called are at this line it is something called are at this line it is something called the bezels correction the bezels correction the bezels correction but this is how I want it in our case but this is how I want it in our case but this is how I want it in our case bienvar inv then becomes basically bienvar inv then becomes basically bienvar inv then becomes basically bienvar plus Epsilon Epsilon is one bienvar plus Epsilon Epsilon is one bienvar plus Epsilon Epsilon is one negative five",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 828,
      "text": "and then it's one over negative five",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 829,
      "text": "and then it's one over negative five",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 830,
      "text": "and then it's one over square root square root square root is the same as raising to the power of is the same as raising to the power of is the same as raising to the power of negative 0.5 right because 0.5 is square negative 0.5 right because 0.5 is square negative 0.5 right because 0.5 is square root and then negative makes it one over root and then negative makes it one over root and then negative makes it one over square root square root",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 831,
      "text": "so BM Bar M is a one over this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 832,
      "text": "uh so BM Bar M is a one over this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 833,
      "text": "uh so BM Bar M is a one over this uh denominator here and then we can see denominator here and then we can see denominator here and then we can see that b and raw which is the X hat here that b and raw which is the X hat here that b and raw which is the X hat here is equal to the BN diff the numerator is equal to the BN diff the numerator is equal to the BN diff the numerator multiplied by the multiplied by the multiplied by the um BN bar in um BN bar in um BN bar in and this line here that creates pre-h and this line here that creates pre-h and this line here that creates pre-h pre-act was the last piece we've already pre-act was the last piece we've already pre-act was the last piece we've already back propagated through it back propagated through it back propagated through it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 834,
      "text": "so now what we want to do is we are here so now what we want to do is we are here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 835,
      "text": "so now what we want to do is we are here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 836,
      "text": "and we have B and raw and we have to and we have B and raw and we have to and we have B and raw and we have to first back propagate into B and diff and first back propagate into B and diff and first back propagate into B and diff and B and Bar M B and Bar M B and Bar M",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 837,
      "text": "so now we're here and we have DB and raw",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 838,
      "text": "so now we're here and we have DB and raw",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 839,
      "text": "so now we're here and we have DB and raw and we need to back propagate through and we need to back propagate through and we need to back propagate through this line this line now I've written out the shapes here and now I've written out the shapes here and now I've written out the shapes here and indeed bien VAR m is a shape 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 840,
      "text": "so indeed bien VAR m is a shape 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 841,
      "text": "so indeed bien VAR m is a shape 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 842,
      "text": "so there is a broadcasting happening here there is a broadcasting happening here there is a broadcasting happening here that we have to be careful with but it that we have to be careful with but it that we have to be careful with but it is just an element-wise simple is just an element-wise simple is just an element-wise simple multiplication by now we should be multiplication by now we should be multiplication by now we should be pretty comfortable with that to get DB pretty comfortable with that to get DB pretty comfortable with that to get DB and diff we know that this is just B and and diff we know that this is just B and and diff we know that this is just B and varm varm varm multiplied with multiplied with multiplied with DP and raw and conversely to get dbmring and conversely to get dbmring we need to take the end if we need to take the end if we need to take the end",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 843,
      "text": "if and multiply that by DB and raw so this is the candidate but of course so this is the candidate but of course we need to make sure that broadcasting we need to make sure that broadcasting we need to make sure that broadcasting is obeyed so in particular B and VAR M is obeyed so in particular B and VAR M is obeyed so in particular B and VAR M multiplying with DB and raw multiplying with DB and raw multiplying with DB and raw will be okay and give us 32 by 64 as we will be okay and give us 32 by 64 as we will be okay and give us 32 by 64 as we expect expect expect but dbm VAR inv would be taking a 32 by but dbm VAR inv would be taking a 32 by but dbm VAR inv would be taking a 32 by 64. 64. 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 844,
      "text": "multiplying it by 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 845,
      "text": "so this is a multiplying it by 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 846,
      "text": "so this is a multiplying it by 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 847,
      "text": "so this is a 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 848,
      "text": "but of course DB this uh B and 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 849,
      "text": "but of course DB this uh B and 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 850,
      "text": "but of course DB this uh B and VAR in is only 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 851,
      "text": "so the second VAR in is only 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 852,
      "text": "so the second VAR in is only 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 853,
      "text": "so the second line here needs a sum across the line here needs a sum across the line here needs a sum across the examples and because there's this examples and because there's this examples and because there's this Dimension here we need to make sure that Dimension here we need to make sure that Dimension here we need to make sure that keep them is true keep them is true keep them is true",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 854,
      "text": "so this is the candidate so this is the candidate",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 855,
      "text": "so this is the candidate let's erase this and let's swing down let's erase this and let's swing down let's erase this and let's swing down here here here and implement it and then let's comment and implement it and then let's comment and implement it and then let's comment out dbm barif and DB and diff out dbm barif and DB and diff out dbm barif and DB and diff now we'll actually notice that DB and now we'll actually notice that DB and now we'll actually notice that DB and diff by the way is going to be incorrect diff by the way is going to be incorrect diff by the way is going to be incorrect",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 856,
      "text": "so when I run this so when I run this so when I run this BMR m is correct B and diff is not BMR m is correct B and diff is not BMR m is correct B and diff is not correct and this is actually expected correct and this is actually expected correct and this is actually expected because we're not done with b and diff because we're not done with b and diff because we're not done with b and diff",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 857,
      "text": "so in particular when we slide here we so in particular when we slide here we so in particular when we slide here we see here that b and raw as a function of see here that b and raw as a function of see here that b and raw as a function of B and diff but actually B and far of is B and diff but actually B and far of is B and diff but actually B and far of is a function of B of R which is a function a function of B of R which is a function a function of B of R which is a function of B and df2 which is a function of B of B and df2 which is a function of B of B and df2 which is a function of B and diff and diff and diff so it comes here so bdn diff",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 858,
      "text": "so it comes here so bdn diff so it comes here so bdn diff um these variable names are crazy",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 859,
      "text": "I'm um these variable names are crazy",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 860,
      "text": "I'm um these variable names are crazy",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 861,
      "text": "I'm sorry it branches out into two branches sorry it branches out into two branches sorry it branches out into two branches and we've only done one branch of it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 862,
      "text": "we and we've only done one branch of it we",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 863,
      "text": "and we've only done one branch of it we have to continue our back propagation have to continue our back propagation have to continue our back propagation and eventually come back to B and diff and eventually come back to B and diff and eventually come back to B and diff",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 864,
      "text": "and then we'll be able to do a plus",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 865,
      "text": "and then we'll be able to do a plus",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 866,
      "text": "and then we'll be able to do a plus equals and get the actual card gradient equals and get the actual card gradient equals and get the actual card gradient for now it is good to verify that CMP for now it is good to verify that CMP for now it is good to verify that CMP also works it doesn't just lie to us and also works it doesn't just lie to us and also works it doesn't just lie to us and tell us that everything is always tell us that everything is always tell us that everything is always correct it can in fact detect when your correct it can in fact detect when your correct it can in fact detect when your gradient is not correct",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 867,
      "text": "so it's that's gradient is not correct",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 868,
      "text": "so it's that's gradient is not correct",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 869,
      "text": "so it's that's good to see as well",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 870,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 871,
      "text": "so now we have good to see as well",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 872,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 873,
      "text": "so now we have good to see as well",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 874,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 875,
      "text": "so now we have the derivative here and we're trying to the derivative here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 876,
      "text": "and we're trying to the derivative here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 877,
      "text": "and we're trying to back propagate through this line back propagate through this line back propagate through this line and because we're raising to a power of and because we're raising to a power of and because we're raising to a power of negative 0.5 I brought up the power rule negative 0.5",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 878,
      "text": "I brought up the power rule negative 0.5 I brought up the power rule and we see that basically we have that and we see that basically we have that and we see that basically we have that the BM bar will now be we bring down the the BM bar will now be we bring down the the BM bar will now be we bring down the exponent so negative 0.5 times exponent",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 879,
      "text": "so negative 0.5 times exponent so negative 0.5 times uh X which is this uh X which is this uh X which is this and now raised to the power of negative and now raised to the power of negative and now raised to the power of negative 0.5 minus 1 which is negative 1.5 0.5 minus 1 which is negative 1.5 0.5 minus 1 which is negative 1.5 now we would have to also apply a small now we would have to also apply a small now we would have to also apply a small chain rule here in our head because we chain rule here in our head because we chain rule here in our head because we need to take further the derivative of B need to take further the derivative of B need to take further the derivative of B and VAR with respect to this expression and VAR with respect to this expression and VAR with respect to this expression here inside the bracket but because this here inside the bracket but because this here inside the bracket but because this is an elementalized operation and is an elementalized operation and is an elementalized operation and everything is fairly simple that's just everything is fairly simple that's just everything is fairly simple that's just one and so there's nothing to do there one and so there's nothing to do there one and so there's nothing to do there",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 880,
      "text": "so this is the local derivative",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 881,
      "text": "and then so this is the local derivative and then times the global derivative to create times the global derivative to create times the global derivative to create the chain rule this is just times the BM the chain rule this is just times the BM the chain rule this is just times the BM bar have bar have bar have so this is our candidate let me bring so this is our candidate let me bring so this is our candidate let me bring this down this down this down and uncommon to the check and we see that we have the correct",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 882,
      "text": "and we see that we have the correct result result result now before we propagate through the next now before we propagate through the next now before we propagate through the next line I want to briefly talk about the line I want to briefly talk about the line I want to briefly talk about the note here where I'm using the bezels note here where I'm using the bezels note here where I'm using the bezels correction dividing by n",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 883,
      "text": "minus 1 instead correction dividing by n minus 1 instead correction dividing by n minus 1 instead of dividing by n when I normalize here of dividing by n when I normalize here of dividing by n when I normalize here the sum of squares the sum of squares the sum of squares",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 884,
      "text": "now you'll notice that this is departure now you'll notice that this is departure now you'll notice that this is departure from the paper which uses one over n from the paper which uses one over n from the paper which uses one over n instead not one over n minus one their m instead not one over n minus one their m instead not one over n minus one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 885,
      "text": "their m is RN is RN is RN",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 886,
      "text": "and and and um so it turns out that there are two",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 887,
      "text": "um so it turns out that there are two",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 888,
      "text": "um so it turns out that there are two ways of estimating variance of an array ways of estimating variance of an array ways of estimating variance of an array one is the biased estimate which is one one is the biased estimate which is one one is the biased estimate which is one over n and the other one is the unbiased over n and the other one is the unbiased over n and the other one is the unbiased estimate which is one over n minus one estimate which is one over n minus one estimate which is one over n minus one now confusingly in the paper this is uh now confusingly in the paper this is uh now confusingly in the paper this is uh not very clearly described",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 889,
      "text": "and also it's not very clearly described and also it's not very clearly described and also it's a detail that kind of matters I think a detail that kind of matters I think a detail that kind of matters I think um they are using the biased version um they are using the biased version um they are using the biased version training time but later when they are training time but later when they are training time but later when they are talking about the inference they are talking about the inference they are talking about the inference they are mentioning that when they do the mentioning that when they do the mentioning that when they do the inference they are using the unbiased inference they are using the unbiased inference they are using the unbiased estimate which is the n minus one estimate which is the n minus one estimate which is the n minus one version in version in version in um um basically for inference basically for inference basically for inference and to calibrate the running mean and and to calibrate the running mean and and to calibrate the running mean and the running variance basically and so the running variance basically and so the running variance basically",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 890,
      "text": "and so they they actually introduce a trained they they actually introduce a trained they they actually introduce a trained test mismatch where in training they use test mismatch where in training they use test mismatch where in training they use the biased version and in the in test the biased version and in the in test the biased version and in the in test time they use the unbiased version I time they use the unbiased version I time they use the unbiased version I find this extremely confusing you can find this extremely confusing you can find this extremely confusing you can read more about the bezels correction read more about the bezels correction read more about the bezels correction and why uh dividing by n minus one gives and why uh dividing by n minus one gives and why uh dividing by n minus one gives you a better estimate of the variance in you a better estimate of the variance in you a better estimate of the variance in a case where you have population size or a case where you have population size or a case where you have population size or samples for the population samples for the population samples for the population that are very small and that is indeed that are very small and that is indeed that are very small and that is indeed the case for us because we are dealing the case for us because we are dealing the case for us because we are dealing with many patches and these mini matches with many patches and these mini matches with many patches and these mini matches are a small sample of a larger are a small sample of a larger are a small sample of a larger population which is the entire training population which is the entire training population which is the entire training set",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 891,
      "text": "and so it just turns out that if you set and so it just turns out that if you set and so it just turns out that if you just estimate it using one over n that just estimate it using one over n that just estimate it using one over n that actually almost always underestimates actually almost always underestimates actually almost always underestimates the variance and it is a biased the variance and it is a biased the variance and it is a biased estimator and it is advised that you use estimator and it is advised that you use estimator and it is advised that you use the unbiased version and divide by n the unbiased version and divide by n the unbiased version and divide by n minus one and you can go through this minus one and you can go through this minus one and you can go through this article here that I liked that actually article here that I liked that actually article here that I liked that actually describes the full reasoning and I'll describes the full reasoning and I'll describes the full reasoning and I'll link it in the video description link it in the video description link it in the video description now when you calculate the torture now when you calculate the torture now when you calculate the torture variance variance variance you'll notice that they take the you'll notice that they take the you'll notice that they take the unbiased flag whether or not you want to unbiased flag whether or not you want to unbiased flag whether or not you want to divide by n or n minus one confusingly divide by n or n minus one confusingly divide by n or n minus one confusingly they do not mention what the default is they do not mention what the default is they do not mention what the default is for unbiased but I believe unbiased by for unbiased",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 892,
      "text": "but I believe unbiased by for unbiased",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 893,
      "text": "but I believe unbiased by default is true",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 894,
      "text": "I'm not sure why the default is true",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 895,
      "text": "I'm not sure why the default is true",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 896,
      "text": "I'm not sure why the docs here don't cite that docs here don't cite that docs here don't cite that now in The Bachelor now in The Bachelor now in The Bachelor 1D the documentation again is kind of 1D the documentation again is kind of 1D the documentation again is kind of wrong and confusing it says that the wrong and confusing it says that the wrong and confusing it says that the standard deviation is calculated via the standard deviation is calculated via the standard deviation is calculated via the biased estimator biased estimator biased estimator but this is actually not exactly right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 897,
      "text": "but this is actually not exactly right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 898,
      "text": "but this is actually not exactly right and people have pointed out that it is and people have pointed out that it is and people have pointed out that it is not right in a number of issues since not right in a number of issues since not right in a number of issues since then because actually the rabbit hole is then because actually the rabbit hole is then because actually the rabbit hole is deeper and they follow the paper exactly deeper and they follow the paper exactly deeper and they follow the paper exactly and they use the biased version for and they use the biased version for and they use the biased version for training",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 899,
      "text": "but when they're estimating the training but when they're estimating the training but when they're estimating the running standard deviation we are using running standard deviation we are using running standard deviation we are using the unbiased version so again there's the unbiased version so again there's the unbiased version so again there's the train test mismatch so long story the train test mismatch so long story the train test mismatch so long story short I'm not a fan of trained test short",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 900,
      "text": "I'm not a fan of trained test short",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 901,
      "text": "I'm not a fan of trained test discrepancies I basically kind of discrepancies",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 902,
      "text": "I basically kind of discrepancies I basically kind of consider consider consider the fact that we use the bias version the fact that we use the bias version the fact that we use the bias version the training time and the unbiased test the training time and the unbiased test the training time and the unbiased test time I basically consider this to be a time I basically consider this to be a time I basically consider this to be a bug and I don't think that there's a bug",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 903,
      "text": "and I don't think that there's a bug",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 904,
      "text": "and I don't think that there's a good reason for that it's not really good reason for that it's not really good reason for that it's not really they don't really go into the detail of they don't really go into the detail of they don't really go into the detail of the reasoning behind it in this paper so the reasoning behind it in this paper",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 905,
      "text": "so the reasoning behind it in this paper so that's why I basically prefer to use the that's why I basically prefer to use the that's why I basically prefer to use the bestless correction in my own work bestless correction in my own work bestless correction in my own work unfortunately Bastion does not take a unfortunately Bastion does not take a unfortunately Bastion does not take a keyword argument that tells you whether keyword argument that tells you whether keyword argument that tells you whether or not you want to use the unbiased or not you want to use the unbiased or not you want to use the unbiased version of the bias version in both version of the bias version in both version of the bias version in both train and test and so therefore anyone train and test and so therefore anyone train and test and so therefore anyone using batch normalization basically in using batch normalization basically in using batch normalization basically in my view has a bit of a bug in the code my view has a bit of a bug in the code my view has a bit of a bug in the code",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 906,
      "text": "um um and this turns out to be much less of a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 907,
      "text": "and this turns out to be much less of a and this turns out to be much less of a problem if your batch mini batch sizes problem if your batch mini batch sizes problem if your batch mini batch sizes are a bit larger but still I just might are a bit larger but still I just might are a bit larger but still I just might kind of uh unpardable",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 908,
      "text": "so maybe someone kind of uh unpardable",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 909,
      "text": "so maybe someone kind of uh unpardable",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 910,
      "text": "so maybe someone can explain why this is okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 911,
      "text": "but for now can explain why this is okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 912,
      "text": "but for now can explain why this is okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 913,
      "text": "but for now I prefer to use the unbiased version I prefer to use the unbiased version I prefer to use the unbiased version consistently both during training and at consistently both during training and at consistently both during training and at this time and that's why I'm using one this time and that's why I'm using one this time and that's why I'm using one over n minus one here over n minus one here over n minus one here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 914,
      "text": "okay so let's now actually back",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 915,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 916,
      "text": "so let's now actually back",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 917,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 918,
      "text": "so let's now actually back propagate through this line propagate through this line propagate through this line",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 919,
      "text": "so so the first thing that I always like to do the first thing that I always like to do the first thing that I always like to do is I like to scrutinize the shapes first is I like to scrutinize the shapes first is I like to scrutinize the shapes first so in particular here looking at the so in particular here looking at the so in particular here looking at the shapes of what's involved I see that b shapes of what's involved I see that b shapes of what's involved I see that b and VAR shape is 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 920,
      "text": "so it's a row and VAR shape is 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 921,
      "text": "so it's a row and VAR shape is 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 922,
      "text": "so it's a row vector and BND if two dot shape is 32 by vector and BND if two dot shape is 32 by vector and BND if two dot shape is 32 by 64. 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 923,
      "text": "so clearly here we're doing a sum over so clearly here we're doing a sum over so clearly here we're doing a sum over the zeroth axis to squash the first the zeroth axis to squash the first the zeroth axis to squash the first dimension of of the shapes here using a dimension of of the shapes here using a dimension of of the shapes here using a sum so that right away actually hints to sum so that right away actually hints to sum so that right away actually hints to me that there will be some kind of a me that there will be some kind of a me that there will be some kind of a replication or broadcasting in the replication or broadcasting in the replication or broadcasting in the backward pass and maybe you're noticing backward pass and maybe you're noticing backward pass and maybe you're noticing the pattern here but basically anytime the pattern here but basically anytime the pattern here but basically anytime you have a sum in the forward pass that you have a sum in the forward pass that you have a sum in the forward pass that turns into a replication or broadcasting turns into a replication or broadcasting turns into a replication or broadcasting in the backward pass along the same in the backward pass along the same in the backward pass along the same Dimension and conversely when we have a Dimension and conversely when we have a Dimension and conversely when we have a replication or a broadcasting in the replication or a broadcasting in the replication or a broadcasting in the forward pass that indicates a variable forward pass that indicates a variable forward pass that indicates a variable reuse and so in the backward pass that reuse",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 924,
      "text": "and so in the backward pass that reuse and so in the backward pass that turns into a sum over the exact same turns into a sum over the exact same turns into a sum over the exact same dimension dimension dimension",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 925,
      "text": "and so hopefully you're noticing that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 926,
      "text": "and so hopefully you're noticing that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 927,
      "text": "and so hopefully you're noticing that Duality that those two are kind of like Duality that those two are kind of like Duality that those two are kind of like the opposite of each other in the the opposite of each other in the the opposite of each other in the forward and backward pass forward and backward pass forward and backward pass now once we understand the shapes the now once we understand the shapes the now once we understand the shapes the next thing I like to do always is I like next thing",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 928,
      "text": "I like to do always is I like next thing I like to do always",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 929,
      "text": "is I like to look at a toy example in my head to to look at a toy example in my head to to look at a toy example in my head to sort of just like understand roughly how sort of just like understand roughly how sort of just like understand roughly how uh the variable the variable uh the variable the variable uh the variable the variable dependencies go in the mathematical dependencies go in the mathematical dependencies go in the mathematical formula formula formula",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 930,
      "text": "so here we have a two-dimensional array so here we have a two-dimensional array so here we have a two-dimensional array of the end of two which we are scaling of the end of two which we are scaling of the end of two which we are scaling by a constant",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 931,
      "text": "and then we are summing uh by a constant",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 932,
      "text": "and then we are summing uh by a constant",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 933,
      "text": "and then we are summing uh vertically over the columns",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 934,
      "text": "so if we vertically over the columns",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 935,
      "text": "so if we vertically over the columns",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 936,
      "text": "so if we have a two by two Matrix a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 937,
      "text": "and then we have a two by two Matrix a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 938,
      "text": "and then we have a two by two Matrix a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 939,
      "text": "and then we sum over the columns and scale we would sum over the columns and scale we would sum over the columns and scale we would get a row Vector B1 B2 and B1 depends on get a row Vector B1 B2 and B1 depends on get a row Vector B1 B2 and B1 depends on a in this way whereas just sum they're a in this way whereas just sum they're a in this way whereas just sum they're scaled of a and B2 in this way where scaled of a and B2 in this way where scaled of a and B2 in this way where it's the second column sump and scale it's the second column sump and scale it's the second column sump and scale and so looking at this basically and so looking at this basically and so looking at this basically what we want to do now is we have the what we want to do now is we have the what we want to do now is we have the derivatives on B1 and B2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 940,
      "text": "and we want to derivatives on B1 and B2 and we want to derivatives on B1 and B2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 941,
      "text": "and we want to back propagate them into Ace",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 942,
      "text": "and so it's back propagate them into Ace",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 943,
      "text": "and so it's back propagate them into Ace",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 944,
      "text": "and so it's clear that just differentiating in your clear that just differentiating in your clear that just differentiating in your head the local derivative here is one head the local derivative here is one head the local derivative here is one over n minus 1 times uh one over n minus 1 times uh one over n minus 1 times uh one uh for each one of these A's and um uh for each one of these A's and um uh for each one of these A's and um basically the derivative of B1 has to basically the derivative of B1 has to basically the derivative of B1 has to flow through The Columns of a flow through The Columns of a flow through The Columns of a scaled by one over n minus one scaled by one over n minus one scaled by one over n minus one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 945,
      "text": "and that's roughly What's Happening Here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 946,
      "text": "and that's roughly What's Happening Here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 947,
      "text": "and that's roughly What's Happening Here so intuitively the derivative flow tells so intuitively the derivative flow tells so intuitively the derivative flow tells us that DB and diff2 us that DB and diff2 us that DB and diff2 will be the local derivative of this will be the local derivative of this will be the local derivative of this operation and there are many ways to do operation and there are many ways to do operation and there are many ways to do this by the way",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 948,
      "text": "but I like to do this by the way",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 949,
      "text": "but I like to do this by the way",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 950,
      "text": "but I like to do something like this torch dot once like something like this torch dot once like something like this torch dot once like of bndf2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 951,
      "text": "so I'll create a large array of bndf2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 952,
      "text": "so I'll create a large array of bndf2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 953,
      "text": "so I'll create a large array two-dimensional of ones two-dimensional of ones two-dimensional of ones",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 954,
      "text": "and then I will scale it so 1.0 divided",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 955,
      "text": "and then I will scale it so 1.0 divided",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 956,
      "text": "and then I will scale it so 1.0 divided by n minus 1. by n minus 1. by n minus 1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 957,
      "text": "so this is a array of so this is a array of so this is a array of um one over n minus one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 958,
      "text": "and that's sort um one over n minus one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 959,
      "text": "and that's sort um one over n minus one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 960,
      "text": "and that's sort of like the local derivative of like the local derivative of like the local derivative and now for the chain rule I will simply and now for the chain rule I will simply and now for the chain rule I will simply just multiply it by dbm bar and notice here what's going to happen and notice here what's going to happen this is 32 by 64 and this is just 1 by this is 32 by 64 and this is just 1 by this is 32 by 64 and this is just 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 961,
      "text": "so I'm letting the broadcasting do 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 962,
      "text": "so I'm letting the broadcasting do 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 963,
      "text": "so I'm letting the broadcasting do the replication because internally in the replication because internally in the replication because internally in pytorch basically dbnbar which is 1 by pytorch basically dbnbar which is 1 by pytorch basically dbnbar which is 1 by 64 row vector 64 row vector 64 row vector well in this multiplication get well in this multiplication get well in this multiplication get um copied vertically until the two are um copied vertically until the two are um copied vertically until the two are of the same shape and then there will be of the same shape and then there will be of the same shape and then there will be an element wise multiply",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 964,
      "text": "and so that uh an element wise multiply",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 965,
      "text": "and so that uh an element wise multiply",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 966,
      "text": "and so that uh so that the broadcasting is basically so that the broadcasting is basically so that the broadcasting is basically doing the replication doing the replication doing the replication",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 967,
      "text": "and I will end up with the derivatives and I will end up with the derivatives and I will end up with the derivatives of DB and diff2 here of DB and diff2 here of DB and diff2 here so this is the candidate solution let's so this is the candidate solution let's so this is the candidate solution let's bring it down here bring it down here bring it down here let's uncomment this line where we check let's uncomment this line where we check let's uncomment this line where we check it and let's hope for the best it and let's hope for the best it and let's hope for the best and indeed we see that this is the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 968,
      "text": "and indeed we see that this is the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 969,
      "text": "and indeed we see that this is the correct formula next up let's correct formula next up let's correct formula next up let's differentiate here and to be in this differentiate here and to be in this differentiate here and to be in this so here we have that b and diff is so here we have that b and diff is so here we have that b and diff is element y squared to create B and F2 element y squared to create B and F2 element y squared to create B and F2",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 970,
      "text": "so this is a relatively simple so this is a relatively simple",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 971,
      "text": "so this is a relatively simple derivative because it's a simple element derivative because it's a simple element derivative because it's a simple element wise operation",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 972,
      "text": "so it's kind of like the wise operation so it's kind of like the wise operation",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 973,
      "text": "so it's kind of like the scalar case and we have that DB and div scalar case and we have that DB and div scalar case and we have that DB and div should be if this is x squared then the should be if this is x squared then the should be if this is x squared then the derivative of this is 2x right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 974,
      "text": "so it's derivative of this is 2x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 975,
      "text": "right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 976,
      "text": "so it's derivative of this is 2x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 977,
      "text": "right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 978,
      "text": "so it's simply 2 times B and if that's the local simply 2 times B and if that's the local simply 2 times B and if that's the local derivative derivative and then times chain Rule and the shape and then times chain Rule and the shape and then times chain Rule and the shape of these is the same they are of the of these is the same they are of the of these is the same they are of the same shape so times this same shape so times this same shape so times this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 979,
      "text": "so that's the backward pass for this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 980,
      "text": "so that's the backward pass for this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 981,
      "text": "so that's the backward pass for this variable let me bring that down here variable let me bring that down here variable let me bring that down here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 982,
      "text": "and now we have to be careful because we and now we have to be careful because we and now we have to be careful because we already calculated dbm depth right so already calculated dbm depth right so already calculated dbm depth right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 983,
      "text": "so this is just the end of the other uh you this is just the end of the other uh you this is just the end of the other uh you know other Branch coming back to B and know other Branch coming back to B and know other Branch coming back to B and diff diff diff because B and diff was already back because B and diff was already back because B and diff was already back propagated to way over here propagated to way over here propagated to way over here from being raw",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 984,
      "text": "so we now completed the from being raw so we now completed the from being raw so we now completed the second branch",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 985,
      "text": "and so that's why I have second branch",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 986,
      "text": "and so that's why I have second branch",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 987,
      "text": "and so that's why I have to do plus equals and if you recall we to do plus equals and if you recall we to do plus equals and if you recall we had an incorrect derivative for being had an incorrect derivative for being had an incorrect derivative for being diff before and I'm hoping that once we diff before and I'm hoping that once we diff before and I'm hoping that once we append this last missing piece we have append this last missing piece we have append this last missing piece we have the exact correctness so let's run the exact correctness",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 988,
      "text": "so let's run the exact correctness so let's run ambient to be in div now actually shows ambient to be in div now actually shows ambient to be in div now actually shows the exact correct derivative the exact correct derivative the exact correct derivative",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 989,
      "text": "um so that's comforting",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 990,
      "text": "okay so let's um so that's comforting",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 991,
      "text": "okay so let's um so that's comforting",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 992,
      "text": "okay so let's now back propagate through this line now back propagate through this line now back propagate through this line here here um the first thing we do of course is we um the first thing we do of course is we um the first thing we do of course is we check the shapes",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 993,
      "text": "and I wrote them out check the shapes",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 994,
      "text": "and I wrote them out check the shapes",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 995,
      "text": "and I wrote them out here and basically the shape of this is here and basically the shape of this is here and basically the shape of this is 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 996,
      "text": "hpbn is the same shape 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 997,
      "text": "hpbn is the same shape 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 998,
      "text": "hpbn is the same shape but B and mean I is a row Vector 1 by but B and mean I is a row Vector 1 by but B and mean I is a row Vector 1 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 999,
      "text": "so this minus here will actually do 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1000,
      "text": "so this minus here will actually do 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1001,
      "text": "so this minus here will actually do broadcasting",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1002,
      "text": "and so we have to be broadcasting",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1003,
      "text": "and so we have to be broadcasting",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1004,
      "text": "and so we have to be careful with that and as a hint to us careful with that and as a hint to us careful with that and as a hint to us again because of The Duality a again because of The Duality a again because of The Duality a broadcasting and the forward pass means broadcasting and the forward pass means broadcasting and the forward pass means a variable reuse and therefore there a variable reuse and therefore there a variable reuse and therefore there will be a sum in the backward pass will be a sum in the backward pass will be a sum in the backward pass",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1005,
      "text": "so let's write out the backward pass so let's write out the backward pass so let's write out the backward pass here now here now here now um um back propagate into the hpbn back propagate into the hpbn back propagate into the hpbn because this is these are the same shape because this is these are the same shape because this is these are the same shape then the local derivative for each one then the local derivative for each one then the local derivative for each one of the elements here is just one for the of the elements here is just one for the of the elements here is just one for the corresponding element in here corresponding element in here corresponding element in here so basically what this means is that the so basically what this means is that the so basically what this means is that the gradient just simply copies it's just a gradient just simply copies it's just a gradient just simply copies it's just a variable assignment it's quality",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1006,
      "text": "so I'm variable assignment it's quality",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1007,
      "text": "so I'm variable assignment it's quality",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1008,
      "text": "so I'm just going to clone this tensor just for just going to clone this tensor just for just going to clone this tensor just for safety to create an exact copy of DB and safety to create an exact copy of DB and safety to create an exact copy of DB and div div div",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1009,
      "text": "and then here to back propagate into and then here to back propagate into and then here to back propagate into this one what I'm inclined to do here is will basically be will basically be uh what is the local derivative",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1010,
      "text": "well uh what is the local derivative",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1011,
      "text": "well uh what is the local derivative",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1012,
      "text": "well it's negative torch.1's like it's negative torch.1's like it's negative torch.1's like of the shape of uh B and diff of the shape of uh B and diff of the shape of uh B and",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1013,
      "text": "diff right and then times and then times the um the um the um the derivative here dbf and this here is the back propagation and this here is the back propagation for the replicated B and mean I for the replicated B and mean I for the replicated B and mean",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1014,
      "text": "I so I still have to back propagate",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1015,
      "text": "so I still have to back propagate",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1016,
      "text": "so I still have to back propagate through the uh replication in the through the uh replication in the through the uh replication in the broadcasting",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1017,
      "text": "and I do that by doing a broadcasting",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1018,
      "text": "and I do that by doing a broadcasting",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1019,
      "text": "and I do that by doing a sum",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1020,
      "text": "so I'm going to take this whole sum",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1021,
      "text": "so I'm going to take this whole sum",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1022,
      "text": "so I'm going to take this whole thing",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1023,
      "text": "and I'm going to do a sum over the thing",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1024,
      "text": "and I'm going to do a sum over the thing",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1025,
      "text": "and I'm going to do a sum over the zeroth dimension which was the zeroth dimension which was the zeroth dimension which was the replication so if you scrutinize this by the way so if you scrutinize this by the way you'll notice that this is the same you'll notice that this is the same you'll notice that this is the same shape as that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1026,
      "text": "and so what I'm doing uh shape as that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1027,
      "text": "and so what I'm doing uh shape as that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1028,
      "text": "and so what I'm doing uh what I'm doing here doesn't actually what I'm doing here doesn't actually what I'm doing here doesn't actually make that much sense because it's just a make that much sense because it's just a make that much sense because it's just a array of ones multiplying DP and diff",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1029,
      "text": "so array of ones multiplying DP and diff so array of ones multiplying DP and diff",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1030,
      "text": "so in fact I can just do this in fact I can just do this in fact I can just do this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1031,
      "text": "um and that is equivalent um and that is equivalent um and that is equivalent so this is the candidate backward pass",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1032,
      "text": "so this is the candidate backward pass so this is the candidate backward pass let me copy it here and then let me let me copy it here and then let me let me copy it here and then let me comment out this one and this one comment out this one and this one comment out this one and this one enter enter enter",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1033,
      "text": "and it's wrong",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1034,
      "text": "damn damn actually sorry this is supposed to be actually sorry this is supposed to be actually sorry this is supposed to be wrong",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1035,
      "text": "and it's supposed to be wrong wrong and it's supposed to be wrong wrong",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1036,
      "text": "and it's supposed to be wrong because because because we are back propagating from a b and we are back propagating from a b",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1037,
      "text": "and we are back propagating from a b and diff into hpbn",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1038,
      "text": "and but",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1039,
      "text": "we're not done diff into hpbn and but we're not done diff into hpbn and but we're not done because B and mean I depends on hpbn and because B and mean I depends on hpbn and because B and mean I depends on hpbn and there will be a second portion of that there will be a second portion of that there will be a second portion of that derivative coming from this second derivative coming from this second derivative coming from this second Branch so we're not done yet and we Branch so we're not done yet and we Branch so we're not done yet and we expect it to be incorrect so there you expect it to be incorrect so there you expect it to be incorrect so there you go go go uh so let's now back propagate from uh B",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1040,
      "text": "uh so let's now back propagate from uh B uh so let's now back propagate from uh B and mean I into hpbn and mean I into hpbn and mean I into hpbn um um and so here again we have to be careful",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1041,
      "text": "and so here again we have to be careful",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1042,
      "text": "and so here again we have to be careful because there's a broadcasting along because there's a broadcasting along because there's a broadcasting along um or there's a Sum along the zeroth um or there's a Sum along the zeroth um or there's a Sum along the zeroth dimension so this will turn into dimension so this will turn into dimension so this will turn into broadcasting in the backward pass now broadcasting in the backward pass now broadcasting in the backward pass now",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1043,
      "text": "and I'm going to go a little bit faster",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1044,
      "text": "and I'm going to go a little bit faster and I'm going to go a little bit faster on this line because it is very similar on this line because it is very similar on this line because it is very similar to the line that we had before and to the line that we had before and to the line that we had before and multiplies in the past in fact multiplies in the past in fact multiplies in the past in fact",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1045,
      "text": "so the hpbn so the hpbn so the hpbn will be will be will be the gradient will be scaled by 1 over n the gradient will be scaled by 1 over n the gradient will be scaled by 1 over n",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1046,
      "text": "and then basically this gradient here on and then basically this gradient here on and then basically this gradient here on dbn mean I dbn mean I dbn mean I is going to be scaled by 1 over n and is going to be scaled by 1 over n and is going to be scaled by 1 over n",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1047,
      "text": "and then it's going to flow across all the then it's going to flow across all the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1048,
      "text": "then it's going to flow across all the columns and deposit itself into the hpvn columns and deposit itself into the hpvn columns and deposit itself into the hpvn so what we want is this thing scaled by so what we want is this thing scaled by so what we want is this thing scaled by 1",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1049,
      "text": "over n 1 over n 1 over n only put the constant up front here um so scale down the gradient and now we so scale down the gradient and now we so scale down the gradient and now we need to replicate it across all the um need to replicate it across all the um need to replicate it across all the um across all the rows here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1050,
      "text": "so we I like to across all the rows here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1051,
      "text": "so we I like to across all the rows here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1052,
      "text": "so we I like to do that by torch.lunslike of basically do that by torch.lunslike of basically do that by torch.lunslike of basically um hpbn um hpbn um hpbn",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1053,
      "text": "and I will let the broadcasting do the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1054,
      "text": "and I will let the broadcasting do the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1055,
      "text": "and I will let the broadcasting do the work of replication work of replication work of replication",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1056,
      "text": "so like that like that so this is uh the hppn and hopefully so this is uh the hppn and hopefully so this is uh the hppn and hopefully we can plus equals that so this here is broadcasting so this here is broadcasting um",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1057,
      "text": "and then this is the scaling",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1058,
      "text": "so this um and then this is the scaling",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1059,
      "text": "so this um and then this is the scaling so this should be current should be current should be current",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1060,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1061,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1062,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1063,
      "text": "so that completes the back propagation so that completes the back propagation so that completes the back propagation of the bathroom layer and we are now of the bathroom layer and we are now of the bathroom layer and we are now here let's back propagate through the here let's back propagate through the here let's back propagate through the linear layer one here now because linear layer one here now because linear layer one here now because everything is getting a little everything is getting a little everything is getting a little vertically crazy I copy pasted the line vertically crazy I copy pasted the line vertically crazy I copy pasted the line here and let's just back properly here and let's just back properly here and let's just back properly through this one line through this one line through this one line so first of course we inspect the shapes so first of course",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1064,
      "text": "we inspect the shapes so first of course we inspect the shapes and we see that this is 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1065,
      "text": "MCAT and we see that this is 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1066,
      "text": "MCAT and we see that this is 32 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1067,
      "text": "MCAT is 32 by 30.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1068,
      "text": "is 32 by 30.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1069,
      "text": "is 32 by 30.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1070,
      "text": "W1 is 30 30 by 64 and B1 is just 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1071,
      "text": "so W1 is 30 30 by 64 and B1 is just 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1072,
      "text": "so W1 is 30 30 by 64 and B1 is just 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1073,
      "text": "so as I mentioned back propagating through as I mentioned back propagating through as I mentioned back propagating through linear layers is fairly easy just by linear layers is fairly easy just by linear layers is fairly easy just by matching the shapes so let's do that we matching the shapes",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1074,
      "text": "so let's do that we matching the shapes",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1075,
      "text": "so let's do that we have that dmcat have that dmcat have that dmcat should be should be should be um some matrix multiplication of dhbn um some matrix multiplication of dhbn um some matrix multiplication of dhbn with uh W1 and one transpose thrown in with uh W1 and one transpose thrown in with uh W1 and one transpose thrown in there so to make uh MCAT be 32 by 30 there so to make uh MCAT be 32 by 30 there so to make uh MCAT be 32 by 30 I need to take dhpn I need to take dhpn I need to take dhpn 32 by 64 and multiply it by w1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1076,
      "text": "32 by 64 and multiply it by w1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1077,
      "text": "32 by 64 and multiply it by w1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1078,
      "text": "transpose to get the only one I need to end up to get the only one I need to end up with 30 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1079,
      "text": "with 30 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1080,
      "text": "with 30 by 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1081,
      "text": "so to get that I need to take uh MCAT so to get that I need to take uh MCAT so to get that I need to take uh MCAT transpose transpose transpose and multiply that by and multiply that by and multiply that by uh dhpion and finally to get DB1 and finally to get DB1 this is a addition and we saw that this is a addition and we saw that this is a addition and we saw that basically I need to just sum the basically I need to just sum the basically I need to just sum the elements in dhpbn along some Dimension elements in dhpbn along some Dimension elements in dhpbn along some Dimension and to make the dimensions work out I and to make the dimensions work out I and to make the dimensions work out I need to Sum along the zeroth axis here need to Sum along the zeroth axis here need to Sum along the zeroth axis here to eliminate this Dimension and we do to eliminate this Dimension and we do to eliminate this Dimension and we do not keep dims not keep dims not keep dims uh so that we want to just get a single uh so that we want to just get a single uh so that we want to just get a single one-dimensional lecture of 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1082,
      "text": "one-dimensional lecture of 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1083,
      "text": "one-dimensional lecture of 64.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1084,
      "text": "so these are the claimed derivatives so these are the claimed derivatives so these are the claimed derivatives let me put that here and let me let me put that here and let me let me put that here and let me uncomment three lines and cross our uncomment three lines and cross our uncomment three lines and cross our fingers fingers fingers everything is great",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1085,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1086,
      "text": "so we now everything is great",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1087,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1088,
      "text": "so we now everything is great",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1089,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1090,
      "text": "so we now continue almost there we have the continue almost there we have the continue almost there we have the derivative of MCAT",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1091,
      "text": "and we want to derivative of MCAT",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1092,
      "text": "and we want to derivative of MCAT",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1093,
      "text": "and we want to derivative we want to back propagate derivative we want to back propagate derivative we want to back propagate into m into m into m",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1094,
      "text": "so I again copied this line over here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1095,
      "text": "so I again copied this line over here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1096,
      "text": "so I again copied this line over here so this is the forward pass",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1097,
      "text": "and then so this is the forward pass",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1098,
      "text": "and then so this is the forward pass and then this is the shapes so remember that the this is the shapes so remember that the this is the shapes so remember that the shape here was 32 by 30 and the original shape here was 32 by 30 and the original shape here was 32 by 30 and the original shape of M plus 32 by 3 by 10.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1099,
      "text": "so this shape of M plus 32 by 3 by 10.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1100,
      "text": "so this shape of M plus 32 by 3 by 10.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1101,
      "text": "so this layer in the forward pass as you recall layer in the forward pass as you recall layer in the forward pass as you recall did the concatenation of these three did the concatenation of these three did the concatenation of these three 10-dimensional character vectors 10-dimensional character vectors 10-dimensional character vectors",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1102,
      "text": "and so now we just want to undo that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1103,
      "text": "and so now we just want to undo that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1104,
      "text": "and so now we just want to undo that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1105,
      "text": "so this is actually relatively so this is actually relatively so this is actually relatively straightforward operation because uh the straightforward operation because uh the straightforward operation because uh the backward pass of the what is the view backward pass of the what is the view backward pass of the what is the view view is just a representation of the view is just a representation of the view is just a representation of the array it's just a logical form of how array it's just a logical form of how array it's just a logical form of how you interpret the array so let's just you interpret the array so let's just you interpret the array so let's just reinterpret it to be what it was before reinterpret it to be what it was before reinterpret it to be what it was before so in other words the end is not uh 32 so in other words the end is not uh 32 so in other words the end is not uh 32 by 30.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1106,
      "text": "it is basically dmcat by 30.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1107,
      "text": "it is basically dmcat by 30.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1108,
      "text": "it is basically dmcat",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1109,
      "text": "but if you view it as the original shape but if you view it as the original shape but if you view it as the original shape so just m dot shape so just m dot shape so just m dot shape uh you can you can pass in tuples into uh you can you can pass in tuples into uh you can you can pass in tuples into view view view and so this should just be okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1110,
      "text": "we just re-represent that view",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1111,
      "text": "and then we just re-represent that view",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1112,
      "text": "and then we uncomment this line here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1113,
      "text": "and we uncomment this line here and we uncomment this line here and",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1114,
      "text": "hopefully hopefully hopefully",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1115,
      "text": "yeah",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1116,
      "text": "so the derivative of M is correct",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1117,
      "text": "yeah",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1118,
      "text": "so the derivative of M is correct",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1119,
      "text": "yeah",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1120,
      "text": "so the derivative of M is correct so in this case we just have to so in this case we just have to so in this case we just have to re-represent the shape of those re-represent the shape of those re-represent the shape of those derivatives into the original View derivatives into the original View derivatives into the original View",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1121,
      "text": "so now we are at the final line and the so now we are at the final line and the so now we are at the final line and the only thing that's left to back propagate only thing that's left to back propagate only thing that's left to back propagate through is this indexing operation here through is this indexing operation here through is this indexing operation here MSC at xB",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1122,
      "text": "so as I did before I copy MSC at xB so as I did before I copy MSC at xB so as I did before I copy pasted this line here and let's look at pasted this line here and let's look at pasted this line here and let's look at the shapes of everything that's involved the shapes of everything that's involved the shapes of everything that's involved and remind ourselves how this worked and remind ourselves how this worked and remind ourselves how this worked so m.shape was 32 by 3 by 10.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1123,
      "text": "so m.shape was 32 by 3 by 10.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1124,
      "text": "so m.shape was 32 by 3 by 10.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1125,
      "text": "it says 32 examples and then we have it says 32 examples",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1126,
      "text": "and then we have it says 32 examples",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1127,
      "text": "and then we have three characters each one of them has a three characters each one of them has a three characters each one of them has a 10 dimensional embedding 10 dimensional embedding 10 dimensional embedding and this was achieved by taking the and this was achieved by taking the and this was achieved by taking the lookup table C which have 27 possible lookup table C which have 27 possible lookup table C which have 27 possible characters characters characters each of them 10 dimensional and we each of them 10 dimensional and we each of them 10 dimensional and we looked up looked up looked up at the rows that were specified inside at the rows that were specified inside at the rows that were specified inside this tensor xB this tensor xB this tensor xB so XB is 32 by 3",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1128,
      "text": "and it's basically so XB is 32 by 3",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1129,
      "text": "and it's basically so XB is 32 by 3",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1130,
      "text": "and it's basically giving us for each example the Identity giving us for each example the Identity giving us for each example the Identity or the index of which character is part or the index of which character is part or the index of which character is part of that example of that example of that example and so here I'm showing the first five and so here I'm showing the first five and so here I'm showing the first five rows of three of this tensor xB rows of three of this tensor xB rows of three of this tensor xB",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1131,
      "text": "and so we can see that for example here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1132,
      "text": "and so we can see that for example here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1133,
      "text": "and so we can see that for example here it was the first example in this batch it was the first example in this batch it was the first example in this batch is that the first character and the is that the first character and the is that the first character and the first character and the fourth character first character and the fourth character first character and the fourth character comes into the neural net comes into the neural net comes into the neural net and then we want to predict the next",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1134,
      "text": "and then we want to predict the next",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1135,
      "text": "and then we want to predict the next character in a sequence after the character in a sequence after the character in a sequence after the character is one one four character is one one four character is one one four",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1136,
      "text": "so basically What's Happening Here is so basically What's Happening Here is so basically What's Happening Here is there are integers inside XB and each there are integers inside XB and each there are integers inside XB and each one of these integers is specifying one of these integers is specifying one of these integers is specifying which row of C we want to pluck out which row of C we want to pluck out which row of C we want to pluck out right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1137,
      "text": "and then we arrange those rows right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1138,
      "text": "and then we arrange those rows right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1139,
      "text": "and then we arrange those rows that we've plucked out into 32 by 3 by that we've plucked out into 32 by 3 by that we've plucked out into 32 by 3 by 10 tensor",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1140,
      "text": "and we just package them in we 10 tensor",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1141,
      "text": "and we just package them in we 10 tensor",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1142,
      "text": "and we just package them in we just package them into the sensor just package them into the sensor just package them into the sensor and now what's happening is that we have and now what's happening is that we have and now what's happening is that we have D amp D amp D amp",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1143,
      "text": "so for every one of these uh basically so for every one of these uh basically so for every one of these uh basically plucked out rows we have their gradients plucked out rows we have their gradients plucked out rows we have their gradients now now but they're arranged inside this 32 by 3",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1144,
      "text": "but they're arranged inside this 32 by 3",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1145,
      "text": "but they're arranged inside this 32 by 3 by 10 tensor",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1146,
      "text": "so all we have to do now is by 10 tensor",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1147,
      "text": "so all we have to do now is by 10 tensor",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1148,
      "text": "so all we have to do now is we just need to Route this gradient we just need to Route this gradient we just need to Route this gradient backwards through this assignment",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1149,
      "text": "so we backwards through this assignment so we backwards through this assignment so we need to find which row of C that every need to find which row of C that every need to find which row of C that every one of these one of these one of these um 10 dimensional embeddings come from um 10 dimensional embeddings come from um 10 dimensional embeddings come from and then we need to deposit them into DC",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1150,
      "text": "and then we need to deposit them into DC",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1151,
      "text": "and then we need to deposit them into DC",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1152,
      "text": "so we just need to undo the indexing",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1153,
      "text": "and so we just need to undo the indexing",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1154,
      "text": "and so we just need to undo the indexing and of course if any of these rows of C was of course if any of these rows of C was of course if any of these rows of C was used multiple times which almost used multiple times which almost used multiple times which almost certainly is the case like the row one certainly is the case like the row one certainly is the case like the row one and one was used multiple times then we and one was used multiple times then we and one was used multiple times then we have to remember that the gradients that have to remember that the gradients that have to remember that the gradients that arrive there have to add arrive there have to add arrive there have to add so for each occurrence we have to have so for each occurrence we have to have so for each occurrence we have to have an addition an addition an addition so let's now write this out and I don't so let's now write this out and I don't so let's now write this out and I don't actually know if like a much better way actually know if like a much better way actually know if like a much better way to do this than a for Loop unfortunately to do this than a for Loop unfortunately to do this than a for Loop unfortunately in Python in Python in Python",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1155,
      "text": "um so maybe someone can come up with a um so maybe someone can come up with a um so maybe someone can come up with a vectorized efficient operation but for vectorized efficient operation but for vectorized efficient operation but for now let's just use for loops so let me now let's just use for loops so let me now let's just use for loops so let me create a torch.zeros like create a torch.zeros like create a torch.zeros like C to initialize uh just uh 27 by 10 C to initialize uh just uh 27 by 10 C to initialize uh just uh 27 by 10 tensor of all zeros tensor of all zeros tensor of all zeros and then honestly 4K in range XB dot and then honestly 4K in range XB dot and then honestly 4K in range XB dot shape at zero shape at zero shape at zero maybe someone has a better way to do maybe someone has a better way to do maybe someone has a better way to do this but for J and range this but for J and range this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1156,
      "text": "but for J and range be that shape at one be that shape at one be that shape at one this is going to iterate over all the this is going to iterate over all the this is going to iterate over all the um all the elements of XB all these um all the elements of XB all these um all the elements of XB all these integers integers integers and then let's get the index at this and then let's get the index at this and then let's get the index at this position position position so the index is basically x b at KJ so the index is basically x b at KJ so the index is basically x b at KJ so that an example of that like is 11 or so that an example of that like is 11 or so that an example of that like is 11 or 14 and so on 14 and so on 14 and so on and now in the forward pass we took and now in the forward pass we took and now in the forward pass we took",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1157,
      "text": "and we basically took um the row of C at index and we deposited the row of C at index and we deposited it into M at K of J it into M at K of J it into M at K of J that's what happened that's where they that's what happened that's where they that's what happened that's where they are packaged so now we need to go are packaged so now we need to go are packaged so now we need to go backwards and we just need to route backwards and we just need to route backwards",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1158,
      "text": "and we just need to route DM at the position KJ DM at the position KJ DM at the position KJ we now have these derivatives we now have these derivatives we now have these derivatives for each position and it's 10 for each position and it's 10 for each position and it's 10 dimensional dimensional dimensional",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1159,
      "text": "and you just need to go into the correct",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1160,
      "text": "and you just need to go into the correct",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1161,
      "text": "and you just need to go into the correct row of C row of C row of C so DC rather at IX is this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1162,
      "text": "but plus so DC rather at IX is this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1163,
      "text": "but plus so DC rather at IX is this but plus equals equals equals because there could be multiple because there could be multiple because there could be multiple occurrences uh like the same row could occurrences uh like the same row could occurrences uh like the same row could have been used many many times and so have been used many many times and so have been used many many times and so all of those derivatives will just go all of those derivatives will just go all of those derivatives will just go backwards through the indexing and they backwards through the indexing and they backwards through the indexing and they will add will add will add so this is my candidate solution let's uncomment this and cross our let's uncomment this and cross our fingers fingers",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1164,
      "text": "hey hey",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1165,
      "text": "hey",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1166,
      "text": "so that's it we've back propagated so that's it we've back propagated so that's it we've back propagated through through this entire Beast this entire Beast this entire Beast",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1167,
      "text": "so there we go totally makes sense",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1168,
      "text": "so there we go totally makes sense",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1169,
      "text": "so there we go totally makes sense",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1170,
      "text": "so now we come to exercise two it so now we come to exercise two it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1171,
      "text": "so now we come to exercise two it basically turns out that in this first basically turns out that in this first basically turns out that in this first exercise we were doing way too much work exercise we were doing way too much work exercise we were doing way too much work uh we were back propagating way too much uh we were back propagating way too much uh we were back propagating way too much",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1172,
      "text": "and it was all good practice and so on and it was all good practice and",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1173,
      "text": "so on and it was all good practice and so on but it's not what you would do in",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1174,
      "text": "but it's not what you would do in",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1175,
      "text": "but it's not what you would do in practice and the reason for that is for practice and the reason for that is for practice and the reason for that is for example here I separated out this loss example here I separated out this loss example here I separated out this loss calculation over multiple lines and I calculation over multiple lines and I calculation over multiple lines",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1176,
      "text": "and I broke it up all all to like its smallest broke it up all all to like its smallest broke it up all all to like its smallest atomic pieces and we back propagated atomic pieces and we back propagated atomic pieces and we back propagated through all of those individually through all of those individually through all of those individually but it turns out that if you just look but it turns out that if you just look but it turns out that if you just look at the mathematical expression for the at the mathematical expression for the at the mathematical expression for the loss loss loss um then actually you can do the um then actually you can do the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1177,
      "text": "um then actually you can do the differentiation on pen and paper and a differentiation on pen and paper and a differentiation on pen and paper and a lot of terms cancel and simplify and the lot of terms cancel and simplify and the lot of terms cancel and simplify and the mathematical expression you end up with mathematical expression you end up with mathematical expression you end up with can be significantly shorter and easier can be significantly shorter and easier can be significantly shorter and easier to implement than back propagating to implement than back propagating to implement than back propagating through all the little pieces of through all the little pieces of through all the little pieces of everything you've done everything you've done everything you've done so before we had this complicated",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1178,
      "text": "so before we had this complicated so before we had this complicated forward paths going from logits to the forward paths going from logits to the forward paths going from logits to the loss loss but in pytorch everything can just be but in pytorch everything can just be but in pytorch everything can just be glued together into a single call at glued together into a single call at glued together into a single call at that cross entropy you just pass in that cross entropy you just pass in that cross entropy you just pass in logits and the labels and you get the logits and the labels and you get the logits and the labels and you get the exact same loss as I verify here so our exact same loss as I verify here so our exact same loss as I verify here so our previous loss and the fast loss coming previous loss and the fast loss coming previous loss and the fast loss coming from the chunk of operations as a single from the chunk of operations as a single from the chunk of operations as a single mathematical expression is the same but mathematical expression is the same but mathematical expression is the same",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1179,
      "text": "but it's much much faster in a forward pass it's much much faster in a forward pass it's much much faster in a forward pass it's also much much faster in backward",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1180,
      "text": "it's also much much faster in backward it's also much much faster in backward pass and the reason for that is if you pass and the reason for that is if you pass and the reason for that is if you just look at the mathematical form of just look at the mathematical form of just look at the mathematical form of this and differentiate again you will this and differentiate again you will this and differentiate again you will end up with a very small and short end up with a very small and short end up with a very small and short expression so that's what we want to do expression so that's what we want to do expression so that's what we want to do here we want to in a single operation or here we want to in a single operation or here we want to in a single operation or in a single go or like very quickly go in a single go or like very quickly go in a single go or like very quickly go directly to delojits directly to delojits directly to delojits and we need to implement the logits as a and we need to implement the logits as a and we need to implement the logits as a function of logits and yb's function of logits and yb's function of logits and yb's but it will be significantly shorter but it will be significantly shorter but it will be significantly shorter than whatever we did here where to get than whatever we did here where to get than whatever we did here where to get to deluggets we had to go all the way to deluggets we had to go all the way to deluggets we had to go all the way here here so all of this work can be skipped in a so all of this work can be skipped in a so all of this work can be skipped in a much much simpler mathematical much much simpler mathematical much much simpler mathematical expression that you can Implement here expression that you can Implement here expression that you can Implement here so you can give it a shot yourself so you can give it a shot yourself so you can give it a shot yourself basically look at what exactly is the basically look at what exactly is the basically look at what exactly is the mathematical expression of loss and mathematical expression of loss and mathematical expression of loss and differentiate with respect to the logits differentiate with respect to the logits differentiate with respect to the logits so let me show you a hint you can of so let me show you a hint you can of so let me show you a hint you can of course try it fully yourself but if not course try it fully yourself but if not course try it fully yourself but if not I can give you some hint of how to get I can give you some hint of how to get I can give you some hint of how to get started mathematically so basically What's Happening Here is we so basically What's Happening Here is we have logits then there's a softmax that have logits then there's a softmax that have logits then there's a softmax that takes the logits and gives you takes the logits and gives you takes the logits and gives you probabilities then we are using the probabilities then we are using the probabilities then we are using the identity of the correct next character identity of the correct next character identity of the correct next character to pluck out a row of probabilities take to pluck out a row of probabilities take to pluck out a row of probabilities take the negative log of it to get our the negative log of it to get our the negative log of it to get our negative block probability and then we negative block probability and then we negative block probability",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1181,
      "text": "and then we average up all the log probabilities or average up all the log probabilities or average up all the log probabilities or negative block probabilities to get our negative block probabilities to get our negative block probabilities to get our loss loss so basically what we have is for a so basically what we have is for a so basically what we have is for a single individual example rather we have single individual example rather we have single individual example rather we have that loss is equal to negative log that loss is equal to negative log that loss is equal to negative log probability uh where P here is kind of probability uh where P here is kind of probability uh where P here is kind of like thought of as a vector of all the like thought of as a vector of all the like thought of as a vector of all the probabilities so at the Y position where probabilities so at the Y position where probabilities so at the Y position where Y is the label Y is the label Y is the label and we have that P here of course is the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1182,
      "text": "and we have that P here of course is the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1183,
      "text": "and we have that P here of course is the softmax",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1184,
      "text": "so the ith component of P of softmax so the ith component of P of softmax",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1185,
      "text": "so the ith component of P of this probability Vector is just the this probability Vector is just the this probability Vector is just the softmax function so raising all the softmax function so raising all the softmax function so raising all the logits uh basically to the power of E logits uh basically to the power of E logits uh basically to the power of E and normalizing so everything comes to and normalizing so everything comes to and normalizing so everything comes to 1. 1. 1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1186,
      "text": "now if you write out P of Y here you can now if you write out P of Y here you can now if you write out P of Y here you can just write out the soft Max and then just write out the soft Max and then just write out the soft Max and then basically what we're interested in is basically what we're interested in is basically what we're interested in is we're interested in the derivative of we're interested in the derivative of we're interested in the derivative of the loss with respect to the I logit the loss with respect to the I logit the loss with respect to the I logit",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1187,
      "text": "and so basically it's a d by DLI of this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1188,
      "text": "and so basically it's a d by DLI of this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1189,
      "text": "and so basically it's a d by DLI of this expression here expression here expression here where we have L indexed with the where we have L indexed with the where we have L indexed with the specific label Y and on the bottom we specific label Y and on the bottom we specific label Y and on the bottom we have a sum over J of e to the L J and have a sum over J of e to the L J and have a sum over J of e to the L J and the negative block of all that so the negative block of all that so the negative block of all that so potentially give it a shot pen and paper potentially give it a shot pen and paper potentially give it a shot pen and paper and see if you can actually derive the and see if you can actually derive the and see if you can actually derive the expression for the loss by DLI and then expression for the loss by DLI and then expression for the loss by DLI and then we're going to implement it here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1190,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1191,
      "text": "so we're going to implement it here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1192,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1193,
      "text": "so we're going to implement it here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1194,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1195,
      "text": "so I'm going to give away the result here I'm going to give away the result here I'm going to give away the result here so this is some of the math I did to so this is some of the math I did to",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1196,
      "text": "so this is some of the math I did to derive the gradients analytically and so derive the gradients analytically and so derive the gradients analytically",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1197,
      "text": "and so we see here that I'm just applying the we see here that I'm just applying the we see here that I'm just applying the rules of calculus from your first or rules of calculus from your first or rules of calculus from your first or second year of bachelor's degree if you second year of bachelor's degree if you second year of bachelor's degree if you took it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1198,
      "text": "and we see that the expression took it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1199,
      "text": "and we see that the expression took it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1200,
      "text": "and we see that the expression is actually simplify quite a bit you is actually simplify quite a bit you is actually simplify quite a bit you have to separate out the analysis in the have to separate out the analysis in the have to separate out the analysis in the case where the ith index that you're case where the ith index that you're case where the ith index that you're interested in inside logits is either interested in inside logits is either interested in inside logits is either equal to the label or it's not equal to equal to the label or it's not equal to equal to the label or it's not equal to the label",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1201,
      "text": "and then the expression the label and then the expression the label",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1202,
      "text": "and then the expression simplify and cancel in a slightly simplify and cancel in a slightly simplify and cancel in a slightly different way and what we end up with is different way and what we end up with is different way and what we end up with is something very very simple something very very simple something very very simple",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1203,
      "text": "and we either end up with basically and we either end up with basically and we either end up with basically pirai where p is again this Vector of pirai where p is again this Vector of pirai where p is again this Vector of probabilities after a soft Max or P at I probabilities after a soft Max or P at I probabilities after a soft Max or P at I minus 1 where we just simply subtract a minus 1 where we just simply subtract a minus 1 where we just simply subtract a one but in any case we just need to one but in any case we just need to one but in any case we just need to calculate the soft Max p e and then in calculate the soft Max p e and then in calculate the soft Max p e",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1204,
      "text": "and then in the correct Dimension we need to the correct Dimension we need to the correct Dimension we need to subtract one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1205,
      "text": "and that's the gradient the subtract one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1206,
      "text": "and that's the gradient the subtract one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1207,
      "text": "and that's the gradient the form that it takes analytically so let's form that it takes analytically so let's form that it takes analytically so let's implement this basically and we have to implement this basically and we have to implement this basically and we have to keep in mind that this is only done for keep in mind that this is only done for keep in mind that this is only done for a single example",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1208,
      "text": "but here we are working a single example",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1209,
      "text": "but here we are working a single example",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1210,
      "text": "but here we are working with batches of examples with batches of examples with batches of examples",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1211,
      "text": "so we have to be careful of that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1212,
      "text": "and so we have to be careful of that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1213,
      "text": "and so we have to be careful of that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1214,
      "text": "and then the loss for a batch is the average then the loss for a batch is the average then the loss for a batch is the average loss over all the examples so in other loss over all the examples so in other loss over all the examples so in other words is the example for all the words is the example for all the words is the example for all the individual examples is the loss for each individual examples is the loss for each individual examples is the loss for each individual example summed up and then individual example summed up and then individual example summed up and then divided by n and we have to back divided by n and we have to back divided by n and we have to back propagate through that as well and be propagate through that as well and be propagate through that as well and be careful with it careful with it careful with it so deluggets is going to be of that soft so deluggets is going to be of that soft so deluggets is going to be of that soft Max Max Max uh pytorch has a softmax function that uh pytorch has a softmax function that uh pytorch has a softmax function that you can call and we want to apply the you can call",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1215,
      "text": "and we want to apply the you can call",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1216,
      "text": "and we want to apply the softmax on the logits and we want to go softmax on the logits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1217,
      "text": "and we want to go softmax on the logits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1218,
      "text": "and we want to go in the dimension that is one so in the dimension that is one so in the dimension that is one so basically we want to do the softmax basically we want to do the softmax basically we want to do the softmax along the rows of these logits along the rows of these logits along the rows of these logits then at the correct positions we need to then at the correct positions we need to then at the correct positions we need to subtract a 1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1219,
      "text": "so delugits at iterating subtract a 1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1220,
      "text": "so delugits at iterating subtract a 1.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1221,
      "text": "so delugits at iterating over all the rows over all the rows over all the rows and indexing into the columns and indexing into the columns and indexing into the columns provided by the correct labels inside YB provided by the correct labels inside YB provided by the correct labels inside YB we need to subtract one we need to subtract one we need to subtract one and then finally it's the average loss and then finally it's the average loss and then finally it's the average loss that is the loss and in the average that is the loss and in the average that is the loss and in the average there's a one over n of all the losses there's a one over n of all the losses there's a one over n of all the losses added up",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1222,
      "text": "and so we need to also added up",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1223,
      "text": "and so we need to also added up",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1224,
      "text": "and so we need to also propagate through that division propagate through that division propagate through that division so the gradient has to be scaled down by so the gradient has to be scaled down by so the gradient has to be scaled down by by n as well because of the mean by n as well because of the mean by n as well because of the mean",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1225,
      "text": "but this otherwise should be the result but this otherwise should be the result but this otherwise should be the result so now if we verify this so now if we verify this so now if we verify this we see that we don't get an exact match we see that we don't get an exact match we see that we don't get an exact match but at the same time the maximum",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1226,
      "text": "but at the same time the maximum but at the same time the maximum difference from logits from pytorch and difference from logits from pytorch and difference from logits from pytorch and RD logits here is uh on the order of 5e RD logits here is uh on the order of 5e RD logits here is uh on the order of 5e negative 9.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1227,
      "text": "so it's a tiny tiny number negative 9.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1228,
      "text": "so it's a tiny tiny number negative 9.",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1229,
      "text": "so it's a tiny tiny number so because of floating point wantiness so because of floating point wantiness so because of floating point wantiness we don't get the exact bitwise result we don't get the exact bitwise result we don't get the exact bitwise result but we basically get the correct answer",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1230,
      "text": "but we basically get the correct answer",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1231,
      "text": "but we basically get the correct answer approximately approximately approximately now I'd like to pause here briefly now I'd like to pause here briefly now I'd like to pause here briefly before we move on to the next exercise before we move on to the next exercise before we move on to the next exercise because I'd like us to get an intuitive because I'd like us to get an intuitive because I'd like us to get an intuitive sense of what the logits is because it sense of what the logits is because it sense of what the logits is because it has a beautiful and very simple has a beautiful and very simple has a beautiful and very simple explanation honestly explanation honestly explanation honestly um so here I'm taking the logits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1232,
      "text": "and I'm um so here I'm taking the logits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1233,
      "text": "and I'm um so here I'm taking the logits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1234,
      "text": "and I'm visualizing it and we can see that we visualizing it and we can see that we visualizing it and we can see that we have a batch of 32 examples of 27 have a batch of 32 examples of 27 have a batch of 32 examples of 27 characters characters and what is the logits intuitively right and what is the logits intuitively right and what is the logits intuitively right the logits is the probabilities that the the logits is the probabilities that the the logits is the probabilities that the properties Matrix in the forward pass properties Matrix in the forward pass properties Matrix in the forward pass but then here these black squares are",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1235,
      "text": "but then here these black squares are but then here these black squares are the positions of the correct indices the positions of the correct indices the positions of the correct indices where we subtracted a one where we subtracted a one where we subtracted a one and so uh what is this doing right these",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1236,
      "text": "and so uh what is this doing right these",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1237,
      "text": "and so uh what is this doing right these are the derivatives on the logits and so are the derivatives on the logits and so are the derivatives on the logits and so let's look at just the first row here let's look at just the first row here let's look at just the first row here so that's what I'm doing here I'm so that's what I'm doing here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1238,
      "text": "I'm so that's what I'm doing here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1239,
      "text": "I'm clocking the probabilities of these clocking the probabilities of these clocking the probabilities of these logits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1240,
      "text": "and then I'm taking just the logits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1241,
      "text": "and then I'm taking just the logits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1242,
      "text": "and then I'm taking just the first row and this is the probability first row and this is the probability first row",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1243,
      "text": "and this is the probability row",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1244,
      "text": "and then the logits of the first row row and then the logits of the first row row and then the logits of the first row and multiplying by n just for us so that and multiplying by n just for us so that and multiplying by n just for us so that we don't have the scaling by n in here we don't have the scaling by n in here we don't have the scaling by n in here and everything is more interpretable we and everything is more interpretable we and everything is more interpretable we see that it's exactly equal to the see that it's exactly equal to the see that it's exactly equal to the probability of course",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1245,
      "text": "but then the probability of course",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1246,
      "text": "but then the probability of course",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1247,
      "text": "but then the position of the correct index has a position of the correct index has a position of the correct index has a minus equals one so minus one on that minus equals one so minus one on that minus equals one so minus one on that position position and so notice that and so notice that and so notice that um if you take Delo Jets at zero and",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1248,
      "text": "you um if you take Delo Jets at zero and you um if you take Delo Jets at zero and you sum it sum it sum it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1249,
      "text": "it actually sums to zero",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1250,
      "text": "and so you it actually sums to zero",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1251,
      "text": "and so you it actually sums to zero and so you should think of these uh gradients here should think of these uh gradients here should think of these uh gradients here at each cell as like a force at each cell as like a force at each cell as like a force um we are going to be basically pulling um we are going to be basically pulling um we are going to be basically pulling down on the probabilities of the down on the probabilities of the down on the probabilities of the incorrect characters and we're going to incorrect characters and we're going to incorrect characters and we're going to be pulling up on the probability at the be pulling up on the probability at the be pulling up on the probability at the correct index and that's what's correct index and that's what's correct index",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1252,
      "text": "and that's what's basically happening in each row and thus basically happening in each row and thus basically happening in each row and thus the amount of push and pull is exactly the amount of push and pull is exactly the amount of push and pull is exactly equalized because the sum is zero so the equalized because the sum is zero so the equalized because the sum is zero so the amount to which we pull down in the amount to which we pull down in the amount to which we pull down in the probabilities and the demand that we probabilities and the demand that we probabilities and the demand that we push up on the probability of the push up on the probability of the push up on the probability of the correct character is equal correct character is equal correct character is equal so sort of the the repulsion and the so sort of the the repulsion and the so sort of the the repulsion and the attraction are equal and think of the attraction are equal and think of the attraction are equal and think of the neural app now as a like a massive uh neural app now as a like a massive uh neural app now as a like a massive uh pulley system or something like that pulley system or something like that pulley system or something like that we're up here on top of the logits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1253,
      "text": "and we're up here on top of the logits and we're up here on top of the logits",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1254,
      "text": "and we're pulling up we're pulling down the we're pulling up we're pulling down the we're pulling up we're pulling down the properties of Incorrect and pulling up properties of Incorrect and pulling up properties of Incorrect and pulling up the property of the correct and in this the property of the correct and in this the property of the correct and in this complicated pulley system because complicated pulley system because complicated pulley system because everything is mathematically uh just everything is mathematically uh just everything is mathematically uh just determined just think of it as sort of determined just think of it as sort of determined just think of it as sort of like this tension translating to this like this tension translating to this like this tension translating to this complicating pulling mechanism and then complicating pulling mechanism and then complicating pulling mechanism",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1255,
      "text": "and then eventually we get a tug on the weights eventually we get a tug on the weights eventually we get a tug on the weights and the biases and basically in each and the biases and basically in each and the biases and basically in each update we just kind of like tug in the update we just kind of like tug in the update we just kind of like tug in the direction that we like for each of these direction that we like for each of these direction that we like for each of these elements and the parameters are slowly elements and the parameters are slowly elements and the parameters are slowly given in to the tug and that's what given in to the tug and that's what given in to the tug and that's what training in neural net kind of like training in neural net kind of like training in neural net kind of like looks like on a high level looks like on a high level looks like on a high level",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1256,
      "text": "and so I think the the forces of push",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1257,
      "text": "and so I think the the forces of push",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1258,
      "text": "and so I think the the forces of push and pull in these gradients are actually and pull in these gradients are actually and pull in these gradients are actually uh very intuitive here we're pushing and uh very intuitive here we're pushing and uh very intuitive here we're pushing and pulling on the correct answer and the pulling on the correct answer and the pulling on the correct answer and the incorrect answers and the amount of incorrect answers and the amount of incorrect answers and the amount of force that we're applying is actually force that we're applying is actually force that we're applying is actually proportional to uh the probabilities proportional to uh the probabilities proportional to uh the probabilities that came out in the forward pass that came out in the forward pass that came out in the forward pass and so for example if our probabilities and so for example",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1259,
      "text": "if our probabilities and so for example if our probabilities came out exactly correct so they would came out exactly correct so they would came out exactly correct so they would have had zero everywhere except for one have had zero everywhere except for one have had zero everywhere except for one at the correct uh position then the the at the correct uh position then the the at the correct uh position then the the logits would be all a row of zeros for logits would be all a row of zeros for logits would be all a row of zeros for that example there would be no push and that example there would be no push and that example there would be no push and pull so the amount to which your pull so the amount to which your pull so the amount to which your prediction is incorrect is exactly the prediction is incorrect is exactly the prediction is incorrect is exactly the amount by which you're going to get a amount by which you're going to get a amount by which you're going to get a pull or a push in that dimension pull or a push in that dimension pull or a push in that dimension",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1260,
      "text": "so if you have for example a very so if you have for example a very so if you have for example a very confidently mispredicted element here confidently mispredicted element here confidently mispredicted element here then then then um what's going to happen is that um what's going to happen is that um what's going to happen is that element is going to be pulled down very element is going to be pulled down very element is going to be pulled down very heavily and the correct answer is going heavily and the correct answer is going heavily and the correct answer is going to be pulled up to the same amount to be pulled up to the same amount to be pulled up to the same amount and the other characters are not going and the other characters are not going and the other characters are not going to be influenced too much to be influenced too much to be influenced too much so the amounts to which you mispredict so the amounts to which you mispredict so the amounts to which you mispredict is then proportional to the strength of is then proportional to the strength of is then proportional to the strength of the pole and that's happening the pole and that's happening the pole and that's happening independently in all the dimensions of independently in all the dimensions of independently in all the dimensions of this of this tensor",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1261,
      "text": "and it's sort of this of this tensor",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1262,
      "text": "and it's sort of this of this tensor",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1263,
      "text": "and it's sort of very intuitive and varies to think very intuitive and varies to think very intuitive and varies to think through and that's basically the magic through and that's basically the magic through and that's basically the magic of the cross-entropy loss and what it's of the cross-entropy loss and what it's of the cross-entropy loss and what it's doing dynamically in the backward pass doing dynamically in the backward pass doing dynamically in the backward pass of the neural net",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1264,
      "text": "so now we get to of the neural net so now we get to of the neural net so now we get to exercise number three which is a very exercise number three which is a very exercise number three which is a very fun exercise fun exercise fun exercise um depending on your definition of fun um depending on your definition of fun um depending on your definition of fun",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1265,
      "text": "and we are going to do for batch",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1266,
      "text": "and we are going to do for batch",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1267,
      "text": "and we are going to do for batch normalization exactly what we did for normalization exactly what we did for normalization exactly what we did for cross entropy loss in exercise number cross entropy loss in exercise number cross entropy loss in exercise number two that is we are going to consider it two that is we are going to consider it two that is we are going to consider it as a glued single mathematical as a glued single mathematical as a glued single mathematical expression and back propagate through it expression and back propagate through it expression and back propagate through it in a very efficient manner because we in a very efficient manner because we in a very efficient manner because we are going to derive a much simpler are going to derive a much simpler are going to derive a much simpler formula for the backward path of batch formula for the backward path of batch formula for the backward path of batch normalization normalization normalization and we're going to do that using pen and and we're going to do that using pen and and we're going to do that using pen and paper paper paper so previously we've broken up so previously we've broken up so previously we've broken up bastionalization into all of the little bastionalization into all of the little bastionalization into all of the little intermediate pieces and all the atomic intermediate pieces and all the atomic intermediate pieces and all the atomic operations inside it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1268,
      "text": "and then we back operations inside it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1269,
      "text": "and then we back operations inside it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1270,
      "text": "and then we back propagate it through it one by one propagate it through it one by one propagate it through it one by one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1271,
      "text": "now we just have a single sort of now we just have a single sort of now we just have a single sort of forward pass of a batch form and it's forward pass of a batch form",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1272,
      "text": "and it's forward pass of a batch form and it's all glued together all glued together all glued together",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1273,
      "text": "and we see that we get the exact same",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1274,
      "text": "and we see that we get the exact same",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1275,
      "text": "and we see that we get the exact same result as before result as before result as before now for the backward pass we'd like to now for the backward pass we'd like to now for the backward pass we'd like to also Implement a single formula also Implement a single formula also Implement a single formula basically for back propagating through basically for back propagating through basically for back propagating through this entire operation that is the this entire operation that is the this entire operation that is the bachelorization bachelorization bachelorization so in the forward pass previously we so in the forward pass previously we so in the forward pass previously we took hpvn the hidden states of the took hpvn the hidden states of the took hpvn the hidden states of the pre-batch realization and created H pre-batch realization and created H pre-batch realization and created H preact which is the hidden States just preact which is the hidden States just preact which is the hidden States just before the activation before the activation before the activation in the bachelorization paper each pbn is in the bachelorization paper each pbn is in the bachelorization paper each pbn is X and each preact is y X",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1276,
      "text": "and each preact is y X",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1277,
      "text": "and each preact is y",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1278,
      "text": "so in the backward pass what we'd like so in the backward pass what we'd like so in the backward pass what we'd like to do now is we have DH preact",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1279,
      "text": "and we'd to do now is we have DH preact",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1280,
      "text": "and we'd to do now is we have DH preact",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1281,
      "text": "and we'd like to produce d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1282,
      "text": "h previous like to produce d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1283,
      "text": "h previous like to produce d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1284,
      "text": "h previous",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1285,
      "text": "and we'd like to do that in a very",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1286,
      "text": "and we'd like to do that in a very",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1287,
      "text": "and we'd like to do that in a very efficient manner so that's the name of efficient manner",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1288,
      "text": "so that's the name of efficient manner so that's the name of the game calculate the H previan given the game calculate the H previan given the game calculate the H previan given DH preact and for the purposes of this DH preact and for the purposes of this DH preact and for the purposes of this exercise we're going to ignore gamma and exercise we're going to ignore gamma and exercise we're going to ignore gamma and beta and their derivatives because they beta and their derivatives because they beta and their derivatives because they take on a very simple form in a very take on a very simple form in a very take on a very simple form in a very similar way to what we did up above similar way to what we did up above similar way to what we did up above so let's calculate this given that right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1289,
      "text": "so let's calculate this given that right",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1290,
      "text": "so let's calculate this given that right here here so to help you a little bit like I did so to help you a little bit like I did so to help you a little bit like I did before I started off the implementation before I started off the implementation before I started off the implementation here on pen and paper",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1291,
      "text": "and I took two here on pen and paper",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1292,
      "text": "and I took two here on pen and paper",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1293,
      "text": "and I took two sheets of paper to derive the sheets of paper to derive the sheets of paper to derive the mathematical formulas for the backward mathematical formulas for the backward mathematical formulas for the backward pass pass pass and basically to set up the problem uh and basically to set up the problem uh and basically to set up the problem uh just write out the MU Sigma Square just write out the MU Sigma Square just write out the MU Sigma Square variance",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1294,
      "text": "x i hat and Y",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1295,
      "text": "I exactly as in variance x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1296,
      "text": "i hat and Y",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1297,
      "text": "I exactly as in variance x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1298,
      "text": "i hat and Y",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1299,
      "text": "I exactly as in the paper except for the bezel the paper except for the bezel the paper except for the bezel correction correction correction and then and then and then in a backward pass we have the in a backward pass we have the in a backward pass we have the derivative of the loss with respect to derivative of the loss with respect to all the elements of Y and remember that all the elements of Y and remember that all the elements of Y and remember that Y is a vector there's there's multiple Y is a vector there's there's multiple Y is a vector there's there's multiple numbers here numbers here numbers here so we have all the derivatives with so we have all the derivatives with so we have all the derivatives with respect to all the Y's respect to all the Y's respect to all the Y's",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1300,
      "text": "and then there's a demo and a beta",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1301,
      "text": "and and then there's a demo and a beta",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1302,
      "text": "and and then there's a demo and a beta",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1303,
      "text": "and this is kind of like the compute graph this is kind of like the compute graph this is kind of like the compute graph the gamma and the beta there's the X hat the gamma and the beta there's the X hat the gamma and the beta there's the X hat",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1304,
      "text": "and then the MU and the sigma squared and then the MU and the sigma squared and then the MU and the sigma squared and the X",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1305,
      "text": "so we have DL by DYI and we and the X",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1306,
      "text": "so we have DL by DYI and we and the X",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1307,
      "text": "so we have DL by DYI",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1308,
      "text": "and we won't DL by d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1309,
      "text": "x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1310,
      "text": "i for all the I's in won't DL by d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1311,
      "text": "x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1312,
      "text": "i for all the I's in won't DL by d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1313,
      "text": "x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1314,
      "text": "i for all the I's in these vectors these vectors these vectors so this is the compute graph and you",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1315,
      "text": "so this is the compute graph and you",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1316,
      "text": "so this is the compute graph and you have to be careful because I'm trying to have to be careful because I'm trying to have to be careful because I'm trying to note here that these are vectors so note here that these are vectors so note here that these are vectors so there's many nodes here inside x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1317,
      "text": "x hat there's many nodes here inside x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1318,
      "text": "x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1319,
      "text": "hat",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1320,
      "text": "there's many nodes here inside x x hat and Y but mu and sigma sorry Sigma and Y but mu and sigma sorry Sigma and Y but mu and sigma sorry Sigma Square are just individual scalars Square are just individual scalars Square are just individual scalars single numbers",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1321,
      "text": "so you have to be careful single numbers so you have to be careful single numbers so you have to be careful with that you have to imagine there's with that you have to imagine there's with that you have to imagine there's multiple nodes here or you're going to multiple nodes here or you're going to multiple nodes here or you're going to get your math wrong get your math wrong get your math wrong",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1322,
      "text": "um so as an example I would suggest that um so as an example I would suggest that um so as an example I would suggest that you go in the following order one two you go in the following order one two you go in the following order one two three four in terms of the back three four in terms of the back three four in terms of the back propagation so back propagating to X hat propagation so back propagating to X hat propagation so back propagating to X hat then into Sigma Square then into mu and then into Sigma Square then into mu and then into Sigma Square then into mu and then into X then into X then into X um just like in a topological sort in um just like in a topological sort in um just like in a topological sort in micrograd we would go from right to left micrograd we would go from right to left micrograd we would go from right to left you're doing the exact same thing except you're doing the exact same thing except you're doing the exact same thing except you're doing it with symbols and on a you're doing it with symbols and on a you're doing it with symbols and on a piece of paper piece of paper piece of paper",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1323,
      "text": "so for number one uh I'm not giving away so for number one uh I'm not giving away so for number one uh I'm not giving away too much if you want DL of d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1324,
      "text": "x i hat too much if you want DL of d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1325,
      "text": "x i hat too much if you want DL of d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1326,
      "text": "x i hat",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1327,
      "text": "then we just take DL by DYI and multiply then we just take DL by DYI and multiply",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1328,
      "text": "then we just take DL by DYI and multiply it by gamma because of this expression it by gamma because of this expression it by gamma because of this expression here where any individual Yi is just here where any individual Yi is just here where any individual Yi is just gamma times x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1329,
      "text": "i hat plus beta",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1330,
      "text": "so it gamma times x i hat plus beta",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1331,
      "text": "so it gamma times x i hat plus beta",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1332,
      "text": "so it doesn't help you too much there",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1333,
      "text": "but this doesn't help you too much there",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1334,
      "text": "but this doesn't help you too much there",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1335,
      "text": "but this gives you basically the derivatives for gives you basically the derivatives for gives you basically the derivatives for all the X hats and so now try to go all the X hats and so now try to go all the X hats and so now try to go through this computational graph and through this computational graph and through this computational graph and derive what is DL by D Sigma Square derive what is DL by D Sigma Square derive what is DL by D Sigma Square and then what is DL by B mu and then one and then what is DL by B mu and then one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1336,
      "text": "and then what is DL by B mu and then one is D L by DX is D L by DX is D L by DX eventually so give it a go",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1337,
      "text": "and I'm going eventually so give it a go",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1338,
      "text": "and I'm going eventually so give it a go",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1339,
      "text": "and I'm going to be revealing the answer one piece at to be revealing the answer one piece at to be revealing the answer one piece at a time",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1340,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1341,
      "text": "so to get DL by D Sigma a time",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1342,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1343,
      "text": "so to get DL by D Sigma a time",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1344,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1345,
      "text": "so to get DL by D Sigma Square we have to remember again like I Square we have to remember again like I Square we have to remember again like I mentioned that there are many excess X mentioned that there are many excess X mentioned that there are many excess X hats here hats here hats here and remember that Sigma square is just a and remember that Sigma square is just a and remember that Sigma square is just a single individual number here single individual number here single individual number here so when we look at the expression so when we look at the expression so when we look at the expression for the L by D Sigma Square for the L by D Sigma Square for the L by D Sigma Square we have that we have to actually we have that we have to actually we have that we have to actually consider all the possible paths that um consider all the possible paths that um consider all the possible paths that um we basically have that there's many X we basically have that there's many X we basically have that there's many X hats and they all feed off from they all hats and they all feed off from they all hats and they all feed off from they all depend on Sigma Square so Sigma square depend on Sigma Square so Sigma square depend on Sigma Square so Sigma square has a large fan out there's lots of has a large fan out there's lots of has a large fan out there's lots of arrows coming out from Sigma square into arrows coming out from Sigma square into arrows coming out from Sigma square into all the X hats all the X hats all the X hats and then there's a back propagating and then there's a back propagating and then there's a back propagating signal from each X hat into Sigma square signal from each X hat into Sigma square signal from each X hat into Sigma square and that's why we actually need to sum and that's why we actually need to sum and that's why we actually need to sum over all those I's from I equal to 1 to over all those I's from I equal to 1 to over all those I's from I equal to 1 to m m m of the DL by d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1346,
      "text": "x i hat which is the of the DL by d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1347,
      "text": "x i hat which is the of the DL by d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1348,
      "text": "x i hat which is the global gradient global gradient global gradient times the x i Hat by D Sigma Square times the x i Hat by D Sigma Square times the x i Hat by D Sigma Square which is the local gradient which is the local gradient which is the local gradient of this operation here of this operation here of this operation here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1349,
      "text": "and then mathematically I'm just working and then mathematically I'm just working and then mathematically I'm just working it out here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1350,
      "text": "and I'm simplifying",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1351,
      "text": "and you it out here and I'm simplifying and you it out here and I'm simplifying and you get a certain expression for DL by D get a certain expression for DL by D get a certain expression for DL by D Sigma square",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1352,
      "text": "and we're going to be using Sigma square",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1353,
      "text": "and we're going to be using Sigma square",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1354,
      "text": "and we're going to be using this expression when we back propagate this expression when we back propagate this expression when we back propagate into mu and then eventually into X so into mu and then eventually into X so into mu and then eventually into X so now let's continue our back propagation now let's continue our back propagation now let's continue our back propagation into mu",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1355,
      "text": "so what is D L by D mu now again into mu",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1356,
      "text": "so what is D L by D mu now again into mu so what is D L by D mu now again be careful that mu influences X hat and be careful that mu influences X hat and be careful that mu influences X hat and X hat is actually lots of values so for X hat is actually lots of values so for X hat is actually lots of values so for example if our mini batch size is 32 as example if our mini batch size is 32 as example if our mini batch size is 32 as it is in our example that we were it is in our example that we were it is in our example that we were working on then this is 32 numbers and working on then this is 32 numbers and working on then this is 32 numbers and 32 arrows going back to mu and then mu 32 arrows going back to mu and then mu 32 arrows going back to mu and then mu going to Sigma square is just a single going to Sigma square is just a single going to Sigma square is just a single Arrow because Sigma square is a scalar Arrow because Sigma square is a scalar Arrow because Sigma square is a scalar so in total there are 33 arrows so in total there are 33 arrows so in total there are 33 arrows emanating from you and then all of them emanating from you and then all of them emanating from you and then all of them have gradients coming into mu and they have gradients coming into mu and they have gradients coming into mu and they all need to be summed up all need to be summed up all need to be summed up and so that's why when we look at the and so that's why when we look at the and so that's why when we look at the expression for DL by D mu I am summing expression for DL by D mu I am summing expression for DL by D mu I am summing up over all the gradients of DL by d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1357,
      "text": "x i up over all the gradients of DL by d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1358,
      "text": "x i up over all the gradients of DL by d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1359,
      "text": "x i hat times the x i Hat by being mu hat times the x i Hat by being mu hat times the x i Hat by being mu uh so that's the that's this arrow",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1360,
      "text": "and uh so that's the that's this arrow",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1361,
      "text": "and uh so that's the that's this arrow",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1362,
      "text": "and that's 32 arrows here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1363,
      "text": "and then plus the that's 32 arrows here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1364,
      "text": "and then plus the that's 32 arrows here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1365,
      "text": "and then plus the one Arrow from here which is the L by one Arrow from here which is the L by one Arrow from here which is the L by the sigma Square Times the sigma squared the sigma Square Times the sigma squared the sigma Square Times the sigma squared by D mu by D mu by D mu so now we have to work out that so now we have to work out that so now we have to work out that expression and let me just reveal the expression and let me just reveal the expression and let me just reveal the rest of it rest of it rest of it uh simplifying here is not complicated uh simplifying here is not complicated uh simplifying here is not complicated the first term",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1366,
      "text": "and you just get an the first term",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1367,
      "text": "and you just get an the first term",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1368,
      "text": "and you just get an expression here expression here for the second term though there's for the second term though there's for the second term though there's something really interesting that something really interesting that something really interesting that happens happens happens when we look at the sigma squared by D when we look at the sigma squared by D when we look at the sigma squared by D mu and we simplify mu and we simplify mu and we simplify at one point if we assume that in a at one point if we assume that in a at one point if we assume that in a special case where mu is actually the special case where mu is actually the special case where mu is actually the average of X",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1369,
      "text": "I's as it is in this case average of X",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1370,
      "text": "I's as it is in this case average of X",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1371,
      "text": "I's as it is in this case then if we plug that in then actually then if we plug that in then actually then if we plug that in then actually the gradient vanishes and becomes the gradient vanishes and becomes the gradient vanishes and becomes exactly zero and that makes the entire exactly zero and that makes the entire exactly zero and that makes the entire second term cancel second term cancel second term cancel and so these uh if you just have a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1372,
      "text": "and so these uh if you just have a",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1373,
      "text": "and so these uh if you just have a mathematical expression like this and mathematical expression like this and mathematical expression like this and you look at D Sigma Square by D mu you you look at D Sigma Square by D mu you you look at D Sigma Square by D mu you would get some mathematical formula for would get some mathematical formula for would get some mathematical formula for how mu impacts Sigma Square how mu impacts Sigma Square how mu impacts Sigma Square but if it is the special case that Nu is but if it is the special case that Nu is but if it is the special case that Nu is actually equal to the average as it is actually equal to the average as it is actually equal to the average as it is in the case of pastoralization that in the case of pastoralization that in the case of pastoralization that gradient will actually vanish and become gradient will actually vanish and become gradient will actually vanish and become zero so the whole term cancels and we zero",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1374,
      "text": "so the whole term cancels and we zero so the whole term cancels and we just get a fairly straightforward just get a fairly straightforward just get a fairly straightforward expression here for DL by D mu",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1375,
      "text": "okay and expression here for DL by D mu",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1376,
      "text": "okay and expression here for DL by D mu",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1377,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1378,
      "text": "and now we get to the craziest part which is now we get to the craziest part which is now we get to the craziest part which is uh deriving DL by dxi which is uh deriving DL by dxi which is uh deriving DL by dxi which is ultimately what we're after ultimately what we're after ultimately what we're after now let's count now let's count now let's count first of all how many numbers are there first of all how many numbers are there first of all how many numbers are there inside X as I mentioned there are 32 inside X as I mentioned there are 32 inside X as I mentioned there are 32 numbers there are 32 Little X I's and numbers there are 32 Little X I's and numbers there are 32 Little X",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1379,
      "text": "I's and let's count the number of arrows let's count the number of arrows",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1380,
      "text": "let's count the number of arrows emanating from each x i emanating from each x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1381,
      "text": "i emanating from each x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1382,
      "text": "i there's an arrow going to Mu an arrow there's an arrow going to Mu an arrow there's an arrow going to Mu an arrow going to Sigma Square going to Sigma Square going to Sigma Square and then there's an arrow going to X hat",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1383,
      "text": "and then there's an arrow going to X hat",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1384,
      "text": "and then there's an arrow going to X hat",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1385,
      "text": "but this Arrow here let's scrutinize",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1386,
      "text": "but this Arrow here let's scrutinize",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1387,
      "text": "but this Arrow here let's scrutinize that a little bit that a little bit that a little bit each",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1388,
      "text": "x i hat is just a function of x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1389,
      "text": "i each",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1390,
      "text": "x i hat is just a function of x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1391,
      "text": "i each",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1392,
      "text": "x i hat is just a function of x i and all the other scalars",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1393,
      "text": "so x i hat and all the other scalars",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1394,
      "text": "so x i hat and all the other scalars",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1395,
      "text": "so x i hat only depends on x i and none of the only depends on x i and none of the only depends on x i and none of the other X's other X's other X's and so therefore there are actually in",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1396,
      "text": "and so therefore there are actually in",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1397,
      "text": "and so therefore there are actually in this single Arrow there are 32 arrows this single Arrow there are 32 arrows this single Arrow there are 32 arrows but those 32 arrows are going exactly but those 32 arrows are going exactly but those 32 arrows are going exactly parallel they don't interfere and parallel they don't interfere and parallel they don't interfere and they're just going parallel between x they're just going parallel between x they're just going parallel between x and x hat you can look at it that way and x hat you can look at it that way and x hat you can look at it that way and so how many arrows are emanating and so how many arrows are emanating and so how many arrows are emanating from each x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1398,
      "text": "i there are three arrows mu from each x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1399,
      "text": "i there are three arrows mu from each x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1400,
      "text": "i there are three arrows mu Sigma squared and the associated X hat Sigma squared and the associated X hat Sigma squared and the associated X hat and so in back propagation we now need and so in back propagation we now need and so in back propagation we now need to apply the chain rule and we need to to apply the chain rule and we need to to apply the chain rule and we need to add up those three contributions add up those three contributions add up those three contributions so here's what that looks like if I just so here's what that looks like if I just so here's what that looks like if I just write that out write that out write that out we have uh we're going through",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1401,
      "text": "we're we have uh we're going through",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1402,
      "text": "we're we have uh we're going through we're chaining through mu Sigma square and chaining through mu Sigma square and chaining through mu Sigma square and through X hat and those three terms are through X hat and those three terms are through X hat and those three terms are just here just here just here now we already have three of these",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1403,
      "text": "we now we already have three of these",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1404,
      "text": "we now we already have three of these we have d l by d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1405,
      "text": "x i hat have d l by d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1406,
      "text": "x i hat have d l by d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1407,
      "text": "x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1408,
      "text": "i hat we have DL by D mu which we derived here we have DL by D mu which we derived here we have DL by D mu which we derived here and we have DL by D Sigma Square",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1409,
      "text": "which and we have DL by D Sigma Square which and we have DL by D Sigma Square which we derived here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1410,
      "text": "but we need three other we derived here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1411,
      "text": "but we need three other we derived here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1412,
      "text": "but we need three other terms here terms here terms here the this one this one and this one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1413,
      "text": "so I the this one this one and this one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1414,
      "text": "so I the this one this one and this one",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1415,
      "text": "so I invite you to try to derive them",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1416,
      "text": "it's invite you to try to derive them",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1417,
      "text": "it's invite you to try to derive them",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1418,
      "text": "it's not that complicated you're just looking not that complicated you're just looking not that complicated you're just looking at these Expressions here and at these Expressions here and at these Expressions here and differentiating with respect to x i differentiating with respect to x i differentiating with respect to x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1419,
      "text": "i so give it a shot",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1420,
      "text": "but here's the result or at least what I got or at least what I got um um",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1421,
      "text": "yeah",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1422,
      "text": "I'm just I'm just differentiating",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1423,
      "text": "yeah",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1424,
      "text": "I'm just I'm just differentiating",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1425,
      "text": "yeah",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1426,
      "text": "I'm just I'm just differentiating with respect to x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1427,
      "text": "i for all these with respect to x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1428,
      "text": "i for all these with respect to x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1429,
      "text": "i for all these expressions and honestly I don't think expressions and honestly I don't think expressions and honestly I don't think there's anything too tricky here it's there's anything too tricky here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1430,
      "text": "it's there's anything too tricky here it's basic calculus basic calculus basic calculus now it gets a little bit more tricky is now it gets a little bit more tricky is now it gets a little bit more tricky",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1431,
      "text": "is we are now going to plug everything we are now going to plug everything we are now going to plug everything together so all of these terms together so all of these terms together so all of these terms multiplied with all of these terms and multiplied with all of these terms and multiplied with all of these terms and add it up according to this formula and add it up according to this formula and add it up according to this formula and that gets a little bit hairy",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1432,
      "text": "so what that gets a little bit hairy",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1433,
      "text": "so what that gets a little bit hairy",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1434,
      "text": "so what ends up happening is ends up happening is ends up happening is uh uh uh you get a large expression and the thing you get a large expression and the thing you get a large expression and the thing to be very careful with here of course to be very careful with here of course to be very careful with here of course is we are working with a DL by dxi for is we are working with a DL by dxi for is we are working with a DL by dxi for specific I here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1435,
      "text": "but when we are plugging specific I here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1436,
      "text": "but when we are plugging specific I here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1437,
      "text": "but when we are plugging in some of these terms in some of these terms in some of these terms like say like say like say um um this term here deal by D signal squared this term here deal by D signal squared this term here deal by D signal squared you see how the L by D Sigma squared I you see how the L by D Sigma squared I you see how the L by D Sigma squared I end up with an expression",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1438,
      "text": "and I'm end up with an expression",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1439,
      "text": "and I'm end up with an expression",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1440,
      "text": "and I'm iterating over little",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1441,
      "text": "I's here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1442,
      "text": "but I iterating over little",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1443,
      "text": "I's here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1444,
      "text": "but I iterating over little",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1445,
      "text": "I's here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1446,
      "text": "but I can't use I as the variable when I plug can't use I as the variable when I plug can't use I as the variable when I plug in here because this is a different I in here because this is a different I in here because this is a different I from this eye from this eye from this eye",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1447,
      "text": "this I here is just a place or like a this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1448,
      "text": "I here is just a place or like a this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1449,
      "text": "I here is just a place or like a local variable for for a for Loop in local variable for for a for Loop in local variable for for a for Loop in here so here when I plug that in you here so here when I plug that in you here so here when I plug that in you",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1450,
      "text": "notice that I rename the I to a j notice that I rename the I to a j notice that I rename the I to a j because I need to make sure that this J because I need to make sure that this J because I need to make sure that this J is not that this J is not this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1451,
      "text": "I this J is not that this J is not this I this J is not that this J is not this I this J is like like a little local iterator is like like a little local iterator is like like a little local iterator over 32 terms",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1452,
      "text": "and so you have to be over 32 terms",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1453,
      "text": "and so you have to be over 32 terms",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1454,
      "text": "and so you have to be careful with that when you're plugging careful with that when you're plugging careful with that when you're plugging in the expressions from here to here you in the expressions from here to here you in the expressions from here to here you may have to rename eyes into J's and you may have to rename eyes into J's and you may have to rename eyes into J's and you have to be very careful what is actually have to be very careful what is actually have to be very careful what is actually an I with respect to the L by t x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1455,
      "text": "i an I with respect to the L by t x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1456,
      "text": "i an I with respect to the L by t x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1457,
      "text": "i so some of these are J's some of these so some of these are J's some of these so some of these are J's some of these are I's are",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1458,
      "text": "I's are",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1459,
      "text": "I's",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1460,
      "text": "and then we simplify this expression",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1461,
      "text": "and then we simplify this expression",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1462,
      "text": "and then we simplify this expression",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1463,
      "text": "and I guess like the big thing to notice",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1464,
      "text": "and I guess like the big thing to notice",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1465,
      "text": "and I guess like the big thing to notice here is a bunch of terms just kind of here is a bunch of terms just kind of here is a bunch of terms just kind of come out to the front",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1466,
      "text": "and you can come out to the front and you can come out to the front and you can refactor them there's a sigma squared refactor them there's a sigma squared refactor them there's a sigma squared plus Epsilon raised to the power of plus Epsilon raised to the power of plus Epsilon raised to the power of negative three over two uh this Sigma negative three over two uh this Sigma negative three over two uh this Sigma squared plus Epsilon can be actually squared plus Epsilon can be actually squared plus Epsilon can be actually separated out into three terms each of separated out into three terms each of separated out into three terms each of them are Sigma squared plus Epsilon to them are Sigma squared plus Epsilon to them are Sigma squared plus Epsilon to the negative one over two",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1467,
      "text": "so the three the negative one over two",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1468,
      "text": "so the three the negative one over two",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1469,
      "text": "so the three of them multiplied is equal to this and of them multiplied is equal to this and of them multiplied is equal to this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1470,
      "text": "and then those three terms can go different then those three terms can go different then those three terms can go different places because of the multiplication so places because of the multiplication so places because of the multiplication so one of them actually comes out to the one of them actually comes out to the one of them actually comes out to the front and will end up here outside one front and will end up here outside one front and will end up here outside one of them joins up with this term and one of them joins up with this term and one of them joins up with this term and one of them joins up with this other term of them joins up with this other term of them joins up with this other term and then when you simplify the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1471,
      "text": "and then when you simplify the and then when you simplify the expression you'll notice that some of expression you'll notice that some of expression you'll notice that some of these terms that are coming out are just these terms that are coming out are just these terms that are coming out are just the x i hats the x i hats the x i hats so you can simplify just by rewriting so you can simplify just by rewriting so you can simplify just by rewriting that that that and what we end up with at the end is a and what we end up with at the end is a and what we end up with at the end is a fairly simple mathematical expression fairly simple mathematical expression fairly simple mathematical expression over here that I cannot simplify further over here that I cannot simplify further over here that I cannot simplify further but basically you'll notice that it only but basically you'll notice that it only but basically you'll notice that it only uses the stuff we have and it derives uses the stuff we have and it derives uses the stuff we have and it derives the thing we need so we have the L by d the thing we need",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1472,
      "text": "so we have the L by d the thing we need so we have the L by d y for all the I's and those are used y for all the I's and those are used y for all the I's and those are used plenty of times here and also in plenty of times here and also in plenty of times here and also in addition what we're using is these",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1473,
      "text": "x i addition what we're using is these",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1474,
      "text": "x i addition what we're using is these",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1475,
      "text": "x i hats and XJ hats and they just come from hats and XJ hats and they just come from hats and XJ hats and they just come from the forward pass the forward pass the forward pass and otherwise this is a simple and otherwise this is a simple and otherwise this is a simple expression and it gives us DL by d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1476,
      "text": "x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1477,
      "text": "i expression",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1478,
      "text": "and it gives us DL by d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1479,
      "text": "x",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1480,
      "text": "i expression",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1481,
      "text": "and it gives us DL by d",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1482,
      "text": "x i for all the I's",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1483,
      "text": "and that's ultimately for all the I's",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1484,
      "text": "and that's ultimately for all the I's",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1485,
      "text": "and that's ultimately what we're interested in what we're interested in what we're interested in so that's the end of Bachelor backward",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1486,
      "text": "so that's the end of Bachelor backward",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1487,
      "text": "so that's the end of Bachelor backward pass analytically let's now implement pass analytically let's now implement pass analytically let's now implement this final result this final result this final result",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1488,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1489,
      "text": "so I implemented the expression",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1490,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1491,
      "text": "so I implemented the expression",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1492,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1493,
      "text": "so I implemented the expression into a single line of code here and you into a single line of code here and you into a single line of code here and you can see that the max diff is Tiny so can see that the max diff is Tiny so can see that the max diff is Tiny",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1494,
      "text": "so this is the correct implementation of this is the correct implementation of this is the correct implementation of this formula",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1495,
      "text": "now I'll just uh this formula now I'll just uh this formula now I'll just uh basically tell you that getting this basically tell you that getting this basically tell you that getting this formula here from this mathematical formula here from this mathematical formula here from this mathematical expression was not trivial and there's a expression was not trivial and there's a expression was not trivial and there's a lot going on packed into this one lot going on packed into this one lot going on packed into this one formula and this is a whole exercise by formula and this is a whole exercise by formula and this is a whole exercise by itself because you have to consider the itself because you have to consider the itself because you have to consider the fact that this formula here is just for fact that this formula here is just for fact that this formula here is just for a single neuron and a batch of 32 a single neuron and a batch of 32 a single neuron and a batch of 32 examples but what I'm doing here is I'm examples but what I'm doing here is I'm examples but what I'm doing here is I'm actually we actually have 64 neurons and actually we actually have 64 neurons and actually we actually have 64 neurons and so this expression has to in parallel",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1496,
      "text": "so this expression has to in parallel so this expression has to in parallel evaluate the bathroom backward pass for evaluate the bathroom backward pass for evaluate the bathroom backward pass for all of those 64 neurons in parallel all of those 64 neurons in parallel all of those 64 neurons in parallel independently so this has to happen independently so this has to happen independently so this has to happen basically in every single basically in every single basically in every single um um column of the inputs here column of the inputs here column of the inputs here and in addition to that you see how and in addition to that you see how and in addition to that you see how there are a bunch of sums here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1497,
      "text": "and we there are a bunch of sums here and we there are a bunch of sums here and we need to make sure that when I do those need to make sure that when I do those need to make sure that when I do those sums that they broadcast correctly onto sums that they broadcast correctly onto sums that they broadcast correctly onto everything else that's here everything else that's here everything else that's here and so getting this expression is just and so getting this expression is just and so getting this expression is just like highly non-trivial",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1498,
      "text": "and I invite you like highly non-trivial",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1499,
      "text": "and I invite you like highly non-trivial",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1500,
      "text": "and I invite you to basically look through it and step to basically look through it and step to basically look through it and step through it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1501,
      "text": "and it's a whole exercise to through it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1502,
      "text": "and it's a whole exercise to through it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1503,
      "text": "and it's a whole exercise to make sure that this this checks out but make sure that this this checks out but make sure that this this checks out but once all the shapes are green and once once all the shapes are green and once once all the shapes are green and once you convince yourself that it's correct you convince yourself that it's correct you convince yourself that it's correct you can also verify that Patrick's gets you can also verify that Patrick's gets you can also verify that Patrick's gets the exact same answer as well and so the exact same answer as well and so the exact same answer as well and so that gives you a lot of peace of mind that gives you a lot of peace of mind that gives you a lot of peace of mind that this mathematical formula is that this mathematical formula is that this mathematical formula is correctly implemented here and correctly implemented here and correctly implemented here and broadcasted correctly and replicated in broadcasted correctly and replicated in broadcasted correctly and replicated in parallel for all of the 64 neurons parallel for all of the 64 neurons parallel for all of the 64 neurons inside this bastrum layer",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1504,
      "text": "okay and inside this bastrum layer",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1505,
      "text": "okay and inside this bastrum layer",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1506,
      "text": "okay and finally exercise number four asks you to finally exercise number four asks you to finally exercise number four asks you to put it all together",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1507,
      "text": "and uh here we have put it all together",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1508,
      "text": "and uh here we have put it all together",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1509,
      "text": "and uh here we have a redefinition of the entire problem so a redefinition of the entire problem so a redefinition of the entire problem",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1510,
      "text": "so you see that we reinitialize the neural you see that we reinitialize the neural you see that we reinitialize the neural nut from scratch and everything",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1511,
      "text": "and then nut from scratch and everything",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1512,
      "text": "and then nut from scratch and everything",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1513,
      "text": "and then here instead of calling loss that here instead of calling loss that here instead of calling loss that backward we want to have the manual back backward we want to have the manual back backward we want to have the manual back propagation here as we derived It Up propagation here as we derived It Up propagation here as we derived It Up Above so go up copy paste all the chunks Above so go up copy paste all the chunks Above so go up copy paste all the chunks of code that we've already derived put of code that we've already derived put of code that we've already derived put them here and drive your own gradients them here and drive your own gradients them here and drive your own gradients and then optimize this neural nut and then optimize this neural nut and then optimize this neural nut basically using your own gradients all basically using your own gradients all basically using your own gradients all the way to the calibration of The the way to the calibration of The the way to the calibration of The Bachelor and the evaluation of the loss Bachelor and the evaluation of the loss Bachelor and the evaluation of the loss",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1514,
      "text": "and I was able to achieve quite a good",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1515,
      "text": "and I was able to achieve quite a good",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1516,
      "text": "and I was able to achieve quite a good loss basically the same loss you would loss basically the same loss you would loss basically the same loss you would achieve before and that shouldn't be achieve before and that shouldn't be achieve before and that shouldn't be surprising because all we've done is surprising because all we've done is surprising because all we've done is we've really gotten to Lost That we've really gotten to Lost That we've really gotten to Lost That backward",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1517,
      "text": "and we've pulled out all the backward",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1518,
      "text": "and we've pulled out all the backward",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1519,
      "text": "and we've pulled out all the code code code and inserted it here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1520,
      "text": "but those gradients and inserted it here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1521,
      "text": "but those gradients and inserted it here",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1522,
      "text": "but those gradients are identical and everything is are identical and everything is are identical and everything is identical and the results are identical identical and the results are identical identical and the results are identical it's just that we have full visibility it's just that we have full visibility it's just that we have full visibility on exactly what goes on under the hood on exactly what goes on under the hood on exactly what goes on under the hood I'll plot that backward in this specific I'll plot that backward in this specific I'll plot that backward in this specific case and this is all of our code this is case and this is all of our code this is case and this is all of our code this is the full backward pass using basically the full backward pass using basically the full backward pass using basically the simplified backward pass for the the simplified backward pass for the the simplified backward pass for the cross entropy loss and the mass cross entropy loss and the mass cross entropy loss and the mass generalization so back propagating generalization so back propagating generalization so back propagating through cross entropy the second layer through cross entropy the second layer through cross entropy the second layer the 10 H nonlinearity the batch the 10 H nonlinearity the batch the 10 H nonlinearity the batch normalization normalization uh through the first layer and through uh through the first layer and through uh through the first layer and through the embedding",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1523,
      "text": "and so you see that this the embedding and so you see that this the embedding and so you see that this is only maybe what is this 20 lines of is only maybe what is this 20 lines of is only maybe what is this 20 lines of code or something like that and that's code or something like that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1524,
      "text": "and that's code or something like that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1525,
      "text": "and that's what gives us gradients and now we can what gives us gradients and now we can what gives us gradients and now we can potentially erase losses backward",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1526,
      "text": "so the potentially erase losses backward so the potentially erase losses backward so the way I have the code set up is you should way I have the code set up is you should way",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1527,
      "text": "I have the code set up is you should be able to run this entire cell once you be able to run this entire cell once you be able to run this entire cell once you fill this in and this will run for only fill this in and this will run for only fill this in and this will run for only 100 iterations and then break 100 iterations and then break 100 iterations and then break and it breaks because it gives you an",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1528,
      "text": "and it breaks because it gives you an",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1529,
      "text": "and it breaks because it gives you an opportunity to check your gradients opportunity to check your gradients opportunity to check your gradients against pytorch against pytorch against pytorch so here our gradients we see are not so here our gradients we see are not so here our gradients we see are not exactly equal they are approximately exactly equal they are approximately exactly equal they are approximately equal and the differences are tiny equal and the differences are tiny equal and the differences are tiny wanting negative 9 or so",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1530,
      "text": "and I don't wanting negative 9 or so",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1531,
      "text": "and I don't wanting negative 9 or so",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1532,
      "text": "and I don't exactly know where they're coming from exactly know where they're coming from exactly know where they're coming from to be honest to be honest to be honest um so once we have some confidence that um so once we have some confidence that um so once we have some confidence that the gradients are basically correct we the gradients are basically correct we the gradients are basically correct we can take out the gradient tracking can take out the gradient tracking can take out the gradient tracking we can disable this breaking statement we can disable this breaking statement we can disable this breaking statement and then we can",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1533,
      "text": "and then we can",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1534,
      "text": "and then we can basically disable lost of backward we basically disable lost of backward we basically disable lost of backward we don't need it anymore it feels amazing don't need it anymore it feels amazing don't need it anymore it feels amazing to say that to say that to say that and",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1535,
      "text": "then here when we are doing the and then here when we are doing the and then here when we are doing the update we're not going to use P dot grad update we're not going to use P dot grad update we're not going to use P dot grad this is the old way of pytorch we don't this is the old way of pytorch we don't this is the old way of pytorch we don't have that anymore because we're not have that anymore because we're not have that anymore because we're not doing backward we are going to use this doing backward we are going to use this doing backward we are going to use this update where we you see that I'm update where we you see that I'm update where we you see that I'm iterating over iterating over iterating over I've arranged the grads to be in the I've arranged the grads to be in the I've arranged the grads to be in the same order as the parameters and",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1536,
      "text": "I'm same order as the parameters and",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1537,
      "text": "I'm same order as the parameters and I'm zipping them up the gradients and the zipping them up the gradients and the zipping them up the gradients and the parameters into p and grad and then here parameters into p and grad and then here parameters into p and grad and then here I'm going to step with just the grad I'm going to step with just the grad I'm going to step with just the grad that we derived manually that we derived manually that we derived manually so the last piece so the last piece so the last piece um is that none of this now requires um",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1538,
      "text": "is that none of this now requires um is that none of this now requires gradients from pytorch and so one thing gradients from pytorch and so one thing gradients from pytorch and so one thing you can do here you can do here you can do here um um is you can do with no grad and offset is you can do with no grad and offset is you can do with no grad and offset this whole code block this whole code block this whole code block and really what you're saying is you're and really what you're saying is you're and really what you're saying is you're telling Pat George",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1539,
      "text": "that hey",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1540,
      "text": "I'm not telling Pat George that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1541,
      "text": "hey I'm not telling Pat George that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1542,
      "text": "hey I'm not going to call backward on any of this going to call backward on any of this going to call backward on any of this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1543,
      "text": "and this allows pytorch to be a bit more and this allows pytorch to be a bit more and this allows pytorch to be a bit more efficient with all of it efficient with all of it efficient with all of it",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1544,
      "text": "and then we should be able to just uh",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1545,
      "text": "and then we should be able to just uh",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1546,
      "text": "and then we should be able to just uh run this run this run this and and it's running it's running it's running",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1547,
      "text": "and you see that losses backward is",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1548,
      "text": "and you see that losses backward is",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1549,
      "text": "and you see that losses backward is commented out commented out commented out and we're optimizing and we're optimizing and we're optimizing so we're going to leave this run",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1550,
      "text": "and uh so we're going to leave this run",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1551,
      "text": "and uh so we're going to leave this run and uh hopefully we get a good result hopefully we get a good result hopefully we get a good result",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1552,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1553,
      "text": "so I allowed the neural net to",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1554,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1555,
      "text": "so I allowed the neural net to",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1556,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1557,
      "text": "so I allowed the neural net to finish optimization finish optimization finish optimization then here I calibrate the bachelor then here I calibrate the bachelor then here I calibrate the bachelor parameters because I did not keep track parameters because I did not keep track parameters because I did not keep track of the running mean and very variants in of the running mean and very variants in of the running mean and very variants in their training Loop their training Loop their training Loop then here I ran the loss and you see then here I ran the loss and you see then here I ran the loss and you see that we actually obtained a pretty good that we actually obtained a pretty good that we actually obtained a pretty good loss very similar to what we've achieved loss very similar to what we've achieved loss very similar to what we've achieved before before before",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1558,
      "text": "and then here I'm sampling from the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1559,
      "text": "and then here I'm sampling from the",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1560,
      "text": "and then here I'm sampling from the model and we see some of the name like model",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1561,
      "text": "and we see some of the name like model and we see some of the name like gibberish that we're sort of used to so gibberish that we're sort of used to so gibberish that we're sort of used to so basically the model worked and samples basically the model worked and samples basically the model worked and samples uh pretty decent results compared to uh pretty decent results compared to uh pretty decent results compared to what we were used to so everything is what we were used to so everything is what we were used to so everything is the same",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1562,
      "text": "but of course the big deal is the same",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1563,
      "text": "but of course the big deal is the same",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1564,
      "text": "but of course the big deal is that we did not use lots of backward we that we did not use lots of backward we that we did not use lots of backward we did not use package Auto grad and we did not use package Auto grad and we did not use package Auto grad and we estimated our gradients ourselves by estimated our gradients ourselves by estimated our gradients ourselves by hand hand hand",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1565,
      "text": "and so hopefully you're looking at this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1566,
      "text": "and so hopefully you're looking at this",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1567,
      "text": "and so hopefully you're looking at this the backward pass of this neural net and the backward pass of this neural net and the backward pass of this neural net and you're thinking to yourself actually you're thinking to yourself",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1568,
      "text": "actually you're thinking to yourself actually that's not too complicated that's not too complicated that's not too complicated um um each one of these layers is like three each one of these layers is like three each one of these layers is like three lines of code or something like that and lines of code or something like that and lines of code or something like that and most of it is fairly straightforward most of it is fairly straightforward most of it is fairly straightforward potentially with the notable exception potentially with the notable exception potentially with the notable exception of the batch normalization backward pass of the batch normalization backward pass of the batch normalization backward pass",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1569,
      "text": "otherwise it's pretty good",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1570,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1571,
      "text": "and otherwise it's pretty good",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1572,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1573,
      "text": "and otherwise it's pretty good",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1574,
      "text": "okay",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1575,
      "text": "and that's everything I wanted to cover for that's everything I wanted to cover for that's everything I wanted to cover for this lecture",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1576,
      "text": "so hopefully you found this this lecture",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1577,
      "text": "so hopefully you found this this lecture",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1578,
      "text": "so hopefully you found this interesting",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1579,
      "text": "and what I liked about it interesting and what I liked about it interesting",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1580,
      "text": "and what I liked about it honestly is that it gave us a very nice honestly is that it gave us a very nice honestly is that it gave us a very nice diversity of layers to back propagate diversity of layers to back propagate diversity of layers to back propagate through and through and through and um I think it gives a pretty nice",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1581,
      "text": "and um I think it gives a pretty nice",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1582,
      "text": "and um I think it gives a pretty nice and comprehensive sense of how these comprehensive sense of how these comprehensive sense of how these backward passes are implemented and how backward passes are implemented and how backward passes are implemented and how they work and you'd be able to derive they work",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1583,
      "text": "and you'd be able to derive they work",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1584,
      "text": "and you'd be able to derive them yourself but of course in practice them yourself",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1585,
      "text": "but of course in practice them yourself but of course in practice you probably don't want to and you want you probably don't want to and you want you probably don't want to and you want to use the pythonograd but hopefully you to use the pythonograd but hopefully you to use the pythonograd",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1586,
      "text": "but hopefully you have some intuition about how gradients have some intuition about how gradients have some intuition about how gradients flow backwards through the neural net flow backwards through the neural net flow backwards through the neural net starting at the loss and how they flow starting at the loss and how they flow starting at the loss and how they flow through all the variables and all the through all the variables and all the through all the variables and all the intermediate results intermediate results intermediate results and if you understood a good chunk of it and if you understood a good chunk of it and if you understood a good chunk of it and if you have a sense of that then you",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1587,
      "text": "and if you have a sense of that then you and if you have a sense of that then you can count yourself as one of these buff can count yourself as one of these buff can count yourself as one of these buff doji's on the left instead of the uh doji's on the left instead of the uh doji's on the left instead of the uh those on the right here now in the next those on the right here now in the next those on the right here now in the next lecture we're actually going to go to lecture we're actually going to go to lecture we're actually going to go to recurrent neural nuts lstms and all the recurrent neural nuts lstms and all the recurrent neural nuts lstms and all the other variants of RNs and we're going to other variants of RNs and we're going to other variants of RNs and we're going to start to complexify the architecture and start to complexify the architecture and start to complexify the architecture and start to achieve better uh log start to achieve better uh log start to achieve better uh log likelihoods and so I'm really looking likelihoods",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1588,
      "text": "and so I'm really looking likelihoods",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1589,
      "text": "and so I'm really looking forward to that",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    },
    {
      "id": 1590,
      "text": "and I'll see you then",
      "start_time": "00:00:02.389",
      "end_time": "01:55:25.280"
    }
  ]
}