{
  "video_id": "PaCmpygFfXo",
  "sentences": [
    {
      "id": 1,
      "text": "hi everyone hope you're well hi",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 2,
      "text": "everyone hope you're well and next up what i'd like to do is i'd and next up what i'd like to do is i'd and next up what i'd like to do is i'd like to build out make more like to build out make more like to build out make more like micrograd before it make more is a like micrograd before it make more is a like micrograd before it make more is a repository that i have on my github repository that i have on my github repository that i have on",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 3,
      "text": "my github webpage webpage webpage you can look at it you can look at it you can look at it but just like with micrograd i'm going but just like with micrograd i'm going but just like with micrograd i'm going to build it out step by step",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 4,
      "text": "and i'm to build it out step by step",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 5,
      "text": "and i'm to build it out step by step",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 6,
      "text": "and i'm going to spell everything out so we're going to spell everything out so we're going to spell everything out so we're going to build it out slowly and going to build it out slowly and going to build it out slowly and together together together now what is make more now what is make more now what is make more make more as the name suggests make more as the name suggests make more as the name suggests makes more of things that you give it makes more of things that you give it makes more of things that you give it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 7,
      "text": "so here's an example so here's an example",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 8,
      "text": "so here's an example names.txt is an example dataset to make names.txt is an example dataset to make names.txt is an example dataset to make more more more and when you look at names.txt you'll and when you look at names.txt you'll and when you look at names.txt you'll find that it's a very large data set of find that it's a very large data set of find that it's a very large data set of names names names",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 9,
      "text": "so so so here's lots of different types of names here's lots of different types of names here's lots of different types of names in fact i believe there are 32 000 names in fact i believe there are 32 000 names in fact i believe there are 32 000 names that i've sort of found randomly on the that i've sort of found randomly on the that i've sort of found randomly on the government website government website government website",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 10,
      "text": "and if you train make more on this data and if you train make more on this data and if you train make more on this data set it will learn to make more of things set it will learn to make more of things set it will learn to make more of things like this like this like this and in particular in this case that will and in particular in this case that will and in particular in this case that will mean more things that sound name-like mean more things that sound name-like mean more things that sound name-like but are actually unique names but are actually unique names but are actually unique names",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 11,
      "text": "and maybe if you have a baby",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 12,
      "text": "and you're",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 13,
      "text": "and maybe if you have a baby and you're",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 14,
      "text": "and maybe if you have a baby and you're trying to assign name maybe you're trying to assign name maybe you're trying to assign name maybe you're looking for a cool new sounding unique looking for a cool new sounding unique looking for a cool new sounding unique name make more might help you name make more might help you name make more might help you so here are some example generations",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 15,
      "text": "so here are some example generations so here are some example generations from the neural network from the neural network from the neural network once we train it on our data set once we train it on our data set once we train it on our data set so here's some example so here's some example so here's some example unique names that it will generate unique names that it will generate unique names that it will generate dontel dontel dontel irot irot irot zhendi zhendi zhendi and so on and so all these are sound and so on and so all these are sound and so on and so all these are sound name like",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 16,
      "text": "but they're not of course name like",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 17,
      "text": "but they're not of course name like",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 18,
      "text": "but they're not of course names names so under the hood make more is a so under the hood make more is a so under the hood make more is a character level language model",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 19,
      "text": "so what character level language model so what character level language model",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 20,
      "text": "so what that means is that it is treating every that means is that it is treating every that means is that it is treating every single line here as an example and single line here as an example and single line here as an example and within each example it's treating them within each example it's treating them within each example it's treating them all as sequences of individual all as sequences of individual all as sequences of individual characters so r e e s e is this example characters",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 21,
      "text": "so r e e s e is this example characters so r e e s e is this example",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 22,
      "text": "and that's the sequence of characters and that's the sequence of characters and that's the sequence of characters and that's the level on which we are",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 23,
      "text": "and that's the level on which we are",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 24,
      "text": "and that's the level on which we are building out make more and what it means building out make more and what it means building out make more and what it means to be a character level language model to be a character level language model to be a character level language model then is that it's just uh sort of then is that it's just uh sort of then is that it's just uh sort of modeling those sequences of characters modeling those sequences of characters modeling those sequences of characters and it knows how to predict the next and it knows how to predict the next and it knows how to predict the next character in the sequence character in the sequence character in the sequence now we're actually going to implement a now we're actually going to implement a now we're actually going to implement a large number of character level language large number of character level language large number of character level language models in terms of the neural networks models in terms of the neural networks models in terms of the neural networks that are involved in predicting the next that are involved in predicting the next that are involved in predicting the next character in a sequence so very simple character in a sequence so very simple character in a sequence so very simple bi-gram and back of work models bi-gram and back of work models bi-gram and back of work models multilingual perceptrons recurrent multilingual perceptrons recurrent multilingual perceptrons recurrent neural networks all the way to modern neural networks all the way to modern neural networks all the way to modern transformers in fact the transformer transformers in fact the transformer transformers in fact the transformer that we will build will be basically the that we will build will be basically the that we will build will be basically the equivalent transformer to gpt2 if you equivalent transformer to gpt2 if you equivalent transformer to gpt2 if you have heard of gpt uh so that's kind of a have heard of gpt uh so that's kind of a have heard of gpt uh so that's kind of a big deal it's a modern network and by big deal it's a modern network and by big deal it's a modern network and by the end of the series you will actually the end of the series you will actually the end of the series you will actually understand how that works um on the understand how that works um on the understand how that works um on the level of characters now to give you a level of characters now to give you a level of characters now to give you a sense of the extensions here uh after sense of the extensions here uh after sense of the extensions here uh after characters we will probably spend some characters we will probably spend some characters we will probably spend some time on the word level so that we can time on the word level so that we can time on the word level so that we can generate documents of words not just generate documents of words not just generate documents of words not just little you know segments of characters little",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 25,
      "text": "you know segments of characters little",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 26,
      "text": "you know segments of characters but we can generate entire large much",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 27,
      "text": "but we can generate entire large much",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 28,
      "text": "but we can generate entire large much larger documents larger documents larger documents",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 29,
      "text": "and then we're probably going to go into",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 30,
      "text": "and then we're probably going to go into",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 31,
      "text": "and then we're probably going to go into images and image text images and image text images and image text networks such as dolly stable diffusion networks such as dolly stable diffusion networks such as dolly stable diffusion and so on but for now we have to start and so on but for now we have to start and so on but for now we have to start here character level language modeling here character level language modeling here character level language modeling let's go let's go let's go so like before we are starting with a so like before we are starting with a so like before we are starting with a completely blank jupiter notebook page completely blank jupiter notebook page completely blank jupiter notebook page the first thing is i would like to the first thing is i would like to the first thing is i would like to basically load up the dataset names.txt basically load up the dataset names.txt basically load up the dataset names.txt so we're going to open up names.txt for so we're going to open up names.txt for so we're going to open up names.txt for reading reading reading and we're going to read in everything",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 32,
      "text": "and we're going to read in everything",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 33,
      "text": "and we're going to read in everything into a massive string into a massive string into a massive string",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 34,
      "text": "and then because it's a massive string",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 35,
      "text": "and then because it's a massive string",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 36,
      "text": "and then because it's a massive string we'd only like the individual words",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 37,
      "text": "and we'd only like the individual words",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 38,
      "text": "and we'd only like the individual words and put them in the list put them in the list put them in the list",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 39,
      "text": "so let's call split lines so let's call split lines so let's call split lines on that string on that string on that string to get all of our words as a python list to get all of our words as a python list to get all of our words as a python list of strings of strings of strings so basically we can look at for example so basically we can look at for example",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 40,
      "text": "so basically we can look at for example the first 10 words the first 10 words the first 10 words and we have that it's a list of emma and we have that it's a list of emma and we have that it's a list of emma olivia eva and so on olivia eva and so on olivia eva and so on and if we look at and if we look at and if we look at the top of the page here that is indeed the top of the page here that is indeed the top of the page here that is indeed what we see what we see what we see um um um",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 41,
      "text": "so that's good",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 42,
      "text": "so that's good",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 43,
      "text": "so that's good this list actually makes me feel that this list actually makes me feel that this list actually makes me feel that this is probably sorted by frequency this is probably sorted by frequency this is probably sorted by frequency",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 44,
      "text": "but okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 45,
      "text": "so",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 46,
      "text": "but okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 47,
      "text": "so",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 48,
      "text": "but okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 49,
      "text": "so these are the words now we'd like to these are the words now we'd like to these are the words now we'd like to actually like learn a little bit more actually like learn a little bit more actually like learn a little bit more about this data set let's look at the about this data set let's look at the about this data set let's look at the total number of words we expect this to total number of words we expect this to total number of words we expect this to be roughly 32 000 be roughly 32 000 be roughly 32 000 and then what is the for example and then what is the for example and then what is the for example shortest word shortest word shortest word so min of so min of so min of length of each word for w inwards length of each word for w inwards length of each word for w inwards so the shortest word will be length",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 50,
      "text": "so the shortest word will be length so the shortest word will be length two two two and max of one w for w in words so the and max of one w for w in words so the and max of one w for w in words so the longest word will be longest word will be longest word will be 15 characters 15 characters 15 characters so let's now think through our very so let's now think through our very so let's now think through our very first language model first language model first language model as i mentioned a character level as i mentioned a character level as i mentioned a character level language model is predicting the next language model is predicting the next language model is predicting the next character in a sequence given already character in a sequence given already character in a sequence given already some concrete sequence of characters some concrete sequence of characters some concrete sequence of characters before it before it before it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 51,
      "text": "now we have to realize here is that now we have to realize here is that now we have to realize here is that every single word here like isabella is every single word here like isabella is every single word here like isabella is actually quite a few examples packed in actually quite a few examples packed in actually quite a few examples packed in to that single word to that single word to that single word because what is an existence of a word because what is an existence of a word because what is an existence of a word like isabella in the data set telling us like isabella in the data set telling us like isabella in the data set telling us really it's saying that really it's saying that really it's saying that the character i is a very likely the character i is a very likely the character i is a very likely character to come first in the sequence character to come first in the sequence character to come first in the sequence of a name of a name of a name the character s is likely to come the character s is likely to come the character s is likely to come after i after i after i the character a is likely to come after the character a is likely to come after the character a is likely to come after is is is the character b is very likely to come the character b is very likely to come the character b is very likely to come after isa and so on all the way to a after isa and so on all the way to a after isa and so on all the way to a following isabel following isabel following isabel",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 52,
      "text": "and then there's one more example",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 53,
      "text": "and then there's one more example",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 54,
      "text": "and then there's one more example actually packed in here actually packed in here actually packed in here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 55,
      "text": "and that is that and that is that and that is that after there's isabella after there's isabella after there's isabella the word is very likely to end the word is very likely to end the word is very likely to end",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 56,
      "text": "so that's one more sort of explicit so that's one more sort of explicit so that's one more sort of explicit piece of information that we have here piece of information that we have here piece of information that we have here that we have to be careful with that we have to be careful with that we have to be careful with",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 57,
      "text": "and so there's a lot backed into a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 58,
      "text": "and so there's a lot backed into a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 59,
      "text": "and so there's a lot backed into a single individual word in terms of the single individual word in terms of the single individual word in terms of the statistical structure of what's likely statistical structure of what's likely statistical structure of what's likely to follow in these character sequences to follow in these character sequences to follow in these character sequences and then of course we don't have just an and then of course we don't have just an and then of course we don't have just an individual word we actually have 32 000 individual word we actually have 32 000 individual word we actually have 32 000 of these",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 60,
      "text": "and so there's a lot of of these",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 61,
      "text": "and so there's a lot of of these",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 62,
      "text": "and so there's a lot of structure here to model structure here to model structure here to model now in the beginning what i'd like to now in the beginning what i'd like to now in the beginning what i'd like to start with is i'd like to start with start with is i'd like to start with start with is i'd like to start with building a bi-gram language model building a bi-gram language model building a bi-gram language model now in the bigram language model we're now in the bigram language model we're now in the bigram language model we're always working with just always working with just always working with just two characters at a time two characters at a time two characters at a time",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 63,
      "text": "so we're only looking at one character so we're only looking at one character so we're only looking at one character that we are given and we're trying to that we are given and we're trying to that we are given and we're trying to predict the next character in the predict the next character in the predict the next character in the sequence sequence sequence",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 64,
      "text": "so um what characters are likely to so um what characters are likely to so um what characters are likely to follow are what characters are likely to follow are what characters are likely to follow are what characters are likely to follow a and so on and we're just follow a and so on and we're just follow a and so on and we're just modeling that kind of a little local modeling that kind of a little local modeling that kind of a little local structure structure structure and we're forgetting the fact that we and we're forgetting the fact that we and we're forgetting the fact that we may have a lot more information we're may have a lot more information we're may have a lot more information we're always just looking at the previous always just looking at the previous always just looking at the previous character to predict the next one so character to predict the next one so character to predict the next one so it's a very simple and weak language it's a very simple and weak language it's a very simple and weak language model",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 65,
      "text": "but i think it's a great place to model",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 66,
      "text": "but i think it's a great place to model",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 67,
      "text": "but i think it's a great place to start start start so now let's begin by looking at these so now let's begin by looking at these so now let's begin by looking at these bi-grams in our data set and what they bi-grams in our data set and what they bi-grams in our data set and what they look like and these bi-grams again are look like and these bi-grams again are look like and these bi-grams again are just two characters in a row just two characters in a row just two characters in a row",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 68,
      "text": "so for w in words so for w in words so for w in words each w here is an individual word a each w here is an individual word a each w here is an individual word a string string string we want to iterate uh for we want to iterate uh for we want to iterate uh for we're going to iterate this word we're going to iterate this word we're going to iterate this word with consecutive characters so two with consecutive characters so two with consecutive characters so two characters at a time sliding it through characters at a time sliding it through characters at a time sliding it through the word now a interesting nice way cute the word now a interesting nice way cute the word now a interesting nice way cute way to do this in python by the way is way to do this in python by the way is way to do this in python by the way is doing something like this for character doing something like this for character doing something like this for character one character two in zip off one character two in zip off one character two in zip off w and w at one w and w at one w and w at one one column one column one column print print print character one character two character one character two character one character two and let's not do all the words let's and let's not do all the words let's and let's not do all the words let's just do the first three words and i'm just do the first three words and i'm just do the first three words and i'm going to show you in a second how this going to show you in a second how this going to show you in a second how this works works works but for now basically as an example but for now basically as an example but for now basically as an example let's just do the very first word alone let's just do the very first word alone let's just do the very first word alone emma emma emma you see how we have a emma and this will you see how we have a emma and this will you see how we have a emma and this will just print e m m m a just print e m m m a just print e m m m a and the reason this works is because w and the reason this works is because w and the reason this works is because w is the string emma w at one column is is the string emma w at one column is is the string emma w at one column is the string",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 69,
      "text": "mma the string mma the string mma and zip and zip and zip takes two iterators and it pairs them up takes two iterators and it pairs them up takes two iterators and it pairs them up and then creates an iterator over the and then creates an iterator over the and then creates an iterator over the tuples of their consecutive entries tuples of their consecutive entries tuples of their consecutive entries and if any one of these lists is shorter and if any one of these lists is shorter and if any one of these lists is shorter than the other then it will just than the other then it will just than the other then it will just halt and return halt and return halt and return so basically that's why we return em mmm",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 70,
      "text": "so basically that's why we return em",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 71,
      "text": "mmm",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 72,
      "text": "so basically that's why we return em",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 73,
      "text": "mmm ma ma ma",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 74,
      "text": "but then because this iterator second",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 75,
      "text": "but then because this iterator second but then because this iterator second one here runs out of elements zip just one here runs out of elements zip just one here runs out of elements zip just ends",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 76,
      "text": "and that's why we only get these ends",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 77,
      "text": "and that's why we only get these ends",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 78,
      "text": "and that's why we only get these tuples so pretty cute tuples so pretty cute tuples so pretty cute",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 79,
      "text": "so these are the consecutive elements in so these are the consecutive elements in so these are the consecutive elements in the first word now we have to be careful the first word now we have to be careful the first word now we have to be careful because we actually have more because we actually have more because we actually have more information here than just these three information here than just these three information here than just these three examples as i mentioned we know that e examples as i mentioned we know that e examples as i mentioned we know that e is the is very likely to come first and is the is very likely to come first and is the is very likely to come first",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 80,
      "text": "and we know that a in this case is coming we know that a in this case is coming we know that a in this case is coming last last last so one way to do this is basically we're so one way to do this is basically we're so one way to do this is basically we're going to create going to create going to create a special array here all a special array here all a special array here all characters characters characters and um we're going to hallucinate a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 81,
      "text": "and um we're going to hallucinate a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 82,
      "text": "and um we're going to hallucinate a special start token here special start token here special start token here i'm going to i'm going to i'm going to call it like special start call it like special start call it like special start",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 83,
      "text": "so this is a list of one element so this is a list of one element",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 84,
      "text": "so this is a list of one element plus plus plus w w w and then plus a special end character and then plus a special end character and then plus a special end character and the reason i'm wrapping the list of and the reason i'm wrapping the list of and the reason i'm wrapping the list of w here is because w is a string emma w here is because w is a string emma w here is because w is a string emma list of w will just have the individual list of w will just have the individual list of w will just have the individual characters in the list characters in the list characters in the list",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 85,
      "text": "and then and then and then doing this again now but not iterating doing this again now but not iterating doing this again now but not iterating over w's but over the characters over w's but over the characters over w's but over the characters will give us something like this will give us something like this will give us something like this so e is likely so this is a bigram of so e is likely so this is a bigram of so e is likely so this is a bigram of the start character and e and this is a the start character and e and this is a the start character and e and this is a bigram of the bigram of the bigram of the a and the special end character a and the special end character a and the special end character and now we can look at for example what and now we can look at for example what and now we can look at for example what this looks like for this looks like for this looks like for olivia or eva olivia or eva olivia or eva and indeed we can actually",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 86,
      "text": "and indeed we can actually",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 87,
      "text": "and indeed we can actually potentially do this for the entire data potentially do this for the entire data potentially do this for the entire data set",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 88,
      "text": "but we won't print that that's going set",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 89,
      "text": "but we won't print that that's going set",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 90,
      "text": "but we won't print that that's going to be too much to be too much to be too much",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 91,
      "text": "but these are the individual character but these are the individual character but these are the individual character diagrams and we can print them diagrams and we can print them diagrams and we can print them now in order to learn the statistics now in order to learn the statistics now in order to learn the statistics about which characters are likely to about which characters are likely to about which characters are likely to follow other characters the simplest way follow other characters the simplest way follow other characters the simplest way in the bigram language models is to in the bigram language models is to in the bigram language models is to simply do it by counting simply do it by counting simply do it by counting so we're basically just going to count so we're basically just going to count so we're basically just going to count how often any one of these combinations how often any one of these combinations how often any one of these combinations occurs in the training set occurs in the training set occurs in the training set in these words in these words in these words so we're going to need some kind of a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 92,
      "text": "so we're going to need some kind of a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 93,
      "text": "so we're going to need some kind of a dictionary that's going to maintain some dictionary that's going to maintain some dictionary that's going to maintain some counts for every one of these diagrams counts for every one of these diagrams counts for every one of these diagrams so let's use a dictionary",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 94,
      "text": "b so let's use a dictionary",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 95,
      "text": "b so let's use a dictionary b and this will map these bi-grams so and this will map these bi-grams so and this will map these bi-grams so bi-gram is a tuple of character one bi-gram is a tuple of character one bi-gram is a tuple of character one character two character two character two and then b at bi-gram and then b at bi-gram and then b at bi-gram will be b dot get of bi-gram will be b dot get of bi-gram will be b dot get of bi-gram which is basically the same as b at which is basically the same as b at which is basically the same as b at bigram bigram bigram but in the case that bigram is not in but in the case that bigram is not in but in the case that bigram is not in the dictionary",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 96,
      "text": "b we would like to by the dictionary b we would like to by the dictionary",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 97,
      "text": "b we would like to by default return to zero default return to zero default return to zero plus one plus one plus one so this will basically add up all the so this will basically add up all the so this will basically add up all the bigrams and count how often they occur bigrams and count how often they occur bigrams and count how often they occur let's get rid of printing let's get rid of printing let's get rid of printing or rather or rather or rather let's keep the printing and let's just let's keep the printing and let's just let's keep the printing and let's just inspect what b is in this case inspect what b is in this case inspect what b is in this case and we see that many bi-grams occur just and we see that many bi-grams occur just and we see that many bi-grams occur just a single time this one allegedly a single time this one allegedly a single time this one allegedly occurred three times occurred three times occurred three times so a was an ending character three times so a was an ending character three times so a was an ending character three times and that's true for all of these words and that's true for all of these words and that's true for all of these words all of emma olivia and eva and with a all of emma olivia and eva and with a all of emma olivia and eva and with a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 98,
      "text": "so that's why this occurred three times",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 99,
      "text": "oops i should not have printed oops i should not have printed i'm going to erase that i'm going to erase that i'm going to erase that let's kill this let's kill this let's kill this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 100,
      "text": "let's just run let's just run let's just run and now b will have the statistics of and now b will have the statistics of and now b will have the statistics of the entire data set the entire data set the entire data set so these are the counts across all the so these are the counts across all the so these are the counts across all the words of the individual pie grams words of the individual pie grams words of the individual pie grams and we could for example look at some of and we could for example look at some of and we could for example look at some of the most common ones and least common the most common ones and least common the most common ones and least common ones ones ones this kind of grows in python but the way this kind of grows in python but the way this kind of grows in python but the way to do this the simplest way i like is we to do this the simplest way i like is we to do this the simplest way i like is we just use b dot items just use b dot items just use b dot items b dot items returns b dot items returns b dot items returns the tuples of the tuples of the tuples of key value in this case the keys are key value in this case the keys are key value in this case the keys are the character diagrams and the values the character diagrams and the values the character diagrams and the values are the counts are the counts are the counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 101,
      "text": "and so then what we want to do is we",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 102,
      "text": "and so then what we want to do is we",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 103,
      "text": "and so then what we want to do is we want to do sorted of this sorted of this but by default sort is on the first on the first item of a tuple",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 104,
      "text": "but we want on the first item of a tuple",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 105,
      "text": "but we want to sort by the values which are the to sort by the values which are the to sort by the values which are the second element of a tuple that is the second element of a tuple that is the second element of a tuple that is the key value key value key value so we want to use the key",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 106,
      "text": "so we want to use the key",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 107,
      "text": "so we want to use the key equals lambda equals lambda equals lambda that takes the key value that takes the key value that takes the key value and returns and returns and returns the key value at the one not at zero but the key value at the one not at zero but the key value at the one not at zero but at one which is the count so we want to at one which is the count so we want to at one which is the count so we want to sort by the count sort by the count sort by the count of these elements of these elements of these elements and actually we wanted to go backwards",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 108,
      "text": "and actually we wanted to go backwards",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 109,
      "text": "and actually we wanted to go backwards so here we have is so here we have is so here we have is the bi-gram q and r occurs only a single the bi-gram q and r occurs only a single the bi-gram q and r occurs only a single time time time dz occurred only a single time dz occurred only a single time dz occurred only a single time and when we sort this the other way and when we sort this the other way and when we sort this the other way around around around we're going to see the most likely we're going to see the most likely we're going to see the most likely bigrams so we see that n was bigrams",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 110,
      "text": "so we see that n was bigrams so we see that n was very often an ending character very often an ending character very often an ending character many many times and apparently n almost many many times and apparently n almost many many times and apparently n almost always follows an a always follows an a always follows an a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 111,
      "text": "and that's a very likely combination as and that's a very likely combination as and that's a very likely combination as well",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 112,
      "text": "well well so so this is kind of the individual counts this is kind of the individual counts this is kind of the individual counts that we achieve over the entire data set that we achieve over the entire data set that we achieve over the entire data set now it's actually going to be now it's actually going to be now it's actually going to be significantly more convenient for us to significantly more convenient for us to significantly more convenient for us to keep this information in a keep this information in a keep this information in a two-dimensional array instead of a two-dimensional array instead of a two-dimensional array instead of a python dictionary python dictionary python dictionary",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 113,
      "text": "so so we're going to store this information we're going to store this information we're going to store this information in a 2d array in a 2d array in a 2d array and and and the rows are going to be the first the rows are going to be the first the rows are going to be the first character of the bigram and the columns character of the bigram and the columns character of the bigram and the columns are going to be the second character and are going to be the second character and are going to be the second character and each entry in this two-dimensional array each entry in this two-dimensional array each entry in this two-dimensional array will tell us how often that first will tell us how often that first will tell us how often that first character files the second character in character files the second character in character files the second character in the data set the data set the data set so in particular the array so in particular the array so in particular the array representation that we're going to use representation that we're going to use representation that we're going to use or the library is that of pytorch or the library is that of pytorch or the library is that of pytorch and pytorch is a deep and pytorch is a deep and pytorch is a deep learning neural network framework but learning neural network framework but learning neural network framework but part of it is also this torch.tensor part of it is also this torch.tensor part of it is also this torch.tensor which allows us to create which allows us to create which allows us to create multi-dimensional arrays and manipulate multi-dimensional arrays and manipulate multi-dimensional arrays and manipulate them very efficiently them very efficiently them very efficiently",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 114,
      "text": "so so let's import pytorch which you can do by let's import pytorch which you can do by let's import pytorch which you can do by import torch import torch import torch",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 115,
      "text": "and then we can create",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 116,
      "text": "and then we can create",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 117,
      "text": "and then we can create arrays arrays arrays so let's create a array of zeros",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 118,
      "text": "so let's create a array of zeros so let's create a array of zeros",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 119,
      "text": "and we give it a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 120,
      "text": "and we give it a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 121,
      "text": "and we give it a size of this array let's create a three size of this array",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 122,
      "text": "let's create a three size of this array let's create a three by five array as an example by five array as an example by five array as an example",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 123,
      "text": "and and this is a three by five array of zeros this is a three by five array of zeros this is a three by five array of zeros and by default you'll notice a.d type and by default you'll notice a.d type and by default you'll notice a.d type which is short for data type is float32 which is short for data type is float32 which is short for data type is float32",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 124,
      "text": "so these are single precision floating so these are single precision floating so these are single precision floating point numbers point numbers point numbers because we are going to represent counts because we are going to represent counts because we are going to represent counts let's actually use d type as torch dot let's actually use d type as torch dot let's actually use d type as torch dot and 32 and 32 and 32 so these are so these are so these are 32-bit integers 32-bit integers 32-bit integers so now you see that we have integer data so now you see that we have integer data so now you see that we have integer data inside this tensor inside this tensor inside this tensor now tensors allow us to really now tensors allow us to really now tensors allow us to really manipulate all the individual entries manipulate all the individual entries manipulate all the individual entries and do it very efficiently and do it very efficiently and do it very efficiently so for example if we want to change this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 125,
      "text": "so for example if we want to change this so for example if we want to change this bit bit bit we have to index into the tensor and in we have to index into the tensor and in we have to index into the tensor and in particular here this is the first row particular here this is the first row particular here this is the first row and the and the and the because it's zero indexed so this is row because it's zero indexed so this is row because it's zero indexed so this is row index one and column index zero one two index one and column index zero one two index one and column index zero one two three three three",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 126,
      "text": "so a at one comma three we can set that so a at one comma three we can set that so a at one comma three we can set that to one to one to one and then a we'll have a 1 over there",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 127,
      "text": "and then a we'll have a 1 over there and then a we'll have a 1 over there we can of course also do things like we can of course also do things like we can of course also do things like this so now a will be 2 over there this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 128,
      "text": "so now a will be 2 over there this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 129,
      "text": "so now a will be 2 over there or 3. or 3. or 3.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 130,
      "text": "and also we can for example say a 0 0 is and also we can for example say a 0 0 is and also we can for example say a 0 0 is 5 5 5 and then a will have a 5 over here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 131,
      "text": "and then a will have a 5 over here and then a will have a 5 over here so that's how we can index into the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 132,
      "text": "so that's how we can index into the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 133,
      "text": "so that's how we can index into the arrays now of course the array that we arrays now of course the array that we arrays now of course the array that we are interested in is much much bigger so are interested in is much much bigger so are interested in is much much bigger so for our purposes we have 26 letters of for our purposes we have 26 letters of for our purposes we have 26 letters of the alphabet the alphabet the alphabet",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 134,
      "text": "and then we have two special characters",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 135,
      "text": "and then we have two special characters and then we have two special characters s and e s and e s and e",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 136,
      "text": "so uh we want 26 plus 2 or 28 by 28",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 137,
      "text": "so uh we want 26 plus 2 or 28 by 28",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 138,
      "text": "so uh we want 26 plus 2 or 28 by 28 array array array and let's call it the capital n because and let's call it the capital n because and let's call it the capital n because it's going to represent sort of the it's going to represent sort of the it's going to represent sort of the counts counts counts let me erase this stuff let me erase this stuff let me erase this stuff",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 139,
      "text": "so that's the array that starts at zeros so that's the array that starts at zeros so that's the array that starts at zeros 28 by 28 28 by 28 28 by 28 and now let's copy paste this and now let's copy paste this and now let's copy paste this here here here but",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 140,
      "text": "instead of having a dictionary b but instead of having a dictionary b but instead of having a dictionary b which we're going to erase we now have which we're going to erase we now have which we're going to erase we now have an n an n an n now the problem here is that we have now the problem here is that we have now the problem here is that we have these characters which are strings but these characters which are strings but these characters which are strings but we have to now we have to now we have to now um basically index into a um basically index into a um basically index into a um array",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 141,
      "text": "and we have to index using um array and we have to index using um array",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 142,
      "text": "and we have to index using integers so we need some kind of a integers so we need some kind of a integers so we need some kind of a lookup table from characters to integers lookup table from characters to integers lookup table from characters to integers",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 143,
      "text": "so let's construct such a character so let's construct such a character",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 144,
      "text": "so let's construct such a character array array and the way we're going to do this is and the way we're going to do this is and the way we're going to do this is we're going to take all the words which we're going to take all the words which we're going to take all the words which is a list of strings is a list of strings is a list of strings we're going to concatenate all of it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 145,
      "text": "we're going to concatenate all of it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 146,
      "text": "we're going to concatenate all of it into a massive string so this is just into a massive string so this is just into a massive string",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 147,
      "text": "so this is just simply the entire data set as a single simply the entire data set as a single simply the entire data set as a single string string we're going to pass this to the set we're going to pass this to the set we're going to pass this to the set constructor which takes this massive constructor which takes this massive constructor which takes this massive string string and throws out duplicates because sets and throws out duplicates because sets and throws out duplicates because sets do not allow duplicates do not allow duplicates do not allow duplicates so set of this will just be the set of so set of this will just be the set of so set of this will just be the set of all the lowercase characters all the lowercase characters all the lowercase characters and there should be a total of 26 of and there should be a total of 26 of and there should be a total of 26 of them and now we actually don't want a set we",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 148,
      "text": "and now we actually don't want a set we want a list want a list want a list",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 149,
      "text": "but we don't want a list sorted in some",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 150,
      "text": "but we don't want a list sorted in some",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 151,
      "text": "but we don't want a list sorted in some weird arbitrary way we want it to be weird arbitrary way we want it to be weird arbitrary way we want it to be sorted sorted sorted from a to z from a to z from a to z so sorted list so sorted list so sorted list so those are our characters now what we want is this lookup table as now what we want is this lookup table as i mentioned so let's create a special i mentioned so let's create a special i mentioned so let's create a special s2i i will call it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 152,
      "text": "s2i",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 153,
      "text": "i will call it s2i",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 154,
      "text": "i will call it um s is string or character and this um s is string or character and this um s is string or character and this will be an s2i mapping will be an s2i mapping will be an s2i mapping for for for is in enumerate of these characters is in enumerate of these characters is in enumerate of these characters so enumerate basically gives us this so enumerate basically gives us this so enumerate basically gives us this iterator over the integer index and the iterator over the integer index and the iterator over the integer index and the actual element of the list",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 155,
      "text": "and then we actual element of the list and then we actual element of the list",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 156,
      "text": "and then we are mapping the character to the integer are mapping the character to the integer are mapping the character to the integer so s2i",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 157,
      "text": "so s2i",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 158,
      "text": "so s2i is a mapping from a to 0 b to 1 etc all is a mapping from a to 0 b to 1 etc all is a mapping from a to 0 b to 1 etc all the way from z to 25 and that's going to be useful here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 159,
      "text": "but and that's going to be useful here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 160,
      "text": "but we actually also have to specifically we actually also have to specifically we actually also have to specifically set that s will be 26 set that s will be 26 set that s will be 26 and s to i at e will be 27 right because and s to i at e will be 27 right because and s to i at e will be 27 right because z was 25.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 161,
      "text": "z was 25.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 162,
      "text": "z was 25.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 163,
      "text": "so those are the lookups and now we can so those are the lookups and now we can so those are the lookups and now we can come here and we can map come here and we can map come here and we can map both character 1 and character 2 to both character 1 and character 2 to both character 1 and character 2 to their integers their integers their integers so this will be s2i at character 1",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 164,
      "text": "so this will be s2i at character 1",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 165,
      "text": "so this will be s2i at character 1 and ix2 will be s2i of character 2. and ix2 will be s2i of character 2. and ix2 will be s2i of character 2.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 166,
      "text": "and now we should be able to and now we should be able to and now we should be able to do this line but using our array so n",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 167,
      "text": "at do this line but using our array so n at do this line but using our array so n at x1 ix2 this is the two-dimensional array x1 ix2",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 168,
      "text": "this is the two-dimensional array x1 ix2",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 169,
      "text": "this is the two-dimensional array indexing i've shown you before indexing i've shown you before indexing i've shown you before and honestly just plus equals one and honestly just plus equals one and honestly just plus equals one because everything starts at because everything starts at because everything starts at zero zero zero",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 170,
      "text": "so this should so this should so this should work work work and give us a large 28 by 28 array and give us a large 28 by 28 array and give us a large 28 by 28 array of all these counts so of all these counts so of all these counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 171,
      "text": "so if we print n if we print n if we print n this is the array",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 172,
      "text": "but of course it looks this is the array",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 173,
      "text": "but of course it looks this is the array",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 174,
      "text": "but of course it looks ugly so let's erase this ugly mess and ugly",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 175,
      "text": "so let's erase this ugly mess and ugly so let's erase this ugly mess and let's try to visualize it a bit more let's try to visualize it a bit more let's try to visualize it a bit more nicer nicer nicer",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 176,
      "text": "so for that we're going to use a library so for that we're going to use a library so for that we're going to use a library called matplotlib called matplotlib called matplotlib so matplotlib allows us to create so matplotlib allows us to create so matplotlib allows us to create figures so we can do things like plt figures so we can do things like plt figures so we can do things like plt item show of the counter array item show of the counter array item show of the counter array",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 177,
      "text": "so this is the 28x28 array so this is the 28x28 array so this is the 28x28 array and this is structure but even this i and this is structure but even this i and this is structure but even this i would say is still pretty ugly would say is still pretty ugly would say is still pretty ugly",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 178,
      "text": "so we're going to try to create a much",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 179,
      "text": "so we're going to try to create a much",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 180,
      "text": "so we're going to try to create a much nicer visualization of it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 181,
      "text": "and i wrote a nicer visualization of it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 182,
      "text": "and i wrote a nicer visualization of it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 183,
      "text": "and i wrote a bunch of code for that bunch of code for that bunch of code for that the first thing we're going to need is the first thing we're going to need is the first thing we're going to need is we're going to need to invert we're going to need to invert we're going to need to invert this array here this dictionary so s2i this array here this dictionary so s2i this array here this dictionary so s2i is mapping from s to i is mapping from s to i is mapping from s to i and in i2s",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 184,
      "text": "we're going to reverse this and in i2s",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 185,
      "text": "we're going to reverse this and in i2s",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 186,
      "text": "we're going to reverse this dictionary so iterator of all the items dictionary so iterator of all the items dictionary so iterator of all the items and just reverse that array and just reverse that array and just reverse that array",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 187,
      "text": "so i2s",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 188,
      "text": "so",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 189,
      "text": "i2s so i2s maps inversely from 0 to a 1 to b etc maps inversely from 0 to a 1 to b etc maps inversely from 0 to a 1 to b etc so we'll need that",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 190,
      "text": "so we'll need that",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 191,
      "text": "so we'll need that",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 192,
      "text": "and then here's the code that i came up and then here's the code that i came up and then here's the code that i came up with to try to make this a little bit with to try to make this a little bit with to try to make this a little bit nicer we create a figure we create a figure we plot we plot we plot n n n",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 193,
      "text": "and then we do",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 194,
      "text": "and then we visualize a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 195,
      "text": "and then we do",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 196,
      "text": "and then we visualize a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 197,
      "text": "and then we do",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 198,
      "text": "and then we visualize a bunch of things later let me just run it bunch of things later let me just run it bunch of things later let me just run it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 199,
      "text": "so you get a sense of what this is okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 200,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 201,
      "text": "so you see here that we have so you see here that we have so you see here that we have the array spaced out the array spaced out the array spaced out and every one of these is basically like and every one of these is basically like and every one of these is basically like b follows g zero times b follows g zero times b follows g zero times b follows h 41 times b follows h 41 times b follows h 41 times um so a follows j 175 times um so a follows j 175 times um so a follows j 175 times and so what you can see that i'm doing",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 202,
      "text": "and so what you can see that i'm doing",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 203,
      "text": "and so what you can see that i'm doing here is first",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 204,
      "text": "i show that entire array here is first i show that entire array here is first i show that entire array",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 205,
      "text": "and then i iterate over all the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 206,
      "text": "and then i iterate over all the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 207,
      "text": "and then i iterate over all the individual little cells here individual little cells here individual little cells here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 208,
      "text": "and i create a character string here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 209,
      "text": "and i create a character string here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 210,
      "text": "and i create a character string here which is the inverse mapping i2s of the which is the inverse mapping i2s of the which is the inverse mapping i2s of the integer",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 211,
      "text": "i and the integer j",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 212,
      "text": "so those are integer i and the integer j",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 213,
      "text": "so those are integer i and the integer j",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 214,
      "text": "so those are the bi-grams in a character the bi-grams in a character the bi-grams in a character representation representation representation and then i plot just the diagram text",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 215,
      "text": "and then i plot just the diagram text",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 216,
      "text": "and then i plot just the diagram text",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 217,
      "text": "and then i plot the number of times that",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 218,
      "text": "and then i plot the number of times that",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 219,
      "text": "and then i plot the number of times that this bigram occurs this bigram occurs this bigram occurs now the reason that there's a dot item now the reason that there's a dot item now the reason that there's a dot item here is because when you index into here is because when you index into here is because when you index into these arrays these are torch tensors these arrays these are torch tensors these arrays these are torch tensors you see that we still get a tensor back you see that we still get a tensor back you see that we still get a tensor back",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 220,
      "text": "so the type of this thing you'd think it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 221,
      "text": "so the type of this thing you'd think it so the type of this thing you'd think it would be just an integer 149",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 222,
      "text": "but it's would be just an integer 149",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 223,
      "text": "but it's would be just an integer 149",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 224,
      "text": "but it's actually a torch.tensor actually a torch.tensor actually a torch.tensor",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 225,
      "text": "and so and so",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 226,
      "text": "and so if you do dot item then it will pop out if you do dot item then it will pop out if you do dot item then it will pop out that in individual integer that in individual integer that in individual integer so it will just be 149.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 227,
      "text": "so it will just be 149.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 228,
      "text": "so it will just be 149.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 229,
      "text": "so that's what's happening there",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 230,
      "text": "and so that's what's happening there",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 231,
      "text": "and so that's what's happening there and these are just some options to make it these are just some options to make it these are just some options to make it look nice",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 232,
      "text": "look nice look nice",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 233,
      "text": "so what is the structure of this array we have all these counts and we see that we have all these counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 234,
      "text": "and we see that some of them occur often and some of some of them occur often and some of some of them occur often and some of them do not occur often them do not occur often them do not occur often now if you scrutinize this carefully you now if you scrutinize this carefully you now if you scrutinize this carefully you will notice that we're not actually will notice that we're not actually will notice that we're not actually being very clever being very clever being very clever that's because when you come over here that's because when you come over here that's because when you come over here you'll notice that for example we have you'll notice that for example we have you'll notice that for example we have an entire row of completely zeros and an entire row of completely zeros and an entire row of completely zeros and that's because the end character that's because the end character that's because the end character is never possibly going to be the first is never possibly going to be the first is never possibly going to be the first character of a bi-gram because we're character of a bi-gram because we're character of a bi-gram because we're always placing these end tokens all at always placing these end tokens all at always placing these end tokens all at the end of the diagram the end of the diagram the end of the diagram similarly we have entire columns zeros similarly we have entire columns zeros similarly we have entire columns zeros here because the s here because the s here because the s character will never possibly be the character will never possibly be the character will never possibly be the second element of a bigram because we second element of a bigram because we second element of a bigram because we always start with s",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 235,
      "text": "and we end with e always start with s",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 236,
      "text": "and we end with e always start with s",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 237,
      "text": "and we end with e",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 238,
      "text": "and we only have the words in between and we only have the words in between and we only have the words in between so we have an entire column of zeros",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 239,
      "text": "an so we have an entire column of zeros",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 240,
      "text": "an so we have an entire column of zeros an entire row of zeros and in this little entire row of zeros and in this little entire row of zeros and in this little two by two matrix here as well the only two by two matrix here as well the only two by two matrix here as well the only one that can possibly happen is if s one that can possibly happen is if s one that can possibly happen is if s directly follows e directly follows e directly follows e that can be non-zero if we have a word that can be non-zero if we have a word that can be non-zero if we have a word that has no letters so in that case that has no letters so in that case that has no letters so in that case there's no letters in the word it's an there's no letters in the word it's an there's no letters in the word it's an empty word and we just have s follows e empty word and we just have s follows e empty word and we just have s follows e",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 241,
      "text": "but the other ones are just not possible but the other ones are just not possible but the other ones are just not possible",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 242,
      "text": "and so we're basically wasting space and and so we're basically wasting space",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 243,
      "text": "and and so we're basically wasting space and not only that but the s and the e are not only that but the s and the e are not only that but the s and the e are getting very crowded here getting very crowded here getting very crowded here i was using these brackets because i was using these brackets because i was using these brackets because there's convention and natural language there's convention and natural language there's convention and natural language processing to use these kinds of processing to use these kinds of processing to use these kinds of brackets to denote special tokens brackets to denote special tokens brackets to denote special tokens but we're going to use something else",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 244,
      "text": "but we're going to use something else",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 245,
      "text": "but we're going to use something else so let's fix all this and make it so let's fix all this and make it so let's fix all this and make it prettier prettier prettier we're not actually going to have two we're not actually going to have two we're not actually going to have two special tokens we're only going to have special tokens we're only going to have special tokens we're only going to have one special token one special token one special token",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 246,
      "text": "so so we're going to have n by n we're going to have n by",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 247,
      "text": "n we're going to have n by n array of 27 by 27 instead array of 27 by 27 instead array of 27 by 27 instead instead of having two instead of having two instead of having two we will just have one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 248,
      "text": "and i will call it we will just have one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 249,
      "text": "and i will call it we will just have one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 250,
      "text": "and i will call it a dot a dot a dot",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 251,
      "text": "okay okay let me swing this over here let me swing this over here let me swing this over here now one more thing that i would like to now one more thing that i would like to now one more thing that i would like to do is i would actually like to make this do is i would actually like to make this do is i would actually like to make this special character half position zero special character half position zero special character half position zero",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 252,
      "text": "and i would like to offset all the other and i would like to offset all the other",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 253,
      "text": "and i would like to offset all the other letters off i find that a little bit letters off",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 254,
      "text": "i find that a little bit letters off",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 255,
      "text": "i find that a little bit more more pleasing pleasing pleasing",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 256,
      "text": "so so we need a plus one here so that the we need a plus one here so that the we need a plus one here so that the first character which is a will start at first character which is a will start at first character which is a will start at one one one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 257,
      "text": "so s2i",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 258,
      "text": "so s2i will now be a starts at one and dot is 0 will now be a starts at one and dot is 0 will now be a starts at one and dot is 0 and and i2s of course we're not changing this i2s of course we're not changing this i2s of course we're not changing this because i2s just creates a reverse because i2s just creates a reverse because i2s just creates a reverse mapping and this will work fine",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 259,
      "text": "so 1 is mapping and this will work fine",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 260,
      "text": "so 1 is mapping and this will work fine",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 261,
      "text": "so 1 is a 2 is b a 2 is b a 2 is b 0 is dot 0 is dot 0 is dot so we've reversed that here so we've reversed that here so we've reversed that here we have we have we have a dot and a dot a dot and a dot a dot and a dot this should work fine this should work fine this should work fine make sure i start at zeros make sure i start at zeros make sure i start at zeros count count count",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 262,
      "text": "and then here we don't go up to 28 we go",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 263,
      "text": "and then here we don't go up to 28 we go",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 264,
      "text": "and then here we don't go up to 28 we go up to 27 up to 27 up to 27 and this should just work",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 265,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 266,
      "text": "so we see that dot never happened it's so we see that dot never happened it's so we see that dot never happened it's at zero because we don't have empty at zero because we don't have empty at zero because we don't have empty words words words then this row here now is just uh very then this row here now is just uh very then this row here now is just uh very simply the um simply the um simply the um counts for all the first letters so counts for all the first letters so counts for all the first letters",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 267,
      "text": "so uh j starts a word h starts a word i uh j starts a word h starts a word i uh j starts a word h starts a word i starts a word etc and then these are all starts a word etc and then these are all starts a word etc and then these are all the ending the ending the ending characters characters and in between we have the structure of and in between we have the structure of and in between we have the structure of what characters follow each other what characters follow each other what characters follow each other so this is the counts array of our",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 268,
      "text": "so this is the counts array of our",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 269,
      "text": "so this is the counts array of our entire entire entire data set so this array actually has all data set so this array actually has all data set so this array actually has all the information necessary for us to the information necessary for us to the information necessary for us to actually sample from this bigram actually sample from this bigram actually sample from this bigram uh character level language model uh character level language model uh character level language model and um roughly speaking what we're going and um roughly speaking what we're going and um roughly speaking what we're going to do is we're just going to start to do is we're just going to start to do is we're just going to start following these probabilities and these following these probabilities and these following these probabilities and these counts and we're going to start sampling counts and we're going to start sampling counts and we're going to start sampling from the from the model from the from the model from the from the model so in the beginning of course so in the beginning of course so in the beginning of course we start with the dot the start token",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 270,
      "text": "we start with the dot the start token we start with the dot the start token dot dot dot so to sample the first character of a so to sample the first character of a so to sample the first character of a name we're looking at this row here name we're looking at this row here name we're looking at this row here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 271,
      "text": "so we see that we have the counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 272,
      "text": "and so we see that we have the counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 273,
      "text": "and so we see that we have the counts and those concepts terminally are telling us those concepts terminally are telling us those concepts terminally are telling us how often any one of these characters is how often any one of these characters is how often any one of these characters is to start a word to start a word to start a word",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 274,
      "text": "so if we take this n",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 275,
      "text": "so if we take this n",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 276,
      "text": "so if we take this n",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 277,
      "text": "and we grab the first row and we grab the first row and we grab the first row we can do that by using just indexing as we can do that by using just indexing as we can do that by using just indexing as zero zero and then using this notation column for and then using this notation column for and then using this notation column for the rest of that row the rest of that row the rest of that row so n zero colon so n zero colon so n zero colon is indexing into the zeroth is indexing into the zeroth is indexing into the zeroth row",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 278,
      "text": "and then it's grabbing all the row",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 279,
      "text": "and then it's grabbing all the row",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 280,
      "text": "and then it's grabbing all the columns columns columns and so this will give us a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 281,
      "text": "and so this will give us a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 282,
      "text": "and so this will give us a one-dimensional array one-dimensional array one-dimensional array of the first row so zero four four ten of the first row",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 283,
      "text": "so zero four four ten of the first row so zero four four ten you know zero four four ten one three",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 284,
      "text": "oh you know zero four four ten one three",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 285,
      "text": "oh you know zero four four ten one three oh six one five four two etc it's just the six one five four two etc it's just the six one five four two etc it's just the first row the shape of this first row the shape of this first row the shape of this is 27",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 286,
      "text": "it's just the row of 27 is 27",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 287,
      "text": "it's just the row of 27 is 27",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 288,
      "text": "it's just the row of 27 and the other way that you can do this and the other way that you can do this and the other way that you can do this also is you just you don't need to also is you just you don't need to also is you just you don't need to actually give this actually give this actually give this you just grab the zeroth row like this you just grab the zeroth row like this you just grab the zeroth row like this this is equivalent this is equivalent this is equivalent now these are the counts now these are the counts now these are the counts and now what we'd like to do is we'd and now what we'd like to do is we'd and now what we'd like to do is we'd like to basically um sample from this like to basically um sample from this like to basically um sample from this since these are the raw counts we since these are the raw counts we since these are the raw counts we actually have to convert this to actually have to convert this to actually have to convert this to probabilities probabilities probabilities so we create a probability vector",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 289,
      "text": "so we create a probability vector so we create a probability vector so we'll take n of zero so we'll take n of zero so we'll take n of zero and we'll actually convert this to float",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 290,
      "text": "and we'll actually convert this to float",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 291,
      "text": "and we'll actually convert this to float first first first",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 292,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 293,
      "text": "so these integers are converted to",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 294,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 295,
      "text": "so these integers are converted to",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 296,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 297,
      "text": "so these integers are converted to float float float floating point numbers and the reason floating point numbers and the reason floating point numbers and the reason we're creating floats is because we're we're creating floats is because we're we're creating floats is because we're about to normalize these counts about to normalize these counts about to normalize these counts so to create a probability distribution so to create a probability distribution so to create a probability distribution here we want to divide here we want to divide here we want to divide we basically want to do p p p divide p we basically want to do p p p divide p we basically want to do p p p divide p that sum",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 298,
      "text": "and now we get a vector of smaller and now we get a vector of smaller numbers and these are now probabilities numbers and these are now probabilities numbers and these are now probabilities",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 299,
      "text": "so of course because we divided by the so of course because we divided by the so of course because we divided by the sum the sum of p now is 1. sum the sum of p now is 1. sum the sum of p now is 1.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 300,
      "text": "so this is a nice proper probability so this is a nice proper probability",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 301,
      "text": "so this is a nice proper probability distribution it sums to 1 and this is distribution it sums to 1 and this is distribution it sums to 1 and this is giving us the probability for any single giving us the probability for any single giving us the probability for any single character to be the first character to be the first character to be the first character of a word character of a word character of a word so now we can try to sample from this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 302,
      "text": "so now we can try to sample from this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 303,
      "text": "so now we can try to sample from this distribution to sample from these distribution to sample from these distribution to sample from these distributions we're going to use distributions we're going to use distributions we're going to use storch.multinomial which i've pulled up storch.multinomial which i've pulled up storch.multinomial which i've pulled up here here so torch.multinomial returns uh samples from the multinomial probability samples from the multinomial probability distribution which is a complicated way distribution which is a complicated way distribution which is a complicated way of saying you give me probabilities and of saying you give me probabilities and of saying you give me probabilities and i will give you integers which are i will give you integers which are i will give you integers which are sampled sampled sampled according to the property distribution according to the property distribution according to the property distribution so this is the signature of the method so this is the signature of the method",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 304,
      "text": "so this is the signature of the method and to make everything deterministic and to make everything deterministic and to make everything deterministic we're going to use a generator object in we're going to use a generator object in we're going to use a generator object in pytorch pytorch pytorch so this makes everything deterministic so this makes everything deterministic so this makes everything deterministic",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 305,
      "text": "so when you run this on your computer so when you run this on your computer so when you run this on your computer you're going to the exact get the exact you're going to the exact get the exact you're going to the exact get the exact same results that i'm getting here on my same results that i'm getting here on my same results that i'm getting here on my computer computer computer",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 306,
      "text": "so let me show you how this works here's the deterministic way of creating here's the deterministic way of creating a torch generator object a torch generator object a torch generator object seeding it with some number that we can seeding it with some number that we can seeding it with some number that we can agree on agree on agree on so that seeds a generator gets gives us so that seeds a generator gets gives us so that seeds a generator gets gives us an object g an object g an object g",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 307,
      "text": "and then we can pass that g",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 308,
      "text": "and then we can pass that g",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 309,
      "text": "and then we can pass that g to a function to a function to a function that creates um that creates um that creates um here random numbers twerk.rand creates here random numbers twerk.rand creates here random numbers twerk.rand creates random numbers three of them random numbers three of them random numbers three of them",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 310,
      "text": "and it's using this generator object to and it's using this generator object to and it's using this generator object to as a source of randomness as a source of randomness as a source of randomness so so without normalizing it without normalizing it without normalizing it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 311,
      "text": "i can just print i can just print i can just print this is sort of like numbers between 0 this is sort of like numbers between 0 this is sort of like numbers between 0 and 1 that are random according to this and 1 that are random according to this and 1 that are random according to this thing",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 312,
      "text": "and whenever i run it again thing",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 313,
      "text": "and whenever i run it again thing",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 314,
      "text": "and whenever i run it again i'm always going to get the same result i'm always going to get the same result i'm always going to get the same result because i keep using the same generator because i keep using the same generator because i keep using the same generator object which i'm seeing here object which i'm seeing here object which i'm seeing here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 315,
      "text": "and then if i divide and then if i divide and then if i divide to normalize i'm going to get a nice to normalize i'm going to get a nice to normalize i'm going to get a nice probability distribution of just three probability distribution of just three probability distribution of just three elements elements elements",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 316,
      "text": "and then we can use torsion multinomial",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 317,
      "text": "and then we can use torsion multinomial",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 318,
      "text": "and then we can use torsion multinomial to draw samples from it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 319,
      "text": "so this is what to draw samples from it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 320,
      "text": "so this is what to draw samples from it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 321,
      "text": "so this is what that looks like that looks like that looks like tertiary multinomial we'll take the tertiary multinomial we'll take the tertiary multinomial we'll take the torch tensor torch tensor torch tensor of probability distributions of probability distributions of probability distributions then we can ask for a number of samples then we can ask for a number of samples then we can ask for a number of samples let's say 20.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 322,
      "text": "let's say 20.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 323,
      "text": "let's say 20.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 324,
      "text": "replacement equals true means that when replacement equals true means that when replacement equals true means that when we draw an element we draw an element we draw an element we will uh we can draw it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 325,
      "text": "and then we we will uh we can draw it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 326,
      "text": "and then we we will uh we can draw it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 327,
      "text": "and then we can put it back into the list of can put it back into the list of can put it back into the list of eligible indices to draw again eligible indices to draw again eligible indices to draw again",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 328,
      "text": "and we have to specify replacement as and we have to specify replacement as and we have to specify replacement as true because by default uh for some true because by default uh for some true because by default uh for some reason it's false reason it's false reason it's false",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 329,
      "text": "and i think",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 330,
      "text": "and i think",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 331,
      "text": "and i think you know it's just something to be you know it's just something to be you know it's just something to be careful with careful with careful with and the generator is passed in here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 332,
      "text": "so and the generator is passed in here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 333,
      "text": "so and the generator is passed in here so we're going to always get deterministic we're going to always get deterministic we're going to always get deterministic results the same results",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 334,
      "text": "so if i run results the same results so if i run results the same results so if i run these two these two these two we're going to get a bunch of samples we're going to get a bunch of samples we're going to get a bunch of samples from this distribution from this distribution from this distribution now you'll notice here that the now you'll notice here that the now you'll notice here that the probability for the probability for the probability for the first element in this tensor is 60 first element in this tensor is 60 first element in this tensor is 60 so in these 20 samples we'd expect 60 of so in these 20 samples we'd expect 60 of so in these 20 samples we'd expect 60 of them to be zero them to be zero them to be zero we'd expect thirty percent of them to be we'd expect thirty percent of them to be we'd expect thirty percent of them to be one one and because the the element index two and because the the element index two and because the the element index two has only ten percent probability very has only ten percent probability very has only ten percent probability very few of these samples should be two and few of these samples should be two and few of these samples should be two and indeed we only have a small number of indeed we only have a small number of indeed we only have a small number of twos twos twos and we can sample as many as we'd like and we can sample as many as we'd like and we can sample as many as we'd like and the more we sample the more and the more we sample the more and the more we sample the more these numbers should um roughly have the these numbers should um roughly have the these numbers should um roughly have the distribution here distribution here distribution here so we should have lots of zeros",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 335,
      "text": "so we should have lots of zeros so we should have lots of zeros half as many um half as many um half as many um ones",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 336,
      "text": "and we should have um three times ones and we should have um three times ones and we should have um three times as few as few as few",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 337,
      "text": "oh sorry s few ones and three times as",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 338,
      "text": "oh sorry s few ones and three times as",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 339,
      "text": "oh sorry s few ones and",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 340,
      "text": "three times as few uh few uh few uh twos twos so you see that we have very few twos we",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 341,
      "text": "so you see that we have very few twos",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 342,
      "text": "we",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 343,
      "text": "so you see that we have very few twos we have some ones and most of them are zero have some ones and most of them are zero have some ones and most of them are zero",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 344,
      "text": "so that's what torsion multinomial is so that's what torsion multinomial is so that's what torsion multinomial is doing doing doing for us here for us here for us here we are interested in this row we've we are interested in this row we've we are interested in this row we've created this p here p here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 345,
      "text": "and now we can sample from it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 346,
      "text": "and now we can sample from it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 347,
      "text": "and now we can sample from it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 348,
      "text": "so if we use the same",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 349,
      "text": "so if we use the same",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 350,
      "text": "so if we use the same seed seed seed and then we sample from this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 351,
      "text": "and then we sample from this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 352,
      "text": "and then we sample from this distribution let's just get one sample distribution let's just get one sample distribution let's just get one sample then we see that the sample is say 13.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 353,
      "text": "then we see that the sample is say 13.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 354,
      "text": "then we see that the sample is say 13.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 355,
      "text": "so this will be the index so this will be the index so this will be the index and let's you see how it's a tensor that and let's you see how it's a tensor that and let's you see how it's a tensor that wraps 13 we again have to use that item wraps 13 we again have to use that item wraps 13 we again have to use that item to pop out that integer to pop out that integer to pop out that integer and now index would be just the number and now index would be just the number and now index would be just the number 13. 13. 13.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 356,
      "text": "and of course the um we can do and of course the um we can do and of course the um we can do we can map the i2s of ix to figure out we can map the i2s of ix to figure out we can map the i2s of ix to figure out exactly which character exactly which character exactly which character we're sampling here we're sampling m we're sampling here we're sampling m we're sampling here we're sampling m",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 357,
      "text": "so we're saying that the first character",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 358,
      "text": "so we're saying that the first character",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 359,
      "text": "so we're saying that the first character is is in our generation in our generation in our generation and just looking at the road here and just looking at the road here and just looking at the road here m was drawn and you we can see that m m was drawn",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 360,
      "text": "and you we can see that m m was drawn and you we can see that m actually starts a large number of words actually starts a large number of words actually starts a large number of words uh m uh m uh m started 2 500 words out of 32 000 words started 2 500 words out of 32 000 words started 2 500 words out of 32 000 words so almost so almost so almost a bit less than 10 percent of the words a bit less than 10 percent of the words a bit less than 10 percent of the words start with them",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 361,
      "text": "so this was actually a start with them",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 362,
      "text": "so this was actually a start with them",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 363,
      "text": "so this was actually a fairly likely character to draw fairly likely character to draw fairly likely character to draw um um so that would be the first character of so that would be the first character of so that would be the first character of our work and now we can continue to our work and now we can continue to our work and now we can continue to sample more characters because now we sample more characters because now we sample more characters because now we know that m started know that m started know that m started m is already sampled m is already sampled m is already sampled so now to draw the next character we so now to draw the next character we so now to draw the next character we will come back here and we will look for will come back here and we will look for will come back here and we will look for the row the row the row that starts with m that starts with m that starts with m",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 364,
      "text": "so you see m",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 365,
      "text": "so you see m",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 366,
      "text": "so you see m",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 367,
      "text": "and we have a row here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 368,
      "text": "and we have a row here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 369,
      "text": "and we have a row here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 370,
      "text": "so we see that m dot is so we see that m dot is so we see that m dot is 516 m a is this many and b is this many 516 m a is this many and b is this many 516 m a is this many and b is this many etc so these are the counts for the next etc so these are the counts for the next etc so these are the counts for the next row and that's the next character that row",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 371,
      "text": "and that's the next character that row",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 372,
      "text": "and that's the next character that we are going to now generate we are going to now generate we are going to now generate",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 373,
      "text": "so i think we are ready to actually",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 374,
      "text": "just so i think we are ready to actually",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 375,
      "text": "just so i think we are ready to actually just write out the loop because i think write out the loop because i think write out the loop because i think you're starting to get a sense of how you're starting to get a sense of how you're starting to get a sense of how this is going to go this is going to go this is going to go the um the um the um we always begin at we always begin at we always begin at index 0 because that's the start token index 0 because that's the start token index 0 because that's the start token",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 376,
      "text": "and then while true and then while true and then while true we're going to grab the row we're going to grab the row we're going to grab the row corresponding to index corresponding to index corresponding to index that we're currently on so that's p that we're currently on",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 377,
      "text": "so that's p that we're currently on so that's p",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 378,
      "text": "so that's n array at ix",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 379,
      "text": "so that's n array at ix",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 380,
      "text": "so that's n array at ix converted to float is",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 381,
      "text": "rp",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 382,
      "text": "then we normalize then we normalize this p to sum to one i accidentally ran the infinite loop we i accidentally ran the infinite loop we normalize p to something one normalize p to something one normalize p to something",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 383,
      "text": "one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 384,
      "text": "then we need this generator object then we need this generator object then we need this generator object now we're going to initialize up here now we're going to initialize up here now we're going to initialize up here and we're going to draw a single sample and we're going to draw a single sample and we're going to draw a single sample from this distribution and then this is going to tell us what and then this is going to tell us what index is going to be next index is going to be next index is going to be next if the index sampled is if the index sampled is if the index sampled is 0 0 0",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 385,
      "text": "then that's now the end token then that's now the end token then that's now the end token",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 386,
      "text": "so we will break so we will break so we will break otherwise we are going to print otherwise we are going to print otherwise we are going to print s2i of ix",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 387,
      "text": "and uh that's pretty much it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 388,
      "text": "we're just and uh that's pretty much",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 389,
      "text": "it we're just uh this should work",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 390,
      "text": "okay more uh this should work",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 391,
      "text": "okay more uh this should work",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 392,
      "text": "okay more",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 393,
      "text": "so that's that's the name that we've",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 394,
      "text": "so that's that's the name that we've",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 395,
      "text": "so that's that's the name that we've sampled we started with m the next step sampled we started with m the next step sampled we started with m the next step was o then r and then dot and this dot we it here as well and this dot we it here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 396,
      "text": "as well so so let's now do this a few times so let's actually create an so let's actually create an out list here and instead of printing we're going to and instead of printing we're going to append append append so out that append this character and then here let's just print it at the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 397,
      "text": "and then here let's just print it at the end so let's just join up all the outs end",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 398,
      "text": "so let's just join up all the outs end",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 399,
      "text": "so let's just join up all the outs and we're just going to print more okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 400,
      "text": "and we're just going to print more okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 401,
      "text": "and we're just going to print more okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 402,
      "text": "now we're always getting the same result now we're always getting the same result now we're always getting the same result because of the generator because of the generator because of the generator",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 403,
      "text": "so if we want to do this a few times we so if we want to do this a few times we so if we want to do this a few times we can go for i in range can go for i in range can go for i in range 10 we can sample 10 names 10 we can sample 10 names 10 we can sample 10 names and we can just do that 10 times and we can just do that 10 times and we can just do that 10 times and these are the names that we're and these are the names that we're and these are the names that we're getting out getting out getting out let's do 20.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 404,
      "text": "i'll be honest with you this doesn't i'll be honest with you this doesn't look right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 405,
      "text": "look right look right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 406,
      "text": "so i started a few minutes to convince so i started a few minutes to convince",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 407,
      "text": "so i started a few minutes to convince myself that it actually is right myself that it actually is right myself that it actually is right the reason these samples are so terrible the reason these samples are so terrible the reason these samples are so terrible is that bigram language model is that bigram language model is that bigram language model is actually look just like really is actually look just like really is actually look just like really terrible terrible terrible we can generate a few more here we can generate a few more here we can generate a few more here and you can see that they're kind of and you can see that they're kind of and you can see that they're kind of like their name like a little bit like like their name like a little bit like like their name like a little bit like yanu o'reilly etc",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 408,
      "text": "but they're just like yanu o'reilly etc but they're just like yanu o'reilly etc but they're just like totally messed up um totally messed up um totally messed up um",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 409,
      "text": "and i mean the reason that this is so",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 410,
      "text": "and i mean the reason that this is so",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 411,
      "text": "and i mean the reason that this is so bad like we're generating h as a name bad like we're generating h as a name bad like we're generating h as a name",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 412,
      "text": "but you have to think through but you have to think through but you have to think through it from the model's eyes it doesn't know it from the model's eyes it doesn't know it from the model's eyes it doesn't know that this h is the very first",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 413,
      "text": "h all it that this h is the very first",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 414,
      "text": "h all it that this h is the very first",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 415,
      "text": "h all it knows is that h was previously and now knows is that h was previously and now knows is that h was previously and now how likely is h the last character",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 416,
      "text": "well how likely is h the last character",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 417,
      "text": "well how likely is h the last character",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 418,
      "text": "well it's somewhat",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 419,
      "text": "it's somewhat",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 420,
      "text": "it's somewhat likely",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 421,
      "text": "and so it just makes it last likely",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 422,
      "text": "and so it just makes it last likely",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 423,
      "text": "and so it just makes it last character it doesn't know that there character it doesn't know that there character it doesn't know that there were other things before it or there were other things before it or there were other things before it or there were not other things before it and so were not other things before it and so were not other things before it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 424,
      "text": "and so that's why it's generating all these that's why it's generating all these that's why it's generating all these like like like nonsense names nonsense names nonsense names another way to do this is to convince yourself that this is to convince yourself that this is actually doing something reasonable even actually doing something reasonable even actually doing something reasonable even though it's so terrible is though it's so terrible is though it's so terrible is these little piece here are 27 right these little piece here are 27 right these little piece here are 27 right like 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 425,
      "text": "like 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 426,
      "text": "like 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 427,
      "text": "so how about if we did something like so how about if we did something like so how about if we did something like this this this instead of p having any structure instead of p having any structure instead of p having any structure whatsoever whatsoever whatsoever how about if p was just how about if p was just how about if p was just torch dot once of 27 of 27 by default this is a float 32 so this is by default this is a float 32",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 428,
      "text": "so this is by default this is a float 32",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 429,
      "text": "so this is fine divide 27 fine divide 27 fine divide 27",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 430,
      "text": "so what i'm doing here is this is the so what i'm doing here is this is the so what i'm doing here is this is the uniform distribution which will make uniform distribution which will make uniform distribution which will make everything equally likely everything equally likely everything equally likely and we can sample from that",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 431,
      "text": "so let's see and we can sample from that so let's see and we can sample from that so let's see if that does any better if that does any better if that does any better",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 432,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 433,
      "text": "so it's okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 434,
      "text": "so it's okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 435,
      "text": "so it's this is what you have from a model that this is what you have from a model that this is what you have from a model that is completely untrained where everything is completely untrained where everything is completely untrained where everything is equally likely so it's obviously",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 436,
      "text": "is equally likely so it's obviously is equally likely",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 437,
      "text": "so it's obviously garbage",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 438,
      "text": "and then if we have a trained garbage and then if we have a trained garbage",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 439,
      "text": "and then if we have a trained model which is trained on just bi-grams model which is trained on just bi-grams model which is trained on just bi-grams this is what we get so you can see that this is what we get so you can see that this is what we get so you can see that it is more name-like it is actually it is more name-like it is actually it is more name-like it is actually working it's just um working",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 440,
      "text": "it's just um working it's just um my gram is so terrible",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 441,
      "text": "and we have to do my gram is so terrible and we have to do my gram is so terrible and we have to do better now next i would like to fix an better now next i would like to fix an better now next i would like to fix an inefficiency that we have going on here inefficiency that we have going on here inefficiency that we have going on here because what we're doing here is we're because what we're doing here is we're because what we're doing here is we're always fetching a row of n from the always fetching a row of n from the always fetching a row of n from the counts matrix up ahead counts matrix up ahead counts matrix up ahead",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 442,
      "text": "and then we're always doing the same",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 443,
      "text": "and then we're always doing the same",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 444,
      "text": "and then we're always doing the same things we're converting to float and things we're converting to float and things we're converting to float",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 445,
      "text": "and we're dividing and we're doing this we're dividing and we're doing this we're dividing and we're doing this every single iteration of this loop and every single iteration of this loop and every single iteration of this loop and we just keep renormalizing these rows we just keep renormalizing these rows we just keep renormalizing these rows over and over again",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 446,
      "text": "and it's extremely over and over again",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 447,
      "text": "and it's extremely over and over again",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 448,
      "text": "and it's extremely inefficient and wasteful",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 449,
      "text": "so what i'd inefficient and wasteful so what i'd inefficient and wasteful so what i'd like to do is i'd like to actually like to do is i'd like to actually like to do is i'd like to actually prepare a matrix capital p that will prepare a matrix capital p that will prepare a matrix capital p that will just have the probabilities in it so in just have the probabilities in it so in just have the probabilities in it so in other words it's going to be the same as other words it's going to be the same as other words it's going to be the same as the capital n matrix here of counts but the capital n matrix here of counts but the capital n matrix here of counts but every single row will have the row of every single row will have the row of every single row will have the row of probabilities uh that is normalized to 1 probabilities uh that is normalized to 1 probabilities uh that is normalized to 1 indicating the probability distribution indicating the probability distribution indicating the probability distribution for the next character given the for the next character given the for the next character given the character before it character before it character before it um as defined by which row we're in um as defined by which row we're in um as defined by which row we're in so basically what we'd like to do is so basically what we'd like to do is so basically what we'd like to do is we'd like to just do it up front here we'd like to just do it up front here we'd like to just do it up front here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 450,
      "text": "and then we would like to just use that",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 451,
      "text": "and then we would like to just use that",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 452,
      "text": "and then we would like to just use that row here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 453,
      "text": "so here we would like to just row here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 454,
      "text": "so here we would like to just row here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 455,
      "text": "so here we would like to just do p equals p of ix instead do p equals p of ix instead do p equals p of ix instead",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 456,
      "text": "okay okay the other reason i want to do this is the other reason i want to do this is the other reason i want to do this is not just for efficiency",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 457,
      "text": "but also i would not just for efficiency",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 458,
      "text": "but also i would not just for efficiency",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 459,
      "text": "but also i would like us to practice like us to practice like us to practice these n-dimensional tensors and i'd like these n-dimensional tensors and i'd like these n-dimensional tensors and i'd like us to practice their manipulation and us to practice their manipulation and us to practice their manipulation and especially something that's called especially something that's called especially something that's called broadcasting that we'll go into in a broadcasting that we'll go into in a broadcasting that we'll go into in a second second second we're actually going to have to become we're actually going to have to become we're actually going to have to become very good at these tensor manipulations very good at these tensor manipulations very good at these tensor manipulations because if we're going to build out all because if we're going to build out all because if we're going to build out all the way to transformers we're going to the way to transformers we're going to the way to transformers we're going to be doing some pretty complicated um be doing some pretty complicated um be doing some pretty complicated um array operations for efficiency and we array operations for efficiency and we array operations for efficiency and we need to really understand that and be need to really understand that and be need to really understand that and be very good at it very good at it very good at it so intuitively what we want to do is we so intuitively what we want to do is we so intuitively what we want to do is we first want to grab the floating point first want to grab the floating point first want to grab the floating point copy of n copy of n copy of n",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 460,
      "text": "and i'm mimicking the line here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 461,
      "text": "and i'm mimicking the line here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 462,
      "text": "and i'm mimicking the line here basically basically basically",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 463,
      "text": "and then we want to divide all the rows",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 464,
      "text": "and then we want to divide all the rows",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 465,
      "text": "and then we want to divide all the rows so that they sum to 1.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 466,
      "text": "so that they sum to 1.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 467,
      "text": "so that they sum to 1.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 468,
      "text": "so we'd like to do something like this p",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 469,
      "text": "so we'd like to do something like this p",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 470,
      "text": "so we'd like to do something like this p divide p dot sum divide p dot sum divide p dot sum",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 471,
      "text": "but",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 472,
      "text": "but but now we have to be careful now we have to be careful now we have to be careful because p dot sum actually because p dot sum actually because p dot sum actually produces a sum produces a sum produces a sum",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 473,
      "text": "sorry equals and that float copy sorry equals and that float copy sorry equals and that float copy p dot sum produces a um p dot sum produces a um p dot sum produces a um sums up all of the counts of this entire sums up all of the counts of this entire sums up all of the counts of this entire matrix n and gives us a single number of matrix n and gives us a single number of matrix n and gives us a single number of just the summation of everything so just the summation of everything so just the summation of everything so that's not the way we want to define that's not the way we want to define that's not the way we want to define divide we want to simultaneously and in divide we want to simultaneously and in divide we want to simultaneously and in parallel divide all the rows parallel divide all the rows parallel divide all the rows by their respective sums by their respective sums by their respective sums",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 474,
      "text": "so what we have to do now is we have to so what we have to do now is we have to so what we have to do now is we have to go into documentation for torch.sum go into documentation for torch.sum go into documentation for torch.sum and we can scroll down here to a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 475,
      "text": "and we can scroll down here to a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 476,
      "text": "and we can scroll down here to a definition that is relevant to us which definition that is relevant to us which definition that is relevant to us which is where we don't only provide an input is where we don't only provide an input is where we don't only provide an input array that we want to sum",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 477,
      "text": "but we also array that we want to sum",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 478,
      "text": "but we also array that we want to sum but we also provide the dimension along which we provide the dimension along which we provide the dimension along which we want to sum want to sum want to sum and in particular we want to sum up and in particular we want to sum up and in particular we want to sum up over rows over rows over rows",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 479,
      "text": "right right right now one more argument that i want you to now one more argument that i want you to now one more argument that i want you to pay attention to here is the keep them pay attention to here is the keep them pay attention to here is the keep them is false is false is false if keep them is true then the output if keep them is true then the output if keep them is true then the output tensor is of the same size as input tensor is of the same size as input tensor is of the same size as input except of course the dimension along except of course the dimension along except of course the dimension along which is summed which will become just which is summed which will become just which is summed which will become just one one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 480,
      "text": "but if you pass in keep them as false",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 481,
      "text": "but if you pass in keep them as false",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 482,
      "text": "but if you pass in keep them as false then this dimension is squeezed out and then this dimension is squeezed out and then this dimension is squeezed out and so torch.sum not only does the sum",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 483,
      "text": "and so torch.sum not only does the sum and so torch.sum not only does the sum and collapses dimension to be of size one collapses dimension to be of size one collapses dimension to be of size one but in addition it does what's called a but in addition it does what's called a but in addition it does what's called a squeeze where it squeezes out it squeeze where it squeezes out it squeeze where it squeezes out it squeezes out that dimension squeezes out that dimension squeezes out that dimension",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 484,
      "text": "so so basically what we want here is we basically what we want here is we basically what we want here is we instead want to do p dot sum of some instead want to do p dot sum of some instead want to do p dot sum of some axis axis axis and in particular notice that p dot and in particular notice that p dot and in particular notice that p dot shape is 27 by 27 shape is 27 by 27 shape is 27 by 27 so when we sum up across axis zero then",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 485,
      "text": "so when we sum up across axis zero then",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 486,
      "text": "so when we sum up across axis zero then we would be taking the zeroth dimension we would be taking the zeroth dimension we would be taking the zeroth dimension and we would be summing across it and we would be summing across it and we would be summing across it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 487,
      "text": "so when keep them as true so when keep them as true so when keep them as true then this thing will not only give us then this thing will not only give us then this thing will not only give us the counts across um the counts across um the counts across um along the columns along the columns along the columns but notice that basically the shape of but notice that basically the shape of but notice that basically the shape of this is 1 by 27 we just get a row vector this is 1 by 27 we just get a row vector this is 1 by 27 we just get a row vector and the reason we get a row vector here and the reason we get a row vector here and the reason we get a row vector here again is because we passed in zero again is because we passed in zero again is because we passed in zero dimension so this zero dimension becomes dimension",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 488,
      "text": "so this zero dimension becomes dimension so this zero dimension becomes one and we've done a sum one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 489,
      "text": "and we've done a sum one and we've done a sum and we get a row",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 490,
      "text": "and so basically we've and we get a row",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 491,
      "text": "and so basically we've and we get a row",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 492,
      "text": "and so basically we've done the sum done the sum done the sum this way this way this way vertically and arrived at just a single vertically and arrived at just a single vertically and arrived at just a single 1 by 27 1 by 27 1 by 27 vector of counts vector of counts vector of counts what happens when you take out keep them what happens when you take out keep them what happens when you take out keep them is that we just get 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 493,
      "text": "so it squeezes is that we just get 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 494,
      "text": "so it squeezes is that we just get 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 495,
      "text": "so it squeezes out that dimension and we just get out that dimension",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 496,
      "text": "and we just get out that dimension",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 497,
      "text": "and we just get a one-dimensional vector of size 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 498,
      "text": "now we don't actually want now we don't actually want one by 27 row vector because that gives one by 27 row vector because that gives one by 27 row vector because that gives us the counts or the sums across us the counts or the sums across us the counts or the sums across the columns the columns the columns we actually want to sum the other way we actually want to sum the other way we actually want to sum the other way along dimension one and you'll see that along dimension one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 499,
      "text": "and you'll see that along dimension one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 500,
      "text": "and you'll see that the shape of this is 27 by one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 501,
      "text": "so it's a the shape of this is 27 by one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 502,
      "text": "so it's a the shape of this is 27 by one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 503,
      "text": "so it's a column vector it's a 27 by one column vector",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 504,
      "text": "it's a 27 by one column vector it's a 27 by one vector of counts vector of counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 505,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 506,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 507,
      "text": "and that's because what's happened here and that's because what's happened here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 508,
      "text": "and that's because what's happened here is that we're going horizontally and is that we're going horizontally and is that we're going horizontally and this 27 by 27 matrix becomes a 27 by 1 this 27 by 27 matrix becomes a 27 by 1 this 27 by 27 matrix becomes a 27 by 1 array array now you'll notice by the way that um the now you'll notice by the way that um the now you'll notice by the way that um the actual numbers actual numbers actual numbers of these counts are identical of these counts are identical of these counts are identical",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 509,
      "text": "and that's because this special array of and that's because this special array of and that's because this special array of counts here comes from bi-gram counts here comes from bi-gram counts here comes from bi-gram statistics",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 510,
      "text": "and actually it just so statistics and actually it just so statistics",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 511,
      "text": "and actually it just so happens by chance happens by chance happens by chance or because of the way this array is or because of the way this array is or because of the way this array is constructed that the sums along the constructed that the sums along the constructed that the sums along the columns or along the rows horizontally columns or along the rows horizontally columns or along the rows horizontally or vertically is identical or vertically is identical or vertically is identical",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 512,
      "text": "but actually what we want to do in this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 513,
      "text": "but actually what we want to do in this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 514,
      "text": "but actually what we want to do in this case is we want to sum across the case is we want to sum across the case is we want to sum across the rows rows rows horizontally",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 515,
      "text": "so what we want here is p horizontally",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 516,
      "text": "so what we want here is p horizontally",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 517,
      "text": "so what we want here is p that sum of one with keep in true that sum of one with keep in true that sum of one with keep in true 27 by one column vector 27 by one column vector 27 by one column vector and now what we want to do is we want to and now what we want to do is we want to and now what we want to do is we want to divide by that now we have to be careful here again is now we have to be careful here again is it possible to take it possible to take it possible to take what's a um p dot shape you see here 27",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 518,
      "text": "what's a um p dot shape you see here 27",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 519,
      "text": "what's a um p dot shape you see here 27 by 27 is it possible to take a 27 by 27 by 27 is it possible to take a 27 by 27 by 27 is it possible to take a 27 by 27 array and divide it by what is a 27 by 1 array and divide it by what is a 27 by 1 array and divide it by what is a 27 by 1 array array is that an operation that you can do is that an operation that you can do is that an operation that you can do and whether or not you can perform this and whether or not you can perform this and whether or not you can perform this operation is determined by what's called operation is determined by what's called operation is determined by what's called broadcasting rules",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 520,
      "text": "so if you just search broadcasting rules",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 521,
      "text": "so if you just search broadcasting rules",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 522,
      "text": "so if you just search broadcasting semantics in torch broadcasting semantics in torch broadcasting semantics in torch you'll notice that there's a special you'll notice that there's a special you'll notice that there's a special definition for definition for definition for what's called broadcasting that uh for what's called broadcasting that uh for what's called broadcasting that uh for whether or not um these two uh arrays whether or not um these two uh arrays whether or not um these two uh arrays can be combined in a binary operation can be combined in a binary operation can be combined in a binary operation like division like division like division so the first condition is each tensor",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 523,
      "text": "so the first condition is each tensor",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 524,
      "text": "so the first condition is each tensor has at least one dimension which is the has at least one dimension which is the has at least one dimension which is the case for us case for us case for us and then when iterating over the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 525,
      "text": "and then when iterating over the and then when iterating over the dimension sizes starting at the trailing dimension sizes starting at the trailing dimension sizes starting at the trailing dimension dimension dimension the dimension sizes must either be equal the dimension sizes must either be equal the dimension sizes must either be equal one of them is one or one of them does one of them is one or one of them does one of them is one or one of them does not exist not exist not exist",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 526,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 527,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 528,
      "text": "so let's do that we need to align the so let's do that we need to align the so let's do that we need to align the two arrays and their shapes which is two arrays and their shapes which is two arrays and their shapes which is very easy because both of these shapes very easy because both of these shapes very easy because both of these shapes have two elements so they're aligned have two elements so they're aligned have two elements so they're aligned then we iterate over from the from the then we iterate over from the from the then we iterate over from the from the right and going to the left right and going to the left right and going to the left each dimension must be either equal one each dimension must be either equal one each dimension must be either equal one of them is a one or one of them does not of them is a one or one of them does not of them is a one or one of them does not exist so in this case they're not equal exist so in this case they're not equal exist so in this case they're not equal",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 529,
      "text": "but one of them is a one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 530,
      "text": "so this is fine but one of them is a one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 531,
      "text": "so this is fine but one of them is a one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 532,
      "text": "so this is fine",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 533,
      "text": "and then this dimension they're both and then this dimension they're both",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 534,
      "text": "and then this dimension they're both equal equal equal",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 535,
      "text": "so uh this is fine",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 536,
      "text": "so uh this is fine",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 537,
      "text": "so uh this is fine",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 538,
      "text": "so all the dimensions are fine",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 539,
      "text": "and so all the dimensions are fine",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 540,
      "text": "and so all the dimensions are fine and therefore the this operation is therefore the this operation is therefore the this operation is broadcastable",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 541,
      "text": "so that means that this broadcastable so that means that this broadcastable so that means that this operation is allowed operation is allowed operation is allowed and what is it that these arrays do when and what is it that these arrays do when and what is it that these arrays do when you divide 27 by 27 by 27 by one you divide 27 by 27 by 27 by one you divide 27 by 27 by 27 by one what it does is that it takes this what it does is that it takes this what it does is that it takes this dimension one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 542,
      "text": "and it stretches it out it dimension one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 543,
      "text": "and it stretches it out it dimension one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 544,
      "text": "and it stretches it out it copies it to match copies it to match copies it to match 27 here in this case 27 here in this case 27 here in this case so in our case it takes this column so in our case it takes this column so in our case it takes this column vector which is 27 by 1 vector which is 27 by 1 vector which is 27 by 1 and it copies it 27 times and it copies it 27 times and it copies it 27 times to make to make to make these both be 27 by 27 internally you these both be 27 by 27 internally you these both be 27 by 27 internally you can think of it that way and so it can think of it that way and so it can think of it that way and so it copies those counts copies those counts copies those counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 545,
      "text": "and then it does an element-wise",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 546,
      "text": "and then it does an element-wise",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 547,
      "text": "and then it does an element-wise division division division which is what we want because these which is what we want because these which is what we want because these counts we want to divide by them on counts we want to divide by them on counts we want to divide by them on every single one of these columns in every single one of these columns in every single one of these columns in this matrix this matrix this matrix",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 548,
      "text": "so this actually we expect will so",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 549,
      "text": "this actually we expect will so this actually we expect will normalize normalize normalize every single row every single row every single row",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 550,
      "text": "and we can check that this is true by and we can check that this is true by and we can check that this is true by taking the first row for example and taking the first row for example and taking the first row for example and taking its sum we expect this to be taking its sum we expect this to be taking its sum we expect this to be 1. because it's not normalized 1. because it's not normalized 1. because it's not normalized and then we expect this now because if and then we expect this now because if and then we expect this now because if we actually correctly normalize all the we actually correctly normalize all the we actually correctly normalize all the rows we expect to get the exact same rows we expect to get the exact same rows we expect to get the exact same result here so let's run this result here so let's run this result here so let's run this it's the exact same result it's the exact same result it's the exact same result this is correct",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 551,
      "text": "so now i would like to this is correct",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 552,
      "text": "so now i would like to this is correct",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 553,
      "text": "so now i would like to scare you a little bit scare you a little bit scare you a little bit uh you actually have to like i basically uh you actually have to like i basically uh you actually have to like i basically encourage you very strongly to read encourage you very strongly to read encourage you very strongly to read through broadcasting semantics through broadcasting semantics through broadcasting semantics and i encourage you to treat this with",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 554,
      "text": "and i encourage you to treat this with and i encourage you to treat this with respect",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 555,
      "text": "and it's not something to play respect and it's not something to play respect",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 556,
      "text": "and it's not something to play fast and loose with it's something to fast and loose with it's something to fast and loose with it's something to really respect really understand and really respect really understand and really respect really understand and look up maybe some tutorials for look up maybe some tutorials for look up maybe some tutorials for broadcasting and practice it and be broadcasting and practice it and be broadcasting and practice it and be careful with it because you can very careful with it because you can very careful with it because you can very quickly run into books let me show you quickly run into books let me show you quickly run into books let me show you what i mean you see how here we have p dot sum of you see how here we have p dot sum of one keep them as true one keep them as true one keep them as true the shape of this is 27 by one let me the shape of this is 27 by one let me the shape of this is 27 by one let me take out this line just so we have the n take out this line just so we have the n take out this line just so we have the n",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 557,
      "text": "and then we can see the counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 558,
      "text": "and then we can see the counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 559,
      "text": "and then we can see the counts we can see that this is a all the counts we can see that this is a all the counts we can see that this is a all the counts across all the across all the across all the rows rows",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 560,
      "text": "and it's a 27 by one column vector",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 561,
      "text": "right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 562,
      "text": "and it's a 27 by one column vector",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 563,
      "text": "right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 564,
      "text": "and it's a 27 by one column vector right now suppose that i tried to do the now suppose that i tried to do the now suppose that i tried to do the following following following",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 565,
      "text": "but i erase keep them just true here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 566,
      "text": "but i erase keep them just true here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 567,
      "text": "but i erase keep them just true here what does that do if keep them is not what does that do if keep them is not what does that do if keep them is not true",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 568,
      "text": "it's false then remember according true",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 569,
      "text": "it's false then remember according true",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 570,
      "text": "it's false then remember according to documentation it gets rid of this to documentation it gets rid of this to documentation it gets rid of this dimension one it squeezes it out so dimension one it squeezes it out so dimension one it squeezes it out",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 571,
      "text": "so basically we just get all the same basically we just get all the same basically we just get all the same counts the same result except the shape counts the same result except the shape counts the same result except the shape of it is not 27 by 1",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 572,
      "text": "it is just 27 the of it is not 27 by 1",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 573,
      "text": "it is just 27 the of it is not 27 by 1",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 574,
      "text": "it is just 27 the one disappears one disappears one disappears",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 575,
      "text": "but all the counts are the same but all the counts are the same but all the counts are the same",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 576,
      "text": "so you'd think that this divide that so you'd think that this divide that so you'd think that this divide that would uh would work would uh would work would uh would work first of all can we even uh write this first of all can we even uh write this first of all can we even uh write this and will it is it even is it even and will it is it even is it even and will it is it even is it even expected to run is it broadcastable expected to run is it broadcastable expected to run is it broadcastable let's determine if this result is let's determine if this result is let's determine if this result is broadcastable broadcastable broadcastable p.summit one is shape p.summit one is shape p.summit one is shape is 27. is 27. is 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 577,
      "text": "this is 27 by 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 578,
      "text": "so 27 by 27 broadcasting into 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 579,
      "text": "so now broadcasting into 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 580,
      "text": "so now rules of broadcasting number one align rules of broadcasting number one align rules of broadcasting number one align all the dimensions on the right done now all the dimensions on the right done now all the dimensions on the right done now iteration over all the dimensions iteration over all the dimensions iteration over all the dimensions starting from the right going to the starting from the right going to the starting from the right going to the left left left all the dimensions must either be equal all the dimensions must either be equal all the dimensions must either be equal one of them must be one or one that does one of them must be one or one that does one of them must be one or one that does not exist so here they are all equal not exist so here they are all equal not exist so here they are all equal here the dimension does not exist here the dimension does not exist here the dimension does not exist so internally what broadcasting will do so internally what broadcasting will do so internally what broadcasting will do is it will create a one here is it will create a one here is it will create a one here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 581,
      "text": "and then",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 582,
      "text": "and then we see that one of them is a one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 583,
      "text": "and we see that one of them is a one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 584,
      "text": "and we see that one of them is a one and this will get copied and this will run this will get copied and this will run this will get copied and this will run this will broadcast this will broadcast this will broadcast",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 585,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 586,
      "text": "so you'd expect this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 587,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 588,
      "text": "so you'd expect this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 589,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 590,
      "text": "so you'd expect this to work this broadcast and this we can divide this broadcast and this we can divide this this now if i run this you'd expect it to now if i run this you'd expect it to now if i run this you'd expect it to work but work but work",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 591,
      "text": "but it doesn't it doesn't it doesn't uh you actually get garbage you get a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 592,
      "text": "uh you actually get garbage you get a uh you actually get garbage you get a wrong dissolve because this is actually wrong dissolve because this is actually wrong dissolve because this is actually a bug a bug a bug this keep them equals true this is a bug this is a bug in both cases we are doing in both cases we are doing in both cases we are doing the correct counts we are summing up the correct counts we are summing up the correct counts we are summing up across the rows across the rows across the rows but keep them is saving us and making it but keep them is saving us and making it but keep them is saving us and making it work so in this case work so in this case work so in this case i'd like to encourage you to potentially i'd like to encourage you to potentially i'd like to encourage you to potentially like pause this video at this point and like pause this video at this point and like pause this video at this point and try to think about why this is buggy and try to think about why this is buggy and try to think about why this is buggy and why the keep dim was necessary here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 593,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 594,
      "text": "so the reason to do so the reason to do so the reason to do for this is i'm trying to hint it here for this is i'm trying to hint it here for this is i'm trying to hint it here when i was sort of giving you a bit of a when i was sort of giving you a bit of a when i was sort of giving you a bit of a hint on how this works hint on how this works hint on how this works this this 27 vector 27 vector 27 vector internally inside the broadcasting this internally inside the broadcasting this internally inside the broadcasting this becomes a 1 by 27 becomes a 1 by 27 becomes a 1 by 27 and 1 by 27 is a row vector",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 595,
      "text": "right and 1 by 27 is a row vector right and 1 by 27 is a row vector right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 596,
      "text": "and now we are dividing 27 by 27 by 1 by and now we are dividing 27 by 27 by 1 by and now we are dividing 27 by 27 by 1 by 27 27 27 and torch will replicate this dimension and torch will replicate this dimension and torch will replicate this dimension",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 597,
      "text": "so basically so basically so basically uh it will take uh it will take uh it will take it will take this it will take this it will take this row vector and it will copy it row vector and it will copy it row vector and it will copy it vertically now vertically now vertically now 27 times so the 27 by 27 lies exactly 27 times so the 27 by 27 lies exactly 27 times so the 27 by 27 lies exactly and element wise divides and element wise divides and element wise divides and so basically what's happening here and so basically what's happening here and so basically what's happening here is is we're actually normalizing the columns we're actually normalizing the columns we're actually normalizing the columns instead of normalizing the rows instead of normalizing the rows instead of normalizing the rows so you can check that what's happening so you can check that what's happening so you can check that what's happening here is that p at zero which is the here is that p at zero which is the here is that p at zero which is the first row of p dot sum first row of p dot sum first row of p dot sum is not one it's seven is not one it's seven is not one it's seven it is the first column as an example it is the first column as an example it is the first column as an example that sums to one that sums to one that sums to one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 598,
      "text": "so so to summarize where does the issue come to summarize where does the issue come to summarize where does the issue come from the issue comes from the silent from the issue comes from the silent from the issue comes from the silent adding of a dimension here because in adding of a dimension here because in adding of a dimension here because in broadcasting rules you align on the broadcasting rules you align on the broadcasting rules you align on the right and go from right to left",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 599,
      "text": "and if right and go from right to left and if right and go from right to left and if dimension doesn't exist you create it dimension doesn't exist you create it dimension doesn't exist you create it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 600,
      "text": "so that's where the problem happens we so that's where the problem happens we so that's where the problem happens we still did the counts correctly we did still did the counts correctly we did still did the counts correctly we did the counts across the rows and we got the counts across the rows and we got the counts across the rows and we got the the counts on the right here as a the the counts on the right here as a the the counts on the right here as a column vector but because the keep column vector but because the keep column vector but because the keep things was true this this uh this things was true this this uh this things was true this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 601,
      "text": "this uh this dimension was discarded and now we just dimension was discarded and now we just dimension was discarded and now we just have a vector of 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 602,
      "text": "and because of have a vector of 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 603,
      "text": "and because of have a vector of 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 604,
      "text": "and because of broadcasting the way it works this broadcasting the way it works this broadcasting the way it works this vector of 27 suddenly becomes a row vector of 27 suddenly becomes a row vector of 27 suddenly becomes a row vector vector vector and then this row vector gets replicated and then this row vector gets replicated and then this row vector gets replicated vertically and that every single point vertically and that every single point vertically and that every single point we are dividing by the by the count in the opposite direction in the opposite direction",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 605,
      "text": "so uh so uh so uh so this thing just uh doesn't work this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 606,
      "text": "so this thing just uh doesn't work this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 607,
      "text": "so this thing just uh doesn't work this needs to be keep things equal true in needs to be keep things equal true in needs to be keep things equal true in this case this case this case",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 608,
      "text": "so then so then so then then we have that p at zero is then we have that p at zero is then we have that p at zero is normalized normalized normalized and conversely the first column you'd and conversely the first column",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 609,
      "text": "you'd and conversely the first column you'd expect to potentially not be normalized expect to potentially not be normalized expect to potentially not be normalized and this is what makes it work and this is what makes it work and this is what makes it work so pretty subtle",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 610,
      "text": "and uh hopefully this so pretty subtle",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 611,
      "text": "and uh hopefully this so pretty subtle",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 612,
      "text": "and uh hopefully this helps to scare you that you should have helps to scare you that you should have helps to scare you that you should have a respect for broadcasting be careful a respect for broadcasting be careful a respect for broadcasting be careful check your work uh and uh understand how check your work uh and uh understand how check your work uh and uh understand how it works under the hood and make sure it works under the hood and make sure it works under the hood and make sure that it's broadcasting in the direction that it's broadcasting in the direction that it's broadcasting in the direction that you like otherwise you're going to that",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 613,
      "text": "you like otherwise you're going to that you like otherwise you're going to introduce very subtle bugs very hard to introduce very subtle bugs very hard to introduce very subtle bugs very hard to find bugs and uh just be careful one find bugs and uh just be careful one find bugs and uh just be careful one more note on efficiency we don't want to more note on efficiency we don't want to more note on efficiency we don't want to be doing this here because this creates be doing this here because this creates be doing this here because this creates a completely new tensor that we store a completely new tensor that we store a completely new tensor that we store into p into p into p we prefer to use in place operations if we prefer to use in place operations if we prefer to use in place operations if possible possible possible so this would be an in-place operation so this would be an in-place operation so this would be an in-place operation it has the potential to be faster it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 614,
      "text": "it has the potential to be faster it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 615,
      "text": "it has the potential to be faster it doesn't create new memory doesn't create new memory doesn't create new memory under the hood and then let's erase this under the hood and then let's erase this under the hood and then let's erase this we don't need it we don't need it we don't need it and let's and let's and let's also also also um just do fewer",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 616,
      "text": "just so i'm not wasting um just do fewer",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 617,
      "text": "just so i'm not wasting um just do fewer",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 618,
      "text": "just so i'm not wasting space space space",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 619,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 620,
      "text": "so we're actually in a pretty good",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 621,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 622,
      "text": "so we're actually in a pretty good",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 623,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 624,
      "text": "so we're actually in a pretty good spot now spot now spot now we trained a bigram language model and we trained a bigram language model and we trained a bigram language model and we trained it really just by counting uh we trained it really just by counting uh we trained it really just by counting uh how frequently any pairing occurs and how frequently any pairing occurs and how frequently any pairing occurs and then normalizing so that we get a nice then normalizing so that we get a nice then normalizing so that we get a nice property distribution property distribution property distribution so really these elements of this array p",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 625,
      "text": "so really these elements of this array p",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 626,
      "text": "so really these elements of this array p are really the parameters of our biogram are really the parameters of our biogram are really the parameters of our biogram language model giving us and summarizing language model giving us and summarizing language model giving us and summarizing the statistics of these bigrams the statistics of these bigrams the statistics of these bigrams so we train the model",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 627,
      "text": "and then we know so we train the model",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 628,
      "text": "and then we know so we train the model",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 629,
      "text": "and then we know how to sample from a model we just how to sample from a model we just how to sample from a model we just iteratively uh sample the next character iteratively uh sample the next character iteratively uh sample the next character and feed it in each time and get a next and feed it in each time and get a next and feed it in each time and get a next character character character now what i'd like to do is i'd like to now what i'd like to do is i'd like to now what i'd like to do is i'd like to somehow evaluate the quality of this somehow evaluate the quality of this somehow evaluate the quality of this model we'd like to somehow summarize the model we'd like to somehow summarize the model we'd like to somehow summarize the quality of this model into a single quality of this model into a single quality of this model into a single number how good is it at predicting number how good is it at predicting number how good is it at predicting the training set the training set the training set and as an example so in the training set and as an example so in the training set and as an example so in the training set we can evaluate now the training loss we can evaluate now the training loss we can evaluate now the training loss and this training loss is telling us and this training loss is telling us and this training loss is telling us about about about sort of the quality of this model in a sort of the quality of this model in a sort of the quality of this model in a single number just like we saw in single number just like we saw in single number just like we saw in micrograd micrograd micrograd",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 630,
      "text": "so let's try to think through the so let's try to think through the so let's try to think through the quality of the model and how we would quality of the model and how we would quality of the model and how we would evaluate it evaluate it evaluate it basically what we're going to do is basically what we're going to do is basically what we're going to do is we're going to copy paste this code we're going to copy paste this code we're going to copy paste this code that we previously used for counting that we previously used for counting that we previously used for counting",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 631,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 632,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 633,
      "text": "and let me just print these diagrams and let me just print these diagrams and let me just print these diagrams first we're gonna use f strings first we're gonna use f strings first we're gonna use f strings",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 634,
      "text": "and i'm gonna print character one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 635,
      "text": "and i'm gonna print character one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 636,
      "text": "and i'm gonna print character one followed by character two these are the followed by character two these are the followed by character two these are the diagrams",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 637,
      "text": "and then i don't wanna do it diagrams",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 638,
      "text": "and then i don't wanna do it diagrams",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 639,
      "text": "and then i don't wanna do it for all the words just do the first for all the words just do the first for all the words just do the first three words so here we have emma olivia three words so here we have emma olivia three words so here we have emma olivia and ava bigrams and ava bigrams and ava bigrams now what we'd like to do is we'd like to now what we'd like to do is we'd like to now what we'd like to do is we'd like to basically look at the probability that basically look at the probability that basically look at the probability that the model assigns to every one of these the model assigns to every one of these the model assigns to every one of these diagrams diagrams diagrams so in other words we can look at the so in other words we can look at the so in other words we can look at the probability which is probability which is probability which is summarized in the matrix b summarized in the matrix b summarized in the matrix b of i x 1 x 2 of i x 1 x 2 of i x 1 x 2",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 640,
      "text": "and then we can print it here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 641,
      "text": "and then we can print it here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 642,
      "text": "and then we can print it here as probability as probability as probability and because these properties are way too and because these properties are way too and because these properties are way too large let me present large let me present large let me present or call in 0.4 f or call in 0.4 f or call in 0.4 f to like truncate it a bit to like truncate it a bit to like truncate it a bit",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 643,
      "text": "so what do we have here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 644,
      "text": "right we're so what do we have here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 645,
      "text": "right we're so what do we have here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 646,
      "text": "right we're looking at the probabilities that the looking at the probabilities that the looking at the probabilities that the model assigns to every one of these model assigns to every one of these model assigns to every one of these bigrams in the dataset bigrams in the dataset bigrams in the dataset and so we can see some of them are four",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 647,
      "text": "and so we can see some of them are four",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 648,
      "text": "and so we can see some of them are four percent three percent etc percent three percent etc percent",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 649,
      "text": "three percent etc just to have a measuring stick in our just to have a measuring stick in our just to have a measuring stick in our mind by the way um we have 27 possible mind by the way um we have 27 possible mind by the way um we have 27 possible characters or tokens and if everything characters or tokens and if everything characters or tokens and if everything was equally likely then you'd expect all was equally likely then you'd expect all was equally likely then you'd expect all these probabilities these probabilities these probabilities to be to be to be four percent roughly four percent roughly four percent roughly so anything above four percent means so anything above four percent means so anything above four percent means that we've learned something useful from that we've learned something useful from that we've learned something useful from these bigram statistics and you see that these bigram statistics",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 650,
      "text": "and you see that these bigram statistics",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 651,
      "text": "and you see that roughly some of these are four percent roughly some of these are four percent roughly some of these are four percent but some of them are as high as 40 but some of them are as high as 40 but some of them are as high as 40 percent percent percent 35 percent",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 652,
      "text": "and so on so you see that the 35 percent",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 653,
      "text": "and so on so you see that the 35 percent",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 654,
      "text": "and so on so you see that the model actually assigned a pretty high model actually assigned a pretty high model actually assigned a pretty high probability to whatever's in the probability to whatever's in the probability to whatever's in the training set",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 655,
      "text": "and so that's a good thing training set",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 656,
      "text": "and so that's a good thing training set",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 657,
      "text": "and so that's a good thing um basically if you have a very good um basically if you have a very good um basically if you have a very good model you'd expect that these model you'd expect that these model you'd expect that these probabilities should be near one because probabilities should be near one because probabilities should be near one because that means that your model is correctly that means that your model is correctly that means that your model is correctly predicting what's going to come next predicting what's going to come next predicting what's going to come next especially on the training set where you especially on the training set where you especially on the training set where you where you trained your model where you trained your model where you trained your model",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 658,
      "text": "so so now we'd like to think about how can we now we'd like to think about how can we now we'd like to think about how can we summarize these probabilities into a summarize these probabilities into a summarize these probabilities into a single number that measures the quality single number that measures the quality single number that measures the quality of this model of this model of this model now when you look at the literature into now when you look at the literature into now when you look at the literature into maximum likelihood estimation and maximum likelihood estimation and maximum likelihood estimation and statistical modeling and so on statistical modeling and so on statistical modeling and so on you'll see that what's typically used you'll see that what's typically used you'll see that what's typically used here is something called the likelihood here is something called the likelihood here is something called the likelihood and the likelihood is the product of all and the likelihood is the product of all and the likelihood is the product of all of these probabilities of these probabilities of these probabilities and so the product of all these and so the product of all these and so the product of all these probabilities is the likelihood and it's probabilities is the likelihood and it's probabilities is the likelihood and it's really telling us about the probability really telling us about the probability really telling us about the probability of the entire data set assigned uh of the entire data set assigned uh of the entire data set assigned uh assigned by the model that we've trained assigned by the model that we've trained assigned by the model that we've trained and that is a measure of quality and that is a measure of quality and that is a measure of quality",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 659,
      "text": "so the product of these so the product of these so the product of these should be as high as possible should be as high as possible should be as high as possible when you are training the model and when when you are training the model and when when you are training the model and when you have a good model your pro your you have a good model your pro your you have a good model your pro your product of these probabilities should be product of these probabilities should be product of these probabilities should be very high very high very high um um now because the product of these now because the product of these now because the product of these probabilities is an unwieldy thing to probabilities is an unwieldy thing to probabilities is an unwieldy thing to work with you can see that all of them work with you can see that all of them work with you can see that all of them are between zero and one so your product are between zero and one so your product are between zero and one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 660,
      "text": "so your product of these probabilities will be a very of these probabilities will be a very of these probabilities will be a very tiny number tiny number tiny number um um so so for convenience what people work with for convenience what people work with for convenience what people work with usually is not the likelihood but they usually is not the likelihood but they usually is not the likelihood but they work with what's called the log work with what's called the log work with what's called the log likelihood likelihood likelihood",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 661,
      "text": "so so the product of these is the likelihood the product of these is the likelihood the product of these is the likelihood to get the log likelihood we just have to get the log likelihood we just have to get the log likelihood we just have to take the log of the probability to take the log of the probability to take the log of the probability",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 662,
      "text": "and so the log of the probability here i",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 663,
      "text": "and so the log of the probability here i",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 664,
      "text": "and so the log of the probability here i have the log of x from zero to one have the log of x from zero to one have the log of x from zero to one the log is a you see here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 665,
      "text": "monotonic",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 666,
      "text": "the log is a you see here monotonic",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 667,
      "text": "the log is a you see here monotonic transformation of the probability transformation of the probability transformation of the probability where if you pass in one where if you pass in one where if you pass in one you get zero you get zero you get zero so probability one gets your log so probability one gets your log so probability one gets your log probability of zero probability of zero probability of zero and then as you go lower and lower and then as you go lower and lower and then as you go lower and lower probability the log will grow more and probability the log will grow more and probability the log will grow more and more negative until all the way to more negative until all the way to more negative until all the way to negative infinity at zero so here we have a log prob which is so here we have a log prob which is really just a torch.log of probability really just a torch.log of probability really just a torch.log of probability",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 668,
      "text": "let's print it out to get a sense of let's print it out to get a sense of let's print it out to get a sense of what that looks like what that looks like what that looks like log prob log prob log prob also",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 669,
      "text": "0.4 f",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 670,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 671,
      "text": "so as you can see when we plug in so as you can see when we plug in so as you can see when we plug in numbers that are very close some of our numbers that are very close some of our numbers that are very close some of our higher numbers we get closer and closer higher numbers we get closer and closer higher numbers we get closer and closer to zero to zero to zero and then if we plug in very bad",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 672,
      "text": "and then if we plug in very bad",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 673,
      "text": "and then if we plug in very bad probabilities we get more and more probabilities we get more and more probabilities we get more and more negative number that's bad negative number that's bad negative number",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 674,
      "text": "that's bad",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 675,
      "text": "so so and the reason we work with this is for and the reason we work with this is for and the reason we work with this is for a large extent convenience",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 676,
      "text": "right a large extent convenience right a large extent convenience right because we have mathematically that if because we have mathematically that if because we have mathematically that if you have some product a times b times c you have some product a times b times",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 677,
      "text": "c you have some product a times b times c of all these probabilities right of all these probabilities right of all these probabilities right the likelihood is the product of all the likelihood is the product of all the likelihood is the product of all these probabilities these probabilities then the log then the log then the log of these of these of these is just log of a plus is just log of a plus is just log of a plus log of b plus log of c if you remember your logs plus log of c if you remember your logs from your from your from your high school or undergrad and so on high school or undergrad and so on high school or undergrad and so on so we have that basically so we have that basically",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 678,
      "text": "so we have that basically the likelihood of the product the likelihood of the product the likelihood of the product probabilities the log likelihood is just probabilities the log likelihood is just probabilities the log likelihood is just the sum of the logs of the individual the sum of the logs of the individual the sum of the logs of the individual probabilities probabilities",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 679,
      "text": "so so log likelihood log likelihood log likelihood starts at zero starts at zero starts at zero and then log likelihood here we can just and then log likelihood here we can just and then log likelihood here we can just accumulate simply f strings f strings maybe you're familiar with this maybe you're familiar with this maybe you're familiar with this so log likelihood is negative 38.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 680,
      "text": "okay now now now we actually want um we actually want um we actually want um",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 681,
      "text": "so how high can log likelihood get it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 682,
      "text": "so how high can log likelihood get it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 683,
      "text": "so how high can log likelihood get it can go to zero so when all the can go to zero so when all the can go to zero so when all the probabilities are one log likelihood probabilities are one log likelihood probabilities are one log likelihood will be zero and then when all the will be zero and then when all the will be zero and then when all the probabilities are lower this will grow probabilities are lower this will grow probabilities are lower this will grow more and more negative more and more negative more and more negative now we don't actually like this because now we don't actually like this because now we don't actually like this because what we'd like is a loss function and a what we'd like is a loss function and a what we'd like is a loss function and a loss function has the semantics that low loss function has the semantics that low loss function has the semantics that low is good is good is good because we're trying to minimize the because we're trying to minimize the because we're trying to minimize the loss so we actually need to invert this loss",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 684,
      "text": "so we actually need to invert this loss",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 685,
      "text": "so we actually need to invert this and that's what gives us something",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 686,
      "text": "and that's what gives us something",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 687,
      "text": "and that's what gives us something called the negative log likelihood negative log likelihood is just negative negative log likelihood is just negative of the log likelihood these are f strings by the way if you'd these are f strings by the way if you'd like to look this up like to look this up like to look this up negative log likelihood equals negative log likelihood equals negative log likelihood equals so negative log likelihood now is just so negative log likelihood now is just so negative log likelihood now is just negative of it and so the negative log negative of it and so the negative log negative of it and so the negative log block load is a very nice loss function block load is a very nice loss function block load is a very nice loss function because um because um because um the lowest it can get is zero the lowest it can get is zero the lowest it can get is zero and the higher it is the worse off the and the higher it is the worse off the and the higher it is the worse off the predictions are that you're making predictions are that you're making predictions are that you're making and then one more modification to this and then one more modification to this and then one more modification to this that sometimes people do is that for that sometimes people do is that for that sometimes people do is that for convenience uh they actually like to convenience",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 688,
      "text": "uh they actually like to convenience uh they actually like to normalize by they like to make it an normalize by they like to make it an normalize by they like to make it an average instead of a sum average instead of a sum average instead of a sum",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 689,
      "text": "and so uh here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 690,
      "text": "and so uh here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 691,
      "text": "and so uh here let's just keep some counts as well let's just keep some counts as well let's just keep some counts as well so n plus equals one so n plus equals one so n plus equals one starts at zero starts at zero and",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 692,
      "text": "then here and then here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 693,
      "text": "and then here um we can have sort of like a normalized um we can have sort of like a normalized um we can have sort of like a normalized log likelihood log likelihood um if we just normalize it by the count if we just normalize it by the count then we will sort of get the average then we will sort of get the average then we will sort of get the average log likelihood so this would be log likelihood so this would be log likelihood so this would be usually our loss function here is what usually our loss function here is what usually our loss function here is what this we would this is what we would use this we would this is what we would use this we would this is what we would use uh so our loss function for the training",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 694,
      "text": "uh so our loss function for the training uh so our loss function for the training set assigned by the model is 2.4 that's set assigned by the model is 2.4 that's set assigned by the model is 2.4 that's the quality of this model the quality of this model the quality of this model and the lower it is the better off we and the lower it is the better off we and the lower it is the better off we are and the higher it is the worse off are and the higher it is the worse off are and the higher it is the worse off",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 695,
      "text": "we are we are we are and and the job of our you know training is to the job of our you know training is to the job of our you know training is to find the parameters that minimize the find the parameters that minimize the find the parameters that minimize the negative log likelihood loss negative log likelihood loss negative log likelihood loss and that would be like a high quality and that would be like a high quality and that would be like a high quality model",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 696,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 697,
      "text": "so to summarize i actually model okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 698,
      "text": "so to summarize i actually model okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 699,
      "text": "so to summarize i actually wrote it out here wrote it out here wrote it out here so our goal is to maximize likelihood so our goal is to maximize likelihood so our goal is to maximize likelihood which is the which is the which is the product of all the probabilities product of all the probabilities product of all the probabilities assigned by the model assigned by the model assigned by the model and we want to maximize this likelihood and we want to maximize this likelihood and we want to maximize this likelihood with respect to the model parameters and with respect to the model parameters and with respect to the model parameters and in our case the model parameters here in our case the model parameters here in our case the model parameters here are defined in the table these numbers are defined in the table these numbers are defined in the table these numbers the probabilities the probabilities the probabilities are are are the model parameters sort of in our the model parameters sort of in our the model parameters sort of in our program language models so far but you program language models so far but you program language models so far but you have to keep in mind that here we are have to keep in mind that here we are have to keep in mind that here we are storing everything in a table format the storing everything in a table format the storing everything in a table format the probabilities but what's coming up as a probabilities but what's coming up as a probabilities but what's coming up as a brief preview is that these numbers will brief preview is that these numbers will brief preview is that these numbers will not be kept explicitly but these numbers not be kept explicitly but these numbers not be kept explicitly but these numbers will be calculated by a neural network will be calculated by a neural network will be calculated by a neural network so that's coming up so that's coming up so that's coming up and we want to change and tune the and we want to change and tune the and we want to change and tune the parameters of these neural networks we parameters of these neural networks we parameters of these neural networks we want to change these parameters to want to change these parameters to want to change these parameters to maximize the likelihood the product of maximize the likelihood the product of maximize the likelihood the product of the probabilities the probabilities now maximizing the likelihood is now maximizing the likelihood is now maximizing the likelihood is equivalent to maximizing the log equivalent to maximizing the log equivalent to maximizing the log likelihood because log is a monotonic likelihood because log is a monotonic likelihood because log is a monotonic function function function here's the graph of log here's the graph of log here's the graph of log and basically all it is doing is it's and basically all it is doing is it's and basically all it is doing is it's just scaling your um you can look at it just scaling your um you can look at it just scaling your um you can look at it as just a scaling of the loss function as just a scaling of the loss function as just a scaling of the loss function and so the optimization problem here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 700,
      "text": "and",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 701,
      "text": "and so the optimization problem here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 702,
      "text": "and",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 703,
      "text": "and so the optimization problem here and here are actually equivalent because here are actually equivalent because here are actually equivalent because this is just scaling you can look at it this is just scaling you can look at it this is just scaling you can look at it that way that way that way and so these are two identical and so these are two identical",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 704,
      "text": "and so these are two identical optimization problems optimization problems optimization problems um um maximizing the log-likelihood is maximizing the log-likelihood is maximizing the log-likelihood is equivalent to minimizing the negative equivalent to minimizing the negative equivalent to minimizing the negative log likelihood and then in practice log likelihood and then in practice log likelihood and then in practice people actually minimize the average people actually minimize the average people actually minimize the average negative log likelihood to get numbers negative log likelihood to get numbers negative log likelihood to get numbers like 2.4 like 2.4 like 2.4 and then this summarizes the quality of and then this summarizes the quality of and then this summarizes the quality of your model and we'd like to minimize it your model",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 705,
      "text": "and we'd like to minimize it your model",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 706,
      "text": "and we'd like to minimize it and make it as small as possible and make it as small as possible and make it as small as possible and the lowest it can get is zero and the lowest it can get is zero and the lowest it can get is zero and the lower it is and the lower it is and the lower it is the better off your model is because the better off your model is because the better off your model is because it's signing it's assigning high it's signing it's assigning high",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 707,
      "text": "it's signing it's assigning high probabilities to your data now let's probabilities to your data now let's probabilities to your data now let's estimate the probability over the entire estimate the probability over the entire estimate the probability over the entire training set just to make sure that we training set just to make sure that we training set just to make sure that we get something around 2.4 let's run this get something around 2.4 let's run this get something around 2.4 let's run this over the entire oops over the entire oops over the entire oops let's take out the print segment as well let's take out the print segment as well let's take out the print segment as well okay 2.45 or the entire training set",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 708,
      "text": "okay 2.45 or the entire training set okay 2.45 or the entire training set now what i'd like to show you is that now what i'd like to show you is that now what i'd like to show you is that you can actually evaluate the you can actually evaluate the you can actually evaluate the probability for any word that you want probability for any word that you want probability for any word that you want like for example like for example like for example if we just test a single word andre and if we just test a single word andre and if we just test a single word andre and bring back the print statement bring back the print statement bring back the print statement then you see that andre is actually kind then you see that andre is actually kind then you see that andre is actually kind of like an unlikely word like on average of like an unlikely word like on average of like an unlikely word like on average we take we take we take three three log probability to represent it and log probability to represent it and log probability to represent it and roughly that's because ej apparently is roughly that's because ej apparently is roughly that's because ej apparently is very uncommon as an example very uncommon as an example very uncommon as an example now now think through this um think through this um think through this um when i take andre and i append q",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 709,
      "text": "and i when i take andre and i append q",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 710,
      "text": "and i when i take andre and i append q",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 711,
      "text": "and i test the probability of it under q we actually get we actually get infinity infinity infinity and that's because jq has a zero percent and that's because jq has a zero percent and that's because jq has a zero percent probability according to our model",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 712,
      "text": "so probability according to our model so probability according to our model so the log likelihood the log likelihood the log likelihood so the log of zero will be negative",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 713,
      "text": "so the log of zero will be negative so the log of zero will be negative infinity we get infinite loss infinity we get infinite loss infinity we get infinite loss",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 714,
      "text": "so this is kind of undesirable right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 715,
      "text": "so this is kind of undesirable right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 716,
      "text": "so this is kind of undesirable right because we plugged in a string that because we plugged in a string that because we plugged in a string that could be like a somewhat reasonable name could be like a somewhat reasonable name could be like a somewhat reasonable name but basically what this is saying is but basically what this is saying is but basically what this is saying is that this model is exactly zero percent that this model is exactly zero percent that this model is exactly zero percent likely to uh to predict this likely to uh to predict this likely to uh to predict this name name name and our loss is infinity on this example and our loss is infinity on this example and our loss is infinity on this example and really what the reason for that is and really what the reason for that is and really what the reason for that is that j that j that j is followed by q is followed by q is followed by q uh zero times uh zero times uh zero times uh where's q jq is zero and so jq is uh uh where's q jq is zero and so jq is uh uh where's q jq is zero and so jq is uh zero percent likely zero percent likely zero percent likely",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 717,
      "text": "so it's actually kind of gross",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 718,
      "text": "and so it's actually kind of gross",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 719,
      "text": "and so it's actually kind of gross and people don't like this too much to fix people don't like this too much to fix people don't like this too much to fix this there's a very simple fix that this there's a very simple fix that this there's a very simple fix that people like to do to sort of like smooth people like to do to sort of like smooth people like to do to sort of like smooth out your model a little bit",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 720,
      "text": "and it's out your model a little bit",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 721,
      "text": "and it's out your model a little bit",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 722,
      "text": "and it's called model smoothing and roughly called model smoothing and roughly called model smoothing and roughly what's happening is that we will eight what's happening is that we will eight",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 723,
      "text": "what's happening is that we will eight we will add some fake counts we will add some fake counts we will add some fake counts so so imagine adding a count of one to imagine adding a count of one to imagine adding a count of one to everything everything",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 724,
      "text": "everything",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 725,
      "text": "so we add a count of one so we add a count of one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 726,
      "text": "so we add a count of one like this like this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 727,
      "text": "and then we recalculate the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 728,
      "text": "and then we recalculate the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 729,
      "text": "and then we recalculate the probabilities probabilities and that's model smoothing and you can and that's model smoothing and you can and that's model smoothing and you can add as much as you like you can add five add as much as you like you can add five add as much as you like you can add five and it will give you a smoother model and it will give you a smoother model and it will give you a smoother model and the more you add here and the more you add here and the more you add here the more the more the more uniform model you're going to have and uniform model you're going to have and uniform model you're going to have and the less you add the less you add the less you add the more peaked model you are going to the more peaked model you are going to the more peaked model you are going to have of course have of course have of course",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 730,
      "text": "so one is like a pretty decent count to so one is like a pretty decent count to so one is like a pretty decent count to add add add and that will ensure that there will be and that will ensure that there will be and that will ensure that there will be no zeros in our probability matrix p",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 731,
      "text": "no zeros in our probability matrix p",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 732,
      "text": "no zeros in our probability matrix p and so this will of course change the and so this will of course change the and so this will of course change the generations a little bit in this case it generations a little bit in this case it generations a little bit in this case it didn't but in principle it could didn't but in principle it could didn't but in principle it could but what that's going to do now is that but what that's going to do now is that but what that's going to do now is that nothing will be infinity unlikely nothing will be infinity unlikely nothing will be infinity unlikely",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 733,
      "text": "so now so now so now our model will predict some other our model will predict some other our model will predict some other probability and we see that jq now has a probability and we see that jq now has a probability and we see that jq now has a very small probability so the model very small probability so the model very small probability",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 734,
      "text": "so the model still finds it very surprising that this still finds it very surprising that this still finds it very surprising that this was a word or a bigram but we don't get was a word or a bigram but we don't get was a word or a bigram but we don't get negative infinity so it's kind of like a negative infinity so it's kind of like a negative infinity so it's kind of like a nice fix that people like to apply nice fix that people like to apply nice fix that people like to apply sometimes and it's called model sometimes",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 735,
      "text": "and it's called model sometimes",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 736,
      "text": "and it's called model smoothing",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 737,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 738,
      "text": "so we've now trained a smoothing",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 739,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 740,
      "text": "so we've now trained a smoothing",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 741,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 742,
      "text": "so we've now trained a respectable bi-gram character level respectable bi-gram character level respectable bi-gram character level language model and we saw that we both language model and we saw that we both language model and we saw that we both sort of trained the model by looking at sort of trained the model by looking at sort of trained the model by looking at the counts of all the bigrams and the counts of all the bigrams and the counts of all the bigrams and normalizing the rows to get probability normalizing the rows to get probability normalizing the rows to get probability distributions distributions distributions we saw that we can also then use those we saw that we can also then use those we saw that we can also then use those parameters of this model to perform parameters of this model to perform parameters of this model to perform sampling of new words sampling of new words sampling of new words so we sample new names according to so we sample new names according to so we sample new names according to those distributions and we also saw that those distributions and we also saw that those distributions and we also saw that we can evaluate the quality of this we can evaluate the quality of this we can evaluate the quality of this model and the quality of this model is model and the quality of this model is model and the quality of this model is summarized in a single number which is summarized in a single number which is summarized in a single number which is the negative log likelihood and the the negative log likelihood and the the negative log likelihood and the lower this number is the better the lower this number is the better the lower this number is the better the model is model is model is because it is giving high probabilities because it is giving high probabilities because it is giving high probabilities to the actual next characters in all the to the actual next characters in all the to the actual next characters in all the bi-grams in our training set bi-grams in our training set bi-grams in our training set",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 743,
      "text": "so that's all well and good",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 744,
      "text": "but we've so that's all well and good",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 745,
      "text": "but we've so that's all well and good",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 746,
      "text": "but we've arrived at this model explicitly by arrived at this model explicitly by arrived at this model explicitly by doing something that felt sensible we doing something that felt sensible we doing something that felt sensible we were just performing counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 747,
      "text": "and then we were just performing counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 748,
      "text": "and then we were just performing counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 749,
      "text": "and then we were normalizing those counts were normalizing those counts were normalizing those counts now what i would like to do is i would now what i would like to do is i would now what i would like to do is i would like to take an alternative approach we like to take an alternative approach we like to take an alternative approach we will end up in a very very similar will end up in a very very similar will end up in a very very similar position but the approach will look very position but the approach will look very position but the approach will look very different because i would like to cast different because i would like to cast different because i would like to cast the problem of bi-gram character level the problem of bi-gram character level the problem of bi-gram character level language modeling into the neural language modeling into the neural language modeling into the neural network framework network framework network framework in the neural network framework we're in the neural network framework we're in the neural network framework we're going to approach things slightly going to approach things slightly going to approach things slightly differently but again end up in a very differently",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 750,
      "text": "but again end up in a very differently",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 751,
      "text": "but again end up in a very similar spot i'll go into that later now similar spot i'll go into that later now similar spot i'll go into that later now our neural network is going to be a our neural network is going to be a our neural network is going to be a still a background character level still a background character level still a background character level language model",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 752,
      "text": "so it receives a single language model so it receives a single language model so it receives a single character as an input character as an input character as an input then there's neural network with some then there's neural network with some then there's neural network with some weights or some parameters w weights or some parameters w weights or some parameters w and it's going to output the probability and it's going to output the probability and it's going to output the probability distribution over the next character in distribution over the next character in distribution over the next character in a sequence it's going to make guesses as a sequence it's going to make guesses as a sequence it's going to make guesses as to what is likely to follow this to what is likely to follow this to what is likely to follow this character that was input to the model character that was input to the model character that was input to the model and then in addition to that we're going and then in addition to that we're going and then in addition to that we're going to be able to evaluate any setting of to be able to evaluate any setting of to be able to evaluate any setting of the parameters of the neural net because the parameters of the neural net because the parameters of the neural net because we have the loss function we have the loss function we have the loss function the negative log likelihood so we're the negative log likelihood so we're the negative log likelihood so we're going to take a look at its probability going to take a look at its probability going to take a look at its probability distributions and we're going to use the distributions and we're going to use the distributions and we're going to use the labels labels labels which are basically just the identity of which are basically just the identity of which are basically just the identity of the next character in that diagram the the next character in that diagram the the next character in that diagram the second character second character second character so knowing what second character so knowing what second character so knowing what second character actually comes next in the bigram allows actually comes next in the bigram allows actually comes next in the bigram allows us to then look at what how high of us to then look at what how high of us to then look at what how high of probability the model assigns to that probability the model assigns to that probability the model assigns to that character character and then we of course want the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 753,
      "text": "and then we of course want the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 754,
      "text": "and then we of course want the probability to be very high probability to be very high probability to be very high and that is another way of saying that and that is another way of saying that and that is another way of saying that the loss is low the loss is low the loss is low",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 755,
      "text": "so we're going to use gradient-based so we're going to use gradient-based so we're going to use gradient-based optimization then to tune the parameters optimization then to tune the parameters optimization then to tune the parameters of this network because we have the loss of this network because we have the loss of this network because we have the loss function and we're going to minimize it function and we're going to minimize it function and we're going to minimize it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 756,
      "text": "so we're going to tune the weights",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 757,
      "text": "so so we're going to tune the weights",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 758,
      "text": "so so we're going to tune the weights so that the neural net is correctly that the neural net is correctly that the neural net is correctly predicting the probabilities for the predicting the probabilities for the predicting the probabilities for the next character next character next character so let's get started the first thing i so let's get started the first thing i so let's get started the first thing i want to do is i want to compile the want to do is i want to compile the want to do is i want to compile the training set of this neural network training set of this neural network training set of this neural network",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 759,
      "text": "right so right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 760,
      "text": "so right so create create create the training set the training set of all the bigrams of all the bigrams of all the bigrams",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 761,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 762,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 763,
      "text": "and and here here i'm going to copy paste this code i'm going to copy paste this code i'm going to copy paste this code because this code iterates over all the because this code iterates over all the because this code iterates over all the programs programs programs so here we start with the words we so here we start with the words we so here we start with the words we iterate over all the bygrams and iterate over all the bygrams and iterate over all the bygrams and previously as you recall we did the previously as you recall we did the previously as you recall we did the counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 764,
      "text": "but now we're not going to do counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 765,
      "text": "but now we're not going to do counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 766,
      "text": "but now we're not going to do counts we're just creating a training counts we're just creating a training counts we're just creating a training set set set now this training set will be made up of now this training set will be made up of now this training set will be made up of two lists we have the we have the inputs inputs inputs and the targets and the targets and the targets the the labels the the labels the the labels and these bi-grams will denote x",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 767,
      "text": "y those and these bi-grams will denote x y those and these bi-grams will denote x y those are the characters right are the characters right are the characters",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 768,
      "text": "right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 769,
      "text": "and so we're given the first character and so we're given the first character and so we're given the first character of the bi-gram and then we're trying to of the bi-gram and then we're trying to of the bi-gram and then we're trying to predict the next one predict the next one predict the next one both of these are going to be integers both of these are going to be integers both of these are going to be integers so here we'll take x's that append is so here we'll take x's that append is so here we'll take x's that append is just just just x1 ystat append ix2 x1 ystat append ix2 x1 ystat append ix2",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 770,
      "text": "and then here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 771,
      "text": "and then here we actually don't want lists of integers we actually don't want lists of integers we actually don't want lists of integers we will create tensors out of these so we will create tensors out of these so we will create tensors out of these so axis is torch.tensor of axis and wise a axis is torch.tensor of axis and wise a axis is torch.tensor of axis and wise a storage.tensor of ys storage.tensor of ys storage.tensor of ys and then",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 772,
      "text": "and then we don't actually want to take all the we don't actually want to take all the we don't actually want to take all the words just yet because i want everything words just yet because i want everything words just yet because i want everything to be manageable to be manageable to be manageable",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 773,
      "text": "so let's just do the first word which is so let's just do the first word which is so let's just do the first word which is emma emma",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 774,
      "text": "and then it's clear what these x's",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 775,
      "text": "and and then it's clear what these x's",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 776,
      "text": "and and then it's clear what these x's and y's would be y's would be y's would be here let me print here let me print here let me print character 1 character 2 just so you see character 1 character 2 just so you see character 1 character 2 just so you see what's going on here what's going on here what's going on here so the bigrams of these characters is so the bigrams of these characters is so the bigrams of these characters is dot e e m m m a a dot so this single dot e e m m m a a dot so this single dot e e m m m a a dot so this single word as i mentioned has one two three word as i mentioned has one two three word as i mentioned has one two three four five examples for our neural four five examples for our neural four five examples for our neural network network network there are five separate examples in emma there are five separate examples in emma there are five separate examples in emma and those examples are summarized here and those examples are summarized here and those examples are summarized here when the input to the neural network is when the input to the neural network is when the input to the neural network is integer 0",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 777,
      "text": "integer 0 integer 0",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 778,
      "text": "the desired label is integer 5 which the desired label is integer 5 which the desired label is integer 5 which corresponds to e when the input to the corresponds to e when the input to the corresponds to e when the input to the neural network is 5 we want its weights neural network is 5 we want its weights neural network is 5 we want its weights to be arranged so that 13 gets a very to be arranged so that 13 gets a very to be arranged so that 13 gets a very high probability high probability high probability when 13 is put in we want 13 to have a when 13 is put in we want 13 to have a when 13 is put in we want 13 to have a high probability high probability when 13 is put in we also want 1 to have when 13 is put in we also want 1 to have when 13 is put in we also want 1 to have a high probability a high probability a high probability when one is input we want zero to have a when one is input we want zero to have a when one is input we want zero to have a very high probability so there are five very high probability so there are five very high probability so there are five separate input examples to a neural nut separate input examples to a neural nut separate input examples to a neural nut in this data set i wanted to add a tangent of a node of i wanted to add a tangent of a node of caution to be careful with a lot of the caution to be careful with a lot of the caution to be careful with a lot of the apis of some of these frameworks apis of some of these frameworks apis of some of these frameworks you saw me silently use torch.tensor you saw me silently use torch.tensor you saw me silently use torch.tensor with a lowercase t with a lowercase t with a lowercase t and the output looked right and the output looked right and the output looked right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 779,
      "text": "but you should be aware that there's but you should be aware that there's",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 780,
      "text": "but you should be aware that there's actually two ways of constructing a actually two ways of constructing a actually two ways of constructing a tensor there's a torch.lowercase tensor tensor there's a torch.lowercase tensor tensor there's a torch.lowercase tensor and there's also a torch.capital tensor and there's also a torch.capital tensor and there's also a torch.capital tensor class which you can also construct class which you can also construct class which you can also construct so you can actually call both you can so you can actually call both you can so you can actually call both you can also do torch.capital tensor also do torch.capital tensor also do torch.capital tensor and you get a nexus and wise as well and you get a nexus and wise as well and you get a nexus and wise as well",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 781,
      "text": "so that's not confusing at all so that's not confusing at all so that's not confusing at all um um there are threads on what is the there are threads on what is the there are threads on what is the difference between these two difference between these two difference between these two and um and um and um unfortunately the docs are just like not unfortunately the docs are just like not unfortunately the docs are just like not clear on the difference and when you clear on the difference and when you clear on the difference and when you look at the the docs of lower case look at the the docs of lower case look at the the docs of lower case tensor construct tensor with no autograd tensor construct tensor with no autograd tensor construct tensor with no autograd history by copying data history by copying data history by copying data it's just like it doesn't it's just like it doesn't it's just like it doesn't it doesn't make sense",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 782,
      "text": "so the actual it doesn't make sense so the actual it doesn't make sense so the actual difference as far as i can tell is difference as far as i can tell is difference as far as i can tell is explained eventually in this random explained eventually in this random explained eventually in this random thread that you can google thread that you can google thread that you can google and really it comes down to and really it comes down to and really it comes down to i believe i believe i believe that um that um that um what is this what is this what is this torch.tensor in first d-type the data torch.tensor in first d-type the data torch.tensor in first d-type the data type automatically while torch.tensor type automatically while torch.tensor type automatically while torch.tensor just returns a float tensor just returns a float tensor just returns a float tensor i would recommend stick to i would recommend stick to i would recommend stick to torch.lowercase tensor torch.lowercase tensor torch.lowercase tensor",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 783,
      "text": "so um so um so um indeed we see that when i indeed we see that when i indeed we see that when i construct this with a capital t the data construct this with a capital t the data construct this with a capital t the data type here of xs is float32 type here of xs is float32 type here of xs is float32 but towards that lowercase tensor but towards that lowercase tensor but towards that lowercase tensor you see how it's now x dot d type is now you see how it's now x dot d type is now you see how it's now x dot d type is now integer integer integer",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 784,
      "text": "so um so um it's advised that you use lowercase t",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 785,
      "text": "it's advised that you use lowercase t",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 786,
      "text": "it's advised that you use lowercase t and you can read more about it if you and you can read more about it if you and you can read more about it if you like in some of these threads but like in some of these threads but like in some of these threads but basically basically um um i'm pointing out some of these things i'm pointing out some of these things i'm pointing out some of these things because i want to caution you and i want because i want to caution you and i want because i want to caution you",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 787,
      "text": "and i want you to re get used to reading a lot of you to re get used to reading a lot of you to re get used to reading a lot of documentation and reading through a lot documentation and reading through a lot documentation and reading through a lot of of of q and a's and threads like this q and a's and threads like this q and a's and threads like this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 788,
      "text": "and and you know some of the stuff is you know some of the stuff is you know some of the stuff is unfortunately not easy and not very well unfortunately not easy and not very well unfortunately not easy and not very well documented and you have to be careful documented and you have to be careful documented and you have to be careful out there what we want here is integers out there what we want here is integers out there what we want here is integers because that's what makes uh sense because that's what makes uh sense because that's what makes uh sense um um and so and so lowercase tensor is what we are using lowercase tensor is what we are using lowercase tensor is what we are using",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 789,
      "text": "okay now we want to think through how okay now we want to think through how okay now we want to think through how we're going to feed in these examples we're going to feed in these examples we're going to feed in these examples into a neural network into a neural network into a neural network now it's not quite as straightforward as now it's not quite as straightforward as now it's not quite as straightforward as plugging it in because these examples plugging it in because these examples plugging it in because these examples right now are integers so there's like a right now are integers so there's like a right now are integers so there's like a 0 5 or 13 it gives us the index of the 0 5 or 13 it gives us the index of the 0 5 or 13 it gives us the index of the character and you can't just plug an character and you can't just plug an character and you can't just plug an integer index into a neural net integer index into a neural net integer index into a neural net these neural nets right are sort of made these neural nets right are sort of made these neural nets right are sort of made up of these neurons up of these neurons up of these neurons and and these neurons have weights and as you these neurons have weights and as you these neurons have weights and as you saw in micrograd these weights act saw in micrograd these weights act saw in micrograd these weights act multiplicatively on the inputs w x plus multiplicatively on the inputs w x plus multiplicatively on the inputs w x plus b there's 10 h's and so on and so it b there's 10 h's and so on",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 790,
      "text": "and so it b there's 10 h's and so on and so it doesn't really make sense to make an doesn't really make sense to make an doesn't really make sense to make an input neuron take on integer values that input neuron take on integer values that input neuron take on integer values that you feed in and then multiply on with you feed in and then multiply on with you feed in and then multiply on with weights weights weights so instead so instead so instead a common way of encoding integers is a common way of encoding integers is a common way of encoding integers is what's called one hot encoding what's called one hot encoding what's called one hot encoding in one hot encoding in one hot encoding in one hot encoding we take an integer like 13 and we create we take an integer like 13 and we create we take an integer like 13 and we create a vector that is all zeros except for a vector that is all zeros except for a vector that is all zeros except for the 13th dimension which we turn to a the 13th dimension which we turn to a the 13th dimension which we turn to a one and then that vector can feed into a one and then that vector can feed into a one and then that vector can feed into a neural net neural net neural net now conveniently now conveniently now conveniently uh pi torch actually has something uh pi torch actually has something uh pi torch actually has something called the one hot function inside torching and functional function inside torching and functional it takes a tensor made up of integers it takes a tensor made up of integers it takes a tensor made up of integers um um long is a is a as an integer long is a is a as an integer long is a is a as an integer um um and it also takes a number of classes",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 791,
      "text": "um and it also takes a number of classes um and it also takes a number of classes um which is how large you want your uh which is how large you want your uh which is how large you want your uh tensor uh your vector to be tensor uh your vector to be tensor uh your vector to be so here let's import so here let's import so here let's import torch.n.functional",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 792,
      "text": "sf this is a common torch.n.functional sf this is a common torch.n.functional sf this is a common way of importing it way of importing it way of importing it and then let's do f.1 hot",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 793,
      "text": "and then let's do f.1 hot",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 794,
      "text": "and then let's do f.1 hot",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 795,
      "text": "and we feed in the integers that we want and we feed in the integers that we want and we feed in the integers that we want to encode so we can actually feed in the to encode so we can actually feed in the to encode so we can actually feed in the entire array of x's entire array of x's entire array of x's and we can tell it that num classes is and we can tell it that num classes is and we can tell it that num classes is 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 796,
      "text": "27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 797,
      "text": "27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 798,
      "text": "so it doesn't have to try to guess it it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 799,
      "text": "so it doesn't have to try to guess it it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 800,
      "text": "so it doesn't have to try to guess it it may have guessed that it's only 13 and may have guessed that it's only 13 and may have guessed that it's only 13 and would give us an incorrect result would give us an incorrect result would give us an incorrect result so this is the one hot let's call this x",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 801,
      "text": "so this is the one hot let's call this x",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 802,
      "text": "so this is the one hot let's call this x inc for x encoded and then we see that x encoded that",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 803,
      "text": "and then we see that x encoded that shape is 5 by 27 shape is 5 by 27 shape is 5 by 27 and uh we can also visualize it plt.i am and uh we can also visualize it plt.i am and uh we can also visualize it plt.i am show of x inc show of x inc show of x inc to make it a little bit more clear to make it a little bit more clear to make it a little bit more clear because this is a little messy because this is a little messy because this is a little messy so we see that we've encoded all the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 804,
      "text": "so we see that we've encoded all the so we see that we've encoded all the five examples uh into vectors we have five examples uh into vectors we have five examples uh into vectors we have five examples so we have five rows and five examples",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 805,
      "text": "so we have five rows and five examples so we have five rows and each row here is now an example into a each row here is now an example into a each row here is now an example into a neural nut neural",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 806,
      "text": "nut neural nut",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 807,
      "text": "and we see that the appropriate bit is and we see that the appropriate bit is",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 808,
      "text": "and we see that the appropriate bit is turned on as a one and everything else turned on as a one and everything else turned on as a one and everything else is zero is zero is zero",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 809,
      "text": "so um so um here for example the zeroth bit is here for example the zeroth bit is here for example the zeroth bit is turned on the fifth bit is turned on turned on the fifth bit is turned on turned on the fifth bit is turned on 13th bits are turned on for both of 13th bits are turned on for both of 13th bits are turned on for both of these examples",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 810,
      "text": "and then the first bit these examples and then the first bit these examples",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 811,
      "text": "and then the first bit here is turned on here is turned on here is turned on so that's how we can encode so that's how we can encode so that's how we can encode integers into vectors and then these integers into vectors and then these integers into vectors and then these vectors can feed in to neural nets one vectors can feed in to neural nets one vectors can feed in to neural nets one more issue to be careful with here by more issue to be careful with here by more issue to be careful with here by the way is the way is the way is let's look at the data type of encoding let's look at the data type of encoding let's look at the data type of encoding we always want to be careful with data we always want to be careful with data we always want to be careful with data types types types what would you expect",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 812,
      "text": "x encoding's data what would you expect x encoding's data what would you expect x encoding's data type to be type to be type to be when we're plugging numbers into neural when we're plugging numbers into neural when we're plugging numbers into neural nuts we don't want them to be integers nuts we don't want them to be integers nuts we don't want them to be integers we want them to be floating point we want them to be floating point we want them to be floating point numbers that can take on various values numbers that can take on various values numbers that can take on various values but the d type here is actually 64-bit but the d type here is actually 64-bit but the d type here is actually 64-bit integer integer and the reason for that i suspect is and the reason for that i suspect is and the reason for that i suspect is that one hot received a 64-bit integer that one hot received a 64-bit integer that one hot received a 64-bit integer here and it returned the same data type here and it returned the same data type here and it returned the same data type and when you look at the signature of and when you look at the signature of and when you look at the signature of one hot it doesn't even take a d type a one hot it doesn't even take a d type a one hot it doesn't even take a d type a desired data type of the output tensor desired data type of the output tensor desired data type of the output tensor",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 813,
      "text": "and so we can't in a lot of functions in",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 814,
      "text": "and so we can't in a lot of functions in",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 815,
      "text": "and so we can't in a lot of functions in torch we'd be able to do something like torch we'd be able to do something like torch we'd be able to do something like d type equal storage.float32 d type equal storage.float32 d type equal storage.float32 which is what we want but one heart does which is what we want but one heart does which is what we want but one heart does not support that not support that not support that",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 816,
      "text": "so instead we're going to want to cast so instead we're going to want to cast so instead we're going to want to cast this to float like this this to float like this this to float like this so that these so that these so that these everything is the same everything is the same everything is the same everything looks the same but the d-type everything looks the same but the d-type everything looks the same but the d-type is float32 and floats can feed into is float32 and floats can feed into is float32 and floats can feed into neural nets so now let's construct our neural nets so now let's construct our neural nets so now let's construct our first neuron first neuron first neuron this neuron will look at these input this neuron will look at these input this neuron will look at these input vectors vectors vectors and as you remember from micrograd these",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 817,
      "text": "and as you remember from micrograd these and as you remember from micrograd these neurons basically perform a very simple neurons basically perform a very simple neurons basically perform a very simple function w x plus b where w x is a dot function w x plus b where w x is a dot function w x plus b where w x is a dot product product product",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 818,
      "text": "right right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 819,
      "text": "so we can achieve the same thing here so we can achieve the same thing here so we can achieve the same thing here let's first define the weights of this let's first define the weights of this let's first define the weights of this neuron basically what are the initial neuron basically what are the initial neuron basically what are the initial weights at initialization for this weights at initialization for this weights at initialization for this neuron neuron neuron",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 820,
      "text": "let's initialize them with torch.rendin let's initialize them with torch.rendin let's initialize them with torch.rendin torch.rendin torch.rendin torch.rendin is um is um is um fills a tensor with random numbers fills a tensor with random numbers fills a tensor with random numbers drawn from a normal distribution drawn from a normal distribution drawn from a normal distribution and a normal distribution and a normal distribution and a normal distribution has has has a probability density function like this a probability density function like this a probability density function like this and so most of the numbers drawn from and so most of the numbers drawn from and so most of the numbers drawn from this distribution will be around 0",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 821,
      "text": "this distribution will be around 0",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 822,
      "text": "this distribution will be around 0",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 823,
      "text": "but some of them will be as high as but some of them will be as high as but some of them will be as high as almost three and so on and very few almost three and so on and very few almost three and so on and very few numbers will be above three in magnitude numbers will be above three in magnitude numbers will be above three in magnitude so we need to take a size as an input",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 824,
      "text": "so we need to take a size as an input",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 825,
      "text": "so we need to take a size as an input here here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 826,
      "text": "and i'm going to use size as to be 27 by and i'm going to use size as to be 27 by and i'm going to use size as to be 27 by one one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 827,
      "text": "so so 27 by one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 828,
      "text": "and then let's visualize w so 27 by one and then let's visualize w so 27 by one and then let's visualize w so w is a column vector of 27 numbers w is a column vector of 27 numbers w is a column vector of 27 numbers and and these weights are then multiplied by the these weights are then multiplied by the these weights are then multiplied by the inputs inputs so now to perform this multiplication we so now to perform this multiplication we so now to perform this multiplication we can take x encoding",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 829,
      "text": "and we can multiply can take x encoding",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 830,
      "text": "and we can multiply can take x encoding and we can multiply it with w it with w it with w this is a matrix multiplication operator this is a matrix multiplication operator this is a matrix multiplication operator in pi torch in pi torch in pi torch and the output of this operation is five and the output of this operation is five and the output of this operation is five by one by one by one the reason is five by five is the the reason is five by five is the the reason is five by five is the following following we took x encoding which is five by we took x encoding which is five by we took x encoding which is five by twenty seven and we multiplied it by twenty seven and we multiplied it by twenty seven and we multiplied it by twenty seven by one twenty seven by one twenty seven by one and and in matrix multiplication in matrix multiplication in matrix multiplication you see that the output will become five you see that the output will become five you see that the output will become five by one because these 27 by one because these 27 by one because these 27 will multiply and add will multiply and add will multiply and add so basically what we're seeing here outs so basically what we're seeing here outs so basically what we're seeing here outs out of this operation out of this operation out of this operation is we are seeing the five is we are seeing the five is we are seeing the five activations activations activations of this neuron of this neuron of this neuron on these five inputs on these five inputs on these five inputs and we've evaluated all of them in and we've evaluated all of them in and we've evaluated all of them in parallel we didn't feed in just a single parallel we didn't feed in just a single parallel we didn't feed in just a single input to the single neuron we fed in input to the single neuron we fed in input to the single neuron we fed in simultaneously all the five inputs into simultaneously all the five inputs into simultaneously all the five inputs into the same neuron the same neuron the same neuron and in parallel patrol has evaluated and in parallel patrol has evaluated and in parallel patrol has evaluated the wx plus b",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 831,
      "text": "but here is just the wx the wx plus b",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 832,
      "text": "but here is just the wx the wx plus b",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 833,
      "text": "but here is just the wx there's no bias there's no bias there's no bias it has value w times x for all of them it has value w times x for all of them",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 834,
      "text": "it has value w times x for all of them independently now instead of a single independently now instead of a single independently now instead of a single neuron though i would like to have 27 neuron though i would like to have 27 neuron though i would like to have 27 neurons and i'll show you in a second neurons and i'll show you in a second neurons and i'll show you in a second why i want 27 neurons why i want 27 neurons why i want 27 neurons so instead of having just a 1 here which so instead of having just a 1 here which so instead of having just a 1 here which is indicating this presence of one is indicating this presence of one is indicating this presence of one single neuron single neuron single neuron we can use 27 we can use 27 we can use 27 and then when w is 27 by 27 and then when w is 27 by 27 and then when w is 27 by 27 this will in parallel evaluate all the this will in parallel evaluate all the this will in parallel evaluate all the 27 neurons on all the 5 inputs 27 neurons on all the 5 inputs 27 neurons on all the 5 inputs giving us a much better much much bigger giving us a much better much much bigger giving us a much better much much bigger result so now what we've done is 5 by 27 result so now what we've done is 5 by 27 result so now what we've done is 5 by 27 multiplied 27 by 27 multiplied 27 by 27 multiplied 27 by 27 and the output of this is now 5 by 27 and the output of this is now 5 by 27 and the output of this is now 5 by 27",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 835,
      "text": "so we can see that the shape of this is 5 by 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 836,
      "text": "is 5 by 27.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 837,
      "text": "so what is every element here telling us so what is every element here telling us so what is every element here telling us",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 838,
      "text": "right right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 839,
      "text": "it's telling us for every one of 27 it's telling us for every one of 27 it's telling us for every one of 27 neurons that we created what is the firing rate of those neurons what is the firing rate of those neurons on every one of those five examples on every one of those five examples on every one of those five examples",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 840,
      "text": "so so",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 841,
      "text": "the element for example 3 comma 13 the element for example 3 comma 13 the element for example 3 comma 13 is giving us the firing rate of the 13th is giving us the firing rate of the 13th is giving us the firing rate of the 13th neuron neuron looking at the third input looking at the third input looking at the third input and the way this was achieved is by a and the way this was achieved is by a and the way this was achieved is by a dot product dot product dot product between the third between the third between the third input input input and the 13th column and the 13th column and the 13th column of this w matrix here of this w matrix here of this w matrix here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 842,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 843,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 844,
      "text": "so so using matrix multiplication we can very using matrix multiplication we can very using matrix multiplication we can very efficiently evaluate efficiently evaluate efficiently evaluate the dot product between lots of input the dot product between lots of input the dot product between lots of input examples in a batch examples in a batch examples in a batch and lots of neurons where all those and lots of neurons where all those and lots of neurons where all those neurons have weights in the columns of neurons have weights in the columns of neurons have weights in the columns of those w's those w's those w's and in matrix multiplication we're just and in matrix multiplication we're just and in matrix multiplication we're just doing those dot products and doing those dot products and doing those dot products and in parallel just to show you that this in parallel just to show you that this in parallel just to show you that this is the case we can take x",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 845,
      "text": "and we can is the case we can take x",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 846,
      "text": "and we can is the case we can take x",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 847,
      "text": "and we can take the third take the third take the third row row row and we can take the w and take its 13th and we can take the w and take its 13th and we can take the w and take its 13th column",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 848,
      "text": "and then we can do",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 849,
      "text": "and then we can do x and get three x and get three x and get three elementwise multiply with w at 13.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 850,
      "text": "elementwise multiply with w at 13.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 851,
      "text": "elementwise multiply with w at 13.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 852,
      "text": "and sum that up that's wx plus b and sum that up that's wx plus b and sum that up that's wx plus",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 853,
      "text": "b well there's no plus",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 854,
      "text": "b it's just wx dot well there's",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 855,
      "text": "no plus b it's just wx dot",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 856,
      "text": "well there's no plus",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 857,
      "text": "b it's just wx dot product product and that's",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 858,
      "text": "and that's",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 859,
      "text": "and that's this number this number this number",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 860,
      "text": "so you see that this is just being done so you see that this is just being done so you see that this is just being done efficiently by the matrix multiplication efficiently by the matrix multiplication efficiently by the matrix multiplication operation operation operation for all the input examples and for all for all the input examples and for all for all the input examples and for all the output neurons of this first layer the output neurons of this first layer the output neurons of this first layer",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 861,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 862,
      "text": "so we fed our 27-dimensional inputs",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 863,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 864,
      "text": "so we fed our 27-dimensional inputs",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 865,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 866,
      "text": "so we fed our 27-dimensional inputs into a first layer of a neural net that into a first layer of a neural net that into a first layer of a neural net that has 27 neurons right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 867,
      "text": "so we have 27 has 27 neurons right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 868,
      "text": "so we have 27 has 27 neurons right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 869,
      "text": "so we have 27 inputs and now we have 27 neurons these inputs and now we have 27 neurons these inputs and now we have 27 neurons these neurons perform w times",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 870,
      "text": "x they don't neurons perform w times x they don't neurons perform w times x they don't have a bias and they don't have a have a bias and they don't have a have a bias and they don't have a non-linearity like 10 h we're going to non-linearity like 10 h we're going to non-linearity like 10 h we're going to leave them to be a linear layer leave them to be a linear layer leave them to be a linear layer in addition to that we're not going to in addition to that we're not going to in addition to that we're not going to have any other layers this is going to have any other layers this is going to have any other layers this is going to be it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 871,
      "text": "it's just going to be be it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 872,
      "text": "it's just going to be be it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 873,
      "text": "it's just going to be the dumbest smallest simplest neural net the dumbest smallest simplest neural net the dumbest smallest simplest neural net which is just a single linear layer which is just a single linear layer which is just a single linear layer",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 874,
      "text": "and now i'd like to explain what i want",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 875,
      "text": "and now i'd like to explain what i want and now i'd like to explain what i want those 27 outputs to be those 27 outputs to be those 27 outputs to be intuitively what we're trying to produce intuitively what we're trying to produce intuitively what we're trying to produce here for every single input example is here for every single input example is here for every single input example is we're trying to produce some kind of a we're trying to produce some kind of a we're trying to produce some kind of a probability distribution for the next probability distribution for the next probability distribution for the next character in a sequence character in a sequence character in a sequence and there's 27 of them and there's 27 of them and there's 27 of them",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 876,
      "text": "but we have to come up with like precise",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 877,
      "text": "but we have to come up with like precise",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 878,
      "text": "but we have to come up with like precise semantics for exactly how we're going to semantics for exactly how we're going to semantics for exactly how we're going to interpret these 27 numbers that these interpret these 27 numbers that these interpret these 27 numbers that these neurons take on neurons take on neurons take on now intuitively now intuitively now intuitively you see here that these numbers are you see here that these numbers are you see here that these numbers are negative and some of them are positive negative and some of them are positive negative and some of them are positive etc etc etc",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 879,
      "text": "and that's because these are coming out and that's because these are coming out and that's because these are coming out of a neural net layer initialized with of a neural net layer initialized with of a neural net layer initialized with these normal distribution normal distribution parameters parameters parameters but what we want is we want something but what we want is we want something",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 880,
      "text": "but what we want is we want something like we had here like we had here like we had here like each row here like each row here like each row here told us the counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 881,
      "text": "and then we told us the counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 882,
      "text": "and then we told us the counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 883,
      "text": "and then we normalized the counts to get normalized the counts to get normalized the counts to get probabilities and we want something probabilities and we want something probabilities and we want something similar to come out of the neural net similar to come out of the neural net similar to come out of the neural net but what we just have right now is just but what we just have right now is just but what we just have right now is just some negative and positive numbers some negative and positive numbers some negative and positive numbers now we want those numbers to somehow now we want those numbers to somehow now we want those numbers to somehow represent the probabilities for the next represent the probabilities for the next represent the probabilities for the next character character but you see that probabilities",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 884,
      "text": "they they",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 885,
      "text": "but you see that probabilities",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 886,
      "text": "they they",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 887,
      "text": "but you see that probabilities they they have a special structure they um have a special structure they um have a special structure they um they're positive numbers and they sum to they're positive numbers and they sum to they're positive numbers and they sum to one one and so that doesn't just come out of a",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 888,
      "text": "and so that doesn't just come out of a and so that doesn't just come out of a neural net neural net",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 889,
      "text": "and then they can't be counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 890,
      "text": "and then they can't be counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 891,
      "text": "and then they can't be counts because these counts are positive and because these counts are positive and because these counts are positive and counts are integers counts are integers counts are integers so counts are also not really a good",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 892,
      "text": "so counts are also not really a good",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 893,
      "text": "so counts are also not really a good thing to output from a neural net thing to output from a neural net thing to output from a neural net so instead what the neural net is going so instead what the neural net is going so instead what the neural net is going to output and how we are going to to output and how we are going to to output and how we are going to interpret the um interpret the um interpret the um the 27 numbers is that these 27 numbers the 27 numbers is that these 27 numbers the 27 numbers is that these 27 numbers are giving us log counts are giving us log counts are giving us log counts basically um so instead of giving us counts directly so instead of giving us counts directly so instead of giving us counts directly like in this table they're giving us log like in this table they're giving us log like in this table they're giving us log counts counts and to get the counts we're going to and to get the counts we're going to and to get the counts we're going to take the log counts and we're going to take the log counts and we're going to take the log counts and we're going to exponentiate them exponentiate them exponentiate them now now exponentiation exponentiation exponentiation takes the following form takes the following form takes the following form it takes numbers it takes numbers it takes numbers that are negative or they are positive that are negative or they are positive that are negative or they are positive it takes the entire real line it takes the entire real line it takes the entire real line and then if you plug in negative numbers and then if you plug in negative numbers and then if you plug in negative numbers you're going to get e to the x you're going to get e to the x you're going to get e to the x which is uh always below one which is uh always below one which is uh always below one so you're getting numbers lower than one so you're getting numbers lower than one so you're getting numbers lower than one and if you plug in numbers greater than and if you plug in numbers greater than and if you plug in numbers greater than zero you're getting numbers greater than zero you're getting numbers greater than zero you're getting numbers greater than one all the way growing to the infinity one all the way growing to the infinity one all the way growing to the infinity and this here grows to zero and this here grows to zero and this here grows to zero so basically we're going to so basically we're going to so basically we're going to take these numbers take these numbers take these numbers here and instead of them being positive and instead of them being positive and instead of them being positive and negative and all over the place we're negative and all over the place we're negative and all over the place we're going to interpret them as log counts going to interpret them as log counts going to interpret them as log counts and then we're going to element wise",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 894,
      "text": "and then we're going to element wise",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 895,
      "text": "and then we're going to element wise exponentiate these numbers exponentiate these numbers exponentiate these numbers exponentiating them now gives us exponentiating them now gives us exponentiating them now gives us something like this something like this something like this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 896,
      "text": "and you see that these numbers now",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 897,
      "text": "and you see that these numbers now",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 898,
      "text": "and you see that these numbers now because they went through an exponent because they went through an exponent because they went through an exponent all the negative numbers turned into all the negative numbers turned into all the negative numbers turned into numbers below 1 like 0.338 and all the numbers below 1 like 0.338 and all the numbers below 1 like 0.338 and all the positive numbers originally turned into positive numbers originally turned into positive numbers originally turned into even more positive numbers sort of even more positive numbers sort of even more positive numbers sort of greater than one greater than one greater than one so like for example",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 899,
      "text": "so like for example",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 900,
      "text": "so like for example seven seven seven is some positive number over here is some positive number over here is some positive number over here that is greater than zero that is greater than zero that is greater than zero but exponentiated outputs here but exponentiated outputs here but exponentiated outputs here basically give us something that we can basically give us something that we can basically give us something that we can use and interpret as the equivalent of use and interpret as the equivalent of use and interpret as the equivalent of counts originally so you see these counts originally",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 901,
      "text": "so you see these counts originally",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 902,
      "text": "so you see these counts here 112 7 51 1 etc counts here 112 7 51 1 etc counts here 112 7 51 1 etc the neural net is kind of now predicting the neural net is kind of now predicting the neural net is kind of now predicting uh uh uh counts counts and these counts are positive numbers and these counts are positive numbers and these counts are positive numbers they can never be below zero so that they can never be below zero so that they can never be below zero so that makes sense makes sense makes sense and uh they can now take on various",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 903,
      "text": "and uh they can now take on various",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 904,
      "text": "and uh they can now take on various values values values depending on the settings of w depending on the settings of w depending on the settings of w so let me break this down so let me break this down so let me break this down we're going to interpret these to be the we're going to interpret these to be the we're going to interpret these to be the log counts in other words for this that is often in other words for this that is often used is so-called logits used is so-called logits used is so-called logits these are logits log counts these are logits log counts these are logits log counts then these will be sort of the counts then these will be sort of the counts then these will be sort of the counts largest exponentiated largest exponentiated largest exponentiated and this is equivalent to the n matrix and this is equivalent to the n matrix and this is equivalent to the n matrix sort of the n sort of the n sort of the n array that we used previously remember array that we used previously remember array that we used previously remember this was the n this was the n this was the n this is the the array of counts this is the the array of counts this is the the array of counts and each row here are the counts for the and each row here are the counts for the and each row here are the counts for the for the um for the um for the um next character sort of so those are the counts and now the so those are the counts and now the probabilities are just the counts um probabilities are just the counts um probabilities are just the counts um normalized normalized and so um and so um and so um i'm not going to find the same",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 905,
      "text": "but i'm not going to find the same",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 906,
      "text": "but i'm not going to find the same",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 907,
      "text": "but basically i'm not going to scroll all basically i'm not going to scroll all basically i'm not going to scroll all over the place over the place over the place we've already done this we want to we've already done this we want to we've already done this we want to counts that sum counts that sum counts that sum along the first dimension",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 908,
      "text": "and we want to along the first dimension and we want to along the first dimension",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 909,
      "text": "and we want to keep them as true keep them as true keep them as true we've went over this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 910,
      "text": "and this is how we we've went over this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 911,
      "text": "and this is how we we've went over this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 912,
      "text": "and this is how we normalize the rows of our counts matrix normalize the rows of our counts matrix normalize the rows of our counts matrix to get our probabilities to get our probabilities to get our probabilities props props props so now these are the probabilities so now these are the probabilities so now these are the probabilities and and these are the counts that we ask these are the counts that we ask these are the counts that we ask currently and now when i show the currently and now when i show the currently and now when i show the probabilities probabilities you see that um you see that um you see that um every row here every row here every row here of course of course of course will sum to 1 will sum to 1 will sum to 1 because they're normalized because they're normalized because they're normalized and the shape of this and the shape of this and the shape of this is 5 by 27 is 5 by 27 is 5 by 27 and so really what we've achieved is for",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 913,
      "text": "and so really what we've achieved is for",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 914,
      "text": "and so really what we've achieved is for every one of our five examples every one of our five examples every one of our five examples we now have a row that came out of a we now have a row that came out of a we now have a row that came out of a neural net neural net and because of the transformations here and because of the transformations here and because of the transformations here we made sure that this output of this we made sure that this output of this we made sure that this output of this neural net now are probabilities or we neural net now are probabilities or we neural net now are probabilities or we can interpret to be probabilities can interpret to be probabilities can interpret to be probabilities",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 915,
      "text": "so so our wx here gave us logits our wx here gave us logits our wx here gave us logits",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 916,
      "text": "and then we interpret those to be log",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 917,
      "text": "and then we interpret those to be log",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 918,
      "text": "and then we interpret those to be log counts counts we exponentiate to get something that we exponentiate to get something that we exponentiate to get something that looks like counts looks like counts looks like counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 919,
      "text": "and then we normalize those counts to and then we normalize those counts to and then we normalize those counts to get a probability distribution get a probability distribution get a probability distribution and all of these are differentiable and all of these are differentiable and all of these are differentiable operations operations operations so what we've done now is we're taking so what we've done now is we're taking so what we've done now is we're taking inputs we have differentiable operations inputs we have differentiable operations inputs we have differentiable operations that we can back propagate through that we can back propagate through that we can back propagate through and we're getting out probability and we're getting out probability and we're getting out probability distributions distributions",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 920,
      "text": "so so for example for the zeroth example that for example for the zeroth example that for example for the zeroth example that fed in fed in fed in right which was um right which was um right which was um the zeroth example here was a one-half the zeroth example here was a one-half the zeroth example here was a one-half vector of zero vector of zero vector of zero and um and um it basically corresponded to feeding in it basically corresponded to feeding in it basically corresponded to feeding in this example here so we're feeding in a this example here so we're feeding in a this example here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 921,
      "text": "so we're feeding in a dot into a neural net and the way we fed dot into a neural net and the way we fed dot into a neural net and the way we fed the dot into a neural net is that we the dot into a neural net is that we the dot into a neural net is that we first got its index first got its index first got its index then we one hot encoded it then we one hot encoded it then we one hot encoded it then it went into the neural net and out then it went into the neural net and out then it went into the neural net and out came came came this distribution of probabilities this distribution of probabilities this distribution of probabilities and its shape and its shape and its shape is 27 there's 27 numbers and we're going is 27 there's 27 numbers and we're going is 27 there's 27 numbers and we're going to interpret this as the neural nets to interpret this as the neural nets to interpret this as the neural nets assignment for how likely every one of assignment for how likely every one of assignment for how likely every one of these characters um these characters um these characters um the 27 characters are to come next the 27 characters are to come next the 27 characters are to come next and as we tune the weights w and as we tune the weights w and as we tune the weights w we're going to be of course getting we're going to be of course getting we're going to be of course getting different probabilities out for any different probabilities out for any different probabilities out for any character that you input character that you input character that you input",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 922,
      "text": "and so now the question is just can we",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 923,
      "text": "and so now the question is just can we",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 924,
      "text": "and so now the question is just can we optimize and find a good w optimize and find a good w optimize and find a good w such that the probabilities coming out such that the probabilities coming out such that the probabilities coming out are pretty good and the way we measure are pretty good and the way we measure are pretty good and the way we measure pretty good is by the loss function",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 925,
      "text": "okay pretty good is by the loss function",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 926,
      "text": "okay pretty good is by the loss function",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 927,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 928,
      "text": "so i organized everything into a single",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 929,
      "text": "so i organized everything into a single",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 930,
      "text": "so i organized everything into a single summary so that hopefully it's a bit summary so that hopefully it's a bit summary so that hopefully it's a bit more clear so it starts here more clear so it starts here more clear",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 931,
      "text": "so it starts here with an input data set with an input data set with an input data set we have some inputs to the neural net we have some inputs to the neural net we have some inputs to the neural net and we have some labels for the correct and we have some labels for the correct and we have some labels for the correct next character in a sequence these are next character in a sequence these are next character in a sequence these are integers integers integers here i'm using uh torch generators now here i'm using uh torch generators now here i'm using uh torch generators now so that you see the same numbers",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 932,
      "text": "that i so that you see the same numbers that i",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 933,
      "text": "so that you see the same numbers that i see see see",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 934,
      "text": "and i'm generating um",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 935,
      "text": "and i'm generating um",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 936,
      "text": "and i'm generating um 27 neurons weights 27 neurons weights 27 neurons weights and each neuron here receives 27 inputs then here we're going to plug in all the then here we're going to plug in all the input examples x's into a neural net so input examples x's into a neural net so input examples x's into a neural net so here this is a forward pass here this is a forward pass here this is a forward pass first we have to encode all of the first we have to encode all of the first we have to encode all of the inputs into one hot representations inputs into one hot representations inputs into one hot representations",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 937,
      "text": "so we have 27 classes we pass in these",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 938,
      "text": "so we have 27 classes we pass in these",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 939,
      "text": "so we have 27 classes we pass in these integers and integers and integers and x inc becomes a array that is 5 by 27 x inc becomes a array that is 5 by 27 x inc becomes a array that is 5 by 27 zeros except for a few ones zeros except for a few ones zeros except for a few ones we then multiply this in the first layer we then multiply this in the first layer we then multiply this in the first layer of a neural net to get logits of a neural net to get logits of a neural net to get logits exponentiate the logits to get fake exponentiate the logits to get fake exponentiate the logits to get fake counts sort of counts sort of counts sort of and normalize these counts to get and normalize these counts to get and normalize these counts to get probabilities probabilities so we lock these last two lines by the so we lock these last two lines by the so we lock these last two lines by the way here are called the softmax way here are called the softmax way here are called the softmax which i pulled up here soft max is a which i pulled up here soft max is a which i pulled up here soft max is a very often used layer in a neural net very often used layer in a neural net very often used layer in a neural net that takes these z's which are logics that takes these z's which are logics that takes these z's which are logics exponentiates them exponentiates them exponentiates them and divides and normalizes it's a way of and divides and normalizes it's a way of and divides and normalizes it's a way of taking taking taking outputs of a neural net layer and these outputs of a neural net layer and these outputs of a neural net layer and these these outputs can be positive or these outputs can be positive or these outputs can be positive or negative negative negative and it outputs probability distributions and it outputs probability distributions and it outputs probability distributions it outputs something that is always it outputs something that is always it outputs something that is always sums to one and are positive numbers sums to one and are positive numbers sums to one and are positive numbers just like probabilities just like probabilities just like probabilities",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 940,
      "text": "um so it's kind of like a normalization um so it's kind of like a normalization",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 941,
      "text": "um so it's kind of like a normalization function if you want to think of it that function if you want to think of it that function if you want to think of it that way",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 942,
      "text": "and you can put it on top of any way and you can put it on top of any way and you can put it on top of any other linear layer inside a neural net other linear layer inside a neural net other linear layer inside a neural net and it basically makes a neural net and it basically makes a neural net and it basically makes a neural net output probabilities that's very often output probabilities that's very often output probabilities that's very often used and we used it as well here used",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 943,
      "text": "and we used it as well here used",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 944,
      "text": "and we used it as well here so this is the forward pass and that's so this is the forward pass",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 945,
      "text": "and that's so this is the forward pass and that's how we made a neural net output how we made a neural net output how we made a neural net output probability probability probability now now you'll notice that you'll notice that you'll notice that um all of these all of these this entire forward pass is made up of this entire forward pass is made up of this entire forward pass is made up of differentiable differentiable differentiable layers everything here we can back layers everything here we can back layers everything here we can back propagate through and we saw some of the propagate through and we saw some of the propagate through and we saw some of the back propagation in micrograd back propagation in micrograd back propagation in micrograd this is just this is just this is just multiplication and addition all that's multiplication and addition all that's multiplication and addition all that's happening here is just multiply and then happening here is just multiply",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 946,
      "text": "and then happening here is just multiply and then add and we know how to backpropagate add",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 947,
      "text": "and we know how to backpropagate add",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 948,
      "text": "and we know how to backpropagate through them through them through them exponentiation we know how to exponentiation we know how to exponentiation we know how to backpropagate through backpropagate through backpropagate through",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 949,
      "text": "and then here we are summing",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 950,
      "text": "and then here we are summing",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 951,
      "text": "and then here we are summing and sum is is easily backpropagable as and sum is is easily backpropagable as and sum is is easily backpropagable as well well and division as well so everything here and division as well so everything here and division as well so everything here is differentiable operation is differentiable operation is differentiable operation and we can back propagate through and we can back propagate through and we can back propagate through now we achieve these probabilities which now we achieve these probabilities which now we achieve these probabilities which are 5 by 27 are 5 by 27 are 5 by 27 for every single example we have a for every single example we have a for every single example we have a vector of probabilities that's into one vector of probabilities that's into one vector of probabilities that's into one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 952,
      "text": "and then here i wrote a bunch of stuff",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 953,
      "text": "and then here i wrote a bunch of stuff",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 954,
      "text": "and then here i wrote a bunch of stuff to sort of like break down uh the to sort of like break down uh the to sort of like break down uh the examples examples examples",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 955,
      "text": "so we have five examples making up emma so we have five examples making up emma so we have five examples making up emma",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 956,
      "text": "right right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 957,
      "text": "and there are five bigrams inside emma and there are five bigrams inside emma and there are five bigrams inside emma so bigram example a bigram example1 is so bigram example a bigram example1 is so bigram example a bigram example1 is that e is the beginning character right that e is the beginning character right that e is the beginning character right after dot after dot after dot and the indexes for these are zero and and the indexes for these are zero and and the indexes for these are zero and five five five",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 958,
      "text": "so then we feed in a zero",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 959,
      "text": "so then we feed in a zero",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 960,
      "text": "so then we feed in a zero that's the input of the neural net that's the input of the neural net that's the input of the neural net we get probabilities from the neural net we get probabilities from the neural net we get probabilities from the neural net that are 27 numbers that are 27 numbers that are 27 numbers and then the label is 5 because e",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 961,
      "text": "and then the label is 5 because e",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 962,
      "text": "and then the label is 5 because e actually comes after dot actually comes after dot actually comes after dot",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 963,
      "text": "so that's the label so that's the label so that's the label",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 964,
      "text": "and then and then we use this label 5 to index into the we use this label 5 to index into the we use this label 5 to index into the probability distribution here probability distribution here probability distribution here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 965,
      "text": "so so this this index 5 here is 0 1 2 3 4 5.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 966,
      "text": "it's this index 5 here is 0 1 2 3 4 5.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 967,
      "text": "it's this index 5 here is 0 1 2 3 4 5.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 968,
      "text": "it's this number here number here number here which is here which is here which is here so that's basically the probability",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 969,
      "text": "so that's basically the probability so that's basically the probability assigned by the neural net to the actual assigned by the neural net to the actual assigned by the neural net to the actual correct character correct character correct character you see that the network currently you see that the network currently you see that the network currently thinks that this next character that e thinks that this next character that e thinks that this next character that e following dot is only one percent likely following dot is only one percent likely following dot is only one percent likely which is of course not very good right which is of course not very good right which is of course not very good right because this actually is a training because this actually is a training because this actually is a training example and the network thinks this is example and the network thinks this is example and the network thinks this is currently very very unlikely",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 970,
      "text": "but that's currently very very unlikely",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 971,
      "text": "but that's currently very very unlikely",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 972,
      "text": "but that's just because we didn't get very lucky in just because we didn't get very lucky in just because we didn't get very lucky in generating a good setting of w so right generating a good setting of w so right generating a good setting of w so right now this network things it says unlikely now this network things it says unlikely now this network things it says unlikely and 0.01 is not a good outcome and 0.01 is not a good outcome and 0.01 is not a good outcome so the log likelihood then is very so the log likelihood then is very so the log likelihood then is very negative negative and the negative log likelihood is very and the negative log likelihood is very and the negative log likelihood is very positive positive positive and so four is a very high negative log and so four is a very high negative log and so four is a very high negative log likelihood and that means we're going to likelihood and that means we're going to likelihood and that means we're going to have a high loss have a high loss have a high loss because what is the loss the loss is because what is the loss the loss is because what is the loss the loss is just the average negative log likelihood just the average negative log likelihood just the average negative log likelihood so the second character is em so the second character is em so the second character is em",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 973,
      "text": "and you see here that also the network",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 974,
      "text": "and you see here that also the network",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 975,
      "text": "and you see here that also the network thought that m following e is very thought that m following e is very thought that m following e is very unlikely one percent the for m following m",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 976,
      "text": "i thought it was the for m following m i thought it was two percent two percent two percent and for a following m it actually and for a following m it actually and for a following m it actually thought it was seven percent likely so thought it was seven percent likely so thought it was seven percent likely",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 977,
      "text": "so just by chance this one actually has a just by chance this one actually has a just by chance this one actually has a pretty good probability and",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 978,
      "text": "therefore pretty good probability and therefore pretty good probability and therefore pretty low negative log likelihood pretty low negative log likelihood pretty low negative log likelihood and finally here it thought this was one and finally here it thought this was one and finally here it thought this was one percent likely percent likely percent likely so overall our average negative log so overall our average negative log so overall our average negative log likelihood which is the loss the total likelihood which is the loss the total likelihood which is the loss the total loss that summarizes loss that summarizes loss that summarizes basically the how well this network basically the how well this network basically the how well this network currently works at least on this one currently works at least on this one currently works at least on this one word not on the full data suggested one word not on the full data suggested one word not on the full data suggested one word is 3.76 which is actually very word is 3.76 which is actually very word is 3.76 which is actually very fairly high loss this is not a very good fairly high loss this is not a very good fairly high loss this is not a very good setting of w's setting of w's setting of w's now here's what we can do now here's what we can do now here's what we can do we're currently getting 3.76 we're currently getting 3.76 we're currently getting 3.76 we can actually come here and we can we can actually come here and we can we can actually come here and we can change our w we can resample it so let change our w we can resample it so let change our w we can resample it so let me just add one to have a different seed me just add one to have a different seed me just add one to have a different seed and then we get a different w and then we get a different w",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 979,
      "text": "and then we get a different w",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 980,
      "text": "and then we can rerun this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 981,
      "text": "and then we can rerun this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 982,
      "text": "and then we can rerun this and with this different c with this and with this different c with this and with this different c with this different setting of w's we now get 3.37 different setting of w's we now get 3.37 different setting of w's we now get 3.37 so this is a much better w right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 983,
      "text": "and so this is a much better w right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 984,
      "text": "and so this is a much better w right and that",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 985,
      "text": "and it's better because the that",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 986,
      "text": "and it's better because the that",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 987,
      "text": "and it's better because the probabilities just happen to come out probabilities just happen to come out probabilities just happen to come out higher for the for the characters that higher for the for the characters that higher for the for the characters that actually are next actually are next actually are next",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 988,
      "text": "and so you can imagine actually just",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 989,
      "text": "and so you can imagine actually just",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 990,
      "text": "and so you can imagine actually just resampling this you know we can try two resampling this you know we can try two resampling this you know we can try two",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 991,
      "text": "so",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 992,
      "text": "so okay this was not very good",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 993,
      "text": "okay this was not very good",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 994,
      "text": "okay this was not very good let's try one more let's try one more let's try one more we can try three we can try three we can try three",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 995,
      "text": "okay this was terrible setting because",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 996,
      "text": "okay this was terrible setting because",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 997,
      "text": "okay this was terrible setting because we have a very high loss we have a very high loss we have a very high loss",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 998,
      "text": "so anyway i'm going to erase this what i'm doing here which is just guess what i'm doing here which is just guess and check of randomly assigning and check of randomly assigning and check of randomly assigning parameters and seeing if the network is parameters and seeing if the network is parameters and seeing if the network is good that is uh amateur hour that's not good that is uh amateur hour that's not good that is uh amateur hour that's not how you optimize a neural net the way how you optimize a neural net the way how you optimize a neural net the way you optimize your neural net is you you optimize your neural net is you you optimize your neural net is you start with some random guess",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 999,
      "text": "and we're start with some random guess and we're start with some random guess and we're going to commit to this one even though going to commit to this one even though going to commit to this one even though it's not very good it's not very good it's not very good",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1000,
      "text": "but now the big deal is we have a loss but now the big deal is we have a loss but now the big deal is we have a loss function function so this loss so this loss so this loss is made up only of differentiable is made up only of differentiable is made up only of differentiable operations and we can minimize the loss operations and we can minimize the loss operations and we can minimize the loss by tuning by tuning by tuning ws ws ws by computing the gradients of the loss by computing the gradients of the loss by computing the gradients of the loss with respect to with respect to with respect to these w matrices these w matrices these w matrices",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1001,
      "text": "and so then we can tune w to minimize",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1002,
      "text": "and so then we can tune w to minimize",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1003,
      "text": "and so then we can tune w to minimize the loss and find a good setting of w the loss and find a good setting of w the loss and find a good setting of w using gradient based optimization so using gradient based optimization so using gradient based optimization so let's see how that will work now things let's see how that will work now things let's see how that will work now things are actually going to look almost are actually going to look almost are actually going to look almost identical to what we had with micrograd identical to what we had with micrograd identical to what we had with micrograd so here so here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1004,
      "text": "so here i pulled up the lecture from micrograd i pulled up the lecture from micrograd i pulled up the lecture from micrograd the notebook",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1005,
      "text": "it's from this repository the notebook",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1006,
      "text": "it's from this repository the notebook",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1007,
      "text": "it's from this repository and when i scroll all the way to the end and when i scroll all the way to the end and when i scroll all the way to the end where we left off with micrograd we had where we left off with micrograd we had where we left off with micrograd we had something very very similar something very very similar something very very similar we had we had we had a number of input examples in this case a number of input examples in this case a number of input examples in this case we had four input examples inside axis we had four input examples inside axis we had four input examples inside axis and we had their targets these are and we had their targets these are and we had their targets these are targets targets targets just like here we have our axes now but just like here we have our axes now but just like here we have our axes now",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1008,
      "text": "but we have five of them",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1009,
      "text": "and they're now we have five of them and they're now we have five of them and they're now integers instead of vectors integers instead of vectors integers instead of vectors but we're going to convert our integers but we're going to convert our integers but we're going to convert our integers to vectors except our vectors will be 27 to vectors except our vectors will be 27 to vectors except our vectors will be 27 large instead of three large large instead of three large large instead of three large and then here what we did is first we and then here what we did is first we and then here what we did is first we did a forward pass where we ran a neural did a forward pass where we ran a neural did a forward pass where we ran a neural net on all of the inputs net on all of the inputs net on all of the inputs to get predictions to get predictions to get predictions our neural net at the time this nfx was our neural net at the time this nfx was our neural net at the time this nfx was a multi-layer perceptron a multi-layer perceptron a multi-layer perceptron our neural net is going to look our neural net is going to look our neural net is going to look different because our neural net is just different because our neural net is just different because our neural net is just a single layer a single layer a single layer single linear layer followed by a soft single linear layer followed by a soft single linear layer followed by a soft max max max",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1010,
      "text": "so that's our neural net so that's our neural net",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1011,
      "text": "so that's our neural net and the loss here was the mean squared and the loss here was the mean squared and the loss here was the mean squared error so we simply subtracted the error so we simply subtracted the error so we simply subtracted the prediction from the ground truth and prediction from the ground truth and prediction from the ground truth and squared it and summed it all up and that squared it and summed it all up and that squared it and summed it all up and that was the loss and loss was the single was the loss and loss was the single was the loss and loss was the single number that summarized the quality of number that summarized the quality of number that summarized the quality of the neural net and when loss is low like the neural net and when loss is low like the neural net and when loss is low like almost zero that means the neural net is almost zero that means the neural net is almost zero that means the neural net is predicting correctly predicting correctly predicting correctly so we had a single number that uh that so we had a single number that uh that so we had a single number that uh that summarized the uh the performance of the summarized the uh the performance of the summarized the uh the performance of the neural net and everything here was neural net and everything here was neural net and everything here was differentiable and was stored in massive differentiable and was stored in massive differentiable and was stored in massive compute graph compute graph compute graph and then we iterated over all the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1012,
      "text": "and then we iterated over all the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1013,
      "text": "and then we iterated over all the parameters we made sure that the parameters we made sure that the parameters we made sure that the gradients are set to zero and we called gradients are set to zero and we called gradients are set to zero and we called lost up backward lost up backward lost up backward and lasted backward initiated back and lasted backward initiated back and lasted backward initiated back propagation at the final output node of propagation at the final output node of propagation at the final output node of loss loss loss",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1014,
      "text": "right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1015,
      "text": "so right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1016,
      "text": "so",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1017,
      "text": "yeah remember these expressions we had",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1018,
      "text": "yeah remember these expressions we had",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1019,
      "text": "yeah remember these expressions we had loss all the way at the end we start loss all the way at the end we start loss all the way at the end we start back propagation",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1020,
      "text": "and we went all the way back propagation",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1021,
      "text": "and we went all the way back propagation",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1022,
      "text": "and we went all the way back back back and we made sure that we populated all and we made sure that we populated all and we made sure that we populated all the parameters dot grad the parameters dot grad the parameters dot grad so that graph started at zero but back so that graph started at zero",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1023,
      "text": "but back",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1024,
      "text": "so that graph started at zero but back propagation filled it in propagation filled it in propagation filled it in and then in the update we iterated over and then in the update we iterated over and then in the update we iterated over all the parameters and we simply did a all the parameters and we simply did a all the parameters",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1025,
      "text": "and we simply did a parameter update where every single parameter update where every single parameter update where every single element of our parameters was nudged in element of our parameters was nudged in element of our parameters was nudged in the opposite direction of the gradient the opposite direction of the gradient the opposite direction of the gradient and so we're going to do the exact same",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1026,
      "text": "and so we're going to do the exact same",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1027,
      "text": "and so we're going to do the exact same thing here thing here thing here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1028,
      "text": "so i'm going to pull this up",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1029,
      "text": "so i'm going to pull this up",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1030,
      "text": "so i'm going to pull this up on the side here so that we have it available and we're so that we have it available",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1031,
      "text": "and we're actually going to do the exact same actually going to do the exact same actually going to do the exact same thing",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1032,
      "text": "so this was the forward pass so thing",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1033,
      "text": "so this was the forward pass so thing",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1034,
      "text": "so this was the forward pass so where we did this where we did this where we did this and probs is our wipe red",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1035,
      "text": "so now we have and probs is our wipe red",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1036,
      "text": "so now we have and probs is our wipe red",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1037,
      "text": "so now we have to evaluate the loss but we're not using to evaluate the loss",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1038,
      "text": "but we're not using to evaluate the loss but we're not using the mean squared error we're using the the mean squared error we're using the the mean squared error we're using the negative log likelihood because we are negative log likelihood because we are negative log likelihood because we are doing classification we're not doing doing classification we're not doing doing classification we're not doing regression as it's called regression as it's called regression as it's called so here we want to calculate loss so here we want to calculate loss so here we want to calculate loss now the way we calculate it is it's just now the way we calculate it is it's just now the way we calculate it is it's just this average negative log likelihood this average negative log likelihood this average negative log likelihood now this probs here now this probs here now this probs here has a shape of 5 by 27 has a shape of 5 by 27 has a shape of 5 by 27 and so to get all the we basically want and so to get all the we basically want and so to get all the we basically want to pluck out the probabilities at the to pluck out the probabilities at the to pluck out the probabilities at the correct indices here correct indices here correct indices here so in particular because the labels are so in particular because the labels are so in particular because the labels are stored here in array wise stored here in array wise stored here in array wise basically what we're after is for the basically what we're after is for the basically what we're after is for the first example we're looking at first example we're looking at first example we're looking at probability of five right at index five probability of five right at index five probability of five right at index five for the second example for the second example for the second example at the the second row or row index one at the the second row or row index one at the the second row or row index",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1039,
      "text": "one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1040,
      "text": "we are interested in the probability we are interested in the probability we are interested in the probability assigned to index 13.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1041,
      "text": "assigned to index 13.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1042,
      "text": "assigned to index 13.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1043,
      "text": "at the second example we also have 13.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1044,
      "text": "at the second example we also have 13.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1045,
      "text": "at the second example we also have 13.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1046,
      "text": "at the third row we want one at the third row we want one at the third row we want one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1047,
      "text": "and then the last row which is four we and then the last row which is four we and then the last row which is four we want zero",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1048,
      "text": "so these are the probabilities want zero so these are the probabilities want zero so these are the probabilities we're interested in right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1049,
      "text": "we're interested in right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1050,
      "text": "we're interested in right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1051,
      "text": "and you can see that they're not amazing",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1052,
      "text": "and you can see that they're not amazing",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1053,
      "text": "and you can see that they're not amazing as we saw above as we saw above as we saw above so these are the probabilities we want so these are the probabilities we want so these are the probabilities we want",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1054,
      "text": "but we want like a more efficient way to",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1055,
      "text": "but we want like a more efficient way to",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1056,
      "text": "but we want like a more efficient way to access these probabilities access these probabilities access these probabilities not just listing them out in a tuple not just listing them out in a tuple not just listing them out in a tuple like this so it turns out that the way like this so it turns out that the way like this so it turns out that the way to do this in pytorch uh one of the ways to do this in pytorch uh one of the ways to do this in pytorch uh one of the ways at least is we can basically pass in at least is we can basically pass in at least is we can basically pass in all of these sorry about that all of these um sorry about that all of these um integers in the vectors integers in the vectors integers in the vectors",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1057,
      "text": "so so the the the these ones you see how they're just 0 1 these ones you see how they're just 0 1 these ones you see how they're just 0 1 2 3 4 2 3 4 2 3 4 we can actually create that using mp we can actually create that using mp we can actually create that using mp not mp sorry torch dot range of 5 not mp sorry torch dot range of 5 not mp sorry torch dot range of 5 0 1 2 3 4. 0 1 2 3 4. 0",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1058,
      "text": "1 2 3 4.",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1059,
      "text": "so we can index here with torch.range of so we can index here with torch.range of so we can index here with torch.range of 5 5 and here we index with ys and here we index with ys",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1060,
      "text": "and here we index with ys and you see that that gives us and you see that that gives us and you see that that gives us exactly these numbers so that plucks out the probabilities of so that plucks out the probabilities of that the neural network assigns to the that the neural network assigns to the that the neural network assigns to the correct next character correct next character correct next character",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1061,
      "text": "now we take those probabilities and we now we take those probabilities and we now we take those probabilities and we don't we actually look at the log don't we actually look at the log don't we actually look at the log probability so we want to dot log probability",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1062,
      "text": "so we want to dot log probability",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1063,
      "text": "so we want to dot log",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1064,
      "text": "and then we want to just",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1065,
      "text": "and then we want to just",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1066,
      "text": "and then we want to just average that up so take the mean of all average that up so take the mean of all average that up so take the mean of all of that of that of that",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1067,
      "text": "and then it's the negative",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1068,
      "text": "and then it's the negative",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1069,
      "text": "and then it's the negative average log likelihood that is the loss average log likelihood that is the loss average log likelihood that is the loss so the loss here is 3.7 something",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1070,
      "text": "and so the loss here is 3.7 something",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1071,
      "text": "and so the loss here is 3.7 something",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1072,
      "text": "and you see that this loss 3.76 3.76 is you see that this loss 3.76 3.76 is you see that this loss 3.76 3.76 is exactly as we've obtained before but exactly as we've obtained before but exactly as we've obtained before but this is a vectorized form of that this is a vectorized form of that this is a vectorized form of that expression expression expression",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1073,
      "text": "so so we get the same loss we get the same loss we get the same loss and the same loss we can consider and the same loss we can consider and the same loss we can consider service part of this forward pass service part of this forward pass service part of this forward pass and we've achieved here now loss and we've achieved here now loss and we've achieved here now loss",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1074,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1075,
      "text": "so we made our way all the way to",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1076,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1077,
      "text": "so we made our way all the way to",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1078,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1079,
      "text": "so we made our way all the way to loss we've defined the forward pass loss we've defined the forward pass loss we've defined the forward pass we forwarded the network and the loss we forwarded the network and the loss we forwarded the network and the loss now we're ready to do the backward pass now we're ready to do the backward pass now we're ready to do the backward pass so backward pass",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1080,
      "text": "we want to first make sure that all the we want to first make sure that all the gradients are reset so they're at zero gradients are reset so they're at zero gradients are reset so they're at zero now in pytorch you can set the gradients now in pytorch you can set the gradients now in pytorch you can set the gradients to be zero but you can also just set it to be zero",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1081,
      "text": "but you can also just set it to be zero",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1082,
      "text": "but you can also just set it to none and setting it to none is more to none and setting it to none is more to none and setting it to none is more efficient and pi torch will interpret efficient and pi torch will interpret efficient and pi torch will interpret none as like a lack of a gradient and is none as like a lack of a gradient and is none as like a lack of a gradient and is the same as zeros the same as zeros the same as zeros",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1083,
      "text": "so this is a way to set to zero the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1084,
      "text": "so this is a way to set to zero the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1085,
      "text": "so this is a way to set to zero the gradient before we do lost that backward we need before we do lost that backward we need one more thing if you remember from one more thing if you remember from one more thing if you remember from micrograd micrograd pytorch actually requires pytorch actually requires pytorch actually requires that we pass in requires grad is true that we pass in requires grad is true that we pass in requires grad is true so that when we tell so that when we tell so that when we tell pythorge that we are interested in pythorge that we are interested in pythorge that we are interested in calculating gradients for this leaf calculating gradients for this leaf calculating gradients for this leaf tensor by default this is false tensor by default this is false tensor by default this is false",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1086,
      "text": "so let me recalculate with that so let me recalculate with that",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1087,
      "text": "so let me recalculate with that and then set to none and lost that and then set to none and lost that and then set to none and lost that backward backward backward now something magical happened when now something magical happened when now something magical happened when lasted backward was run lasted backward was run lasted backward was run because pytorch just like micrograd when because pytorch just like micrograd when because pytorch just like micrograd when we did the forward pass here we did the forward pass here we did the forward pass here it keeps track of all the operations it keeps track of all the operations it keeps track of all the operations under the hood it builds a full under the hood it builds a full under the hood it builds a full computational graph just like the graphs computational graph just like the graphs computational graph just like the graphs we've we've we've produced in micrograd those graphs exist produced in micrograd those graphs exist produced in micrograd those graphs exist inside pi torch inside pi torch inside pi torch",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1088,
      "text": "and so it knows all the dependencies and and so it knows all the dependencies",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1089,
      "text": "and and so it knows all the dependencies and all the mathematical operations of all the mathematical operations of all the mathematical operations of everything everything and when you then calculate the loss and when you then calculate the loss and when you then calculate the loss we can call a dot backward on it we can call a dot backward on it we can call a dot backward on it and that backward then fills in the and that backward then fills in the and that backward then fills in the gradients of gradients of gradients of all the intermediates all the intermediates all the intermediates all the way back to w's which are the all the way back to w's which are the all the way back to w's which are the parameters of our neural net so now we parameters of our neural net so now we parameters of our neural net so now we can do w grad and we see that it has can do w grad",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1090,
      "text": "and we see that it has can do w grad",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1091,
      "text": "and we see that it has structure there's stuff inside it and these gradients and these gradients every single element here every single element here every single element here so w dot shape is 27 by 27 so w dot shape is 27 by 27 so w dot shape is 27 by 27 w grad shape is the same 27 by 27 w grad shape is the same 27 by 27 w grad shape is the same 27 by 27 and every element of w that grad and every element of w that grad and every element of w that grad is telling us is telling us is telling us the influence of that weight on the loss the influence of that weight on the loss the influence of that weight on the loss function function so for example this number all the way so for example this number all the way so for example this number all the way here here if this element the zero zero element of if this element the zero zero element of if this element the zero zero element of w w because the gradient is positive is because the gradient is positive is because the gradient is positive is telling us that this has a positive telling us that this has a positive telling us that this has a positive influence in the loss slightly nudging influence in the loss slightly nudging influence in the loss slightly nudging w w slightly taking w 0 0 slightly taking w 0 0 slightly taking w 0 0 and and adding a small h to it adding a small h to it adding a small h to it would increase the loss would increase the loss would increase the loss mildly because this gradient is positive mildly because this gradient is positive mildly because this gradient is positive some of these gradients are also some of these gradients are also some of these gradients are also negative negative so that's telling us about the gradient so that's telling us about the gradient so that's telling us about the gradient information and we can use this gradient information and we can use this gradient information and we can use this gradient information to update the weights of information to update the weights of information to update the weights of this neural network so let's now do the this neural network so let's now do the this neural network so let's now do the update it's going to be very similar to update it's going to be very similar to update it's going to be very similar to what we had in micrograd we need no loop what we had in micrograd we need no loop what we had in micrograd we need no loop over all the parameters because we only over all the parameters because we only over all the parameters because we only have one parameter uh tensor and that is have one parameter uh tensor and that is have one parameter uh tensor and that is w w",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1092,
      "text": "so we simply do w dot data plus equals so we simply do w dot data plus equals so we simply do w dot data plus equals uh the uh the uh the we can actually copy this almost exactly we can actually copy this almost exactly we can actually copy this almost exactly negative 0.1 times w dot grad and that would be the update to the and that would be the update to the tensor tensor tensor",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1093,
      "text": "so that updates so that updates so that updates the tensor and because the tensor is updated we would because the tensor is updated we would because the tensor is updated we would expect that now the loss should decrease expect that now the loss should decrease expect that now the loss should decrease so so here if i print loss that item that item it was 3.76 right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1094,
      "text": "it was 3.76 right it was 3.76 right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1095,
      "text": "so we've updated the w here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1096,
      "text": "so if i so we've updated the w here so if i so we've updated the w here so if i recalculate forward pass recalculate forward pass recalculate forward pass loss now should be slightly lower so loss now should be slightly lower so loss now should be slightly lower so 3.76 goes to 3.76 goes to 3.76 goes to 3.74 3.74 3.74 and then",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1097,
      "text": "and then we can again set to set grad to none",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1098,
      "text": "and we can again set to set grad to none",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1099,
      "text": "and we can again set to set grad to none and backward backward update update update and now the parameters changed again and now the parameters changed again and now the parameters changed again so if we recalculate the forward pass we so if we recalculate the forward pass we so if we recalculate the forward pass we expect a lower loss again 3.72 okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1100,
      "text": "and this is again doing the we're okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1101,
      "text": "and this is again doing the we're now doing gradient descent and when we achieve a low loss that will and when we achieve a low loss that will mean that the network is assigning high mean that the network is assigning high mean that the network is assigning high probabilities to the correctness probabilities to the correctness probabilities to the correctness characters",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1102,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1103,
      "text": "so i rearranged characters",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1104,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1105,
      "text": "so i rearranged characters",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1106,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1107,
      "text": "so i rearranged everything",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1108,
      "text": "and i put it all together everything",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1109,
      "text": "and i put it all together everything",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1110,
      "text": "and i put it all together from scratch from scratch from scratch so here is where we construct our data so here is where we construct our data so here is where we construct our data set of bigrams set of bigrams set of bigrams you see that we are still iterating only you see that we are still iterating only you see that we are still iterating only on the first word emma on the first word emma on the first word emma i'm going to change that in a second",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1111,
      "text": "i i'm going to change that in a second",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1112,
      "text": "i i'm going to change that in a second i added a number that counts the number of added a number that counts the number of added a number that counts the number of elements in x's so that we explicitly elements in x's so that we explicitly elements in x's so that we explicitly see that number of examples is five see that number of examples is five see that number of examples is five because currently we're just working because currently we're just working because currently we're just working with emma and there's five backgrounds with emma and there's five backgrounds with emma and there's five backgrounds there there there",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1113,
      "text": "and here i added a loop of exactly what",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1114,
      "text": "and here i added a loop of exactly what",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1115,
      "text": "and here i added a loop of exactly what we had before so we had 10 iterations of we had before",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1116,
      "text": "so we had 10 iterations of we had before",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1117,
      "text": "so we had 10 iterations of grainy descent of forward pass backward grainy descent of forward pass backward grainy descent of forward pass backward pass and an update pass and an update pass and an update and so running these two cells and so running these two cells and so running these two cells initialization and gradient descent initialization and gradient descent initialization and gradient descent gives us some improvement gives us some improvement gives us some improvement on on on the loss function the loss function the loss function but now i want to use all the words",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1118,
      "text": "but now i want to use all the words",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1119,
      "text": "but now i want to use all the words and there's not 5 but 228 000 bigrams and there's not 5 but 228 000 bigrams and there's not 5 but 228 000 bigrams now now however this should require",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1120,
      "text": "no however this should require",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1121,
      "text": "no however this should require no modification whatsoever everything modification whatsoever everything modification whatsoever everything should just run because all the code we should just run because all the code we should just run because all the code we wrote doesn't care if there's five wrote doesn't care if there's five wrote doesn't care if there's five migrants or 228 000 bigrams and with migrants or 228 000 bigrams and with migrants or 228 000 bigrams and with everything we should just work",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1122,
      "text": "so everything we should just work so everything we should just work so you see that this will just run you see that this will just run you see that this will just run",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1123,
      "text": "but now we are optimizing over the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1124,
      "text": "but now we are optimizing over the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1125,
      "text": "but now we are optimizing over the entire training set of all the bigrams entire training set of all the bigrams entire training set of all the bigrams and you see now that we are decreasing and you see now that we are decreasing and you see now that we are decreasing very slightly",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1126,
      "text": "so actually we can very slightly",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1127,
      "text": "so actually we can very slightly so actually we can probably afford a larger learning rate and probably for even larger learning and probably for even larger learning rate even 50 seems to work on this very very even 50 seems to work on this very very simple example",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1128,
      "text": "right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1129,
      "text": "so let me simple example",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1130,
      "text": "right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1131,
      "text": "so let me simple example",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1132,
      "text": "right",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1133,
      "text": "so let me re-initialize and let's run 100 re-initialize and let's run 100 re-initialize and let's run 100 iterations iterations iterations see what happens we seem to be we seem to be coming up to some pretty good losses coming up to some pretty good losses coming up to some pretty good losses here 2.47 here 2.47 here 2.47 let me run 100 more let me run 100 more let me run 100 more what is the number that we expect by the what is the number that we expect by the what is the number that we expect by the way in the loss we expect to get way in the loss we expect to get way in the loss we expect to get something around what we had originally something around what we had originally something around what we had originally actually actually actually so all the way back if you remember in so all the way back if you remember in so all the way back if you remember in the beginning of this video when we the beginning of this video when we the beginning of this video when we optimized uh just by counting optimized uh just by counting optimized uh just by counting our loss was roughly 2.47 our loss was roughly 2.47 our loss was roughly 2.47 after we had it smoothing after we had it smoothing after we had it smoothing but before smoothing we had roughly 2.45 but before smoothing we had roughly 2.45 but before smoothing we had roughly 2.45 likelihood likelihood sorry loss sorry loss sorry loss",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1134,
      "text": "and so that's actually roughly the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1135,
      "text": "and so that's actually roughly the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1136,
      "text": "and so that's actually roughly the vicinity of what we expect to achieve vicinity of what we expect to achieve vicinity of what we expect to achieve",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1137,
      "text": "but before we achieved it by counting but before we achieved it by counting but before we achieved it by counting and here we are achieving the roughly",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1138,
      "text": "and here we are achieving the roughly",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1139,
      "text": "and here we are achieving the roughly the same result but with gradient based the same result but with gradient based the same result but with gradient based optimization optimization optimization so we come to about 2.4 so we come to about 2.4 so we come to about 2.4 6 2.45 etc 6 2.45 etc 6 2.45 etc and that makes sense because and that makes sense because and that makes sense because fundamentally we're not taking any fundamentally we're not taking any fundamentally we're not taking any additional information we're still just additional information we're still just additional information we're still just taking in the previous character and taking in the previous character and taking in the previous character and trying to predict the next one but trying to predict the next one but trying to predict the next one but instead of doing it explicitly by instead of doing it explicitly by instead of doing it explicitly by counting and normalizing counting and normalizing counting and normalizing we are doing it with gradient-based we are doing it with gradient-based we are doing it with gradient-based learning",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1140,
      "text": "and it just so happens that the learning and it just so happens that the learning and it just so happens that the explicit approach happens to very well explicit approach happens to very well explicit approach happens to very well optimize the loss function without any optimize the loss function without any optimize the loss function without any need for a gradient based optimization need for a gradient based optimization need for a gradient based optimization because the setup for bigram language because the setup for bigram language because the setup for bigram language models are is so straightforward that's models are is so straightforward that's models are is so straightforward that's so simple we can just afford to estimate so simple we can just afford to estimate so simple we can just afford to estimate those probabilities directly and those probabilities directly and those probabilities directly and maintain them maintain them maintain them in a table in a table in a table but the gradient-based approach is but the gradient-based approach is but the gradient-based approach is significantly more flexible significantly more flexible significantly more flexible",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1141,
      "text": "so we've actually gained a lot",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1142,
      "text": "so we've actually gained a lot",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1143,
      "text": "so we've actually gained a lot because because because what we can do now is what we can do now is what we can do now is we can expand this approach and we can expand this approach and we can expand this approach and complexify the neural net so currently complexify the neural net so currently complexify the neural net",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1144,
      "text": "so currently we're just taking a single character and we're just taking a single character and we're just taking a single character and feeding into a neural net and the neural feeding into a neural net and the neural feeding into a neural net and the neural that's extremely simple but we're about that's extremely simple but we're about that's extremely simple but we're about to iterate on this substantially we're to iterate on this substantially we're to iterate on this substantially we're going to be taking multiple previous going to be taking multiple previous going to be taking multiple previous characters and we're going to be feeding characters and we're going to be feeding characters and we're going to be feeding feeding them into increasingly more feeding them into increasingly more feeding them into increasingly more complex neural nets but fundamentally complex neural nets but fundamentally complex neural nets but fundamentally out the output of the neural net will out the output of the neural net will out the output of the neural net will always just be logics always just be logics always just be logics and those logits will go through the and those logits will go through the and those logits will go through the exact same transformation we are going exact same transformation we are going exact same transformation we are going to take them through a soft max to take them through a soft max to take them through a soft max calculate the loss function and the calculate the loss function and the calculate the loss function and the negative log likelihood and do gradient negative log likelihood and do gradient negative log likelihood and do gradient based optimization and so actually based optimization and so actually based optimization",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1145,
      "text": "and so actually as we complexify the neural nets and as we complexify the neural nets and as we complexify the neural nets and work all the way up to transformers work all the way up to transformers work all the way up to transformers none of this will really fundamentally none of this will really fundamentally none of this will really fundamentally change none of this will fundamentally change none of this will fundamentally change none of this will fundamentally change the only thing that will change change the only thing that will change change the only thing that will change is is the way we do the forward pass where we the way we do the forward pass where we the way we do the forward pass where we take in some previous characters and take in some previous characters and take in some previous characters and calculate logits for the next character calculate logits for the next character calculate logits for the next character in the sequence that will become more in the sequence that will become more in the sequence that will become more complex complex complex and uh",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1146,
      "text": "but we'll use the same machinery and",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1147,
      "text": "uh but we'll use the same machinery and",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1148,
      "text": "uh but we'll use the same machinery to optimize it to optimize it to optimize it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1149,
      "text": "and um and um it's not obvious how we would have it's not obvious how we would have it's not obvious how we would have extended extended extended this bigram approach this bigram approach this bigram approach into the case where there are many more into the case where there are many more into the case where there are many more characters at the input because characters at the input because characters at the input because eventually these tables would get way eventually these tables would get way eventually these tables would get way too large because there's way too many too large because there's way too many too large because there's way too many combinations of what previous characters combinations of what previous characters combinations of what previous characters could be could be could be if you only have one previous character if you only have one previous character if you only have one previous character we can just keep everything in a table we can just keep everything in a table we can just keep everything in a table that counts but if you have the last 10 that counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1150,
      "text": "but if you have the last 10 that counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1151,
      "text": "but if you have the last 10 characters that are input we can't characters that are input we can't characters that are input we can't actually keep everything in the table actually keep everything in the table actually keep everything in the table anymore so this is fundamentally an anymore",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1152,
      "text": "so this is fundamentally an anymore",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1153,
      "text": "so this is fundamentally an unscalable approach and the neural unscalable approach and the neural unscalable approach and the neural network approach is significantly more network approach is significantly more network approach is significantly more scalable and it's something that scalable and it's something that scalable",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1154,
      "text": "and it's something that actually we can improve on over time",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1155,
      "text": "so actually we can improve on over time",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1156,
      "text": "so actually we can improve on over time",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1157,
      "text": "so that's where we will be digging next",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1158,
      "text": "i that's where we will be digging next",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1159,
      "text": "i that's where we will be digging next i wanted to point out two more things wanted to point out two more things wanted to point out two more things number one number one number one",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1160,
      "text": "i want you to notice that i want you to notice that i want you to notice that this this x ink here x ink here x ink here this is made up of one hot vectors and this is made up of one hot vectors and this is made up of one hot vectors and then those one hot vectors are then those one hot vectors are then those one hot vectors are multiplied by this w matrix multiplied by this w matrix multiplied by this w matrix",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1161,
      "text": "and we think of this as multiple neurons and we think of this as multiple neurons",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1162,
      "text": "and we think of this as multiple neurons being forwarded in a fully connected being forwarded in a fully connected being forwarded in a fully connected manner manner manner",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1163,
      "text": "but actually what's happening here is",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1164,
      "text": "but actually what's happening here is",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1165,
      "text": "but actually what's happening here is that for example that for example that for example if you have a one hot vector here that if you have a one hot vector here that if you have a one hot vector here that has a one at say the fifth dimension has a one at say the fifth dimension has a one at say the fifth dimension then because of the way the matrix then because of the way the matrix then because of the way the matrix multiplication works multiplication works multiplication works multiplying that one-half vector with w multiplying that one-half vector with w multiplying that one-half vector with w actually ends up plucking out the fifth actually ends up plucking out the fifth actually ends up plucking out the fifth row of w row of w row of w log logits would become just the fifth log logits would become just the fifth log logits would become just the fifth row of w row of w and that's because of the way the matrix",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1166,
      "text": "and that's because of the way the matrix",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1167,
      "text": "and that's because of the way the matrix multiplication works multiplication works um so that's actually what ends up happening that's actually what ends up happening that's actually what ends up happening so",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1168,
      "text": "but that's actually exactly what so",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1169,
      "text": "but that's actually exactly what so",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1170,
      "text": "but that's actually exactly what happened before happened before happened before because remember all the way up here because remember all the way up here because remember all the way up here we have a bigram we took the first we have a bigram we took the first we have a bigram we took the first character and then that first character character and then that first character character and then that first character indexed into a row of this array here indexed into a row of this array here indexed into a row of this array here and that row gave us the probability and that row gave us the probability and that row gave us the probability distribution for the next character so distribution for the next character so distribution for the next character so the first character was used as a lookup the first character was used as a lookup the first character was used as a lookup into a into a into a matrix here to get the probability matrix here to get the probability matrix here to get the probability distribution distribution distribution",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1171,
      "text": "well that's actually exactly what's well",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1172,
      "text": "that's actually exactly what's well",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1173,
      "text": "that's actually exactly what's happening here because we're taking the happening here because we're taking the happening here because we're taking the index we're encoding it as one hot and index we're encoding it as one hot and index we're encoding it as one hot and multiplying it by w multiplying it by w multiplying it by w so logics literally becomes so logics literally becomes so logics literally becomes the the appropriate row of w the appropriate row of w and that gets just as before and that gets just as before and that gets just as before exponentiated to create the counts exponentiated to create the counts exponentiated to create the counts and then normalized and becomes and then normalized and becomes and then normalized and becomes probability probability so this w here so this w here so this w here is literally is literally is literally the same as this array here the same as this array here the same as this array here but w remember is the log counts not the but w remember is the log counts not the but w remember is the log counts not the counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1174,
      "text": "so it's more precise to say that counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1175,
      "text": "so it's more precise to say that counts",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1176,
      "text": "so it's more precise to say that w exponentiated w exponentiated w exponentiated w dot x is this array w dot x is this array w dot x is this array",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1177,
      "text": "but this array was filled in by counting but this array was filled in by counting",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1178,
      "text": "but this array was filled in by counting and by and by and by basically basically populating the counts of bi-grams populating the counts of bi-grams populating the counts of bi-grams whereas in the gradient-based framework whereas in the gradient-based framework whereas in the gradient-based framework we initialize it randomly",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1179,
      "text": "and then we we initialize it randomly",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1180,
      "text": "and then we we initialize it randomly",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1181,
      "text": "and then we let the loss let the loss let the loss guide us guide us guide us to arrive at the exact same array to arrive at the exact same array to arrive at the exact same array so this array exactly here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1182,
      "text": "so this array exactly here so this array exactly here is is basically the array w at the end of basically the array w at the end of basically the array w at the end of optimization except we arrived at it optimization except we arrived at it optimization except we arrived at it piece by piece by following the loss piece by piece by following the loss piece by piece by following the loss and that's why we also obtain the same and that's why we also obtain the same and that's why we also obtain the same loss function at the end and the second loss function at the end and the second loss function at the end and the second note is if i come here note is if i come here note is if i come here remember the smoothing where we added remember the smoothing where we added remember the smoothing where we added fake counts to our counts fake counts to our counts fake counts to our counts in order to in order to in order to smooth out and make more uniform the smooth out and make more uniform the smooth out and make more uniform the distributions of these probabilities distributions of these probabilities distributions of these probabilities and that prevented us from assigning and that prevented us from assigning and that prevented us from assigning zero probability to zero probability to zero probability to to any one bigram to any one bigram to any one bigram now if i increase the count here now if i increase the count here now if i increase the count here what's happening to the probability what's happening to the probability what's happening to the probability as i increase the count probability as i increase the count probability as i increase the count probability becomes more and more uniform becomes more and more uniform becomes more and more uniform right because these counts go only up to right because these counts go only up to right because these counts go only up to like 900 or whatever so if i'm adding like 900 or whatever so if i'm adding like 900 or whatever so if i'm adding plus a million to every single number plus a million to every single number plus a million to every single number here you can see how here you can see how here you can see how the row and its probability then when we the row and its probability then when we the row and its probability then when we divide is just going to become more and divide is just going to become more and divide is just going to become more and more close to exactly even probability more close to exactly even probability more close to exactly even probability uniform distribution uniform distribution uniform distribution it turns out that the gradient based it turns out that the gradient based it turns out that the gradient based framework has an equivalent to smoothing framework has an equivalent to smoothing framework has an equivalent to smoothing in particular in particular in particular think through these w's here think through these w's here think through these w's here which we initialized randomly which we initialized randomly which we initialized randomly we could also think about initializing we could also think about initializing we could also think about initializing w's to be zero w's to be zero w's to be zero if all the entries of w are zero if all the entries of w are zero if all the entries of w are zero",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1183,
      "text": "then you'll see that logits will become then you'll see that logits will become then you'll see that logits will become all zero all zero all zero and then exponentiating those logics and then exponentiating those logics and then exponentiating those logics becomes all one becomes all one becomes all one and then the probabilities turned out to and then the probabilities turned out to and then the probabilities turned out to be exactly uniform be exactly uniform be exactly uniform so basically when w's are all equal to so basically when w's are all equal to so basically when w's are all equal to each other or say especially zero each other or say especially zero each other or say especially zero then the probabilities come out then the probabilities come out then the probabilities come out completely uniform completely uniform completely uniform",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1184,
      "text": "so so trying to incentivize w to be near zero trying to incentivize w to be near zero trying to incentivize w to be near zero is basically equivalent to is basically equivalent to is basically equivalent to label smoothing and the more you label smoothing and the more you label smoothing and the more you incentivize that in the loss function incentivize that in the loss function incentivize that in the loss function the more smooth distribution you're the more smooth distribution you're the more smooth distribution you're going to achieve going to achieve going to achieve so this brings us to something that's so this brings us to something that's so this brings us to something that's called called called regularization where we can actually regularization where we can actually regularization where we can actually augment the loss function to have a augment the loss function to have a augment the loss function to have a small component that we call a small component that we call a small component that we call a regularization loss regularization loss regularization loss in particular what we're going to do is in particular what we're going to do is in particular what we're going to do is we can take w",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1185,
      "text": "and we can for example we can take w and we can for example we can take w and we can for example square all of its entries square all of its entries square all of its entries",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1186,
      "text": "and then we can um whoops",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1187,
      "text": "and then we can um whoops",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1188,
      "text": "and then we can um whoops sorry about that sorry about that sorry about that we can take all the entries of w and we we can take all the entries of w and we we can take all the entries of w and we can sum them and because we're squaring uh there will and because we're squaring uh there will be no signs anymore um be no signs anymore um be no signs anymore um negatives and positives all get squashed negatives and positives all get squashed negatives and positives all get squashed to be positive numbers to be positive numbers to be positive numbers and then the way this works is you and then the way this works is you and then the way this works is you achieve zero loss if w is exactly or achieve zero loss if w is exactly or achieve zero loss if w is exactly or zero but if w has non-zero numbers you zero but if w has non-zero numbers you zero but if w has non-zero numbers you accumulate loss accumulate loss accumulate loss and so we can actually take this and we and so we can actually take this and we and so we can actually take this and we can add it on here can add it on here can add it on here so we can do something like loss",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1189,
      "text": "plus so we can do something like loss",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1190,
      "text": "plus so we can do something like loss plus w square w square w square dot sum dot sum dot sum or let's actually instead of sum let's or let's actually instead of sum let's or let's actually instead of sum let's take a mean because otherwise the sum take a mean because otherwise the sum take a mean because otherwise the sum gets too large gets too large gets too large",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1191,
      "text": "so mean is like a little bit more so mean is like a little bit more so mean is like a little bit more manageable manageable manageable",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1192,
      "text": "and then we have a regularization loss",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1193,
      "text": "and then we have a regularization loss",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1194,
      "text": "and then we have a regularization loss here say 0.01 times here say 0.01 times here say 0.01 times or something like that you can choose or something like that you can choose or something like that you can choose the regularization strength the regularization strength the regularization strength",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1195,
      "text": "and then we can just optimize this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1196,
      "text": "and and then we can just optimize this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1197,
      "text": "and and then we can just optimize this",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1198,
      "text": "and now this optimization actually has two now this optimization actually has two now this optimization actually has two components not only is it trying to make components not only is it trying to make components not only is it trying to make all the probabilities work out but in all the probabilities work out but in all the probabilities work out but in addition to that there's an additional addition to that there's an additional addition to that there's an additional component that simultaneously tries to component that simultaneously tries to component that simultaneously tries to make all w's be zero because if w's are make all w's be zero because if w's are make all w's be zero because if w's are non-zero you feel a loss and so non-zero you feel a loss and so non-zero you feel a loss and so minimizing this the only way to achieve minimizing this the only way to achieve minimizing this the only way to achieve that is for w to be zero that is for w to be zero that is for w to be zero and so you can think of this as adding and so you can think of this as adding and so you can think of this as adding like a spring force or like a gravity like a spring force or like a gravity like a spring force or like a gravity force that that pushes w to be zero so w force that that pushes w to be zero so w force that that pushes w to be zero so w wants to be zero and the probabilities wants to be zero and the probabilities wants to be zero and the probabilities want to be uniform but they also want to be uniform but they also want to be uniform but they also simultaneously want to match up your simultaneously want to match up your simultaneously want to match up your your probabilities as indicated by the your probabilities as indicated by the your probabilities as indicated by the data data data and so the strength of this and so the strength of this and so the strength of this regularization is exactly controlling regularization is exactly controlling regularization is exactly controlling the amount of counts the amount of counts the amount of counts that you add here that you add here that you add here adding a lot more counts adding a lot more counts adding a lot more counts here here corresponds to corresponds to corresponds to increasing this number increasing this number increasing this number because the more you increase it the because the more you increase it the because the more you increase it the more this part of the loss function more this part of the loss function more this part of the loss function dominates this part and the more these dominates this part and the more these dominates this part and the more these these weights will un be unable to grow these weights will un be unable to grow these weights will un be unable to grow because as they grow because as they grow because as they grow they uh accumulate way too much loss they uh accumulate way too much loss they uh accumulate way too much loss",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1199,
      "text": "and so if this is strong enough",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1200,
      "text": "and so if this is strong enough",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1201,
      "text": "and so if this is strong enough then we are not able to overcome the then we are not able to overcome the then we are not able to overcome the force of this loss and we will never force of this loss and we will never force of this loss and we will never and basically everything will be uniform and basically everything will be uniform and basically everything will be uniform predictions predictions predictions",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1202,
      "text": "so i thought that's kind of cool",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1203,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1204,
      "text": "so i thought that's kind of cool",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1205,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1206,
      "text": "so i thought that's kind of cool",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1207,
      "text": "okay and lastly before we wrap up and lastly before we wrap up and lastly before we wrap up i wanted to show you how you would i wanted to show you how you would i wanted to show you how you would sample from this neural net model sample from this neural net model sample from this neural net model and i copy-pasted the sampling code from and i copy-pasted the sampling code from and i copy-pasted the sampling code from before before before",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1208,
      "text": "where remember that we sampled five where remember that we sampled five where remember that we sampled five times times times and all we did we start at zero we and all we did we start at zero we and all we did we start at zero we grabbed the current ix row of p and that grabbed the current ix row of p and that grabbed the current ix row of p and that was our probability row was our probability row was our probability row from which we sampled the next index and from which we sampled the next index and from which we sampled the next index and just accumulated that and just accumulated that and just accumulated that and break when zero break when zero break when zero and running this and running this and running this gave us these gave us these gave us these results still have the results still have the results still have the p in memory",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1209,
      "text": "so this is fine p in memory so this is fine p in memory",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1210,
      "text": "so this is fine now now the speed doesn't come from the row of b the speed doesn't come from the row of b the speed doesn't come from the row of b instead it comes from this neural net instead it comes from this neural net instead it comes from this neural net first we take ix first we take ix first we take ix",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1211,
      "text": "and we encode it into a one hot row of x",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1212,
      "text": "and we encode it into a one hot row of x",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1213,
      "text": "and we encode it into a one hot row of x inc inc inc this x inc multiplies rw this x inc multiplies rw this x inc multiplies rw which really just plucks out the row of which really just plucks out the row of which really just plucks out the row of w corresponding to ix really that's w corresponding to ix really that's w corresponding to ix really that's what's happening what's happening what's happening and that gets our logits",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1214,
      "text": "and then we and that gets our logits",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1215,
      "text": "and then we and that gets our logits",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1216,
      "text": "and then we normalize those low jets normalize those low jets normalize those low jets exponentiate to get counts and then exponentiate to get counts and then exponentiate to get counts and then normalize to get uh the distribution and normalize to get uh the distribution and normalize to get uh the distribution",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1217,
      "text": "and then we can sample from the distribution then we can sample from the distribution then we can sample from the distribution",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1218,
      "text": "so if i run this kind of anticlimactic or climatic kind of anticlimactic or climatic depending how you look at it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1219,
      "text": "but we get depending how you look at it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1220,
      "text": "but we get depending how you look at it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1221,
      "text": "but we get the exact same result the exact same result the exact same result um um and that's because this is in the and that's because this is in the",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1222,
      "text": "and that's because this is in the identical model not only does it achieve identical model not only does it achieve identical model not only does it achieve the same loss the same loss the same loss",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1223,
      "text": "but but as i mentioned these are identical as i mentioned these are identical as i mentioned these are identical models and this w is the log counts of models and this w is the log counts of models and this w is the log counts of what we've estimated before but we came what we've estimated before but we came what we've estimated before but we came to this answer in a very different way to this answer in a very different way to this answer in a very different way and it's got a very different",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1224,
      "text": "and it's got a very different",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1225,
      "text": "and it's got a very different interpretation but fundamentally this is interpretation but fundamentally this is interpretation but fundamentally this is basically the same model and gives the basically the same model and gives the basically the same model and gives the same samples here and so same samples here and so same samples here",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1226,
      "text": "and so that's kind of cool",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1227,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1228,
      "text": "so we've that's kind of cool",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1229,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1230,
      "text": "so we've that's kind of cool",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1231,
      "text": "okay",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1232,
      "text": "so we've actually covered a lot of ground we actually covered a lot of ground we actually covered a lot of ground we introduced the bigram character level introduced the bigram character level introduced the bigram character level language model language model language model we saw how we can train the model how we we saw how we can train the model how we we saw how we can train the model how we can sample from the model and how we can can sample from the model and how we can can sample from the model and how we can evaluate the quality of the model using evaluate the quality of the model using evaluate the quality of the model using the negative log likelihood loss the negative log likelihood loss the negative log likelihood loss and then we actually trained the model",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1233,
      "text": "and then we actually trained the model",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1234,
      "text": "and then we actually trained the model in two completely different ways that in two completely different ways that in two completely different ways that actually get the same result and the actually get the same result and the actually get the same result and the same model same model same model in the first way we just counted up the in the first way we just counted up the in the first way we just counted up the frequency of all the bigrams and frequency of all the bigrams and frequency of all the bigrams and normalized normalized in a second way we used the in a second way we used the in a second way we used the negative log likelihood loss as a guide negative log likelihood loss as a guide negative log likelihood loss as a guide to optimizing the counts matrix to optimizing the counts matrix to optimizing the counts matrix or the counts array so that the loss is or the counts array so that the loss is or the counts array so that the loss is minimized in the in a gradient-based minimized in the in a gradient-based minimized in the in a gradient-based framework and we saw that both of them framework and we saw that both of them framework and we saw that both of them give the same result give the same result give the same result",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1235,
      "text": "and and that's it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1236,
      "text": "that's it",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1237,
      "text": "that's it now the second one of these the now the second one of these the now the second one of these the gradient-based framework is much more gradient-based framework is much more gradient-based framework is much more flexible and right now our neural flexible and right now our neural flexible and right now our neural network is super simple we're taking a network is super simple we're taking a network is super simple we're taking a single previous character and we're single previous character and we're single previous character and we're taking it through a single linear layer taking it through a single linear layer taking it through a single linear layer to calculate the logits to calculate the logits to calculate the logits this is about to complexify so in the this is about to complexify so in the this is about to complexify so in the follow-up videos we're going to be follow-up videos we're going to be follow-up videos we're going to be taking more and more of these characters taking more and more of these characters taking more and more of these characters and we're going to be feeding them into and we're going to be feeding them into and we're going to be feeding them into a neural net",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1238,
      "text": "but this neural net will a neural net",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1239,
      "text": "but this neural net will a neural net",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1240,
      "text": "but this neural net will still output the exact same thing the still output the exact same thing the still output the exact same thing the neural net will output logits neural net will output logits neural net will output logits and these logits will still be and these logits will still be and these logits will still be normalized in the exact same way and all normalized in the exact same way and all normalized in the exact same way and all the loss and everything else and the the loss and everything else and the the loss and everything else and the gradient gradient-based framework gradient gradient-based framework gradient gradient-based framework everything stays identical it's just everything stays identical it's just everything stays identical it's just that this neural net will now complexify that this neural net will now complexify that this neural net will now complexify all the way to transformers all the way to transformers all the way to transformers so that's gonna be pretty awesome",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1241,
      "text": "and so that's gonna be pretty awesome",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1242,
      "text": "and so that's gonna be pretty awesome",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    },
    {
      "id": 1243,
      "text": "and i'm looking forward to it for now bye",
      "start_time": "00:00:02.230",
      "end_time": "01:57:46.719"
    }
  ]
}