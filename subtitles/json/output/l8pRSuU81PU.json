{
  "video_id": "l8pRSuU81PU",
  "sentences": [
    {
      "id": 1,
      "text": "hi everyone so today we are going to be hi everyone",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2,
      "text": "so today we are going to be continuing our Zero to Hero series and continuing our Zero to Hero series and continuing our Zero to Hero series and in particular today we are going to in particular today we are going to in particular today we are going to reproduce the gpt2 model the 124 million reproduce the gpt2 model the 124 million reproduce the gpt2 model the 124 million version of it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3,
      "text": "so when openi released version of it so when openi released version of it so when openi released gpt2 this was 2019 and they released it gpt2 this was 2019 and they released it gpt2 this was 2019 and they released it with this blog post on top of that they with this blog post on top of that they with this blog post on top of that they released this paper and on top of that released this paper and on top of that released this paper and on top of that they released this code on GitHub so they released this code on GitHub so they released this code on GitHub so open a/ open a/ open a/ gpt2 now when we talk about reproducing gpt2 now when we talk about reproducing gpt2 now when we talk about reproducing gpt2 we have to be careful because in gpt2 we have to be careful because in gpt2 we have to be careful because in particular in this video we're going to particular in this video we're going to particular in this video we're going to be reproducing the 124 million parameter be reproducing the 124 million parameter be reproducing the 124 million parameter model so the thing to realize is that model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 4,
      "text": "so the thing to realize is that model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 5,
      "text": "so the thing to realize is that there's always a miniseries when these there's always a miniseries when these there's always a miniseries when these are releases are made so there are the are releases are made so there are the are releases are made so there are the gpt2 miniseries made up of models at gpt2 miniseries made up of models at gpt2 miniseries made up of models at different sizes and usually the biggest different sizes and usually the biggest different sizes and usually the biggest model is called the model is called the model is called the gpt2 but basically the reason we do that gpt2 but basically the reason we do that gpt2 but basically the reason we do that is because you can put the model sizes is because you can put the model sizes is because you can put the model sizes on the x-axis of plots like this and on on the x-axis of plots like this and on on the x-axis of plots like this and on the Y AIS you put a lot of uh Downstream the Y AIS you put a lot of uh Downstream the Y AIS you put a lot of uh Downstream metrics that you're interested in like metrics that you're interested in like metrics that you're interested in like translation summarization question translation summarization question translation summarization question answering and so on and you can chart answering and so on and you can chart answering and so on and you can chart out these scaling laws so basically as out these scaling laws so basically as out these scaling laws so basically as the model size increases you're getting the model size increases you're getting the model size increases you're getting better and better at Downstream metrics better and better at Downstream metrics better and better at Downstream metrics and so in particular for and so in particular for and so in particular for gpt2 if we scroll down in paper there gpt2 if we scroll down in paper there gpt2 if we scroll down in paper there are four models in the gpt2 miniseries are four models in the gpt2 miniseries are four models in the gpt2 miniseries starting at 124 million all the way up starting at 124 million all the way up starting at 124 million all the way up to 1558 million now the reason my to 1558 million now the reason my to 1558 million now the reason my numbers the way I say them disagree with numbers the way I say them disagree with numbers the way I say them disagree with this table is that this table is wrong this table is that this table is wrong this table is that this table is wrong if you actually go to the uh gpt2 uh if you actually go to the uh gpt2 uh if you actually go to the uh gpt2 uh GitHub repo they sort of say that um GitHub repo they sort of say that um GitHub repo they sort of say that um there was an error in how they added up there was an error in how they added up there was an error in how they added up the parameters but basically this is the the parameters but basically this is the the parameters but basically this is the 124 million parameter model Etc so the 124 million parameter model Etc so the 124 million parameter model Etc so the 124 million parameter had 12 layers in 124 million parameter had 12 layers in 124 million parameter had 12 layers in the Transformer and it had 768 channels the Transformer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 6,
      "text": "and it had 768 channels the Transformer and it had 768 channels in the Transformer 768 dimensions and in the Transformer 768 dimensions and in the Transformer 768 dimensions and I'm going to be assuming some I'm going to be assuming some I'm going to be assuming some familiarity with what these terms mean familiarity with what these terms mean familiarity with what these terms mean because I covered all of this in my because I covered all of this in my because I covered all of this in my previous video let's build gpt2 uh let's previous video let's build gpt2 uh let's previous video let's build gpt2 uh let's build GPT from scratch so I covered that build GPT from scratch so I covered that build GPT from scratch so I covered that in the previous video in this playlist in the previous video in this playlist in the previous video in this playlist now if we do everything correctly and now if we do everything correctly and now if we do everything correctly and everything works out well by the end of everything works out well by the end of everything works out well by the end of this video we're going to see something this video we're going to see something this video we're going to see something like this where we're looking at the like this where we're looking at the like this where we're looking at the validation loss which basically um validation loss which basically um validation loss which basically um measures how good we are at predicting measures how good we are at predicting measures how good we are at predicting the next token in a sequence on some the next token in a sequence on some the next token in a sequence on some validation data that the model has not validation data that the model has not validation data that the model has not seen during training",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 7,
      "text": "and we see that we seen during training and we see that we seen during training and we see that we go from doing that task not very well go from doing that task not very well go from doing that task not very well because we're initializing from scratch because we're initializing from scratch because we're initializing from scratch all the way to doing that task quite all the way to doing that task quite all the way to doing that task quite well um by the end of the training and well um by the end of the training and well um by the end of the training and hopefully we're going to beat the gpt2 hopefully we're going to beat the gpt2 hopefully we're going to beat the gpt2 uh 124 M model uh 124 M model uh 124 M model now previously when they were working on now previously when they were working on now previously when they were working on this this is already 5 years ago",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 8,
      "text": "so this this this is already 5 years ago so this this this is already 5 years ago",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 9,
      "text": "so this was probably a fairly complicated was probably a fairly complicated was probably a fairly complicated optimization at the time and the gpus optimization at the time and the gpus optimization at the time and the gpus and the compute was a lot smaller today and the compute was a lot smaller today and the compute was a lot smaller today you can reproduce this model in roughly you can reproduce this model in roughly you can reproduce this model in roughly an hour or probably less even and it an hour or probably less even and it an hour or probably less even and it will cost you about 10 bucks if you want will cost you about 10 bucks if you want will cost you about 10 bucks if you want to do this on the cloud uh Cloud Compu a to do this on the cloud uh Cloud Compu a to do this on the cloud uh Cloud Compu a sort of computer that you can all rent sort of computer that you can all rent sort of computer that you can all rent and if you pay $10 for that computer you and if you pay $10 for that computer you and if you pay $10 for that computer you wait about an hour or less you can wait about an hour or less you can wait about an hour or less you can actually achieve a model that is as good actually achieve a model that is as good actually achieve a model that is as good as this model that open ey released",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 10,
      "text": "and as this model that open ey released",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 11,
      "text": "and as this model that open ey released",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 12,
      "text": "and uh one more thing to mention is unlike uh one more thing to mention is unlike uh one more thing to mention is unlike many other models open ey did release many other models open ey did release many other models open ey did release the weights for gpt2 so those weights the weights for gpt2 so those weights the weights for gpt2 so those weights are all available in this repository but are all available in this repository but are all available in this repository but the gpt2 paper is not always as good the gpt2 paper is not always as good the gpt2 paper is not always as good with all of the details of training so with all of the details of training so with all of the details of training so in addition to the gpt2 paper we're in addition to the gpt2 paper we're in addition to the gpt2 paper we're going to be referencing the gpt3 paper going to be referencing the gpt3 paper going to be referencing the gpt3 paper which is a lot more Concrete in a lot of which is a lot more Concrete in a lot of which is a lot more Concrete in a lot of the hyp parameters and optimization the hyp parameters and optimization the hyp parameters and optimization settings and so on um and it's not a settings and so on um and it's not a settings and so on um and it's not a huge departure in the architecture from huge departure in the architecture from huge departure in the architecture from the GPT 2 uh version of the model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 13,
      "text": "so the GPT 2 uh version of the model so the GPT 2 uh version of the model so we're going to be referencing both gpt2 we're going to be referencing both gpt2 we're going to be referencing both gpt2 and gpt3 as we try to reproduce gpt2 124 and gpt3 as we try to reproduce gpt2 124 and gpt3 as we try to reproduce gpt2 124 M",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 14,
      "text": "uh so let's go so the first thing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 15,
      "text": "I M",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 16,
      "text": "uh so let's go so the first thing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 17,
      "text": "I M",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 18,
      "text": "uh so let's go so the first thing I would like to do is actually start at would like to do is actually start at would like to do is actually start at the end or at the Target so in other the end or at the Target so in other the end or at the Target so in other words let's load the GPT to 124 M model words let's load the GPT to 124 M model words let's load the GPT to 124 M model as it was released by openi and maybe as it was released by openi and maybe as it was released by openi and maybe take it for a spin let's sample some take it for a spin let's sample some take it for a spin let's sample some tokens from it now the issue with that tokens from it now the issue with that tokens from it now the issue with that is when you go into the code base of is when you go into the code base of is when you go into the code base of gpt2 and you go into the source and you gpt2 and you go into the source and you gpt2 and you go into the source and you click in on the model.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 19,
      "text": "pi you'll realize click in on the model.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 20,
      "text": "pi you'll realize click in on the model.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 21,
      "text": "pi you'll realize that actually this is using tensorflow that actually this is using tensorflow that actually this is using tensorflow so the original gpt2 code here was so the original gpt2 code here was so the original gpt2 code here was written in tensor flow which is written in tensor flow which is written in tensor flow which is um you know not let's just say not used um you know not let's just say not used um you know not let's just say not used as much anymore um so we'd like to use as much anymore um so we'd like to use as much anymore um so we'd like to use pytorch uh because it's a lot friendlier pytorch uh because it's a lot friendlier pytorch uh because it's a lot friendlier easier",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 22,
      "text": "and I just personally like a lot easier",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 23,
      "text": "and I just personally like a lot easier",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 24,
      "text": "and I just personally like a lot more the problem with that is the more the problem with that is the more the problem with that is the initial code is intenser flow we'd like initial code is intenser flow we'd like initial code is intenser flow we'd like to use pytorch so instead uh to get the to use pytorch so instead uh to get the to use pytorch so instead uh to get the target we're going to use the hugging target we're going to use the hugging target we're going to use the hugging face Transformers um code which I like a face Transformers um code which I like a face Transformers um code which I like a lot more so when you go into the lot more so when you go into the lot more so when you go into",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 25,
      "text": "the Transformers source Transformers models Transformers source Transformers models Transformers source Transformers models gpt2 modeling gpt2 Pi you will see that gpt2 modeling gpt2 Pi you will see that gpt2 modeling gpt2 Pi you will see that they have the gpt2 implementation of they have the gpt2 implementation of they have the gpt2 implementation of that Transformer here in this that Transformer here in this that Transformer here in this file um and it's like medium readable file um and it's like medium readable file um and it's like medium readable but not fully readable um but what it but not fully readable um but what it but not fully readable um but what it does is it did all the work of does is it did all the work of does is it did all the work of converting all those weights uh from converting all those weights uh from converting all those weights uh from tensor flow to pytorch Friendly and so tensor flow to pytorch Friendly and so tensor flow to pytorch Friendly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 26,
      "text": "and so it's much easier to load and work with it's much easier to load and work with it's much easier to load and work with so in particular we can look at the so in particular we can look at the so in particular we can look at the gpt2 um model here and we can load it gpt2 um model here and we can load it gpt2 um model here and we can load it using hugging face Transformers so using hugging face Transformers so using hugging face Transformers so swinging over this is what that looks swinging over this is what that looks swinging over this is what that looks like from Transformers import the DP GT2 like from Transformers import the DP GT2 like from Transformers import the DP GT2 LM head model and then from pre-train LM head model and then from pre-train LM head model and then from pre-train gpt2 uh now one awkward thing about this gpt2 uh now one awkward thing about this gpt2 uh now one awkward thing about this is that when you do gpt2 as the model is that when you do gpt2 as the model is that when you do gpt2 as the model that we're loading this actually is the that we're loading this actually is the that we're loading this actually is the 124 million parameter model if you want 124 million parameter model if you want 124 million parameter model if you want the actual the gpt2 the 1.5 billion then the actual the gpt2 the 1.5 billion then the actual the gpt2 the 1.5 billion then you actually want to do- XL",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 27,
      "text": "so this is you actually want to do- XL",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 28,
      "text": "so this is you actually want to do- XL",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 29,
      "text": "so this is the 12 4 M our Target now what we're the 12 4 M our Target now what we're the 12 4 M our Target now what we're doing is when we actually get this we're doing is when we actually get this we're doing is when we actually get this we're initializing the uh pytorch NN module as initializing the uh pytorch NN module as initializing the uh pytorch NN module as defined here in this defined here in this defined here in this class from it I want to get just the class from it I want to get just the class from it I want to get just the state dict which is just a raw tensors state dict which is just a raw tensors state dict which is just a raw tensors so we just have um the tensors of that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 30,
      "text": "so we just have um the tensors of that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 31,
      "text": "so we just have um the tensors of that file and by the way here this is a file and by the way here this is a file and by the way here this is a jupyter notebook",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 32,
      "text": "uh but this is jupyter jupyter notebook",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 33,
      "text": "uh but this is jupyter jupyter notebook",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 34,
      "text": "uh but this is jupyter notebook running inside vs code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 35,
      "text": "uh so I notebook running inside vs code uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 36,
      "text": "so I notebook running inside vs code uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 37,
      "text": "so I like to work with it all in a single like to work with it all in a single like to work with it all in a single sort of interface",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 38,
      "text": "so I like to use vs sort of interface",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 39,
      "text": "so I like to use vs sort of interface",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 40,
      "text": "so I like to use vs code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 41,
      "text": "so this is the jupyter notebook code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 42,
      "text": "so this is the jupyter notebook code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 43,
      "text": "so this is the jupyter notebook extension inside the es extension inside the es extension inside the es code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 44,
      "text": "so when we get the state dick this code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 45,
      "text": "so when we get the state dick this code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 46,
      "text": "so when we get the state dick this is just a dict",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 47,
      "text": "so we can print the key is just a dict",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 48,
      "text": "so we can print the key is just a dict",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 49,
      "text": "so we can print the key and the value which is the tensor and and the value which is the tensor and and the value which is the tensor and let's just look at the shapes so these let's just look at the shapes so these let's just look at the shapes so these are sort of are sort of are sort of the uh different parameters inside the the uh different parameters inside the the uh different parameters inside the gbt2 model and their shape",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 50,
      "text": "so the W gbt2 model and their shape",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 51,
      "text": "so the W gbt2 model and their shape so the W weight for token weight for token weight for token embedding is of size embedding is of size embedding is of size 50257 by 768 where this is coming from 50257 by 768 where this is coming from 50257 by 768 where this is coming from is that we have is that we have is that we have 50257 tokens in the gpt2 vocabulary um 50257 tokens in the gpt2 vocabulary um 50257 tokens in the gpt2 vocabulary um and the tokens by the way these are and the tokens by the way these are and the tokens by the way these are exactly the tokens that we spoken about exactly the tokens that we spoken about exactly the tokens that we spoken about in the previous video on my tokenization in the previous video on my tokenization in the previous video on my tokenization Series so the previous videos just Series so the previous videos just Series so the previous videos just before this I go into a ton of detail on before this I go into a ton of detail on before this I go into a ton of detail on tokenization gpt2 tokenizer happens to tokenization gpt2 tokenizer happens to tokenization gpt2 tokenizer happens to have this many tokens for each have this many tokens for each have this many tokens for each token we have a 768 dimensional token we have a 768 dimensional token we have a 768 dimensional embedding that is the distributed embedding that is the distributed embedding that is the distributed representation that stands in for that representation that stands in for that representation that stands in for that token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 52,
      "text": "so each token is a little string token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 53,
      "text": "so each token is a little string token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 54,
      "text": "so each token is a little string piece and then the 768 numbers are the piece and then the 768 numbers are the piece and then the 768 numbers are the vector that represents that vector that represents that vector that represents that token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 55,
      "text": "and so this is just our lookup token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 56,
      "text": "and so this is just our lookup token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 57,
      "text": "and so this is just our lookup table for tokens and then here we have table for tokens and then here we have table for tokens and then here we have the lookup table for the positions so the lookup table for the positions so the lookup table for the positions so because gbt2 has a maximum sequence because gbt2 has a maximum sequence because gbt2 has a maximum sequence length of length of length of 1024 we have up to 1,24 positions that 1024 we have up to 1,24 positions that 1024 we have up to 1,24 positions that each token can be attending to in the each token can be attending to in the each token can be attending to in the past and every one of those positions in past and every one of those positions in past and every one of those positions in gpd2 has a fixed Vector of gpd2 has a fixed Vector of gpd2 has a fixed Vector of 768 that is learned by 768 that is learned by 768 that is learned by optimization um and so this is the optimization um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 58,
      "text": "and so this is the optimization um and so this is the position embedding and the token position embedding and the token position embedding and the token embedding um and then everything here is embedding um and then everything here is embedding um and then everything here is just the other weights and biases and just the other weights and biases and just the other weights and biases and everything else of this everything else of this everything else of this Transformer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 59,
      "text": "so when you just take for Transformer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 60,
      "text": "so when you just take for Transformer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 61,
      "text": "so when you just take for example the positional embeddings and example the positional embeddings and example the positional embeddings and flatten it out and take just the 20 flatten it out and take just the 20 flatten it out and take just the 20 elements you can see that these are just elements you can see that these are just elements you can see that these are just the parameters these are weights floats the parameters these are weights floats the parameters these are weights floats just we can take and we can plot them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 62,
      "text": "so just we can take and we can plot them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 63,
      "text": "so just we can take and we can plot them so these are the position embeddings and we these are the position embeddings and we these are the position embeddings and we get something like this and you can see get something like this and you can see get something like this and you can see that this has structure and it has that this has structure and it has that this has structure and it has structure because what we what we have structure because what we what we have structure because what we what we have here really is every Row in this here really is every Row in this here really is every Row in this visualization is a different position a visualization is a different position a visualization is a different position a fixed absolute position in um the range fixed absolute position in um the range fixed absolute position in um the range from 0 to from 0 to from 0 to 1024 and each row here is the 1024 and each row here is the 1024 and each row here is the representation of that position and so representation of that position and so representation of that position and so it has structure because these it has structure because these it has structure because these positional embeddings end up learning positional embeddings end up learning positional embeddings end up learning these sinusoids and cosiness um that these sinusoids and cosiness um that these sinusoids and cosiness um that sort of like represent each of these sort of like represent each of these sort of like represent each of these positions and uh each row here stands in positions and uh each row here stands in positions and uh each row here stands in for that position and is processed by for that position and is processed by for that position and is processed by the Transformer to recover all the the Transformer to recover all the the Transformer to recover all the relative positions and uh sort of relative positions and uh sort of relative positions and uh sort of realize which token is where and um realize which token is where and um realize which token is where and um attend to them depending on their attend to them depending on their attend to them depending on their position not just their position not just their position not just their content",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 64,
      "text": "so when we actually just look content so when we actually just look content so when we actually just look into an individual column inside these into an individual column inside these into an individual column inside these and I just grabbed three random columns and I just grabbed three random columns and I just grabbed three random columns you'll see that for example here we are you'll see that for example here we are you'll see that for example here we are focusing on every every single um focusing on every every single um focusing on every every single um Channel and we're looking Channel and we're looking Channel and we're looking at what that channel is doing as a at what that channel is doing as a at what that channel is doing as a function of uh position from one from Z function of uh position from one from Z function of uh position from one from Z to to to 1223 1223 1223 really and we can see that some of these really",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 65,
      "text": "and we can see that some of these really",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 66,
      "text": "and we can see that some of these channels basically like respond more or channels basically like respond more or channels basically like respond more or less to different parts of the position less to different parts of the position less to different parts of the position Spectrum so this green channel uh really Spectrum so this green channel uh really Spectrum",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 67,
      "text": "so this green channel uh really likes to fire for everything after 200 likes to fire for everything after 200 likes to fire for everything after 200 uh up to 800 but not less a lot less and uh up to 800 but not less a lot less and uh up to 800 but not less a lot less and has a sharp drop off here near zero so has a sharp drop off here near zero so has a sharp drop off here near zero so who knows what these embeddings are who knows what these embeddings are who knows what these embeddings are doing and why they are the way they are doing and why they are the way they are doing and why they are the way they are you can tell for example that because you can tell for example that because you can tell for example that because they're a bit more Jagged and they're they're a bit more Jagged and they're they're a bit more Jagged and they're kind of noisy you can tell that this kind of noisy you can tell that this kind of noisy you can tell that this model was not fully trained and the more model was not fully trained and the more model was not fully trained and the more trained this model was the more you trained this model was the more you trained this model was the more you would expect to smooth this out and so would expect to smooth this out and so would expect to smooth this out and so this is telling you that this is a this is telling you that this is a this is telling you that this is a little bit of an undertrained model um little bit of an undertrained model um little bit of an undertrained model um but in principle actually these curves but in principle actually these curves but in principle actually these curves don't even have to be smooth this should don't even have to be smooth this should don't even have to be smooth this should just be totally random noise and in fact just be totally random noise and in fact just be totally random noise and in fact in the beginning of the optimization it in the beginning of the optimization it in the beginning of the optimization it is complete random noise because this is complete random noise because this is complete random noise because this position embedding table is initialized position embedding table is initialized position embedding table is initialized completely at random so in the beginning completely at random so in the beginning completely at random so in the beginning you have jaggedness and the fact that you have jaggedness and the fact that you have jaggedness and the fact that you end up with something smooth is you end up with something smooth",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 68,
      "text": "is you end up with something smooth is already kind of impressive um that that already kind of impressive um that that already kind of impressive um that that just falls out of the optimization just falls out of the optimization just falls out of the optimization because in principle you shouldn't even because in principle you shouldn't even because in principle you shouldn't even be able to get any single graph out of be able to get any single graph out of be able to get any single graph out of this that makes sense",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 69,
      "text": "but we actually this that makes sense",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 70,
      "text": "but we actually this that makes sense",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 71,
      "text": "but we actually get something that looks a little bit get something that looks a little bit get something that looks a little bit noisy",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 72,
      "text": "but for the most part looks noisy but for the most part looks noisy but for the most part looks sinusoidal like um in the original sinusoidal like um in the original sinusoidal like um in the original Transformer um in the original Transformer um in the original Transformer um in the original Transformer paper the attention is all Transformer paper the attention is all Transformer paper the attention is all you need paper the positional embeddings you need paper the positional embeddings you need paper the positional embeddings are actually initialized and fixed if I are actually initialized and fixed if I are actually initialized and fixed if I remember correctly to sinusoids and remember correctly to sinusoids and remember correctly to sinusoids and cosiness of uh different frequencies and cosiness of uh different frequencies and cosiness of uh different frequencies and that's the positional coding",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 73,
      "text": "and it's that's the positional coding",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 74,
      "text": "and it's that's the positional coding",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 75,
      "text": "and it's fixed but in gpt2 these are just fixed but in gpt2 these are just fixed but in gpt2 these are just parameters and they're trained from parameters and they're trained from parameters and they're trained from scratch just like any other parameter uh scratch just like any other parameter uh scratch just like any other parameter",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 76,
      "text": "uh and that seems to work about as well and and that seems to work about as well and and that seems to work about as well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 77,
      "text": "and so what they do is they kind of like so what they do is they kind of like so what they do is they kind of like recover these sinusoidal like features recover these sinusoidal like features recover these sinusoidal like features during the during the during the optimization we can also look at any of optimization we can also look at any of optimization we can also look at any of the other matrices here so here I took the other matrices here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 78,
      "text": "so here I took the other matrices here so here I took the first layer of the the first layer of the the first layer of the Transformer and looking at like one of Transformer and looking at like one of Transformer and looking at like one of its weights and just the first block of its weights and just the first block of its weights and just the first block of 300 by 300 and you see some structure 300 by 300 and you see some structure 300 by 300 and you see some structure but like again like who knows what any but like again like who knows what any but like again like who knows what any of this is if you're into mechanistic of this is if you're into mechanistic of this is if you're into mechanistic interpretability you might get a real interpretability you might get a real interpretability you might get a real kick out of trying to figure out like kick out of trying to figure out like kick out of trying to figure out like what is going on what is this structure what is going on what is this structure what is going on what is this structure and what does this all mean but we're and what does this all mean but we're and what does this all mean but we're not going to be doing that in this video not going to be doing that in this video not going to be doing that in this video but we definitely see that there's some",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 79,
      "text": "but we definitely see that there's some",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 80,
      "text": "but we definitely see that there's some interesting structure and that's kind of interesting structure and that's kind of interesting structure and that's kind of cool what we're mostly interested in is cool what we're mostly interested in is cool what we're mostly interested in is we've loaded the weights of this model we've loaded the weights of this model we've loaded the weights of this model that was released by open Ai and now that was released by open Ai and now that was released by open Ai and now using the hogging face Transformers we using the hogging face Transformers we using the hogging face Transformers we can not just get all the raw weights but can not just get all the raw weights but can not just get all the raw weights",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 81,
      "text": "but we can also get the um what they call we can also get the um what they call we can also get the um what they call Pipeline and sample from it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 82,
      "text": "so this is Pipeline and sample from it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 83,
      "text": "so this is Pipeline and sample from it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 84,
      "text": "so this is the prefix",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 85,
      "text": "hello I'm a language model the prefix",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 86,
      "text": "hello I'm a language model the prefix",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 87,
      "text": "hello I'm a language model comma",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 88,
      "text": "and then we're sampling uh 30 comma",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 89,
      "text": "and then we're sampling uh 30 comma",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 90,
      "text": "and then we're sampling uh 30 tokens and we getting five sequences and tokens and we getting five sequences and tokens and we getting five sequences",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 91,
      "text": "and I ran this and this is what it produced I ran this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 92,
      "text": "and this is what it produced I ran this and this is what it produced um hell language um hell language um hell language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 93,
      "text": "but what I'm really doing is model but what I'm really doing is model but what I'm really doing is making a human readable document there making a human readable document there making a human readable document there are other languages",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 94,
      "text": "but those are dot are other languages but those are dot are other languages but those are dot dot dot so you can read through these if dot dot so you can read through these if dot dot so you can read through these if you like but basically these are five you like but basically these are five you like but basically these are five different completions of the same prefix different completions of the same prefix different completions of the same prefix from this uh gbt from this uh gbt from this uh gbt 2124m now uh if I go here I took this 2124m now uh if I go here I took this 2124m now uh if I go here I took this example from here and sadly even though example from here and sadly even though example from here and sadly even though we are fixing the seed we are getting we are fixing the seed we are getting we are fixing the seed we are getting different Generations from the snippet different Generations from the snippet different Generations from the snippet than what they got so presumably the than what they got so presumably the than what they got so presumably the code changed um but what we see though code changed um but what we see though code changed um but what we see though at this stage that's important is that at this stage that's important is that at this stage that's important is that we are getting coherent text so we've we are getting coherent text",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 95,
      "text": "so we've we are getting coherent text so we've loaded the model successfully we can loaded the model successfully we can loaded the model successfully we can look at all its parameters and the keys look at all its parameters and the keys look at all its parameters and the keys tell us where in the model these come tell us where in the model these come tell us where in the model these come from and we want to actually write our from and we want to actually write our from",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 96,
      "text": "and we want to actually write our own gpt2 class so that we have full own gpt2 class so that we have full own gpt2 class so that we have full understanding of what's happening there understanding of what's happening there understanding of what's happening there we don't want to be working with we don't want to be working with we don't want to be working with something like uh the modeling gpt2 Pi something like uh the modeling gpt2 Pi something like uh the modeling gpt2 Pi because it's just too complicated we because it's just too complicated we because it's just too complicated we want to write this from scratch want to write this from scratch want to write this from scratch ourselves",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 97,
      "text": "so we're going to be ourselves",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 98,
      "text": "so we're going to be ourselves",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 99,
      "text": "so we're going to be implementing the GPT model here in implementing the GPT model here in implementing the GPT model here in parallel and as our first task let's parallel and as our first task let's parallel and as our first task let's load the gpt2 124 M into the class that load the gpt2 124 M into the class that load the gpt2 124 M into the class that we're going to develop here from scratch we're going to develop here from scratch we're going to develop here from scratch that's going to give us confidence that that's going to give us confidence that that's going to give us confidence that we can load the open ey model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 100,
      "text": "and we can load the open ey model and we can load the open ey model and therefore there's a setting of Weights therefore there's a setting of Weights therefore there's a setting of Weights that exactly is the 124 model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 101,
      "text": "but then that exactly is the 124 model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 102,
      "text": "but then that exactly is the 124 model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 103,
      "text": "but then of course what we're going to do is of course what we're going to do is of course what we're going to do is we're going to initialize the model from we're going to initialize the model from we're going to initialize the model from scratch instead and try try to train it scratch instead and try try to train it scratch instead and try try to train it ourselves um on a bunch of documents ourselves um on a bunch of documents ourselves um on a bunch of documents that we're going to get",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 104,
      "text": "and we're going that we're going to get",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 105,
      "text": "and we're going that we're going to get",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 106,
      "text": "and we're going to try to surpass that model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 107,
      "text": "so we're to try to surpass that model so we're to try to surpass that model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 108,
      "text": "so we're going to get different weights and going to get different weights and going to get different weights and everything's going to look different everything's going to look different everything's going to look different hopefully better even um hopefully better even um hopefully better even um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 109,
      "text": "but uh we're going to have a lot of",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 110,
      "text": "but uh we're going to have a lot of",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 111,
      "text": "but uh we're going to have a lot of confidence that because we can load the confidence that because we can load the confidence that because we can load the openi model we are in the same model openi model we are in the same model openi model we are in the same model family and model class and we just have family and model class",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 112,
      "text": "and we just have family and model class",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 113,
      "text": "and we just have to ReDiscover a good setting of the to ReDiscover a good setting of the to ReDiscover a good setting of the weights uh but from scratch so let's now weights uh but from scratch so let's now weights uh but from scratch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 114,
      "text": "so let's now write the gbt2 model and let's load the write the gbt2 model and let's load the write the gbt2 model and let's load the weights and make sure that we can also weights and make sure that we can also weights and make sure that we can also generate text that looks coherent okay generate text that looks coherent okay generate text that looks coherent",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 115,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 116,
      "text": "so let's now swing over to the attention so let's now swing over to the attention so let's now swing over to the attention is all un need paper that started is all un need paper that started is all un need paper that started everything and let's scroll over to the everything and let's scroll over to the everything and let's scroll over to the model architecture the original model architecture the original model architecture the original Transformer now remember that gpt2 is Transformer now remember that gpt2 is Transformer now remember that gpt2 is slightly modified from the or or slightly modified from the or or slightly modified from the or or Transformer in particular we do not have Transformer in particular we do not have Transformer in particular we do not have uh the encoder gpt2 is a decoder only uh the encoder gpt2 is a decoder only uh the encoder gpt2 is a decoder only Transformer as we call it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 117,
      "text": "so this entire Transformer as we call it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 118,
      "text": "so this entire Transformer as we call it so this entire encoder here is missing in addition to encoder here is missing in addition to encoder here is missing in addition to that this cross attention here that was that this cross attention here that was that this cross attention here that was using that encoder is also missing so we using that encoder is also missing so we using that encoder is also missing so we delete this entire part everything else delete this entire part everything else delete this entire part everything else stays almost the same but there are some stays almost the same but there are some stays almost the same",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 119,
      "text": "but there are some differences that we're going to uh sort differences that we're going to uh sort differences that we're going to uh sort of look at here so there are two main of look at here so there are two main of look at here so there are two main differences when we go to the gb2 page differences when we go to the gb2 page differences when we go to the gb2 page under 2.3 model we notice that first under 2.3 model we notice that first under 2.3 model we notice that first there's a reshuffling of the layer Norms there's a reshuffling of the layer Norms there's a reshuffling of the layer Norms so they change place and second an so they change place and second an so they change place and second an additional layer normalization was added additional layer normalization was added additional layer normalization was added here to the final self detention block here to the final self detention block here to the final self detention block",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 120,
      "text": "so basically all the layer Norms here so basically all the layer Norms here so basically all the layer Norms here instead of being after the MLP or after instead of being after the MLP or after instead of being after the MLP or after the attention they SN before it and an the attention they SN before it and an the attention they SN before it and an additional layer Norm gets added here additional layer Norm gets added here additional layer Norm gets added here right before the final right before the final right before the final classifier so now let's Implement some classifier so now let's Implement some classifier so now let's Implement some of the first sort of skeleton NN module of the first sort of skeleton NN module of the first sort of skeleton NN module modules here in our GPT NN module and in modules here in our GPT NN module and in modules here in our GPT NN module and in particular we're going to try to match particular we're going to try to match particular we're going to try to match up this schema here that is used by up this schema here that is used by up this schema here that is used by hugging face Transformers because that hugging face Transformers because that hugging face Transformers because that will make it much easier to load these will make it much easier to load these will make it much easier to load these weights from this state dict",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 121,
      "text": "so we want weights from this state dict",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 122,
      "text": "so we want weights from this state dict",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 123,
      "text": "so we want something that reflects uh this schema something that reflects uh this schema something that reflects uh this schema here so here's what I came up with here so here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 124,
      "text": "'s what I came up with here so here's what I came up with um basically we see that the main um basically we see that the main um basically we see that the main container here that has all the modules container here that has all the modules container here that has all the modules is called Transformer so I'm reflecting is called Transformer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 125,
      "text": "so I'm reflecting is called Transformer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 126,
      "text": "so I'm reflecting that with an NN module dict and this is that with an NN module dict and this is that with an NN module dict and this is basically a module that allows you to basically a module that allows you to basically a module that allows you to index into the subm modules using keys index into the subm modules using keys index into the subm modules using keys just like a dictionary uh just like a dictionary uh just like a dictionary uh strings within it we have the weights of strings within it we have the weights of strings within it we have the weights of the token embeddings",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 127,
      "text": "WT",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 128,
      "text": "and that's an N the token embeddings WT and that's an N the token embeddings WT and that's an N embedding and the weights of the embedding and the weights of the embedding and the weights of the position embeddings which is also just position embeddings which is also just position embeddings which is also just an N embedding and if you remember n an N embedding and if you remember n an N embedding and if you remember n embedding is really just a fancy little embedding is really just a fancy little embedding is really just a fancy little wrapper module around just a single um wrapper module around just a single um wrapper module around just a single um single array of numbers a single uh single array of numbers a single uh single array of numbers a single uh block of numbers just like this it's a block of numbers just like this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 129,
      "text": "it's a block of numbers just like this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 130,
      "text": "it's a single tensor and an embedding is a single tensor and an embedding is a single tensor and an embedding is a glorified um wrapper around a tensor glorified um wrapper around a tensor glorified um wrapper around a tensor that allows you to access its elements that allows you to access its elements that allows you to access its elements uh by indexing into the uh by indexing into the uh by indexing into the rows now in addition to that we see here rows now in addition to that we see here rows now in addition to that we see here that we have a h",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 131,
      "text": "and then there's a this that we have a h",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 132,
      "text": "and then there's a this that we have a h",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 133,
      "text": "and then there's a this is index using numbers instead of is index using numbers instead of is",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 134,
      "text": "index using numbers instead of indexed using strings so there's a h. 0 indexed using strings so there's a h. 0 indexed using strings so there's a h. 0 1 2",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 135,
      "text": "Etc all the way up till h. 11 and 1 2 Etc all the way up till h. 11 and 1 2 Etc all the way up till h. 11 and that's because there are 12 layers here that's because there are 12 layers here that's because there are 12 layers here in this Transformer so to reflect that in this Transformer so to reflect that in this Transformer so to reflect that I'm creating also an H I think that I'm creating also an H",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 136,
      "text": "I think that I'm creating also an H",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 137,
      "text": "I think that probably stands for hidden and instead probably stands for hidden and instead probably stands for hidden and instead of a module dict this is a model list so of a module dict this is a model list so of a module dict this is a model list so we can index it using integers exactly we can index it using integers exactly we can index it using integers exactly as we see here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 138,
      "text": "01 2 Etc and the modular as we see here 01 2 Etc and the modular as we see here 01 2 Etc and the modular list has a n layer blocks and the blocks list has a n layer blocks and the blocks list has a n layer blocks and the blocks are yet to be defined in a module in a are yet to be defined in a module in a are yet to be defined in a module in a bit in addition to that following the bit in addition to that following the bit in addition to that following the gpt2 paper we have we need an additional gpt2 paper we have we need an additional gpt2 paper we have we need an additional final layer Norm that we're going to put final layer Norm that we're going to put final layer Norm that we're going to put in there and then we have the final in there",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 139,
      "text": "and then we have the final in there",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 140,
      "text": "and then we have the final classifier uh the language model head classifier uh the language model head classifier uh the language model head which um projects from 768 the number of which um projects from 768 the number of which um projects from 768 the number of embedding dimensions in this GPT all the embedding dimensions in this GPT all the embedding dimensions in this GPT all the way to the vocab size which is way to the vocab size which is way to the vocab size which is 50257 and gpt2 uses no bias for this 50257 and gpt2 uses no bias for this 50257 and gpt2 uses no bias for this final uh sort of projection",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 141,
      "text": "so this is final uh sort of projection so this is final uh sort of projection so this is the skeleton and you can see that it the skeleton and you can see that it the skeleton and you can see that it reflects this so the wte is the token reflects this so the wte is the token reflects this so the wte is the token embeddings here it's called output embeddings here it's called output embeddings here it's called output embedding but it's really the token embedding",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 142,
      "text": "but it's really the token embedding",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 143,
      "text": "but it's really the token embeddings the PE is the positional embeddings the PE is the positional embeddings the PE is the positional codings uh those two pieces of codings uh those two pieces of codings uh those two pieces of information as we saw previously are information as we saw previously are information as we saw previously are going to add and then go into the going to add and then go into the going to add and then go into the Transformer the H is the all the blocks",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 144,
      "text": "Transformer the H is the all the blocks Transformer the H is the all the blocks in Gray and the LNF is this new layer in Gray and the LNF is this new layer in Gray and the LNF is this new layer that gets added here by the gpt2 model that gets added here by the gpt2 model that gets added here by the gpt2 model and LM head is this linear part here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 145,
      "text": "so and LM head is this linear part here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 146,
      "text": "so and LM head is this linear part here so that's the skeleton of the gpt2 we now that's the skeleton of the gpt2 we now that's the skeleton of the gpt2 we now have to implement the block",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 147,
      "text": "okay so have to implement the block",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 148,
      "text": "okay so have to implement the block",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 149,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 150,
      "text": "so let's now recurse to the block itself so let's now recurse to the block itself so let's now recurse to the block itself so we want to define the block",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 151,
      "text": "um so I'll we want to define the block",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 152,
      "text": "um so I'll we want to define the block",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 153,
      "text": "um so I'll start putting them here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 154,
      "text": "so the block I start putting them here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 155,
      "text": "so the block I start putting them here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 156,
      "text": "so the block I like to write out like like to write out like like to write out like this uh these are some of the this uh these are some of the this uh these are some of the initializations and then this is the initializations and then this is the initializations and then this is the actual forward pass of what this block actual forward pass of what this block actual forward pass of what this block computes and notice here that there's a computes and notice here that there's a computes and notice here that there's a change from the Transformer again that change from the Transformer again that change from the Transformer again that is mentioned in the gpt2 paper so here is mentioned in the gpt2 paper so here is mentioned in the gpt2 paper so here the layer normalizations are after the the layer normalizations are after the the layer normalizations are after the application of attention or feed forward application of attention or feed forward application of attention or feed forward in addition to that note that the in addition to that note that the in addition to that note that the normalizations are inside the residual normalizations are inside the residual normalizations are inside the residual stream you see how feed forward is stream you see how feed forward is stream you see how feed forward is applied and this arrow goes through and applied and this arrow goes through and applied and this arrow goes through and through the normalization so that means through the normalization so that means through the normalization so that means that your residual pathway has that your residual pathway has that your residual pathway has normalizations inside them and this is normalizations inside them and this is normalizations inside them and this is not very good or desirable uh you not very good or desirable uh you not very good or desirable uh you actually prefer to have a single uh actually prefer to have a single uh actually prefer to have a single uh clean residual stream all the way from clean residual stream all the way from clean residual stream all the way from supervision all the way down to the supervision all the way down to the supervision all the way down to the inputs the tokens and this is very inputs the tokens and this is very inputs the tokens and this is very desirable and nice because the gradients desirable and nice because the gradients desirable and nice because the gradients that flow from the top if you remember that flow from the top if you remember that flow from the top if you remember from your microad addition just from your microad addition just from your microad addition just distributes gradients during the distributes gradients during the distributes gradients during the backwards state to both of its branches backwards state to both of its branches backwards state to both of its branches equally so addition is a branch in the equally so addition is a branch in the equally so addition is a branch in the gradients and so that means that the gradients and so that means that the gradients and so that means that the gradients from the top flows straight to gradients from the top flows straight to gradients from the top flows straight to the inputs the tokens through the the inputs the tokens through the the inputs the tokens through the residual Pathways unchanged but then in residual Pathways unchanged but then in residual Pathways unchanged but then in addition to that the gradient also flows addition to that the gradient also flows addition to that the gradient also flows through the blocks and the blocks you through the blocks and the blocks you through the blocks and the blocks you know contribute their own contribution know contribute their own contribution know contribute their own contribution over time and kick in and change the over time and kick in and change the over time and kick in and change the optimization over time but basically optimization over time but basically optimization over time but basically clean residual pathway is desirable from clean residual pathway is desirable from clean residual pathway is desirable from an optimization perspective and then the an optimization perspective and then the an optimization perspective",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 157,
      "text": "and then the this is the pre-normalization version this is the pre-normalization version this is the pre-normalization version where you see that RX first goes through where you see that RX first goes through where you see that RX first goes through the layer normalization and then the the layer normalization and then the the layer normalization and then the attention and then goes uh back out to attention and then goes uh back out to attention and then goes uh back out to go to the L ration number two and the go to the L ration number two and the go to the L ration number two and the multia perceptron sometimes also multia perceptron sometimes also multia perceptron sometimes also referred to as a feed forward Network or referred to as a feed forward Network or referred to as a feed forward Network or an FFN and then that goes into the an FFN and then that goes into the an FFN and then that goes into the residual stream again and the one more residual stream again and the one more residual stream again and the one more thing that is kind of interesting to thing that is kind of interesting to thing that is kind of interesting to note is that recall that attention is a note is that recall that attention is a note is that recall that attention is a communication operation it is where all communication operation it is where all communication operation it is where all the tokens and there's 1,24 tokens lined the tokens and there's 1,24 tokens lined the tokens and there's 1,24 tokens lined up in a sequence and this is where the up in a sequence and this is where the up in a sequence and this is where the tokens communicate this is where they tokens communicate this is where they tokens communicate this is where they exchange information so attention is a exchange information so attention is a exchange information so attention is a um aggregation function it's a pooling um aggregation function it's a pooling um aggregation function it's a pooling function it's a weighted sum function it function it's a weighted sum function it function it's a weighted sum function it is a reduce operation whereas MLP this is a reduce operation whereas MLP this is a reduce operation whereas MLP this uh MLP here happens at every single uh MLP here happens at every single uh MLP here happens at every single token individually there's no token individually there's no token individually there's no information being collected or exchanged information being collected or exchanged information being collected or exchanged between the tokens so the attention is between the tokens so the attention is between the tokens so the attention is the reduce and the MLP is the map and the reduce and the MLP is the map and the reduce and the MLP is the map and what you end up with is that the what you end up with is that the what you end up with is that the Transformer just ends up just being a Transformer just ends up just being a Transformer just ends up just being a repeated application of map produce if repeated application of map produce if repeated application of map produce if you want to think about it that way so you want to think about it that way so you want to think about it that way",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 158,
      "text": "so um this is where they communicate and um this is where they communicate and um this is where they communicate and this is where they think individually this is where they think individually this is where they think individually about the information that they gathered about the information that they gathered about the information that they gathered and every one of these blocks uh and every one of these blocks uh and every one of these blocks uh iteratively refines the um iteratively refines the um iteratively refines the um representation is at the residual stream representation is at the residual stream representation is at the residual stream",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 159,
      "text": "so this is our block um slightly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 160,
      "text": "so this is our block um slightly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 161,
      "text": "so this is our block um slightly modified from this picture",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 162,
      "text": "Okay so let's modified from this picture",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 163,
      "text": "Okay so let's modified from this picture",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 164,
      "text": "Okay so let's now move on to the MLP so the MLP block now move on to the MLP so the MLP block now move on to the MLP so the MLP block uh I implemented as follows uh I implemented as follows uh I implemented as follows it is relatively straightforward we it is relatively straightforward we it is relatively straightforward we basically have two linear projections basically have two linear projections basically have two linear projections here that are sandwiched in between the here that are sandwiched in between the here that are sandwiched in between the G G G nonlinearity so nn.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 165,
      "text": "G approximate is 10h nonlinearity so nn.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 166,
      "text": "G approximate is 10h nonlinearity so nn.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 167,
      "text": "G approximate is 10h now when we swing on uh swing over to now when we swing on uh swing over to now when we swing on uh swing over to the Pyro documentation this is n.g and the Pyro documentation this is n.g and the Pyro documentation this is n.g and it has this format and it has two it has this format and it has two it has this format and it has two versions the original version of G which versions the original version of G which versions the original version of G which we'll step into into in a bit and the we'll step into into in a bit and the we'll step into into in a bit and the approximate version of Galo which we can approximate version of Galo which we can approximate version of Galo which we can request using request using request using 10 so as you can see just as a preview 10 so as you can see just as a preview 10 so as you can see just as a preview here G is a basically like a reu except here G is a basically like a reu except here G is a basically like a reu except there's no flat exactly Flat Tail here there's no flat exactly Flat Tail here there's no flat exactly Flat Tail here at exactly zero but otherwise it looks at exactly zero but otherwise it looks at exactly zero but otherwise it looks very much like a slightly smoother reu very much like a slightly smoother reu very much like a slightly smoother reu it comes from this paper here Gan error it comes from this paper here Gan error",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 168,
      "text": "it comes from this paper here Gan error linear units and uh you can step through linear units and uh you can step through linear units and uh you can step through this paper and there's some mathematical this paper",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 169,
      "text": "and there's some mathematical this paper",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 170,
      "text": "and there's some mathematical calac reasoning that leads to an calac reasoning that leads to an calac reasoning that leads to an interpretation that leads to the interpretation that leads to the interpretation that leads to the specific formulation it has to do with specific formulation it has to do with specific formulation it has to do with stochastic radial risers and the stochastic radial risers and the stochastic radial risers and the expectation of a modification to expectation of a modification to expectation of a modification to Adaptive dropout so you can read through Adaptive dropout so you can read through Adaptive dropout so you can read through all of that if you'd like here and all of that if you'd like here and all of that if you'd like here and there's a little bit of history as to there's a little bit of history as to there's a little bit of history as to why there is an an approximate version why there is an an approximate version why there is an an approximate version of G and that comes from this issue here of G and that comes from this issue here of G and that comes from this issue here as far as I can tell and in this issue as far as I can tell and in this issue as far as I can tell and in this issue Daniel Hendrix mentions that at the time Daniel Hendrix mentions that at the time Daniel Hendrix mentions that at the time when they developed this nonlinearity when they developed this nonlinearity when they developed this nonlinearity the Earth function which you need to the Earth function which you need to the Earth function which you need to evaluate the exact G was very slow in evaluate the exact G was very slow in evaluate the exact G was very slow in tensor flow",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 171,
      "text": "so they ended up basically tensor flow",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 172,
      "text": "so they ended up basically tensor flow",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 173,
      "text": "so they ended up basically developing this approximation and this developing this approximation and this developing this approximation and this approximation that then ended up being approximation that then ended up being approximation that then ended up being picked up by Bert and by GP P2 Etc but picked up by Bert and by GP P2 Etc but picked up by Bert and by GP P2 Etc",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 174,
      "text": "but today there's no real good reason to use today there's no real good reason to use today there's no real good reason to use the approximate version you'd prefer to the approximate version you'd prefer to the approximate version you'd prefer to just use the exact version um because I just use the exact version um because I just use the exact version um because I my expectation is that there's no big my expectation is that there's no big my expectation is that there's no big difference anymore and this is kind of difference anymore and this is kind of difference anymore and this is kind of like a historical um kind of Quirk um like a historical um kind of Quirk um like a historical um kind of Quirk",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 175,
      "text": "um but we are trying to reproduce gpt2",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 176,
      "text": "but we are trying to reproduce gpt2",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 177,
      "text": "but we are trying to reproduce gpt2 exactly and gpt2 used the 10h exactly and gpt2 used the 10h exactly and gpt2 used the 10h approximate version so we prefer to approximate version so we prefer to approximate version so we prefer to stick with stick with stick with that um now one other reason to actually that um now one other reason to actually that um now one other reason to actually just intuitively use G instead of veru just intuitively use G instead of veru just intuitively use G instead of veru is previously in the in videos in the is previously in the in videos in the is previously in the in videos in the past we've spoken about the dead reu past we've spoken about the dead reu past we've spoken about the dead reu neuron problem where in this tale of a neuron problem where in this tale of a neuron problem where in this tale of a reu if it's exactly flat at zero any reu if it's exactly flat at zero any reu if it's exactly flat at zero any activations that fall there will get activations that fall there will get activations that fall there will get exactly zero gradient there's no change",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 178,
      "text": "exactly zero gradient there's no change",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 179,
      "text": "exactly zero gradient there's no change there's no adaptation there's no there's no adaptation there's no there's no adaptation there's no development of the network if any of development of the network if any of development of the network if any of these activations end in this flat these activations end in this flat these activations end in this flat region but the G always contributes a region but the G always contributes a region but the G always contributes a local gradient and so there's always local gradient and so there's always local gradient and so there's always going to be a change always going to be going to be a change always going to be going to be a change always going to be an adaptation and sort of smoothing it an adaptation and sort of smoothing it an adaptation and sort of smoothing it out ends up empirically working better out ends up empirically working better out ends up empirically working better in practice as demonstrated in this in practice as demonstrated in this in practice as demonstrated in this paper and also as demonstrated by it paper and also as demonstrated by it paper and also as demonstrated by it being picked up by the bird paper gbt2 being picked up by the bird paper gbt2 being picked up by the bird paper gbt2 paper and so on so for that reason we paper and so on so for that reason we paper and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 180,
      "text": "so on so for that reason we adopt this nonlinearity uh here in the adopt this nonlinearity uh here in the adopt this nonlinearity uh here in the 10 in the gbt2 reproduction now in more 10 in the gbt2 reproduction now in more 10 in the gbt2 reproduction now in more modern networks also like llama 3 and so modern networks also like llama 3 and so modern networks also like llama 3 and so on this nonlinearity also further on this nonlinearity also further on this nonlinearity also further changes uh to swiglo and other variants changes uh to swiglo and other variants changes uh to swiglo and other variants like that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 181,
      "text": "uh but for gpt2 they Ed this like that uh but for gpt2 they Ed this like that uh but for gpt2 they Ed this approximate approximate approximate G okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 182,
      "text": "and finally we have the attention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 183,
      "text": "G",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 184,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 185,
      "text": "and finally we have the attention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 186,
      "text": "G",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 187,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 188,
      "text": "and finally we have the attention operation so let me paste in my operation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 189,
      "text": "so let me paste in my operation so let me paste in my attention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 190,
      "text": "so I know this is a lot",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 191,
      "text": "so I'm going to",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 192,
      "text": "so I know this is a lot",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 193,
      "text": "so I'm going to go through this a bit quickly a bit go through this a bit quickly a bit go through this a bit quickly a bit slowly but not too slowly because we slowly but not too slowly because we slowly but not too slowly because we have covered this in the previous video have covered this in the previous video have covered this in the previous video",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 194,
      "text": "and I would just point you there",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 195,
      "text": "um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 196,
      "text": "so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 197,
      "text": "and I would just point you there",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 198,
      "text": "um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 199,
      "text": "so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 200,
      "text": "and I would just point you there",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 201,
      "text": "um so this is the attention operation now in this is the attention operation now in this is the attention operation now in the previous video you will remember the previous video you will remember the previous video you will remember this is not just attention this is um this is not just attention this is um this is not just attention this is um multi-headed attention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 202,
      "text": "right and so in multi-headed attention right and so in multi-headed attention right and so in the previous video we had this the previous video we had this the previous video we had this multi-headed attention module and this multi-headed attention module and this multi-headed attention module and this implementation made it obvious that implementation made it obvious that implementation made it obvious that these heads are not actually that these heads are not actually that these heads are not actually that complicated uh there's basically complicated uh there's basically complicated uh there's basically in parallel inside every attention block in parallel inside every attention block in parallel inside every attention block there's multiple heads and they're all there's multiple heads and they're all there's multiple heads and they're all functioning in parallel and uh their functioning in parallel and uh their functioning in parallel and uh their outputs are just being concatenated and outputs are just being concatenated and outputs are just being concatenated and that becomes the output of the that becomes the output of the that becomes the output of the multi-headed attention so the heads are multi-headed attention so the heads are multi-headed attention so the heads are just kind of like parallel streams and just kind of like parallel streams and just kind of like parallel streams and their outputs get their outputs get their outputs get concatenated and so it was very simple concatenated",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 203,
      "text": "and so it was very simple concatenated",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 204,
      "text": "and so it was very simple and made the head be kind of like U and made the head be kind of like U and made the head be kind of like U fairly straightforward in terms of its fairly straightforward in terms of its fairly straightforward in terms of its implementation what happens here is that implementation what happens here is that implementation what happens here is that instead of having two separate modules instead of having two separate modules instead of having two separate modules and indeed many more modules that get and indeed many more modules that get and indeed many more modules that get concatenated all of that is just put concatenated",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 205,
      "text": "all of that is just put concatenated all of that is just put into a single uh self attention uh into a single uh self attention uh into a single uh self attention uh module and instead I'm being very module",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 206,
      "text": "and instead I'm being very module and instead I'm being very careful and doing a bunch of transpose careful and doing a bunch of transpose careful and doing a bunch of transpose split um tensor gymnastics to make this split um tensor gymnastics to make this split um tensor gymnastics to make this very efficient in pych but fundamentally very efficient in pych but fundamentally very efficient in pych but fundamentally and algorithmically nothing is different and algorithmically nothing is different and algorithmically nothing is different from the implementation we saw from the implementation we saw from the implementation we saw before um in this uh give before um in this uh give before um in this uh give repository so to remind you very briefly repository so to remind you very briefly repository so to remind you very briefly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 207,
      "text": "and I don't want to go in this uh into and I don't want to go in this uh into",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 208,
      "text": "and I don't want to go in this uh into this in too many in too much time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 209,
      "text": "but we this in too many in too much time but we this in too many in too much time but we have these tokens lined up in a sequence have these tokens lined up in a sequence have these tokens lined up in a sequence and there's 1,20 of them and then each and there's 1,20 of them and then each and there's 1,20 of them and then each token at this stage of the attention token at this stage of the attention token at this stage of the attention emits three vectors the query key and emits three vectors the query key and emits three vectors the query key and the value and first what happens here um the value and first what happens here um the value and first what happens here um is that the queries and the keys have to is that the queries and the keys have to is that the queries and the keys have to multiply each other to get sort of the multiply each other to get sort of the multiply each other to get sort of the attention um amount like how interesting attention um amount like how interesting attention um amount like how interesting they find each other so they have to they find each other so they have to they find each other so they have to interact multiplicatively so what we're interact multiplicatively",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 210,
      "text": "so what we're interact multiplicatively",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 211,
      "text": "so what we're doing here is we're calculating the qkv doing here is we're calculating the qkv doing here is we're calculating the qkv we splitting it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 212,
      "text": "and then there's a bunch we splitting it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 213,
      "text": "and then there's a bunch we splitting it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 214,
      "text": "and then there's a bunch of gymnastics as I mentioned here and of gymnastics as I mentioned here and of gymnastics as I mentioned here and the way this works is that we're the way this works is that we're the way this works",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 215,
      "text": "is that we're basically making the number of heads and basically making the number of heads and basically making the number of heads and H into a batch Dimension",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 216,
      "text": "and so it's a H into a batch Dimension",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 217,
      "text": "and so it's a H into a batch Dimension",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 218,
      "text": "and so it's a batch Dimension just like B so that in batch Dimension just like B",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 219,
      "text": "so that in batch Dimension just like B so that in these operations that follow pytorch these operations that follow pytorch these operations that follow pytorch treats B and NH as batches and it treats B and NH as batches and it treats B and NH as batches",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 220,
      "text": "and it applies all the operations on all of applies all the operations on all of applies all the operations on all of them in parallel in both the batch and them in parallel in both the batch and them in parallel in both the batch and the the the heads and the operations that get heads and the operations that get heads and the operations that get applied are number one the queries and applied are number one the queries and applied are number one the queries and the keys intera to give us her attention the keys intera to give us her attention the keys intera to give us her attention this is the autoaggressive mask that this is the autoaggressive mask that this is the autoaggressive mask that makes sure that the tokens only attend makes sure that the tokens only attend makes sure that the tokens only attend to tokens before them and never to to tokens before them and never to to tokens before them and never to tokens in the tokens in the tokens in the future the softmax here normalizes the future the softmax here normalizes the future the softmax here normalizes the attention so it sums to one always and attention so it sums to one always and attention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 221,
      "text": "so it sums to one always and then recall from the previous video that then recall from the previous video that then recall from the previous video that doing the attention Matrix multiply with doing the attention Matrix multiply with doing the attention Matrix multiply with the values is basically a way to do a the values is basically a way to do a the values is basically a way to do a weighted sum of the values of the tokens weighted sum of the values of the tokens weighted sum of the values of the tokens that we found interesting at every that we found interesting at every that we found interesting at every single token and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 222,
      "text": "then the final single token and then the final single token and then the final transpose conf VI and view is just transpose conf VI and view is just transpose conf VI and view is just reassembling all of that again and this reassembling all of that again and this reassembling all of that again and this actually performs the concatenation actually performs the concatenation actually performs the concatenation operation so you can step through this operation so you can step through this operation so you can step through this uh slowly if you'd like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 223,
      "text": "um but it is uh slowly if you'd like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 224,
      "text": "um but it is uh slowly if you'd like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 225,
      "text": "um but it is equivalent mathematically to our equivalent mathematically to our equivalent mathematically to our previous implementation is just more previous implementation is just more previous implementation is just more efficient in P torch so that's why I efficient in P torch so that's why I efficient in P torch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 226,
      "text": "so that's why I chose this implementation chose this implementation chose this implementation instead now in addition to that I'm instead now in addition to that I'm instead now in addition to that I'm being careful with how I name my being careful with how I name my being careful with how I name my variables so for example cattin is the variables so for example cattin is the variables so for example cattin is the same as seaten and so actually our keys same as seaten",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 227,
      "text": "and so actually our keys same as seaten and so actually our keys should basically exactly follow the should basically exactly follow the should basically exactly follow the schema of the hugging face train schema of the hugging face train schema of the hugging face train Transformers code and that will make it Transformers code and that will make it Transformers code and that will make it very easy for us to now Port over all very easy for us to now Port over all very easy for us to now Port over all the weights from exactly this sort of the weights from exactly this sort of the weights from exactly this sort of naming conventions because all of our naming conventions because all of our naming conventions because all of our variables are named the same thing but variables are named the same thing but variables are named the same thing but um at this point we have finished the um at this point we have finished the um at this point we have finished the gpt2 implementation and what that allows gpt2 implementation and what that allows gpt2 implementation and what that allows us to do is we don't have to basically us to do is we don't have to basically us to do is we don't have to basically use uh this file from hugging face which use uh this file from hugging face which use uh this file from hugging face which is fairly long is fairly long is fairly long um this um this um this is uh 2,000 lines of code um instead we is uh 2,000 lines of code um instead we is uh 2,000 lines of code um instead we just have a less than 100 lines of code just have a less than 100 lines of code just have a less than 100 lines of code and this is the complete uh gpd2 and this is the complete uh gpd2 and this is the complete uh gpd2 implementation so at this stage we implementation so at this stage we implementation so at this stage we should just be able to take over all the should just be able to take over all the should just be able to take over all the weights set them and then do generation weights set them and then do generation weights set them and then do generation so let's see what that looks like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 228,
      "text": "okay so let's see what that looks like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 229,
      "text": "okay so let's see what that looks like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 230,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 231,
      "text": "so here I've also changed the GPT config",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 232,
      "text": "so here I've also changed the GPT config",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 233,
      "text": "so here I've also changed the GPT config so that the numbers here the H so that the numbers here the H so that the numbers here the H parameters agree with the gpt2 124 M parameters agree with the gpt2 124 M parameters agree with the gpt2 124 M model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 234,
      "text": "so the maximum sequence length model so the maximum sequence length model so the maximum sequence length which I call block size here is 124 the which I call block size here is 124 the which I call block size here is 124 the number of tokens is 50250 257 which if number of tokens is 50250 257 which if number of tokens is 50250 257 which if you watch my tokenizer video know that you watch my tokenizer video know that you watch my tokenizer video know that this is 50,000 m merges BP merges 256",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 235,
      "text": "this is 50,000 m merges BP merges 256 this is 50,000 m merges BP merges 256 bite tokens the leaves of the BP tree bite tokens the leaves of the BP tree bite tokens the leaves of the BP tree and one special end of text token that and one special end of text token that and one special end of text token that delimits different documents and can delimits different documents and can delimits different documents and can start generation as well and there are start generation as well and there are start generation as well and there are 12 layers there are 12 heads in the 12 layers there are 12 heads in the 12 layers there are 12 heads in the attention and the dimension of the attention and the dimension of the attention and the dimension of the Transformers was Transformers was Transformers was 768 so here's how we can now load the 768 so here's how we can now load the 768 so here's how we can now load the parameters from hugging face to uh our parameters from hugging face to uh our parameters from hugging face to uh our code here and initialize the GPT class code here and initialize the GPT class code here and initialize the GPT class with those parameters so let me just with those parameters so let me just with those parameters so let me just copy paste a bunch of code copy paste a bunch of code copy paste a bunch of code here and I'm not going to go through here and I'm not going to go through here and I'm not going to go through this code too slow too quickly too this code too slow too quickly too this code too slow too quickly too slowly because um honestly it's not that slowly because um honestly it's not that slowly because um honestly it's not that interesting it's not that exciting",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 236,
      "text": "we're interesting it's not that exciting we're interesting",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 237,
      "text": "it's not that exciting we're just loading the weights",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 238,
      "text": "so it's kind of just loading the weights so it's kind of just loading the weights",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 239,
      "text": "so it's kind of dry",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 240,
      "text": "but as I mentioned there are four dry but as I mentioned there are four dry but as I mentioned there are four models in this miniseries of gpt2 this models in this miniseries of gpt2 this models in this miniseries of gpt2 this is some of the Jupiter code um code that is some of the Jupiter code um code that is some of the Jupiter code um code that we had here on the right I'm just pting we had here on the right I'm just pting we had here on the right I'm just pting it over these are the hyper parameters it over these are the hyper parameters it over these are the hyper parameters of the gpt2 models uh we're creating the of the gpt2 models uh we're creating the of the gpt2 models uh we're creating the config object and creating our own model config object and creating our own model config object and creating our own model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 241,
      "text": "and then what's Happening Here is we're",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 242,
      "text": "and then what's Happening Here is we're",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 243,
      "text": "and then what's Happening Here is we're creating the state dict both for our creating the state dict both for our creating the state dict both for our model and for the hugging face model and for the hugging face model and for the hugging face model um and then what we're doing here model um and then what we're doing here model um and then what we're doing here is we're going over the hugging face is we're going over the hugging face is we're going over the hugging face model keys and we're copying over those model keys and we're copying over those model keys and we're copying over those tensors and in the process we are kind tensors and in the process we are kind tensors and in the process we are kind of ignoring a few of the buffers they're of ignoring a few of the buffers they're of ignoring a few of the buffers they're not parameters they're buffers so for not parameters they're buffers so for not parameters they're buffers so for example attention dobias uh that's just example attention dobias uh that's just example attention dobias uh that's just used for the autoaggressive mask and so used for the autoaggressive mask and so used for the autoaggressive mask and so we are ignoring some of those masks and we are ignoring some of those masks and we are ignoring some of those masks and uh that's it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 244,
      "text": "and then then one uh that's it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 245,
      "text": "and then then one uh that's it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 246,
      "text": "and then then one additional kind of annoyance is that additional kind of annoyance is that additional kind of annoyance is that this comes from the tensorflow repo and this comes from the tensorflow repo and this comes from the tensorflow repo",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 247,
      "text": "and I'm not sure how this is a little bit I'm not sure how this is a little bit I'm not sure how this is a little bit annoying",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 248,
      "text": "but some of the weights are annoying",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 249,
      "text": "but some of the weights are annoying",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 250,
      "text": "but some of the weights are transposed from what pytorch would want transposed from what pytorch would want transposed from what pytorch would want",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 251,
      "text": "and so manually I hardcoded the weights and so manually I hardcoded the weights and so manually I hardcoded the weights that should be transposed and then we that should be transposed and then we that should be transposed and then we transpose them if that is",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 252,
      "text": "so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 253,
      "text": "and then we transpose them if that is",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 254,
      "text": "so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 255,
      "text": "and then we transpose them if that is",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 256,
      "text": "so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 257,
      "text": "and then we return this model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 258,
      "text": "so the from return this model so the from return this model so the from pre-trained is a pre-trained is a pre-trained is a Constructor or class method in Python Constructor or class method in Python Constructor or class method in Python that Returns the GPT object if we just that Returns the GPT object if we just that Returns the GPT object if we just give it the model type which in our case give it the model type which in our case give it the model type which in our case is gpt2 the smallest model that we're is gpt2 the smallest model that we're is gpt2 the smallest model that we're interested in so this is the code and interested in so this is the code and interested in so this is the code and this is how you would use it and um we this is how you would use it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 259,
      "text": "and um we this is how you would use it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 260,
      "text": "and um we can pop open the terminal here in vs can pop open the terminal here in vs can pop open the terminal here in vs code and we can python train gbt2 pi and code and we can python train gbt2 pi and code and we can python train gbt2 pi and fingers crossed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 261,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 262,
      "text": "so we didn't crash and so crossed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 263,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 264,
      "text": "so we didn't crash",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 265,
      "text": "and so we can load the weights and the biases we can load the weights and the biases we can load the weights and the biases and everything else into our Ann module and everything else into our Ann module and everything else into our Ann module but now let's also get additional but now let's also get additional but now let's also get additional confidence that this is working and confidence that this is working and confidence that this is working and let's try to actually generate from this let's try to actually generate from this let's try to actually generate from this model okay now before we can actually model okay now before we can actually model okay now before we can actually generate from this model we have to be generate from this model we have to be generate from this model we have to be able to forward it we didn't actually able to forward it we didn't actually able to forward it we didn't actually write that code yet",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 266,
      "text": "so here's the write that code yet",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 267,
      "text": "so here's the write that code yet",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 268,
      "text": "so here's the forward forward forward function so the input to the forward is function so the input to the forward is function so the input to the forward is going to be our indices our tokens uh going to be our indices our tokens uh going to be our indices our tokens uh token indices and they are always of token indices and they are always of token indices and they are always of shape B BYT",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 269,
      "text": "and so we have batch shape B BYT",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 270,
      "text": "and so we have batch shape B BYT",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 271,
      "text": "and so we have batch dimension of B",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 272,
      "text": "and then we have the time dimension of B",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 273,
      "text": "and then we have the time dimension of B",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 274,
      "text": "and then we have the time dimension of up to T and the T can't be dimension of up to T and the T can't be dimension of up to T and the T can't be more than the block size the block size more than the block size the block size more than the block size the block size is is the maximum sequence length",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 275,
      "text": "so B is is the maximum sequence length",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 276,
      "text": "so B is is the maximum sequence length",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 277,
      "text": "so B BYT indices arranged is sort of like a BYT indices arranged is sort of like a BYT indices arranged is sort of like a two-dimensional layout and remember that two-dimensional layout and remember that two-dimensional layout and remember that basically every single row of this is of basically every single row of this is of basically every single row of this is of size up to uh block size and this is T size up to uh block size and this is T size up to uh block size and this is T tokens that are in a sequence and then tokens that are in a sequence and then tokens that are in a sequence",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 278,
      "text": "and then we have B independent sequences stacked we have B independent sequences stacked we have B independent sequences stacked up in a batch so that this is up in a batch so that this is up in a batch so that this is efficient now here we are forwarding the efficient now here we are forwarding the efficient now here we are forwarding the position embeddings and the token position embeddings and the token position embeddings and the token embeddings and this code should be very embeddings and this code should be very embeddings and this code should be very recognizable from the previous lecture recognizable from the previous lecture recognizable from the previous lecture",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 279,
      "text": "so um we basically use uh a range which so um we basically use uh a range which so um we basically use uh a range which is kind of like a version of range but is kind of like a version of range but is kind of like a version of range but for pytorch uh and we're iterating from for pytorch uh and we're iterating from for pytorch uh and we're iterating from Z to T and creating this uh positions uh Z to T and creating this uh positions uh Z to T and creating this uh positions uh sort of uh indices sort of uh indices sort of uh indices um and then we are making sure that um and then we are making sure that um and then we are making sure that they're in the same device as idx they're in the same device as idx they're in the same device as idx because we're not going to be training because we're not going to be training because we're not going to be training on only CPU that's going to be too on only CPU that's going to be too on only CPU that's going to be too inefficient we want to be training on inefficient we want to be training on inefficient we want to be training on GPU and that's going to come in in a GPU and that's going to come in in a GPU and that's going to come in in a bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 280,
      "text": "uh then we have the position bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 281,
      "text": "uh then we have the position bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 282,
      "text": "uh then we have the position embeddings and the token embeddings and embeddings and the token embeddings and embeddings and the token embeddings and the addition operation of those two now the addition operation of those two now the addition operation of those two now notice that the position embed are going notice that the position embed are going notice that the position embed are going to be identical for every single row of to be identical for every single row of to be identical for every single row of uh of input",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 283,
      "text": "and so there's broadcasting uh of input",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 284,
      "text": "and so there's broadcasting uh of input",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 285,
      "text": "and so there's broadcasting hidden inside this plus where we have to hidden inside this plus where we have to hidden inside this plus where we have to create an additional Dimension here and create an additional Dimension here and create an additional Dimension here and then these two add up because the same then these two add up because the same then these two add up because the same position embeddings apply at every position embeddings apply at every position embeddings apply at every single row of our example stacked up in single row of our example stacked up in single row of our example stacked up in a batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 286,
      "text": "then we forward the Transformer a batch then we forward the Transformer a batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 287,
      "text": "then we forward the Transformer blocks and finally the last layer norm blocks and finally the last layer norm blocks and finally the last layer norm and the LM head so what comes out after and the LM head so what comes out after and the LM head so what comes out after forward is the logits and if the input forward is the logits and if the input forward is the logits and if the input was B BYT indices then at every single B was B BYT indices then at every single B was B BYT indices then at every single B by T we will calculate the uh logits for by T we will calculate the uh logits for by T we will calculate the uh logits for what token comes next in the sequence so what token comes next in the sequence so what token comes next in the sequence so what is the token B t+1 the one on the what is the token B t+1 the one on the what is the token B t+1 the one on the right of this token and B app size here right of this token and B app size here right of this token and B app size here is the number of possible tokens and so is the number of possible tokens and so is the number of possible tokens and so therefore this is the tensor that we're therefore this is the tensor that we're therefore this is the tensor that we're going to obtain and these low jits are going to obtain and these low jits are going to obtain and these low jits are just a softmax away from becoming just a softmax away from becoming just a softmax away from becoming probabilities so this is the forward probabilities",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 288,
      "text": "so this is the forward probabilities so this is the forward pass of the network and now we can get pass of the network and now we can get pass of the network and now we can get load",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 289,
      "text": "and so we're going to be able to load",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 290,
      "text": "and so we're going to be able to load",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 291,
      "text": "and so we're going to be able to generate from the model generate from the model generate from the model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 292,
      "text": "imminently okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 293,
      "text": "so now we're going to imminently okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 294,
      "text": "so now we're going to imminently okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 295,
      "text": "so now we're going to try to set up the identical thing on the try to set up the identical thing on the try to set up the identical thing on the left here that matches hug and face on left here that matches hug and face on left here that matches hug and face on the right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 296,
      "text": "so here we've sampled from the the right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 297,
      "text": "so here we've sampled from the the right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 298,
      "text": "so here we've sampled from the pipeline and we sampled five times up to pipeline and we sampled five times up to pipeline and we sampled five times up to 30 tokens with the prefix of hello",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 299,
      "text": "I'm a 30 tokens with the prefix of hello",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 300,
      "text": "I'm a 30 tokens with the prefix of hello",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 301,
      "text": "I'm a language model and these are the language model and these are the language model and these are the completions that we achieved so we're completions that we achieved so we're completions that we achieved so we're going to try to replicate that on the going to try to replicate that on the going to try to replicate that on the left here so number turn sequences is left here so number turn sequences is left here so number turn sequences is five max length is 30 so the first thing five max length is 30 so the first thing five max length is 30 so the first thing we do of course is we initialize our we do of course is we initialize our we do of course is we initialize our model then we put it into evaluation model then we put it into evaluation model then we put it into evaluation mode now this is a good practice to put mode now this is a good practice to put mode now this is a good practice to put the model into eval when you're not the model into eval when you're not the model into eval when you're not going to be training it you're just going to be training it you're just going to be training it you're just going to be using it and I don't going to be using it and I don't going to be using it and I don't actually know if this is doing anything actually know if this is doing anything actually know if this is doing anything right now for the following reason our right now for the following reason our right now for the following reason our model up above here contains no modules model up above here contains no modules model up above here contains no modules or layers that actually have a different or layers that actually have a different or layers that actually have a different uh Behavior at training or evaluation uh Behavior at training or evaluation uh Behavior at training or evaluation time so for example Dropout batch norm time so for example Dropout batch norm time so for example Dropout batch norm and a bunch of other layers have this and a bunch of other layers have this and a bunch of other layers have this kind of behavior but all of these layers kind of behavior but all of these layers kind of behavior but all of these layers that we've used here should be identical that we've used here should be identical that we've used here should be identical in both training and evaluation time um in both training and evaluation time um in both training and evaluation time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 302,
      "text": "um so so potentially model that eval does so so potentially model that eval does so so potentially model that eval does nothing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 303,
      "text": "but then I'm not actually sure nothing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 304,
      "text": "but then I'm not actually sure nothing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 305,
      "text": "but then I'm not actually sure if this is the case and maybe pytorch if this is the case and maybe pytorch if this is the case and maybe pytorch internals uh do some clever things internals uh do some clever things internals uh do some clever things depending on the evaluation mode uh depending on the evaluation mode uh depending on the evaluation mode uh inside here the next thing we're doing inside here the next thing we're doing inside here the next thing we're doing here is we are moving the entire model here is we are moving the entire model here is we are moving the entire model to Cuda so we're moving this all of the to Cuda",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 306,
      "text": "so we're moving this all of the to Cuda",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 307,
      "text": "so we're moving this all of the tensors to GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 308,
      "text": "so I'm sshed here to a tensors to GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 309,
      "text": "so I'm sshed here to a tensors to GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 310,
      "text": "so I'm sshed here to a cloud box",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 311,
      "text": "and I have a bunch of gpus on cloud box",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 312,
      "text": "and I have a bunch of gpus on cloud box",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 313,
      "text": "and I have a bunch of gpus on this box",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 314,
      "text": "and here I'm moving the entire this box",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 315,
      "text": "and here I'm moving the entire this box",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 316,
      "text": "and here I'm moving the entire model and all of its members and all of model and all of its members and all of model and all of its members and all of its tensors and everything like that its tensors and everything like that its tensors and everything like that everything gets shipped off to basically everything gets shipped off to basically everything gets shipped off to basically a whole separate computer that is a whole separate computer that is a whole separate computer that is sitting on the GPU and the GPU is sitting on the GPU and the GPU is sitting on the GPU and the GPU is connected to the uh CPU and they can connected to the uh CPU and they can connected to the uh CPU and they can communicate but it's basically a whole communicate but it's basically a whole communicate but it's basically a whole separate computer with its own computer separate computer with its own computer separate computer with its own computer architecture and it's really well architecture and it's really well architecture and it's really well catered to parallel processing tasks catered to parallel processing tasks catered to parallel processing tasks like those of running neural networks so like those of running neural networks so like those of running neural networks so I'm doing this so that the model lives I'm doing this so that the model lives I'm doing this so that the model lives on the GPU a whole separate computer and on the GPU a whole separate computer and on the GPU a whole separate computer and it's just going to make our code a lot it's just going to make our code a lot it's just going to make our code a lot more efficient because all of this stuff more efficient because all of this stuff more efficient because all of this stuff runs a lot more efficiently on the runs a lot more efficiently on the runs a lot more efficiently on the gpus so that's the model gpus so that's the model gpus so that's the model itself now uh the next thing we want to itself now uh the next thing we want to itself now uh the next thing we want to do is we want to start with this as the do is we want to start with this as the do is we want to start with this as the prefix when we do the generation so prefix when we do the generation so prefix when we do the generation so let's actually create those prefix let's actually create those prefix let's actually create those prefix tokens so here's the code that I've tokens so here's the code that I've tokens so here's the code that I've written we're going to import the tich written we're going to import the tich written we're going to import the tich token library from open Ai",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 317,
      "text": "and we're token library from open Ai",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 318,
      "text": "and we're token library from open Ai",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 319,
      "text": "and we're going to get the gpt2 encoding so that's going to get the gpt2 encoding so that's going to get the gpt2 encoding",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 320,
      "text": "so that's the tokenizer for gpt2",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 321,
      "text": "and then we're the tokenizer for gpt2",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 322,
      "text": "and then we're the tokenizer for gpt2",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 323,
      "text": "and then we're going to encode this string and get a going to encode this string and get a going to encode this string and get a list of integers which are the tokens uh list of integers which are the tokens uh list of integers which are the tokens uh now these integers here should actually now these integers here should actually now these integers here should actually be fairly straightforward because we can be fairly straightforward because we can be fairly straightforward because we can just copy paste this string and we can just copy paste this string and we can just copy paste this string and we can sort of inspect what it is in tick sort of inspect what it is in tick sort of inspect what it is in tick tokenizer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 324,
      "text": "so just pasting that in these tokenizer so just pasting that in these tokenizer so just pasting that in these are the tokens that are going to come are the tokens that are going to come are the tokens that are going to come out so this list of integers is what we out so this list of integers is what we out so this list of integers is what we expect tokens to become and as you expect tokens to become and as you expect tokens to become and as you recall if you saw my video of course all recall if you saw my video of course all recall if you saw my video of course all the tokens they're just little string the tokens",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 325,
      "text": "they're just little string the tokens",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 326,
      "text": "they're just little string chunks",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 327,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 328,
      "text": "so these are this is the chunks",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 329,
      "text": "right so these are this is the chunks",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 330,
      "text": "right so these are this is the chunc of this string into gpt2 chunc of this string into gpt2 chunc of this string into gpt2 tokens so once we have those tokens it's tokens so once we have those tokens it's tokens so once we have those tokens it's a list of integers we can create a torch a list of integers we can create a torch a list of integers we can create a torch tensor out of it in this case it's eight tensor out of it in this case it's eight tensor out of it in this case it's eight tokens and then we're going to replicate tokens and then we're going to replicate tokens and then we're going to replicate these eight tokens for five times to get these eight tokens for five times to get these eight tokens for five times to get five rows of eight tokens and that is five rows of eight tokens and that is five rows of eight tokens and that is our initial um input X as I call it here our initial um input X as I call it here our initial um input X as I call it here and it lives on the GPU as well so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 331,
      "text": "X now and it lives on the GPU as well so X now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 332,
      "text": "and it lives on the GPU as well so X now is this idx that we can put into forward is this idx that we can put into forward is this idx that we can put into forward to get our logits so that we know what to get our logits so that we know what to get our logits so that we know what comes as the sixth token comes as the sixth token comes as the sixth token uh sorry as the ninth token in every one uh sorry as the ninth token in every one uh sorry as the ninth token in every one of these five rows",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 333,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 334,
      "text": "and we are now of these five rows",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 335,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 336,
      "text": "and we are now of these five rows",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 337,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 338,
      "text": "and we are now ready to generate so let me paste in one ready to generate so let me paste in one ready to generate so let me paste in one more code block more code block more code block here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 339,
      "text": "um so what's happening here in this here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 340,
      "text": "um so what's happening here in this here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 341,
      "text": "um so what's happening here in this code block is we have this x which is of code block is we have this x which is of code block is we have this x which is of size B BYT",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 342,
      "text": "right so batch by time and size B BYT right so batch by time and size B BYT",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 343,
      "text": "right so batch by time and we're going to be in every iteration of we're going to be in every iteration of we're going to be in every iteration of this loop we're going to be adding a this loop we're going to be adding a this loop we're going to be adding a column of new indices into each one of column of new indices into each one of column of new indices into each one of these rows",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 344,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 345,
      "text": "and so these are the these rows",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 346,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 347,
      "text": "and so these are the these rows",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 348,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 349,
      "text": "and so these are the new indices and we're appending them to new indices and we're appending them to new indices and we're appending them to the the sequence as we're sampling so the the sequence as we're sampling so the the sequence as we're sampling so with each Loop iteration we get one more with each Loop iteration we get one more with each Loop iteration we get one more column into X and all of the operations column into X and all of the operations column into X and all of the operations happen in the context manager of torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 350,
      "text": "happen in the context manager of torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 351,
      "text": "happen in the context manager of torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 352,
      "text": "nograd this is just telling pytorch that nograd this is just telling pytorch that nograd this is just telling pytorch that we're not going to be calling that we're not going to be calling that we're not going to be calling that backward on any of this so it doesn't backward on any of this so it doesn't backward on any of this so it doesn't have to cach all the intermediate have to cach all the intermediate have to cach all the intermediate tensors it's not going to have to tensors it's not going to have to tensors it's not going to have to prepare in any way for a potential prepare in any way for a potential prepare in any way for a potential backward later and this saves a lot of backward later and this saves a lot of backward later and this saves a lot of space and also possibly uh some time so space and also possibly uh some time so space and also possibly uh some time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 353,
      "text": "so we get our low jits we get the loow jits we get our low jits we get the loow jits we get our low jits we get the loow jits at only the last location we throw away at only the last location we throw away at only the last location we throw away all the other low jits uh we don't need all the other low jits uh we don't need all the other low jits uh we don't need them we only care about the last columns them we only care about the last columns them we only care about the last columns low jits so this is being wasteful uh low jits",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 354,
      "text": "so this is being wasteful uh low jits",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 355,
      "text": "so this is being wasteful uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 356,
      "text": "but uh this is just kind of like an",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 357,
      "text": "but uh this is just kind of like an",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 358,
      "text": "but uh this is just kind of like an inefficient implementation of inefficient implementation of inefficient implementation of sampling um so it's correct but sampling um so it's correct but sampling um so it's correct but inefficient so we get the last column of inefficient so we get the last column of inefficient so we get the last column of loow jits pass it through soft Max to loow jits pass it through soft Max to loow jits pass it through soft Max to get our probabilities then here I'm get our probabilities then here I'm get our probabilities then here I'm doing top case sampling of 50 and I'm doing top case sampling of 50 and I'm doing top case sampling of 50 and I'm doing that because this is the hugging doing that because this is the hugging doing that because this is the hugging face default so just looking at the face default so just looking at the face default so just looking at the hugging face docks here of a pipeline um hugging face docks here of a pipeline um hugging face docks here of a pipeline um there's a bunch of there's a bunch of there's a bunch of quarks that go into hugging face and I quarks that go into hugging face and I quarks that go into hugging face",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 359,
      "text": "and I mean it's it's kind of a lot",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 360,
      "text": "honestly mean it's it's kind of a lot",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 361,
      "text": "honestly mean it's it's kind of a lot",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 362,
      "text": "honestly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 363,
      "text": "but I guess the important one that I",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 364,
      "text": "but I guess the important one that I",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 365,
      "text": "but I guess the important one that I noticed is that they're using top K by noticed is that they're using top K by noticed is that they're using top K by default which is 50 and what that does default which is 50 and what that does default which is 50 and what that does is that uh so that's being used here as is that uh so that's being used here as is that uh so that's being used here as well and what that does is basically we well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 366,
      "text": "and what that does is basically we well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 367,
      "text": "and what that does is basically we want to take our probabilities and we want to take our probabilities and we want to take our probabilities and we only want to keep the top 50 only want to keep the top 50 only want to keep the top 50 probabilities and anything that is lower probabilities and anything that is lower probabilities and anything that is lower than the 50th probability uh we just than the 50th probability uh we just than the 50th probability uh we just clamp to zero and renormalize and so clamp to zero and renormalize and so clamp to zero and renormalize and so that way we are never sampling very rare that way we are never sampling very rare that way we are never sampling very rare tokens uh the tokens we're going to be tokens uh the tokens we're going to be tokens uh the tokens we're going to be sampling are always in the top 50 of sampling are always in the top 50 of sampling are always in the top 50 of most likely tokens and this helps keep most likely tokens and this helps keep most likely tokens and this helps keep the model kind of on track and it the model kind of on track and it the model kind of on track and it doesn't blabber on and it doesn't get doesn't blabber on and it doesn't get doesn't blabber on and it doesn't get lost and doesn't go off the rails as lost and doesn't go off the rails as lost and doesn't go off the rails as easily uh and it kind of like um sticks easily uh and it kind of like um sticks easily uh and it kind of like um sticks in the vicinity of likely tokens a lot in the vicinity of likely tokens a lot in the vicinity of likely tokens a lot better so this is the way to do it in better so this is the way to do it in better so this is the way to do it in pytorch and you can step through it if pytorch and you can step through it if pytorch and you can step through it if you like I don't think it's super",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 368,
      "text": "you like I don't think it's super",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 369,
      "text": "you like I don't think it's super insightful",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 370,
      "text": "so I'll speed through it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 371,
      "text": "but insightful",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 372,
      "text": "so I'll speed through it but",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 373,
      "text": "insightful",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 374,
      "text": "so I'll speed through it but roughly speaking we get this new column roughly speaking we get this new column roughly speaking we get this new column of of tokens we append them on x and of of tokens we append them on x and of of tokens we append them on x and basically The Columns of X grow until basically The Columns of X grow until basically The Columns of X grow until this y Loop gets tripped up and then this y Loop gets tripped up and then this y Loop gets tripped up and then finally we have an entire X of size um 5 finally we have an entire X of size um 5 finally we have an entire X of size um 5 by 30 in this case in this example and by 30 in this case in this example and by 30 in this case in this example and we can just basically print all those we can just basically print all those we can just basically print all those individual rows",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 375,
      "text": "so I'm getting all the individual rows",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 376,
      "text": "so I'm getting all the individual rows",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 377,
      "text": "so I'm getting all the rows I'm getting all the tokens that rows I'm getting all the tokens that rows I'm getting all the tokens that were sampled and I'm using the decode were sampled and I'm using the decode were sampled and I'm using the decode function from Tik tokenizer to get back function from Tik tokenizer to get back function from Tik tokenizer to get back the string which we can print and so the string which we can print and so the string which we can print and so terminal new terminal gpt2",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 378,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 379,
      "text": "so these are the generations gpt2",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 380,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 381,
      "text": "so these are the generations that we're getting",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 382,
      "text": "hello I'm a language that we're getting",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 383,
      "text": "hello I'm a language that we're getting",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 384,
      "text": "hello I'm a language model not a model not a model not a program um new line new line Etc hello program um new line new line Etc hello program um new line new line Etc",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 385,
      "text": "hello I'm a language model and one of the main I'm a language model and one of the main I'm a language model and one of the main things that bothers me when they create things that bothers me when they create things that bothers me when they create languages is how easy it becomes to languages is how easy it becomes to languages is how easy it becomes to create something that I me",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 386,
      "text": "so this will create something that I me",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 387,
      "text": "so this will create something that I me",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 388,
      "text": "so this will just like blabber on right in all these just like blabber on right in all these just like blabber on right in all these cases now one thing you will notice is cases now one thing you will notice is cases now one thing you will notice is that these Generations are not the that these Generations are not the that these Generations are not the generations of hugging face here and I generations of hugging face here and I generations of hugging face here and I can't find the discrepancy to be honest can't find the discrepancy to be honest can't find the discrepancy to be honest and I didn't fully go through all these and I didn't fully go through all these",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 389,
      "text": "and I didn't fully go through all these options but probably there's something options",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 390,
      "text": "but probably there's something options",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 391,
      "text": "but probably there's something else hiding in on addition to the top P else hiding in on addition to the top P else hiding in on addition to the top P",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 392,
      "text": "so I'm not able to match it up",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 393,
      "text": "but just so I'm not able to match it up",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 394,
      "text": "but just so I'm not able to match it up but just for correctness um down here Below in for correctness um down here Below in for correctness um down here Below in the juper notebook and using the hugging the juper notebook and using the hugging the juper notebook and using the hugging face model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 395,
      "text": "so this is the hugging face face model so this is the hugging face face model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 396,
      "text": "so this is the hugging face model here I was I replicated the code model here I was I replicated the code model here I was I replicated the code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 397,
      "text": "and if I do this and I run that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 398,
      "text": "then I and if I do this and I run that then I and if I do this and I run that then I am getting the same results so basically am getting the same results so basically am getting the same results so basically the model internals are not wrong it's the model internals are not wrong it's the model internals are not wrong it's just I'm not 100% sure what the pipeline just I'm not 100% sure what the pipeline just I'm not 100% sure what the pipeline does in hugging face and that's why does in hugging face and that's why does in hugging face and that's why we're not able to match them up",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 399,
      "text": "but we're not able to match them up but we're not able to match them up but otherwise the code is correct and we've otherwise the code is correct and we've otherwise the code is correct and we've loaded all the um tensors correctly so loaded all the um tensors correctly so loaded all the um tensors correctly so we're initializing the model correctly we're initializing the model correctly we're initializing the model correctly and everything here works so long story and everything here works so long story and everything here works so long story short uh We've Port it all the weights short",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 400,
      "text": "uh We've Port it all the weights short uh We've Port it all the weights we initialize the gpt2 this is the exact we initialize the gpt2 this is the exact we initialize the gpt2 this is the exact opening gpt2 and it can generate opening gpt2 and it can generate opening gpt2 and it can generate sequences and they look sensible and now sequences and they look sensible and now sequences and they look sensible and now here of course we're initializing with here of course we're initializing with here of course we're initializing with gbt2 model weights",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 401,
      "text": "but now we want to gbt2 model weights",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 402,
      "text": "but now we want to gbt2 model weights",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 403,
      "text": "but now we want to initialize from scratch from random initialize from scratch from random initialize from scratch from random numbers and we want to actually train a numbers",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 404,
      "text": "and we want to actually train a numbers and we want to actually train a model that will give us sequences as model that will give us sequences as model that will give us sequences as good as or better than these ones in good as or better than these ones in good as or better than these ones in quality",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 405,
      "text": "and so that's what we turn to quality",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 406,
      "text": "and so that's what we turn to quality",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 407,
      "text": "and so that's what we turn to next",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 408,
      "text": "so it turns out that using the next so it turns out that using the next so it turns out that using the random model is actually fairly random model is actually fairly random model is actually fairly straightforward because pytorch already straightforward because pytorch already straightforward because pytorch already initializes our model randomly and by initializes our model randomly and by initializes our model randomly and by default so when we create the GPT model default so when we create the GPT model default so when we create the GPT model and the Constructor this is all um all and the Constructor this is all um all and the Constructor this is all um all of these layers and modules have random of these layers and modules have random of these layers and modules have random initializers that are there by default initializers that are there by default initializers that are there by default",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 409,
      "text": "so when these linear layers get created so when these linear layers get created so when these linear layers get created and so on there's default Constructors and so on there's default Constructors and so on there's default Constructors for example using the Javier for example using the Javier for example using the Javier initialization that we saw in the past initialization that we saw in the past initialization that we saw in the past uh to construct the weights of these uh to construct the weights of these uh to construct the weights of these layers and so creating a random model layers and so creating a random model layers and so creating a random model instead of a gpt2 model is actually instead of a gpt2 model is actually instead of a gpt2 model is actually fairly straightforward and we would just fairly straightforward and we would just fairly straightforward and we would just come here and instead we would create come here and instead we would create come here and instead we would create model equals GPT and then we want to use model equals GPT and then we want to use model equals GPT and then we want to use the default config GPT config and the the default config GPT config and the the default config GPT config and the default config uses the 124 M parameters default config uses the 124 M parameters default config uses the 124 M parameters so this is the random model so this is the random model so this is the random model initialization and we can run it and we should be able to get uh it and we should be able to get uh results now the results here of course results now the results here of course results now the results here of course are total garbage carbal and that's are total garbage carbal",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 410,
      "text": "and that's are total garbage carbal and that's because this is random model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 411,
      "text": "and so because this is random model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 412,
      "text": "and so because this is random model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 413,
      "text": "and so we're just getting all these random we're just getting all these random we're just getting all these random token string pieces chunked up totally token string pieces chunked up totally token string pieces chunked up totally at random so that's what we have right at random",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 414,
      "text": "so that's what we have right at random",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 415,
      "text": "so that's what we have right now uh now one more thing I wanted to now uh now one more thing I wanted to now uh now one more thing I wanted to point out by the way is in case you do point out by the way is in case you do point out by the way is in case you do not have Cuda available because you not have Cuda available because you not have Cuda available because you don't have a GPU you can still follow don't have a GPU you can still follow don't have a GPU you can still follow along with uh with what we're doing here along with uh with what we're doing here along with uh with what we're doing here uh to some extent uh and probably not to uh to some extent uh and probably not to uh to some extent uh and probably not to the very end because by the end we're the very end because by the end we're the very end because by the end we're going to be using multiple gpus and going to be using multiple gpus and going to be using multiple gpus and actually doing a serious training run uh actually doing a serious training run uh actually doing a serious training run uh but for now you can actually follow but for now you can actually follow but for now you can actually follow along decently",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 416,
      "text": "okay uh so one thing that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 417,
      "text": "along decently okay uh so one thing that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 418,
      "text": "along decently okay uh so one thing that I like to do in pytorch is I like to I like to do in pytorch is I like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 419,
      "text": "to I like to do in pytorch is I like to autod detect the device that is autod detect the device that is autod detect the device that is available to you so in particular you available to you so in particular you available to you so in particular you could do that like this could do that like this could do that like this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 420,
      "text": "so here we are trying to detect a device so here we are trying to detect a device so here we are trying to detect a device to run on that has the highest compute to run on that has the highest compute to run on that has the highest compute capability you can think about it that capability you can think about it that capability you can think about it that way so by default we start with CPU way so by default we start with CPU way so by default we start with CPU which of course is available everywhere which of course is available everywhere which of course is available everywhere because every single computer will have because every single computer will have because every single computer will have a CPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 421,
      "text": "but then we can try to detect do a CPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 422,
      "text": "but then we can try to detect do a CPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 423,
      "text": "but then we can try to detect do you have a GPU you so use a Cuda and you have a GPU you so use a Cuda and you have a GPU you so use a Cuda and then if you don't have a Cuda uh do you then if you don't have a Cuda uh do you then if you don't have a Cuda uh do you at least have MPS MPS is the back end at least have MPS MPS is the back end at least have MPS MPS is the back end for Apple silicon",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 424,
      "text": "so if you have a for Apple silicon so if you have a for Apple silicon so if you have a Macbook that is fairly new you probably Macbook that is fairly new you probably Macbook that is fairly new you probably have apple silicon on the inside and have apple silicon on the inside and have apple silicon on the inside and then that has a GPU that is actually then that has a GPU that is actually then that has a GPU that is actually fairly capable uh depending on which fairly capable uh depending on which fairly capable uh depending on which MacBook you have",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 425,
      "text": "and so you can use MPS MacBook you have",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 426,
      "text": "and so you can use MPS MacBook you have",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 427,
      "text": "and so you can use MPS which will be potentially faster than which will be potentially faster than which will be potentially faster than CPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 428,
      "text": "and so we can print the device here CPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 429,
      "text": "and so we can print the device here CPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 430,
      "text": "and so we can print the device here now once we have the device we can now once we have the device we can now once we have the device we can actually use it in place of Puda",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 431,
      "text": "so we actually use it in place of Puda so we actually use it in place of Puda",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 432,
      "text": "so we just swap it in and notice that here just swap it in and notice that here just swap it in and notice that here when we call model on X if this x here when we call model on X if this x here when we call model on X if this x here is on CPU instead of GPU then it will is on CPU instead of GPU then it will is on CPU instead of GPU then it will work fine because here in the forward work fine because here in the forward work fine because here in the forward which is where P to will come when we which is where P to will come when we which is where P to will come when we create a pose we were careful to use the create a pose we were careful to use the create a pose we were careful to use the device of idx to create this tensor as device of idx to create this tensor as device of idx to create this tensor as well and so there won't be any mismatch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 433,
      "text": "well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 434,
      "text": "and so there won't be any mismatch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 435,
      "text": "well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 436,
      "text": "and so there won't be any mismatch where one tensor is on CPU one is on GPU where one tensor is on CPU one is on GPU where one tensor is on CPU one is on GPU and uh that you can't combine those",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 437,
      "text": "but and uh that you can't combine those but and uh that you can't combine those",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 438,
      "text": "but here we are um carefully initializing on here we are um carefully initializing on here we are um carefully initializing on the correct device as indicated by the the correct device as indicated by the the correct device as indicated by the input to this model so this will autod input to this model so this will autod input to this model so this will autod detect device for me this will be of detect device for me this will be of detect device for me this will be of course course course GPU so using device GPU so using device GPU so using device Cuda uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 439,
      "text": "but uh you can also run with um Cuda uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 440,
      "text": "but uh you can also run with um Cuda uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 441,
      "text": "but uh you can also run with um as I mentioned another device",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 442,
      "text": "and it's as I mentioned another device and it's as I mentioned another device and it's not going to be too much slower",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 443,
      "text": "so if I not going to be too much slower so if I not going to be too much slower so if I override device here override device here override device here oops if I override device equals oops if I override device equals oops if I override device equals CPU CPU CPU then we'll still print Cuda of course then we'll still print Cuda of course then we'll still print Cuda of course",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 444,
      "text": "but now we're actually using CPU one 2 3",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 445,
      "text": "but now we're actually using CPU one 2 3",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 446,
      "text": "but now we're actually using CPU one 2 3 4 5 6 okay about 6 seconds and actually 4 5 6 okay about 6 seconds and actually 4 5 6 okay about 6 seconds",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 447,
      "text": "and actually we're not using torch compile and stuff we're not using torch compile and stuff we're not using torch compile and stuff like that which will speed up everything like that which will speed up everything like that which will speed up everything a lot faster as well but you can follow a lot faster as well but you can follow a lot faster as well but you can follow even on a CPU I think to a decent extent even on a CPU I think to a decent extent even on a CPU I think to a decent extent",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 448,
      "text": "um so that's note on that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 449,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 450,
      "text": "so I do um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 451,
      "text": "so that's note on that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 452,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 453,
      "text": "so I do um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 454,
      "text": "so that's note on that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 455,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 456,
      "text": "so I do want to loop around eventually into what want to loop around eventually into what want to loop around eventually into what it means to have different devices in it means to have different devices in it means to have different devices in pytorch and what it is exactly that pytorch and what it is exactly that pytorch and what it is exactly that pytorch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 457,
      "text": "does in the background for you pytorch does in the background for you pytorch does in the background for you when you do something like module.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 458,
      "text": "2 when you do something like module.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 459,
      "text": "2 when you do something like module.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 460,
      "text": "2 device or where you take a torch tensor device or where you take a torch tensor device or where you take a torch tensor and do A2 device and what exactly and do A2 device and what exactly and do A2 device and what exactly happens and how that works but for now happens and how that works but for now happens and how that works but for now I'd like to get to training",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 461,
      "text": "and I'd like I'd like to get to training",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 462,
      "text": "and I'd like I'd like to get to training",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 463,
      "text": "and I'd like to start training the model and for now to start training the model and for now to start training the model and for now let's just say the device makes code go let's just say the device makes code go let's just say the device makes code go fast um and let's go into how we can fast um and let's go into how we can fast um and let's go into how we can actually train the model so to train the actually train the model so to train the actually train the model so to train the model we're going to need some data set model we're going to need some data set model we're going to need some data set and for me the best debugging simplest",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 464,
      "text": "and for me the best debugging simplest",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 465,
      "text": "and for me the best debugging simplest data set that I like to use is the tiny data set that I like to use is the tiny data set that I like to use is the tiny Shakespeare data set um and it's Shakespeare data set um and it's Shakespeare data set um and it's available at this URL so you can W get available at this URL so you can W get available at this URL so you can W get it or you can just search tiny it or you can just search tiny it or you can just search tiny Shakespeare data Shakespeare data Shakespeare data set",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 466,
      "text": "and so um I have in my file system set",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 467,
      "text": "and so um I have in my file system set",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 468,
      "text": "and so um I have in my file system as just LS input.txt as just LS input.txt as just LS input.txt",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 469,
      "text": "so I already downloaded it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 470,
      "text": "and here I'm so I already downloaded it and here I'm so I already downloaded it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 471,
      "text": "and here I'm reading the data set getting the first reading the data set getting the first reading the data set getting the first 1,000 characters and printing the first 1,000 characters and printing the first 1,000 characters and printing the first 100 100 100 now remember that gpt2 has uh roughly a now remember that gpt2 has uh roughly a now remember that gpt2 has uh roughly a compression ratio the tokenizer has a compression ratio the tokenizer has a compression ratio the tokenizer has a compression ratio of rly 3 to1 so th000 compression ratio of rly 3 to1 so th000 compression ratio of rly 3 to1 so th000 characters is roughly 300 tokens here uh characters is roughly 300 tokens here uh characters is roughly 300 tokens here uh that will come out of this in the slice that will come out of this in the slice that will come out of this in the slice that we're currently getting so this is that we're currently getting so this is that we're currently getting so this is the first few uh the first few uh the first few uh characters and uh if you want to get a characters and uh if you want to get a characters and uh if you want to get a few more statistics on this we can do few more statistics on this we can do few more statistics on this we can do work count on input.txt work count on input.txt work count on input.txt so we can see that this is uh 40,000 so we can see that this is uh 40,000 so we can see that this is uh 40,000 lines about 200,000 words in this data lines about 200,000 words in this data lines about 200,000 words in this data set and about 1 million bytes in this set and about 1 million bytes in this set and about 1 million bytes in this file and knowing that this file is only file and knowing that this file is only file and knowing that this file is only asky characters there's no crazy unic asky characters there's no crazy unic asky characters there's no crazy unic code here as far as I know",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 472,
      "text": "and so every code here as far as I know",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 473,
      "text": "and so every code here as far as I know",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 474,
      "text": "and so every asky character is encoded with one bite asky character is encoded with one bite asky character is encoded with one bite and so this is uh the same number and so this is uh the same number and so this is uh the same number roughly a million characters inside this roughly a million characters inside this roughly a million characters inside this data set so that's the data set size uh data set so that's the data set size uh data set so that's the data set size uh by default very small and minimal data by default very small and minimal data by default very small and minimal data set for debugging to get us off the set for debugging to get us off the set for debugging to get us off the ground in order to tokenize this data ground in order to tokenize this data ground in order to tokenize this data set we're going to get Tik token set we're going to get Tik token set we're going to get Tik token encoding for gbt2 encode the data uh the encoding for gbt2 encode the data uh the encoding for gbt2 encode the data uh the first um 1,000 characters",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 475,
      "text": "and then I'm first um 1,000 characters",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 476,
      "text": "and then I'm first um 1,000 characters",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 477,
      "text": "and then I'm only going to print the first 24 tokens only going to print the first 24 tokens only going to print the first 24 tokens so these are the tokens as a list of so these are the tokens as a list of so these are the tokens as a list of integers and if you can read gpt2 tokens integers and if you can read gpt2 tokens integers and if you can read gpt2 tokens you will see that 198 here you'll you will see that 198 here you'll you will see that 198 here you'll recognize that as the slashing character recognize that as the slashing character recognize that as the slashing character so that is a new line and then here for so that is a new line and then here for so that is a new line and then here for example we have two new lines so that's example we have two new lines so that's example we have two new lines so that's 198 twice here uh so this is just a 198 twice here uh so this is just a 198 twice here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 478,
      "text": "uh so this is just a tokenization of the first 24 tokens so tokenization of the first 24 tokens so tokenization of the first 24 tokens so what we want to do now is we want to what we want to do now is we want to what we want to do now is we want to actually process these token sequences actually process these token sequences actually process these token sequences and feed them into a Transformer and in and feed them into a Transformer and in and feed them into a Transformer and in particular we want them we want to particular we want them we want to particular we want them we want to rearrange these tokens into this idx rearrange these tokens into this idx rearrange these tokens into this idx variable that we're going to be feeding variable that we're going to be feeding variable that we're going to be feeding into the Transformer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 479,
      "text": "so we don't want a into the Transformer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 480,
      "text": "so we don't want a into the Transformer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 481,
      "text": "so we don't want a single very long onedimensional sequence single very long onedimensional sequence single very long onedimensional sequence we want an entire batch where each we want an entire batch where each we want an entire batch where each sequence is up to uh is basically T sequence is up to uh is basically T sequence is up to uh is basically T tokens and T cannot be larger than the tokens and T cannot be larger than the tokens and T cannot be larger than the maximum sequence length and then we have maximum sequence length",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 482,
      "text": "and then we have maximum sequence length",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 483,
      "text": "and then we have these t uh tlong uh sequences of tokens these t",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 484,
      "text": "uh tlong uh sequences of tokens these t uh tlong uh sequences of tokens and we have B independent examples of and we have B independent examples of and we have B independent examples of sequences",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 485,
      "text": "so how can we create a b BYT sequences",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 486,
      "text": "so how can we create a b BYT sequences",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 487,
      "text": "so how can we create a b BYT tensor that we can feed into the forward tensor that we can feed into the forward tensor that we can feed into the forward out of these onedimensional out of these onedimensional out of these onedimensional sequences so here's my favorite way to sequences so here's my favorite way to sequences so here's my favorite way to to achieve this uh so if we take torch to achieve this uh so if we take torch to achieve this uh so if we take torch and then we create a tensor object out and then we create a tensor object out and then we create a tensor object out of this list of integers and just the of this list of integers and just the of this list of integers and just the first 24 tokens my favorite way to do first 24 tokens my favorite way to do first 24 tokens my favorite way to do this is basically you do a do view of um this is basically you do a do view of um this is basically you do a do view of um of uh for example 4x6 which multiply to of uh for example 4x6 which multiply to of uh for example 4x6 which multiply to 24 and so it's just a two-dimensional 24 and so it's just a two-dimensional 24 and so it's just a two-dimensional rearrangement of these tokens and you'll rearrangement of these tokens and you'll rearrangement of these tokens and you'll is that when you view this is that when you view this is that when you view this onedimensional sequence as onedimensional sequence as onedimensional sequence as two-dimensional 4x6 here the first six two-dimensional 4x6 here the first six two-dimensional 4x6 here the first six uh tokens uh up to here end up being the uh tokens uh up to here end up being the uh tokens uh up to here end up being the first row the next six tokens here end first row the next six tokens here end first row the next six tokens here end up being the second row and so on and so up being the second row and so on and so up being the second row and so on and so basically it's just going to stack up basically it's just going to stack up basically it's just going to stack up this the um every six tokens in this this the um every six tokens in this this the um every six tokens in this case as independent rows and it creates case as independent rows and it creates case as independent rows and it creates a batch of tokens in this case and so a batch of tokens in this case and so a batch of tokens in this case and so for example if we are token 25 in the for example if we are token 25 in the for example if we are token 25 in the Transformer when we feed this in and Transformer when we feed this in and Transformer when we feed this in and this becomes the idx this token is going this becomes the idx this token is going this becomes the idx this token is going to see these three tokens and it's going to see these three tokens and it's going to see these three tokens and it's going to try to predict that 198 comes to try to predict that 198 comes to try to predict that 198 comes next so in this way we are able to next so in this way we are able to next so in this way we are able to create this two-dimensional batch that's create this two-dimensional batch that's create this two-dimensional batch that's that's quite nice now in terms of the that's quite nice now in terms of the that's quite nice now in terms of the label that we're going to need for the label that we're going to need for the label that we're going to need for the Target to calculate the loss function Target to calculate the loss function Target to calculate the loss function how do we get that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 488,
      "text": "well we could write how do we get that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 489,
      "text": "well we could write how do we get that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 490,
      "text": "well we could write some code inside the forward pass some code inside the forward pass some code inside the forward pass because we know that the next uh token because we know that the next uh token because we know that the next uh token in a sequence which is the label is just in a sequence which is the label is just in a sequence which is the label is just to the right of us",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 491,
      "text": "but you'll notice to the right of us but you'll notice to the right of us but you'll notice that actually we for this token at the that actually we for this token at the that actually we for this token at the very end 13 we don't actually have the very end 13 we don't actually have the very end 13 we don't actually have the next correct token because we didn't next correct token because we didn't next correct token because we didn't load it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 492,
      "text": "so uh we actually didn't get load it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 493,
      "text": "so uh we actually didn't get load it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 494,
      "text": "so uh we actually didn't get enough information here so I'll show you enough information here so I'll show you enough information here so I'll show you my favorite way of basically getting my favorite way of basically getting my favorite way of basically getting these batches",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 495,
      "text": "and I like to personally these batches",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 496,
      "text": "and I like to personally these batches",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 497,
      "text": "and I like to personally have not just the input to the have not just the input to the have not just the input to the Transformer which I like to call X but",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 498,
      "text": "I Transformer which I like to call X but I Transformer which I like to call X",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 499,
      "text": "but I also like to create the labels uh tensor also like to create the labels uh tensor also like to create the labels uh tensor which is of the exact same size as X but which is of the exact same size as X but which is of the exact same size as X but contains the targets at every single contains the targets at every single contains the targets at every single position position position and so here's the way that I like to do and so here's the way that I like to do and so here's the way that I like to do that I like to make sure that I fetch that I like to make sure that I fetch that I like to make sure that I fetch plus one uh token because we need the plus one uh token because we need the plus one uh token because we need the ground Truth for the very last token uh ground Truth for the very last token uh ground Truth for the very last",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 500,
      "text": "token uh for for for 13 and then when we're creating the 13",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 501,
      "text": "and then when we're creating the 13 and then when we're creating the input we take everything up to the last input we take everything up to the last input we take everything up to the last token not including and view it as 4x6 token not including and view it as 4x6 token not including and view it as 4x6 and when we're creating targets we do and when we're creating targets we do and when we're creating targets we do the buffer but starting at index one not the buffer but starting at index one not the buffer but starting at index one not index zero so we're skipping the first index zero so we're skipping the first index zero so we're skipping the first element and we view it in the exact same element and we view it in the exact same element and we view it in the exact same size",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 502,
      "text": "and then when I print this size",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 503,
      "text": "and then when I print this size and then when I print this here's what happens where we see that here's what happens where we see that here's what happens where we see that basically as an example for this token basically as an example for this token basically as an example for this token 25 its Target was 198",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 504,
      "text": "and that's now 25 its Target was 198",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 505,
      "text": "and that's now 25 its Target was 198",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 506,
      "text": "and that's now just stored at the exact same position just stored at the exact same position just stored at the exact same position in the Target tensor which is 198 and in the Target tensor which is 198 and in the Target tensor which is 198 and also this last token 13 now has its also this last token 13 now has its also this last token 13 now has its label which is 198 and that's just label which is 198",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 507,
      "text": "and that's just label which is 198 and that's just because we loaded this plus one here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 508,
      "text": "so because we loaded this plus one here so because we loaded this plus one here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 509,
      "text": "so basically this is the way I like to do basically this is the way I like to do basically this is the way I like to do it you take long sequences you uh view it you take long sequences you uh view it you take long sequences you uh view them in two- dimensional terms so that them in two- dimensional terms so that them in two- dimensional terms so that you get batch of time and then we make you get batch of time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 510,
      "text": "and then we make you get batch of time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 511,
      "text": "and then we make sure to load one additional token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 512,
      "text": "so we sure to load one additional token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 513,
      "text": "so we sure to load one additional",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 514,
      "text": "token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 515,
      "text": "so we basically load a buffer of tokens of B * basically load a buffer of tokens of B * basically load a buffer of tokens of B *",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 516,
      "text": "t+ one",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 517,
      "text": "and then we sort of offset things t+ one",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 518,
      "text": "and then we sort of offset things t+ one",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 519,
      "text": "and then we sort of offset things and view them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 520,
      "text": "and then we have two and view them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 521,
      "text": "and then we have two and view them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 522,
      "text": "and then we have two tensors one of them is the input to the tensors one of them is the input to the tensors one of them is the input to the Transformer and the other exactly is the Transformer and the other exactly is the Transformer and the other exactly is the labels and so let's now reorganize this labels and so let's now reorganize this labels and so let's now reorganize this code and um create a very simple data code and um create a very simple data code and um create a very simple data loader object that tries to basically loader object that tries to basically loader object that tries to basically load these tokens and um feed them to load these tokens and um feed them to load these tokens and um feed them to the Transformer and calculate the loss the Transformer and calculate the loss the Transformer and calculate the loss",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 523,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 524,
      "text": "so I reshuffled the code here uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 525,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 526,
      "text": "so I reshuffled the code here uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 527,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 528,
      "text": "so I reshuffled the code here uh accordingly so as you can see here I'm accordingly so as you can see here I'm accordingly so as you can see here I'm temporarily overwriting U to run a CPU temporarily overwriting U to run a CPU temporarily overwriting U to run a CPU and importing TI token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 529,
      "text": "and all of this and importing TI token and all of this and importing TI token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 530,
      "text": "and all of this should look familiar we're loading a should look familiar we're loading a should look familiar we're loading a th000 characters I'm setting BT to just th000 characters I'm setting BT to just th000 characters I'm setting BT to just be 4 and 32 right now just because we're be 4 and 32 right now just because we're be 4 and 32 right now just because we're debugging we just want to have a single debugging we just want to have a single debugging we just want to have a single batch that's very small and all of this batch that's very small and all of this batch that's very small and all of this should now look familiar and follows should now look familiar and follows should now look familiar and follows what we did on the right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 531,
      "text": "and then here what we did on the right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 532,
      "text": "and then here what we did on the right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 533,
      "text": "and then here we get the we create the model and get we get the we create the model and get we get the we create the model and get the lits",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 534,
      "text": "and so so here as you see I the lits",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 535,
      "text": "and so so here as you see I the lits",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 536,
      "text": "and so so here as you see I already ran this only runs in a few already ran this only runs in a few already ran this only runs in a few seconds but because we have a batch of seconds but because we have a batch of seconds but because we have a batch of uh 4X 32 our lits are now of size 4X 32x uh 4X 32 our lits are now of size 4X 32x uh 4X 32 our lits are now of size 4X 32x 50257 so those are the lit for what 50257",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 537,
      "text": "so those are the lit for what 50257",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 538,
      "text": "so those are the lit for what comes next at every position and now we comes next at every position and now we comes next at every position and now we have the labels which are stored in y so have the labels which are stored in y so have the labels which are stored in y so now is the time to calculate the loss now is the time to calculate the loss now is the time to calculate the loss and then do the backward pass and then and then do the backward pass and then and then do the backward pass and then the optimization so let's first the optimization so let's first the optimization so let's first calculate the calculate the calculate the loss",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 539,
      "text": "okay so to calculate the loss we're loss",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 540,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 541,
      "text": "so to calculate the loss we're loss",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 542,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 543,
      "text": "so to calculate the loss we're going to adjust the forward function of going to adjust the forward function of going to adjust the forward function of this NN module in the model and in this NN module in the model and in this NN module in the model and in particular we're not just going to be particular we're not just going to be particular we're not just going to be returning logits",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 544,
      "text": "but also we're going to returning logits",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 545,
      "text": "but also we're going to returning logits",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 546,
      "text": "but also we're going to return the loss uh and we're going to return the loss uh and we're going to return the loss uh and we're going to not just pass in the input in thees but not just pass in the input in thees but not just pass in the input in thees but also the targets uh in y",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 547,
      "text": "and now we will also the targets uh in y and now we will also the targets uh in y and now we will print not Lo just.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 548,
      "text": "shape anymore we're print not Lo just.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 549,
      "text": "shape anymore we're print not Lo just.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 550,
      "text": "shape anymore we're actually going to print the loss actually going to print the loss actually going to print the loss function and then c. exit of zero so function and then c. exit of zero so function and then c. exit of zero so that we skip some of the sampling logic that we skip some of the sampling logic that we skip some of the sampling logic so now let's swing up to the forward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 551,
      "text": "so now let's swing up to the forward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 552,
      "text": "so now let's swing up to the forward function which gets called there because function which gets called there because function which gets called there because now we also have these optional now we also have these optional now we also have these optional targets and when we get the targets we targets and when we get the targets we targets and when we get the targets we can also calculate uh the loss and can also calculate uh the loss and can also calculate uh the loss and remember that we want to basically remember that we want to basically remember that we want to basically return uh log just loss and loss by return uh log just loss and loss by return uh log just loss and loss by default is none default is none default is none",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 553,
      "text": "but",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 554,
      "text": "but but um let's put this here if uh targets is um let's put this here if uh targets is um let's put this here if uh targets is not none then we want to calculate loss not none then we want to calculate loss not none then we want to calculate loss and co-pilot is already getting excited and co-pilot is already getting excited and co-pilot is already getting excited here and calculating the what looks to here and calculating the what looks to here and calculating the what looks to be correct loss it is using the cross be correct loss it is using the cross be correct loss it is using the cross entropy loss as is documented here uh so entropy loss as is documented here uh so entropy loss as is documented here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 555,
      "text": "uh so this is a function in pytorch under the this is a function in pytorch under the this is a function in pytorch under the functional now what is actually functional now what is actually functional now what is actually happening here because it looks a little happening here because it looks a little happening here because it looks a little bit scary uh basically uh the F that bit scary uh basically uh the F that bit scary uh basically uh the F that cross entropy does not like cross entropy does not like cross entropy does not like multi-dimensional inputs it can't take a multi-dimensional inputs it can't take a multi-dimensional inputs it can't take a b BYT by vocap size",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 556,
      "text": "so what's happening b BYT by vocap size",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 557,
      "text": "so what's happening b BYT by vocap size so what's happening here is that we are flattening out this here is that we are flattening out this here is that we are flattening out this three-dimensional tensor into just two three-dimensional tensor into just two three-dimensional tensor into just two Dimensions the First Dimension is going Dimensions the First Dimension is going Dimensions the First Dimension is going to be calculated automatically",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 558,
      "text": "and it's to be calculated automatically",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 559,
      "text": "and it's to be calculated automatically",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 560,
      "text": "and it's going to be B * T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 561,
      "text": "and then the last going to be B * T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 562,
      "text": "and then the last going to be B * T and then the last Dimension is vocap size so basically Dimension is vocap size so basically Dimension is vocap size so basically this is uh flattening out this this is uh flattening out this this is uh flattening out this three-dimensional tensor of logits to three-dimensional tensor of logits to three-dimensional tensor of logits to just be two- dimensional B",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 563,
      "text": "* T all just be two- dimensional B",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 564,
      "text": "* T all just be two- dimensional B",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 565,
      "text": "* T all individual examples and vocap size on uh individual examples and vocap size on uh individual examples and vocap size on uh in terms of the length of each row and in terms of the length of each row and in terms of the length of each row",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 566,
      "text": "and then it's also flattening out the then it's also flattening out the then it's also flattening out the targets which are also two- dimensional targets which are also two- dimensional targets which are also two- dimensional at this stage but we're going to just at this stage",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 567,
      "text": "but we're going to just at this stage but we're going to just flatten them out so they're just a flatten them out so they're just a flatten them out so they're just a single tensor of B * T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 568,
      "text": "and this can then single tensor of B * T and this can then single tensor of B * T and this can then pass into cross entropy to calculate a pass into cross entropy to calculate a pass into cross entropy to calculate a loss which we return so this should loss which we return so this should loss which we return so this should basically at this point run because this basically at this point run because this basically at this point run because this is not too complicated is not too complicated is not too complicated so let's run it and let's see if we so let's run it and let's see if we so let's run it and let's see if we should be printing the loss and here we see that we printed 11 loss and here we see that we printed 11 uh roughly and so uh roughly and so uh roughly and so um and notice that this is the tensor of um and notice that this is the tensor of um and notice that this is the tensor of a single element which is this number 11 a single element which is this number 11 a single element which is this number 11 now we also want to be able to calculate now we also want to be able to calculate now we also want to be able to calculate a reasonable uh kind of starting point a reasonable uh kind of starting point a reasonable uh kind of starting point for a random rationalized Network",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 569,
      "text": "so we for a random rationalized Network so we for a random rationalized Network so we covered this in previous videos but our covered this in previous videos but our covered this in previous videos but our vocabulary size is vocabulary size is vocabulary size is 50257 at initialization of the network 50257 at initialization of the network 50257 at initialization of the network you would hope that um every vocab you would hope that um every vocab you would hope that um every vocab element is getting roughly a uniform element is getting roughly a uniform element is getting roughly a uniform probability uh so that we're not probability uh so that we're not probability uh so that we're not favoring at initialization any token way favoring at initialization any token way favoring at initialization any token way too much we're not confidently wrong at too much we're not confidently wrong at too much we're not confidently wrong at initialization so what we're hoping is initialization so what we're hoping is initialization so what we're hoping is that the probability of any arbitrary that the probability of any arbitrary that the probability of any arbitrary token is roughly 1 over 50,2 57 and now token is roughly 1 over 50,2 57 and now token is roughly 1 over 50,2 57 and now we can sanity check the loss because we can sanity check the loss because we can sanity check the loss because remember that the cross entropy loss is remember that the cross entropy loss is remember that the cross entropy loss is just basically the negative um log just basically the negative um log just basically the negative um log likelihood so if we now take this likelihood so if we now take this likelihood so if we now take this probability and we take it through the probability and we take it through the probability and we take it through the natural logarithm and then we do the natural logarithm and then we do the natural logarithm and then we do the negative that is the loss we expect at negative that is the loss we expect at negative that is the loss we expect at initialization and we covered this in initialization and we covered this in initialization and we covered this in previous videos so I would expect previous videos so I would expect previous videos so I would expect something around 10.82 and we're seeing something around 10.82 and we're seeing something around 10.82 and we're seeing something around 11",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 570,
      "text": "so it's not way off something around 11",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 571,
      "text": "so it's not way off something around 11",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 572,
      "text": "so it's not way off this is roughly the probability I expect this is roughly the probability I expect this is roughly the probability I expect at initialization so that tells me that at initialization so that tells me that at initialization so that tells me that the at initialization or probability the at initialization or probability the at initialization or probability distribtion is roughly diffused it's a distribtion is roughly diffused it's a distribtion is roughly diffused it's a good starting point and we can now uh good starting point",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 573,
      "text": "and we can now uh good starting point",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 574,
      "text": "and we can now uh perform the optimization and tell the perform the optimization and tell the perform the optimization and tell the network which elements you know should network which elements you know",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 575,
      "text": "should network which elements you know should follow correctly in what order so at follow correctly in what order so at follow correctly in what order so at this point we can do a l step backward this point we can do a l step backward this point we can do a l step backward calculate the gradients and do an calculate the gradients and do an calculate the gradients and do an optimization so let's get to that okay optimization so let's get to that okay optimization so let's get to that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 576,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 577,
      "text": "so let's do the optimization now um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 578,
      "text": "so so let's do the optimization now um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 579,
      "text": "so so let's do the optimization now um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 580,
      "text": "so here we here we here we have the loss is this is how we get the have the loss is this is how we get the have the loss is this is how we get the loss but now basically we want a load loss but now basically we want a load loss but now basically we want a load for Loop here so 4 I in range let's do for Loop here so 4 I in range let's do for Loop here so 4 I in range let's do 50 steps or something like that uh let's 50 steps or something like that uh let's 50 steps or something like that uh let's create an Optimizer object in create an Optimizer object in create an Optimizer object in pytorch um and so here we are using the pytorch um and so here we are using the pytorch um and so here we are using the atom um Optimizer which is an atom um Optimizer which is an atom um Optimizer which is an alternative to the stochastic radian alternative to the stochastic radian alternative to the stochastic radian descent Optimizer SGD that we were using descent Optimizer SGD that we were using descent Optimizer SGD that we were using so SGD is a lot simpler atom is a bit so SGD is a lot simpler atom is a bit so SGD is a lot simpler atom is a bit more involved",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 581,
      "text": "and I actually more involved",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 582,
      "text": "and I actually more involved",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 583,
      "text": "and I actually specifically like the atom W variation specifically like the atom W variation specifically like the atom W variation because in my opinion it kind of just because in my opinion it kind of just because in my opinion it kind of just like fixes a bug um so adom w is a bug like fixes a bug",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 584,
      "text": "um so adom w is a bug like fixes a bug",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 585,
      "text": "um so adom w is a bug fix of atom is what I would say when we fix of atom is what I would say when we fix of atom is what I would say when we go to the documentation for atom go to the documentation for atom go to the documentation for atom W",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 586,
      "text": "oh my W",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 587,
      "text": "oh my W",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 588,
      "text": "oh my",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 589,
      "text": "gosh we see um that it takes a bunch of gosh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 590,
      "text": "we see um that it takes a bunch of gosh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 591,
      "text": "we see um that it takes a bunch of hyper parameters and it's a little bit hyper parameters and it's a little bit hyper parameters and it's a little bit more complicated than the SGD we were more complicated than the SGD we were more complicated than the SGD we were looking at before uh because in addition looking at before uh because in addition looking at before uh because in addition to basically updating the parameters to basically updating the parameters to basically updating the parameters with the gradient uh scaled by the with the gradient uh scaled by the with the gradient uh scaled by the Learning rate it keeps these buffers Learning rate it keeps these buffers Learning rate it keeps these buffers around and it keeps two buffers the m around and it keeps two buffers the m around and it keeps two buffers the m and the V which it calls the first and and the V which it calls the first and and the V which it calls the first and the second moment",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 592,
      "text": "so something that the second moment so something that the second moment so something that looks a bit like momentum and something looks a bit like momentum and something looks a bit like momentum and something that looks a bit like RMS prop if you're that looks a bit like RMS prop if you're that looks a bit like RMS prop if you're familiar with it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 593,
      "text": "but you don't have to familiar with it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 594,
      "text": "but you don't have to familiar with it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 595,
      "text": "but you don't have to be it's just kind of a normalization be it's just",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 596,
      "text": "kind of a normalization be it's just kind of a normalization that happens on each gradient element that happens on each gradient element that happens on each gradient element individually and speeds up the individually and speeds up the individually and speeds up the optimization especially for language optimization especially for language optimization especially for language models but I'm not going to go into the models but I'm not going to go into the models but I'm not going to go into the detail right here we're going to treat detail right here we're going to treat detail right here we're going to treat it as a bit of a black box and it just it as a bit of a black box and it just it as a bit of a black box and it just optimizes um the objective faster than optimizes um the objective faster than optimizes um the objective faster than SGD which is what we've seen in the SGD which is what we've seen in the SGD which is what we've seen in the previous lectures so let's use it as a previous lectures so let's use it as a previous lectures so let's use it as a black box in our case uh create the black box in our case uh create the black box in our case uh create the optimizer object and optimizer object and optimizer object and then go through the optimization the first thing to always make sure the the first thing to always make sure the co-pilot did not forget to zero the co-pilot did not forget to zero the co-pilot did not forget to zero the gradients so um always remember that you gradients so um always remember that you gradients so um always remember that you have to start with a zero gradient then have to start with a zero gradient then have to start with a zero gradient then when you get your loss and you do a DOT when you get your loss and you do a DOT when you get your loss and you do a DOT backward dot backward adds to gradients backward dot backward adds to gradients backward dot backward adds to gradients so it deposits gradients it it always so it deposits gradients",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 597,
      "text": "it it always so it deposits gradients it it always does a plus equals on whatever the does a plus equals on whatever the does a plus equals on whatever the gradients are which is why you must set gradients are which is why you must set gradients are which is why you must set them to zero so this accumulates the them to zero so this accumulates the them to zero so this accumulates the gradient from this loss",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 598,
      "text": "and then we call gradient from this loss",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 599,
      "text": "and then we call gradient from this loss",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 600,
      "text": "and then we call the step function on the optimizer to um the step function on the optimizer to um the step function on the optimizer to um update the parameters and to um decrease update the parameters and to um decrease update the parameters and to um decrease the the loss",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 601,
      "text": "and then we print a step and the loss",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 602,
      "text": "and then we print a step and the loss",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 603,
      "text": "and then we print a step and the loss do item is used here because loss loss do item is used here because loss loss do item is used here because loss is a tensor with a single element do is a tensor with a single element do is a tensor with a single element do item will actually uh convert that to a item will actually uh convert that to a item will actually uh convert that to a single float and this float will live single float and this float will live single float and this float will live not will will live on the CPU so this not will will live on the CPU so this not will will live on the CPU so this gets to some of the internals again of gets to some of the internals again of gets to some of the internals again of the devices but loss is a is a tensor the devices but loss is a is a tensor the devices but loss is a is a tensor with a single element and it lifts on with a single element and it lifts on with a single element and it lifts on GPU for me because I'm using gpus when GPU for me because I'm using gpus when GPU for me because I'm using gpus when you call item P torch behind the scenes you call item P torch behind the scenes you call item P torch behind the scenes will take that one-dimensional tensor will take that one-dimensional tensor will take that one-dimensional tensor ship it back to the CPU uh memory and ship it back to the CPU uh memory and ship it back to the CPU uh memory and convert it into a float that we can just convert it into a float that we can just convert it into a float that we can just print so this is the optimization and print so this is the optimization and print so this is the optimization and this should probably just this should probably just this should probably just work let's see what work let's see what work let's see what happens actually sorry let me instead of happens actually sorry let me instead of happens actually sorry let me instead of using CPU override let me delete that so using CPU override let me delete that so using CPU override let me delete that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 604,
      "text": "so this is a bit faster for me and it runs this is a bit faster for me and it runs this is a bit faster for me and it runs on Cuda",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 605,
      "text": "oh expected all tensors to be on the oh expected all tensors to be on the same device but found at least two same device but found at least two same device but found at least two devices Cuda zero and CPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 606,
      "text": "so Cuda zero devices Cuda zero and CPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 607,
      "text": "so Cuda zero devices Cuda zero and CPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 608,
      "text": "so Cuda zero is the zeroth GPU because I actually is the zeroth GPU because I actually is the zeroth GPU because I actually have eight gpus on this box",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 609,
      "text": "uh so the have eight gpus on this box uh so the have eight gpus on this box",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 610,
      "text": "uh so the zeroth GPU in my box and CPU and model zeroth GPU in my box and CPU and model zeroth GPU in my box and CPU and model we have moved to device but when I was we have moved to device but when I was we have moved to device but when I was writing this code I actually introduced writing this code I actually introduced writing this code I actually introduced a bug because buff we never moved to a bug because buff we never moved to a bug because buff we never moved to device and you have to be careful device and you have to be careful device and you have to be careful because you can't just do buff dot two because you can't just do buff dot two because you can't just do buff dot two of of of device um it's not stateful it doesn't device um it's not stateful it doesn't device um it's not stateful it doesn't convert it to be a device it instead uh convert it to be a device it instead uh convert it to be a device it instead uh returns pointer to a new memory which is returns pointer to a new memory which is returns pointer to a new memory which is on the device so you see how we can just on the device",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 611,
      "text": "so you see how we can just on the device so you see how we can just do model that two a device that does not do model that two a device that does not do model that two a device that does not apply to tensors you have to do buff apply to tensors you have to do buff apply to tensors you have to do buff equals equals equals um b.2 device and then this should work um b.2 device and then this should work um b.2 device and then this should work",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 612,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 613,
      "text": "so what do we expect to see",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 614,
      "text": "we okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 615,
      "text": "so what do we expect to see",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 616,
      "text": "we okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 617,
      "text": "so what do we expect to see we expect to see a reasonable loss in the expect to see a reasonable loss in the expect to see a reasonable loss in the beginning",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 618,
      "text": "and then we continue to beginning",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 619,
      "text": "and then we continue to beginning",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 620,
      "text": "and then we continue to optimize just the single batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 621,
      "text": "and so we optimize just the single batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 622,
      "text": "and so we optimize just the single batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 623,
      "text": "and so we want to see that we can overfit this want to see that we can overfit this want to see that we can overfit this single batch we can we can crush this single batch we can we can crush this single batch we can we can crush this little batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 624,
      "text": "and we can perfectly little batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 625,
      "text": "and we can perfectly little batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 626,
      "text": "and we can perfectly predict the indices on just this little predict the indices on just this little predict the indices on just this little batch and indeed that is roughly what batch and indeed that is roughly what batch and indeed that is roughly what we're seeing here we're seeing here we're seeing here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 627,
      "text": "so um we started off at roughly 10.82 11 so um we started off at roughly 10.82 11",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 628,
      "text": "so um we started off at roughly 10.82 11 in this case and then as we continue in this case and then as we continue in this case and then as we continue optimizing on this single batch without optimizing on this single batch without optimizing on this single batch without loading new examples we are making sure loading new examples we are making sure loading new examples we are making sure that we can overfit a single batch and that we can overfit a single batch and that we can overfit a single batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 629,
      "text": "and we are getting to very very low loss",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 630,
      "text": "so we are getting to very very low loss",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 631,
      "text": "so we are getting to very very low loss so the Transformer is memorizing this the Transformer is memorizing this the Transformer is memorizing this single individual batch and one more single individual batch and one more single individual batch and one more thing I didn't mention is uh the thing I didn't mention is uh the thing I didn't mention is uh the learning rate here is 3 E4 which is a learning rate here is 3 E4 which is a learning rate here is 3 E4 which is a pretty good default for most uh pretty good default for most uh pretty good default for most uh optimizations that you want to run at a optimizations that you want to run at a optimizations that you want to run at a very early debugging stage so this is very early debugging stage so this is very early debugging stage",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 632,
      "text": "so this is our simple inter Loop and uh we are our simple inter Loop and uh we are our simple inter Loop and uh we are overfitting a single batch and this overfitting a single batch and this overfitting a single batch and this looks good so now what uh what comes looks good",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 633,
      "text": "so now what uh what comes looks good so now what uh what comes next is we don't just want to overfit a next is we don't just want to overfit a next is we don't just want to overfit a single batch we actually want to do an single batch we actually want to do an single batch we actually want to do an optimization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 634,
      "text": "so we actually need to optimization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 635,
      "text": "so we actually need to optimization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 636,
      "text": "so we actually need to iterate these XY batches and create a iterate these XY batches and create a iterate these XY batches and create a little data loader uh that makes sure little data loader uh that makes sure little data loader uh that makes sure that we're always getting a fresh batch that we're always getting a fresh batch that we're always getting a fresh batch and that we're actually optimizing a and that we're actually optimizing a and that we're actually optimizing a reasonable objective so let's do that reasonable objective",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 637,
      "text": "so let's do that reasonable objective so let's do that next",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 638,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 639,
      "text": "so this is what I came up with next",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 640,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 641,
      "text": "so this is what I came up with next",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 642,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 643,
      "text": "so this is what I came up with",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 644,
      "text": "and I wrote a little data loader and I wrote a little data loader and I wrote a little data loader light",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 645,
      "text": "um so what this data loader does light um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 646,
      "text": "so what this data loader does light um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 647,
      "text": "so what this data loader does is we're importing the token up here is we're importing the token up here is we're importing the token up here we're reading the entire text file from we're reading the entire text file from we're reading the entire text file from this single input.txt this single input.txt this single input.txt tokenizing it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 648,
      "text": "and then we're just tokenizing it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 649,
      "text": "and then we're just tokenizing it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 650,
      "text": "and then we're just printing the number of tokens in total printing the number of tokens in total printing the number of tokens in total and the number of batches in a single and the number of batches in a single and the number of batches in a single Epoch of iterating over this data set so Epoch of iterating over this data set so Epoch of iterating over this data set so how many unique batches do we output how many unique batches do we output how many unique batches do we output before we loop back around the beginning before we loop back around the beginning before we loop back around the beginning of the document and start reading it of the document and start reading it of the document and start reading it again",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 651,
      "text": "so we start off at position zero again",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 652,
      "text": "so we start off at position zero again",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 653,
      "text": "so we start off at position zero",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 654,
      "text": "and then we simply walk the document in",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 655,
      "text": "and then we simply walk the document in",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 656,
      "text": "and then we simply walk the document in batches of B *",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 657,
      "text": "T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 658,
      "text": "so we take chunks of B batches of B *",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 659,
      "text": "T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 660,
      "text": "so we take chunks of B batches of B *",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 661,
      "text": "T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 662,
      "text": "so we take chunks of B * T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 663,
      "text": "and then always Advance by B * T and * T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 664,
      "text": "and then always Advance by B * T and * T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 665,
      "text": "and then always Advance by B * T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 666,
      "text": "and um it's important to note that we're um it's important to note that we're um it's important to note that we're always advancing our position by exactly always advancing our position by exactly always advancing our position by exactly B * T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 667,
      "text": "but when we're fetching the tokens B * T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 668,
      "text": "but when we're fetching the tokens B * T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 669,
      "text": "but when we're fetching the tokens we're actually fetching from current we're actually fetching from current we're actually fetching from current position to B * t",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 670,
      "text": "+ 1",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 671,
      "text": "and we need that position to B * t + 1",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 672,
      "text": "and we need that position to B * t + 1",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 673,
      "text": "and we need that plus one because remember uh we need the plus one because remember uh we need the plus one because remember uh we need the target token target token target token um for the last token in the current um for the last token in the current um for the last token in the current batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 674,
      "text": "and so that way we can do um the batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 675,
      "text": "and so that way we can do um the batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 676,
      "text": "and so that way we can do um the XY exactly as we did it before and if we XY exactly as we did it before and if we XY exactly as we did it before and if we are to um run out of data we'll just are to um run out of data we'll just are to um run out of data we'll just loop back around to zero so this is one loop back around to zero so this is one loop back around to zero so this is one way to write a very very simple data way to write a very very simple data way to write a very very simple data loader um that simply just goes through loader um that simply just goes through loader um that simply just goes through the file in chunks and is good enough the file in chunks and is good enough the file in chunks and is good enough for us uh for current purposes and we're for us uh for current purposes and we're for us uh for current purposes and we're going to complexify it later and now going to complexify it later and now going to complexify it later and now we'd like to come back around here and we'd like to come back around here and we'd like to come back around here and we'd like to actually use our data we'd like to actually use our data we'd like to actually use our data loader so the import Tik token has moved loader so the import Tik token has moved loader so the import Tik token has moved up and actually all of this is now up",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 677,
      "text": "and actually all of this is now up and actually all of this is now useless",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 678,
      "text": "so instead we just want a train useless so instead we just want a train useless",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 679,
      "text": "so instead we just want a train loader for the training data and we want loader for the training data and we want loader for the training data and we want to use the same hyper parameters for to use the same hyper parameters for to use the same hyper parameters for four so B size was four and time was four so B size was four and time was four so B size was four and time was 32 and then here we need to get the XY 32",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 680,
      "text": "and then here we need to get the XY 32",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 681,
      "text": "and then here we need to get the XY for the current batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 682,
      "text": "so let's see if for the current batch so let's see if for the current batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 683,
      "text": "so let's see if copal gets it because this is simple copal gets it because this is simple copal gets it because this is simple enough",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 684,
      "text": "uh so we call the next batch and enough uh so we call the next batch and enough uh so we call the next batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 685,
      "text": "and then we um make sure that we have to then we um make sure that we have to then we um make sure that we have to move our tensors from CPU to the device move our tensors from CPU to the device move our tensors from CPU to the device so here when I converted the tokens so here when I converted the tokens so here when I converted the tokens notice that I didn't actually move these notice that I didn't actually move these notice that I didn't actually move these tokens to the GPU I left them on CPU tokens to the GPU I left them on CPU tokens to the GPU I left them on CPU which is the default um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 686,
      "text": "and that's just which is the default um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 687,
      "text": "and that's just which is the default um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 688,
      "text": "and that's just because I'm trying not to waste too much because I'm trying not to waste too much because I'm trying not to waste too much memory on the GPU in this case this is a memory on the GPU in this case this is a memory on the GPU in this case this is a tiny data set and it would fit uh but tiny data set and it would fit uh but tiny data set and it would fit uh but it's fine to just uh ship it to GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 689,
      "text": "it's fine to just uh ship it to GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 690,
      "text": "it's fine to just uh ship it to GPU right now for for our purposes right now right now for for our purposes right now right now for for our purposes right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 691,
      "text": "so we get the next batch we keep the",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 692,
      "text": "so we get the next batch we keep the",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 693,
      "text": "so we get the next batch we keep the data loader simple CPU class and then data loader simple CPU class and then data loader simple CPU class",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 694,
      "text": "and then here we actually ship it to the GPU and here we actually ship it to the GPU and here we actually ship it to the GPU and do all the computation and uh let's see do all the computation and uh let's see do all the computation and uh let's see if this runs so python train gbt2 pi and if this runs so python train gbt2 pi and if this runs so python train gbt2 pi and what do we expect to see before this what do we expect to see before this what do we expect to see before this actually happens what we expect to see actually happens what we expect to see actually happens what we expect to see is now we're actually getting the next is now we're actually getting the next is now we're actually getting the next batch so we expect to not overfit a batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 695,
      "text": "so we expect to not overfit a batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 696,
      "text": "so we expect to not overfit a single batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 697,
      "text": "and so I expect our loss to single batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 698,
      "text": "and so I expect our loss to single batch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 699,
      "text": "and so I expect our loss to come down but not too much",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 700,
      "text": "and that's come down but not too much",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 701,
      "text": "and that's come down but not too much",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 702,
      "text": "and that's because I still expect it to come down because I still expect it to come down because I still expect it to come down because in the because in the because in the 50257 tokens many of those tokens never 50257 tokens many of those tokens never 50257 tokens many of those tokens never occur in our data set so there are some occur in our data set so there are some occur in our data set so there are some very easy gains to be made here in the very easy gains to be made here in the very easy gains to be made here in the optimization by for example taking the optimization by for example taking the optimization by for example taking the biases of all the loits that never occur biases of all the loits that never occur biases of all the loits that never occur and driving them to negative infinity and driving them to negative infinity and driving them to negative infinity and that would basically just it's just and that would basically just it's just and that would basically just it's just",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 703,
      "text": "that all of these crazy unic codes or that all of these crazy unic codes or that all of these crazy unic codes or different languages those tokens never different languages those tokens never different languages those tokens never occur so their probability should be occur so their probability should be occur so their probability should be very low and so the gains that we should very low and so the gains that we should very low and so the gains that we should be seeing are along the lines of be seeing are along the lines of be seeing are along the lines of basically deleting the usage of tokens basically deleting the usage of tokens basically deleting the usage of tokens that never occur that's probably most of that never occur that's probably most of that never occur that's probably most of the loss gain that we're going to see at the loss gain that we're going to see at the loss gain that we're going to see at this scale right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 704,
      "text": "uh but we shouldn't this scale right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 705,
      "text": "uh but we shouldn't this scale right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 706,
      "text": "uh but we shouldn't come to a zero uh because um we are only come to a zero uh because um we are only come to a zero uh because um we are only doing 50 iterations",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 707,
      "text": "and I don't think doing 50 iterations",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 708,
      "text": "and I don't think doing 50 iterations",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 709,
      "text": "and I don't think that's enough to do an eoch right now so that's enough to do an eoch right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 710,
      "text": "so that's enough to do an eoch right now so let's see what we let's see what we let's see what we got we",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 711,
      "text": "um we have 338,000 got we um we have 338,000 got we um we have 338,000 tokens which makes sense with our 3:1 tokens which makes sense with our 3:1 tokens which makes sense with our 3:1 compression ratio because there are 1 compression ratio because there are 1 compression ratio because there are 1 million uh characters so one Epoch with million uh characters so one Epoch with million uh characters so one Epoch with the current setting of B and T will take the current setting of B and T will take the current setting of B and T will take 2, 600 batches",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 712,
      "text": "and we're only doing 50 2, 600 batches and we're only doing 50 2, 600 batches",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 713,
      "text": "and we're only doing 50 batches of optimization in batches of optimization in batches of optimization in here so we start off in a familiar here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 714,
      "text": "so we start off in a familiar here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 715,
      "text": "so we start off in a familiar territory as expected and then we seem territory as expected",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 716,
      "text": "and then we seem territory as expected and then we seem to come down to about 6.6 so basically to come down to about 6.6 so basically to come down to about 6.6 so basically things seem to be working",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 717,
      "text": "okay right now things seem to be working",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 718,
      "text": "okay right now things seem to be working",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 719,
      "text": "okay right now with respect to our expectations so with respect to our expectations so with respect to our expectations so that's good",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 720,
      "text": "okay next I want to actually that's good",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 721,
      "text": "okay next I want to actually that's good",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 722,
      "text": "okay next I want to actually fix a bug that we have in our code um fix a bug that we have in our code um fix a bug that we have in our code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 723,
      "text": "um it's not a major bug",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 724,
      "text": "but it is a bug it's not a major bug",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 725,
      "text": "but it is a bug it's not a major bug",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 726,
      "text": "but it is a bug with respect to how gpt2 training uh with respect to how gpt2 training uh with respect to how gpt2 training uh should should should happen um happen um happen um so the buck is the following we were not so the buck is the following we were not so the buck is the following we were not being careful enough when we were being careful enough when we were being careful enough when we were loading the weights from hugging face loading the weights from hugging face loading the weights from hugging face and we actually missed a little detail and we actually missed a little detail and we actually missed a little detail so if we come so if we come so if we come here notice that um the shape of these here notice that um the shape of these here notice that um the shape of these two tensors is the same",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 727,
      "text": "so this one here two tensors is the same",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 728,
      "text": "so this one here two tensors is the same",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 729,
      "text": "so this one here is the token embedding at the bottom of is the token embedding at the bottom of is the token embedding at the bottom of the the Transformer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 730,
      "text": "right so and this one here Transformer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 731,
      "text": "right so and this one here Transformer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 732,
      "text": "right so and this one here is the language modeling head at the top is the language modeling head at the top is the language modeling head at the top of the of the of the Transformer and both of these are Transformer and both of these are Transformer and both of these are basically two-dimensional tensors and basically two-dimensional tensors and basically two-dimensional tensors and they shape is identical",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 733,
      "text": "so here the they shape is identical so here the they shape is identical",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 734,
      "text": "so here the first one is the output embedding the first one is the output embedding the first one is the output embedding the token embedding and the second one is token embedding and the second one is token embedding and the second one is this linear layer at the very top the this linear layer at the very top the this linear layer at the very top the classifier layer both of them are of classifier layer both of them are of classifier layer both of them are of shape shape shape 50257 X 50257 X 50257 X 768 um this one here is giving us our 768 um this one here is giving us our 768 um this one here is giving us our token embeddings at the bottom and this token embeddings at the bottom and this token embeddings at the bottom and this one here is taking the 768 channels of one here is taking the 768 channels of one here is taking the 768 channels of the Transformer and trying to upscale the Transformer and trying to upscale the Transformer and trying to upscale that to 50, 257 to get the Lis for the that to 50, 257 to get the Lis for the that to 50, 257 to get the Lis for the next token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 735,
      "text": "so they're both the same",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 736,
      "text": "next",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 737,
      "text": "token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 738,
      "text": "so they're both the same",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 739,
      "text": "next",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 740,
      "text": "token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 741,
      "text": "so they're both the same shape but more than that actually if you shape but more than that actually if you shape but more than that actually if you look at um comparing their elements um look at um comparing their elements um look at um comparing their elements um in pytorch this is an element wise in pytorch this is an element wise in pytorch this is an element wise equality",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 742,
      "text": "so then we use do all",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 743,
      "text": "and we equality so then we use do all",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 744,
      "text": "and we equality so then we use do all",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 745,
      "text": "and we see that every single element is see that every single element is see that every single element is identical and more than that we see that identical and more than that we see that identical and more than that we see that if we actually look at the data pointer if we actually look at the data pointer if we actually look at the data pointer uh this is what this is a way in pytorch uh this is what this is a way in pytorch uh this is what this is a way in pytorch to get the actual pointer to the uh data to get the actual pointer to the uh data to get the actual pointer to the uh data and the storage we see that actually the and the storage we see that actually the and the storage we see that actually the pointer is identical so not only are pointer is identical",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 746,
      "text": "so not only are pointer is identical so not only are these two separate tensors that happen these two separate tensors that happen these two separate tensors that happen to have the same shape and elements to have the same shape and elements to have the same shape and elements they're actually pointing to the they're actually pointing to the they're actually pointing to the identical tensor",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 747,
      "text": "so what's happening identical tensor",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 748,
      "text": "so what's happening identical tensor",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 749,
      "text": "so what's happening here is that this is a common weight here is that this is a common weight here is that this is a common weight tying scheme uh that actually comes from tying scheme uh that actually comes from tying scheme uh that actually comes from the original the original the original um from the original attention is all um from the original attention is all um from the original attention is all you need paper and actually even the you need paper and actually even the you need paper and actually even the reference before it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 750,
      "text": "so if we come reference before it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 751,
      "text": "so if we come reference before it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 752,
      "text": "so if we come here um eddings and softmax in the attention um eddings and softmax in the attention is all you need paper they mentioned is all you need paper they mentioned is all you need paper they mentioned that in our model we shared the same that in our model we shared the same that in our model we shared the same weight Matrix between the two embedding weight Matrix between the two embedding weight Matrix between the two embedding layers and the pre softmax linear layers and the pre softmax linear layers and the pre softmax linear transformation similar to 30 um so this transformation similar to 30 um so this transformation similar to 30 um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 753,
      "text": "so this is an awkward way to phrase that these is an awkward way to phrase that these is an awkward way to phrase that these two are shared and they're tied and two are shared and they're tied and two are shared and they're tied and they're the same Matrix and the 30",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 754,
      "text": "they're the same Matrix and the 30 they're the same Matrix and the 30 reference is this reference is this reference is this paper",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 755,
      "text": "um so this came out in paper",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 756,
      "text": "um so this came out in paper",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 757,
      "text": "um so this came out in 2017 and you can read the full paper but 2017 and you can read the full paper but 2017 and you can read the full paper but basically it argues for this weight basically it argues for this weight basically it argues for this weight tying scheme and I think intuitively the tying scheme",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 758,
      "text": "and I think intuitively the tying scheme",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 759,
      "text": "and I think intuitively the idea for why you might want to do this idea for why you might want to do this idea for why you might want to do this comes from from this paragraph here and comes from from this paragraph here and comes from from this paragraph here and basically you you can observe basically you you can observe basically you you can observe that um you actually want these two that um you actually want these two that um you actually want these two matrices to behave similar in the matrices to behave similar in the matrices to behave similar in the following sense if two tokens are very following sense if two tokens are very following sense if two tokens are very similar semantically like maybe one of similar semantically like maybe one of similar semantically like maybe one of them is all lowercase and the other one them is all lowercase and the other one them is all lowercase and the other one is all uppercase",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 760,
      "text": "or it's the same token is all uppercase",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 761,
      "text": "or it's the same token is all uppercase",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 762,
      "text": "or it's the same token in a different language or something in a different language or something in a different language or something like that if you have similarity between like that if you have similarity between like that if you have similarity between two tokens presumably you would expect two tokens presumably you would expect two tokens presumably you would expect that they are uh nearby in the token that they are uh nearby in the token that they are uh nearby in the token embedding space but in the exact same embedding space but in the exact same embedding space but in the exact same way you'd expect that if you have two way you'd expect that if you have two way you'd expect that if you have two tokens that are similar semantically tokens that are similar semantically tokens that are similar semantically you'd expect them to get the same you'd expect them to get the same you'd expect them to get the same probabilities at the output of a probabilities at the output of a probabilities at the output of a transformer because they are transformer because they are transformer because they are semantically similar and so both semantically similar and so both semantically similar and so both positions in the Transformer at the very positions in the Transformer at the very positions in the Transformer at the very bottom and at the top have this property bottom and at the top have this property bottom and at the top have this property that similar tokens should have similar that similar tokens should have similar that similar tokens should have similar embeddings or similar weights and so embeddings or similar weights and so embeddings or similar weights and so this is what motivates their exploration this is what motivates their exploration this is what motivates their exploration here and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 763,
      "text": "they they kind of you know I here and they they kind of you know I here and they they kind of you know I don't want to go through the entire don't want to go through the entire don't want to go through the entire paper",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 764,
      "text": "and and uh you can go through it paper",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 765,
      "text": "and and uh you can go through it paper",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 766,
      "text": "and and uh you can go through it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 767,
      "text": "but this is what they observe they also",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 768,
      "text": "but this is what they observe they also",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 769,
      "text": "but this is what they observe they also observe that if you look at the output observe that if you look at the output observe that if you look at the output embeddings they also behave like word embeddings they also behave like word embeddings they also behave like word embeddings um if you um if you just kind embeddings um if you um if you just kind embeddings um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 770,
      "text": "if you um if you just kind of try to use those weights as word of try to use those weights as word of try to use those weights as word embeddings um so they kind of observe embeddings um so they kind of observe embeddings um so they kind of observe this similarity they try to tie them and this similarity they try to tie them and this similarity they try to tie them and they observe that they can get much they observe that they can get much they observe that they can get much better performance in that way and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 771,
      "text": "so better performance in that way and so better performance in that way and so this was adopted and the attention is this was adopted and the attention is this was adopted and the attention is all need paper",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 772,
      "text": "and then it was used all need paper",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 773,
      "text": "and then it was used all need paper",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 774,
      "text": "and then it was used again in gpt2 as well again in gpt2 as well again in gpt2 as well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 775,
      "text": "so I couldn't find it in",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 776,
      "text": "the so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 777,
      "text": "I couldn't find it in the so I couldn't find it in the Transformers implementation I'm not sure Transformers implementation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 778,
      "text": "I'm not sure Transformers implementation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 779,
      "text": "I'm not sure where they tie those embeddings but I where they tie those embeddings but I where they tie those embeddings but I can find it in the original gpt2 code U can find it in the original gpt2 code U can find it in the original gpt2 code U introduced by open aai",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 780,
      "text": "so this is um introduced by open aai so this is um introduced by open aai",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 781,
      "text": "so this is um openai gpt2 Source model and here where openai gpt2 Source model and here where openai gpt2 Source model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 782,
      "text": "and here where they are forwarding this model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 783,
      "text": "and this they are forwarding this model and this they are forwarding this model and this is in tensorflow but uh that's okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 784,
      "text": "we is in tensorflow but uh that's okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 785,
      "text": "we is in tensorflow but uh that's",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 786,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 787,
      "text": "we see that they get the wte token see that they get the wte token see that they get the wte token embeddings and then here is the incoder embeddings and then here is the incoder embeddings and then here is the incoder of the token embeddings and the of the token embeddings and the of the token embeddings and the position and then here at the bottom position and then here at the bottom position and then here at the bottom they Ed the WT again to do the lits",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 788,
      "text": "so they Ed the WT again to do the lits so they Ed the WT again to do the lits",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 789,
      "text": "so when they get the loits it's a math Mo when they get the loits it's a math Mo when they get the loits it's a math Mo of uh this output from the Transformer of uh this output from the Transformer of uh this output from the Transformer and the wte tensor is and the wte tensor is and the wte tensor is reused um and so the wte tensor reused um and so the wte tensor reused um and so the wte tensor basically is used twice on the bottom of basically is used twice on the bottom of basically is used twice on the bottom of the Transformer and on the top of the the Transformer and on the top of the the Transformer and on the top of the Transformer and in the backward pass Transformer and in the backward pass Transformer and in the backward pass we'll get gradients contributions from we'll get gradients contributions from we'll get gradients contributions from both branches right and these gradients both branches right and these gradients both branches right and these gradients will add up um on the wte tensor",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 790,
      "text": "um so will add up um on the wte tensor",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 791,
      "text": "um so will add up um on the wte tensor um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 792,
      "text": "so we'll get a contribution from the we'll get a contribution from the we'll get a contribution from the classifier list classifier list classifier list and then at the very end of the and then at the very end of the and then at the very end of the Transformer we'll get a contribution at Transformer we'll get a contribution at Transformer we'll get a contribution at the at the bottom of it float floating the at the bottom of it float floating the at the bottom of it float floating again into the wte uh tensor",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 793,
      "text": "so we want again into the wte uh tensor",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 794,
      "text": "so we want again into the wte uh tensor",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 795,
      "text": "so we want to we are currently not sharing WT and to we are currently not sharing WT and to we are currently not sharing WT and our code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 796,
      "text": "but we want to do our code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 797,
      "text": "but we want to do our code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 798,
      "text": "but we want to do that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 799,
      "text": "um that um that um so weight sharing scheme um and one way so weight sharing scheme um and one way so weight sharing scheme um and one way to do this let's see if goil gets it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 800,
      "text": "oh to do this let's see if goil gets it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 801,
      "text": "oh to do this let's see if goil gets it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 802,
      "text": "oh it does",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 803,
      "text": "okay uh so this is one way to do it does",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 804,
      "text": "okay uh so this is one way to do it does",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 805,
      "text": "okay uh so this is one way to do it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 806,
      "text": "it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 807,
      "text": "it uh uh uh basically relatively straightforward basically relatively straightforward basically relatively straightforward what we're doing here is we're taking what we're doing here is we're taking what we're doing here is we're taking the wte do weight and we're simply uh the wte do weight and we're simply uh the wte do weight and we're simply uh redirecting it to point to the LM head redirecting it to point to the LM head redirecting it to point to the LM head",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 808,
      "text": "so um this basically copies the data",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 809,
      "text": "so um this basically copies the data",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 810,
      "text": "so um this basically copies the data pointer right it copies the reference pointer right it copies the reference pointer right it copies the reference and now the wte weight becomes orphaned and now the wte weight becomes orphaned and now the wte weight becomes orphaned uh the old value of it and uh pytorch uh the old value of it and uh pytorch uh the old value of it and uh pytorch will clean it up python will clean it up will clean it up python will clean it up will clean it up python will clean it up",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 811,
      "text": "and so we are only left with a single and so we are only left with a single",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 812,
      "text": "and so we are only left with a single tensor and it's going to be used twice tensor",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 813,
      "text": "and it's going to be used twice tensor",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 814,
      "text": "and it's going to be used twice in the forward pass and uh this is to my in the forward pass and uh this is to my in the forward pass and uh this is to my knowledge all that's required",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 815,
      "text": "so we knowledge all that's required",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 816,
      "text": "so we knowledge all that's required so we should be able to use this and this should be able to use this and this should be able to use this and this should probably train uh we're just should probably train uh we're just should probably train uh we're just going to basically be using this exact going to basically be using this exact going to basically be using this exact same sensor twice and same sensor twice and same sensor twice and um we weren't being careful with um we weren't being careful with um we weren't being careful with tracking the likelihoods but uh tracking the likelihoods but uh tracking the likelihoods but uh according to the paper and according to according to the paper and according to according to the paper and according to the results you'd actually expect the results you'd actually expect the results you'd actually expect slightly better results doing this and slightly better results doing this and slightly better results doing this and in addition to that one other reason in addition to that one other reason in addition to that one other reason that this is very very nice for us is that this is very very nice for us is that this is very very nice for us is that this is a ton of parameters right that this is a ton of parameters right that this is a ton of parameters",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 817,
      "text": "right uh what is the size here it's 768 * uh what is the size here it's 768 *",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 818,
      "text": "uh what is the size here it's 768 * 50257",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 819,
      "text": "so This Is 40 million parameters 50257",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 820,
      "text": "so This Is 40 million parameters 50257",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 821,
      "text": "so This Is 40 million parameters and this is a 124 million parameter and this is a 124 million parameter and this is a 124 million parameter model so 40 divide 124",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 822,
      "text": "so this is like model so 40 divide 124 so this is like model so 40 divide 124",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 823,
      "text": "so this is like 30% of the parameters are being saved 30% of the parameters are being saved 30% of the parameters are being saved using this weight time scheme and so using this weight time scheme and so using this weight time scheme and so this might be one of the reasons that this might be one of the reasons that this might be one of the reasons that this is working slightly better if this is working slightly better if this is working slightly better if you're not training the model long you're not training the model long you're not training the model long enough because of the weight tying uh enough because of the weight tying uh enough because of the weight tying uh you don't have to train as many you don't have to train as many you don't have to train as many parameters",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 824,
      "text": "and so you become more parameters",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 825,
      "text": "and so you become more parameters",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 826,
      "text": "and so you become more efficient um in terms of the training efficient um in terms of the training efficient um in terms of the training process uh because you have fewer process uh because you have fewer process uh because you have fewer parameters and you're putting in this parameters and you're putting in this parameters and you're putting in this inductive bias that these two embeddings inductive bias that these two embeddings inductive bias that these two embeddings should share similarities between tokens should share similarities between tokens should share similarities between tokens so this is the way time scheme and we've so this is the way time scheme and we've so this is the way time scheme and we've saved a ton of parameters and we expect saved a ton of parameters and we expect saved a ton of parameters and we expect our model to work slightly better our model to work slightly better our model to work slightly better because of the scheme",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 827,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 828,
      "text": "next I would because of the scheme",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 829,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 830,
      "text": "next I would because of the scheme",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 831,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 832,
      "text": "next I would like us to be a bit more careful with like us to be a bit more careful with like us to be a bit more careful with the initialization and to try to follow the initialization and to try to follow the initialization and to try to follow the way gpt2 initialized their model now the way gpt2 initialized their model now the way gpt2 initialized their model now unfortunately the gpt2 paper and the unfortunately the gpt2 paper and the unfortunately the gpt2 paper and the gpt3 paper are not very explicit about gpt3 paper are not very explicit about gpt3 paper are not very explicit about initialization so we kind of have to initialization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 833,
      "text": "so we kind of have to initialization so we kind of have to read between the lines uh and instead of read between the lines uh and instead of read between the lines uh and instead of going to the paper which is quite vague going to the paper which is quite vague going to the paper which is quite vague um there's a bit of information in the um there's a bit of information in the um there's a bit of information in the code that open I released so when we go code that open I released so when we go code that open I released so when we go to the model.py we see that when they to the model.py we see that when they to the model.py we see that when they initialize their weights they are using initialize their weights they are using initialize their weights they are using the standard deviation of the standard deviation of the standard deviation of 0.02 and that's how they they",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 834,
      "text": "so this is 0.02",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 835,
      "text": "and that's how they they so this is 0.02",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 836,
      "text": "and that's how they they so this is a normal distribution for the weights a normal distribution for the weights a normal distribution for the weights and the standard deviation is and the standard deviation is and the standard deviation is 0.02 for the bias they initialize that 0.02 for the bias they initialize that 0.02 for the bias they initialize that with with with zero and then when we scroll down zero and then when we scroll down zero and then when we scroll down here why is this not scrolling here why is this not scrolling here why is this not scrolling um the token embeddings are initialized um the token embeddings are initialized um the token embeddings are initialized at at at 0.02 and position embeddings at 0.01 for 0.02 and position embeddings at 0.01 for 0.02 and position embeddings at 0.01 for some reason so those are the some reason so those are the some reason so those are the initializations",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 837,
      "text": "and we'd like to mirror initializations and we'd like to mirror initializations and we'd like to mirror that in that in that in gpt2 uh in our module here so here's a gpt2 uh in our module here so here's a gpt2 uh in our module here so here's a snippet of code that I sort of came up snippet of code that I sort of came up snippet of code that I sort of came up with very with very with very quickly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 838,
      "text": "so what's happening here is at quickly so what's happening here is at quickly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 839,
      "text": "so what's happening here is at the end of our initializer for the GPT the end of our initializer for the GPT the end of our initializer for the GPT module we're calling the apply function module we're calling the apply function module we're calling the apply function of NN module and that iterates all the of NN module and that iterates all the of NN module and that iterates all the sub modules of this module and uh sub modules of this module and uh sub modules of this module and uh applies in it weights function on them applies in it weights function on them applies in it weights function on them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 840,
      "text": "and so what's happening here is that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 841,
      "text": "and so what's happening here is that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 842,
      "text": "and so what's happening here is that we're in we're iterating all the modules we're in we're iterating all the modules we're in we're iterating all the modules here and if they are an nn. linear here and if they are an nn. linear here and if they are an nn. linear module then we're going to make sure to module then we're going to make sure to module then we're going to make sure to initialize the weight using a normal initialize the weight using a normal initialize the weight using a normal with the standard deviation of with the standard deviation of with the standard deviation of 0.02 if there's a bias in this layer we 0.02",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 843,
      "text": "if there's a bias in this layer we 0.02 if there's a bias in this layer we will make sure to initialize that to will make sure to initialize that to will make sure to initialize that to zero note that zero initialization for zero note that zero initialization for zero note that zero initialization for the bias is not actually the pyto the bias is not actually the pyto the bias is not actually the pyto default um by default the bias here is default um by default the bias here is default um by default the bias here is initialized with a uniform",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 844,
      "text": "so uh that's initialized with a uniform",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 845,
      "text": "so uh that's initialized with a uniform",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 846,
      "text": "so uh that's interesting so we make sure to use zero interesting so we make sure to use zero interesting so we make sure to use zero and for the embedding we're just going and for the embedding we're just going and for the embedding we're just going to use 0.02 and um keep it the same um to use 0.02 and um keep it the same um to use 0.02 and um keep it the same",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 847,
      "text": "um so we're not going to change it to 0.01 so we're not going to change it to 0.01",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 848,
      "text": "so we're not going to change it to 0.01 for positional because it's about the for positional because it's about the for positional because it's about the same",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 849,
      "text": "and then if you look through our same and then if you look through our same",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 850,
      "text": "and then if you look through our model the only other layer that requires model the only other layer that requires model the only other layer that requires initialization and that has parameters initialization and that has parameters initialization and that has parameters is the layer norm and the fighter defer is the layer norm and the fighter defer is the layer norm and the fighter defer initialization sets the scale in the initialization sets the scale in the initialization sets the scale in the layer Norm to be one and the offset in layer Norm to be one and the offset in layer Norm to be one and the offset in the layer Norm to be zero",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 851,
      "text": "so that's the layer Norm to be zero",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 852,
      "text": "so that's the layer Norm to be zero",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 853,
      "text": "so that's exactly what we want",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 854,
      "text": "and so we're just exactly what we want",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 855,
      "text": "and so we're just exactly what we want",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 856,
      "text": "and so we're just going to uh keep it that way",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 857,
      "text": "and so this going to uh keep it that way",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 858,
      "text": "and so this going to uh keep it that way",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 859,
      "text": "and so this is the default initialization if we are is the default initialization if we are is the default initialization if we are following the um where is it the uh gpt2 following the um where is it the uh gpt2 following the um where is it the uh gpt2 uh source code that they released I uh source code that they released I uh source code that they released I would like to point out by the way that would like to point out by the way that would like to point out by the way that um typically the standard deviation here um typically the standard deviation here um typically the standard deviation here on this initialization if you follow the on this initialization if you follow the on this initialization if you follow the Javier initialization would be one of Javier initialization would be one of Javier initialization would be one of over the square root of the number of over the square root of the number of over the square root of the number of features that are incoming into this features that are incoming into this features that are incoming into this layer but if you'll notice actually 0.02 layer but if you'll notice actually 0.02 layer but if you'll notice actually 0.02 is basically consistent with that is basically consistent with that is basically consistent with that because the the model sizes inside these because the the model sizes inside these because the the model sizes inside these Transformers for gpt2 are roughly 768 Transformers for gpt2 are roughly 768 Transformers for gpt2 are roughly 768 1600 Etc so 1 over the square root of 1600 Etc so 1 over the square root of 1600 Etc so 1 over the square root of for example 768 gives us for example 768 gives us for example 768 gives us 0.03 if we plug in 600 1,600 we get 0.03 if we plug in 600 1,600 we get 0.03 if we plug in 600 1,600 we get 0.02 if we plug in three times that 0.02 if we plug in three times that 0.02 if we plug in three times",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 860,
      "text": "that 0.014 Etc so basically 0.02 is roughly 0.014 Etc so basically 0.02 is roughly 0.014 Etc so basically 0.02 is roughly in the vicinity of reasonable values for in the vicinity of reasonable values for in the vicinity of reasonable values for the for um for these initializations the for um for these initializations the for um for these initializations anyway",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 861,
      "text": "so so it's not uh completely anyway",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 862,
      "text": "so so it's not uh completely anyway",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 863,
      "text": "so so it's not uh completely crazy to be hard coding 0.02 here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 864,
      "text": "uh but crazy to be hard coding 0.02 here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 865,
      "text": "uh but crazy to be hard coding 0.02 here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 866,
      "text": "uh but you'd like typically uh some something you'd like typically uh some something you'd like typically uh some something that grows with the model size instead that grows with the model size instead that grows with the model size instead but we will keep this because that is",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 867,
      "text": "but we will keep this because that is",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 868,
      "text": "but we will keep this because that is the gpt2 initialization per their source the gpt2 initialization per their source the gpt2 initialization per their source code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 869,
      "text": "but we are not fully done yet on code but we are not fully done yet on code but we are not fully done yet on initialization because there's one more initialization because there's one more initialization because there's one more caveat here so caveat here so caveat here so here a mod initialization which accounts here a mod initialization which accounts here a mod initialization which accounts for the accumulation on the residual for the accumulation on the residual for the accumulation on the residual path with model depth is used we scale path with model depth is used we scale path with model depth is used we scale the weight of residual layers of the weight of residual layers of the weight of residual layers of initialization by factor of one over squ initialization by factor of one over squ initialization by factor of one over squ of n where n is the number of residual of n where n is the number of residual of n where n is the number of residual layers",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 870,
      "text": "so this is what gbt2 paper says layers",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 871,
      "text": "so this is what gbt2 paper says layers",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 872,
      "text": "so this is what gbt2 paper says so we have not implemented that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 873,
      "text": "yet",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 874,
      "text": "and so we have not implemented that yet",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 875,
      "text": "and so we have not implemented that yet",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 876,
      "text": "and uh we can do so now now I'd like to uh we can do so now now I'd like to uh we can do so now now I'd like to actually kind of like motivate a little actually kind of like motivate a little actually kind of like motivate a little bit what they mean here I think um so bit what they mean here I think um so bit what they mean here I think um so here's roughly what they here's roughly what they here's roughly what they mean if you start out with zeros in your mean if you start out with zeros in your mean if you start out with zeros in your residual stream remember that each residual stream remember that each residual stream remember that each residual stream is a is of this form residual stream is a is of this form residual stream is a is of this form where we continue adding to it X is X where we continue adding to it X is X where we continue adding to it X is X plus something some kind of contribution plus something some kind of contribution plus something some kind of contribution so every single block of the residual uh so every single block of the residual uh so every single block of the residual uh Network contributes some uh amount and Network contributes some uh amount and Network contributes some uh amount and it gets added and so what ends up it gets added",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 877,
      "text": "and so what ends up it gets added",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 878,
      "text": "and so what ends up happening is that the variance of the happening is that the variance of the happening is that the variance of the activations in the residual stream grows activations in the residual stream grows activations in the residual stream grows so here's a small example if we start at so here's a small example if we start at so here's a small example if we start at zero and then we for 100 times uh we zero",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 879,
      "text": "and then we for 100 times uh we zero",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 880,
      "text": "and then we for 100 times uh we have sort of this residual stream of of have sort of this residual stream of of have sort of this residual stream of of 768 uh zeros and then 100 times we add 768 uh zeros and then 100 times we add 768 uh zeros and then 100 times we add um random which is a normal distribution um random which is a normal distribution um random which is a normal distribution zero mean one standard deviation if we zero mean one standard deviation if we zero mean one standard deviation if we add to it then by the end the residual add to it then by the end the residual add to it then by the end the residual stream has grown to have standard stream has grown to have standard stream has grown to have standard deviation of 10 and that's just because deviation of 10 and that's just because deviation of 10 and that's just because um we're always adding um these numbers um we're always adding um these numbers um we're always adding um these numbers and so this scaling factor that they use and so this scaling factor that they use and so this scaling factor that they use here exactly compensates for that growth here exactly compensates for that growth here exactly compensates for that growth so if we take n and we",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 881,
      "text": "basically um so if we take n",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 882,
      "text": "and we basically um so if we take n",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 883,
      "text": "and we basically um scale down every one of these scale down every one of these scale down every one of these contributions into the residual stream contributions into the residual stream contributions into the residual stream by one over theare Ro of n so 1 over by one over theare Ro of n so 1 over by one over theare Ro of n so 1 over theun of n is n to the 0.5 theun of n is n to the 0.5 theun of n is n to the 0.5 right because n the5 is the square root right because n the5 is the square root right because n the5 is the square root and then one over the square root is n.5",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 884,
      "text": "and then one over the square root is n.5",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 885,
      "text": "and then one over the square root is n.5 if we scale it in this way then we see if we scale it in this way then we see if we scale it in this way then we see that we actually get um that we actually get um that we actually get um one one one",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 886,
      "text": "so this is a way to control the growth so this is a way to control the growth so this is a way to control the growth of of activations inside the residual of of activations inside the residual of of activations inside the residual stream in the forward pass",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 887,
      "text": "and so we'd stream in the forward pass",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 888,
      "text": "and so we'd stream in the forward pass",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 889,
      "text": "and so we'd like to initialize in the same way where like to initialize in the same way where like to initialize in the same way where these weights that are at the end of these weights that are at the end of these weights that are at the end of each block",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 890,
      "text": "so this C uh layer uh the gbt each block",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 891,
      "text": "so this C uh layer uh the gbt each block",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 892,
      "text": "so this C uh layer uh the gbt paper proposes to scale down those paper proposes to scale down those paper proposes to scale down those weights by one over the square root of weights by one over the square root of weights by one over the square root of the number of residual the number of residual the number of residual layers so one crude way to implement layers so one crude way to implement layers so one crude way to implement this is the following I don't know if this is the following I don't know if this is the following I don't know if this is uh pyro sanctioned",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 893,
      "text": "but it works this is uh pyro sanctioned but it works this is uh pyro sanctioned but it works for me is we'll do in the for me is we'll do in the for me is we'll do in the initialization see that s that do initialization see that s that do initialization see that s that do special nanog special nanog special nanog GPT uh scale in it is one",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 894,
      "text": "so we're GPT uh scale in it is one",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 895,
      "text": "so we're GPT uh scale in it is one",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 896,
      "text": "so we're setting um kind of like a flag for this setting um kind of like a flag for this setting um kind of like a flag for this module there must be a better way in py module there must be a better way in py module there must be a better way in py torch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 897,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 898,
      "text": "but I don't torch right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 899,
      "text": "but I don't torch right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 900,
      "text": "but I don't know",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 901,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 902,
      "text": "so we're basically attaching know",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 903,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 904,
      "text": "so we're basically attaching know",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 905,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 906,
      "text": "so we're basically attaching this flag and trying to make sure that this flag and trying to make sure that this flag and trying to make sure that it doesn't conflict with anything it doesn't conflict with anything it doesn't conflict with anything previously and then when we come down previously",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 907,
      "text": "and then when we come down previously",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 908,
      "text": "and then when we come down here this STD should be 0.02 by default here this STD should be 0.02 by default here this STD should be 0.02 by default but then",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 909,
      "text": "if but then if but then if haat um module of this thing haat um module of this thing haat um module of this thing then STD * then STD * then STD * equals equals um copal is not guessing correctly uh so um copal is not guessing correctly uh so um copal is not guessing correctly uh so we want one over the square root of the we want one over the square root of the we want one over the square root of the number of layers so number of layers so number of layers",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 910,
      "text": "so um the number of residual layers here is um the number of residual layers here is um the number of residual layers here is twice twice twice times Salt out config layers and then times Salt out config layers and then times Salt out config layers and then this times .5",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 911,
      "text": "so we want to scale down this times .5",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 912,
      "text": "so we want to scale down this times .5",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 913,
      "text": "so we want to scale down that standard deviation and this should that standard deviation and this should that standard deviation and this should be um correct and Implement that I be um correct and Implement that I be um correct and Implement that I should clarify by the way that the two should clarify by the way that the two should clarify by the way that the two times number of layers comes from the times number of layers comes from the times number of layers comes from the fact that every single one of our layers fact that every single one of our layers fact that every single one of our layers in the Transformer actually has two in the Transformer actually has two in the Transformer actually has two blocks that add to the ridal pathway blocks that add to the ridal pathway blocks that add to the ridal pathway",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 914,
      "text": "right we have the attention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 915,
      "text": "and then the right we have the attention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 916,
      "text": "and then the right we have the attention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 917,
      "text": "and then the MLP so that's where the two times comes MLP so that's where the two times comes MLP so that's where the two times comes from and the other thing to mention is from and the other thing to mention is from and the other thing to mention is that uh what's slightly awkward but that uh what's slightly awkward but that uh what's slightly awkward but we're not going to fix it is that um we're not going to fix it is that um we're not going to fix it is that um because we are weight sharing the wte because we are weight sharing the wte because we are weight sharing the wte and the LM head in this iteration of our and the LM head in this iteration of our and the LM head in this iteration of our old subm modules we're going to actually old subm modules we're going to actually old subm modules we're going to actually come around to that tensor twice so come around to that tensor twice so come around to that tensor twice so we're going to first initialize it as an we're going to first initialize it as an we're going to first initialize it as an embedding with 0.02 and then we're going embedding with 0.02 and then we're going embedding with 0.02 and then we're going to come back around it again in a linear to come back around it again in a linear to come back around it again in a linear and initialize it again using 0.02 and and initialize it again using 0.02 and and initialize it again using 0.02 and it's going to be 0.02 because the LM it's going to be 0.02 because the LM it's going to be 0.02 because the LM head is of course not not scaled so it's head is of course not not scaled so it's head is of course not not scaled so it's not going to come here it's just it's not going to come here it's just it's not going to come here it's just it's going to be basically initialized twice going to be basically initialized twice going to be basically initialized twice using the identical same initialization using the identical same initialization using the identical same initialization but that's okay and then scrolling over but that's okay and then scrolling over but that's okay and then scrolling over here I added uh some code here so that here I added uh some code here so that here I added uh some code here so that we have we have we have reproducibility um to set the seeds and reproducibility um to set the seeds and reproducibility um to set the seeds and now we should be able to python train now we should be able to python train now we should be able to python train gpt2 pi and let this running and as far gpt2 pi and let this running and as far gpt2 pi and let this running and as far as I know this is the gpt2 as I know this is the gpt2 as I know this is the gpt2 initialization uh in the way we've initialization uh in the way we've initialization uh in the way we've implemented it right now so this implemented it right now so this implemented it right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 918,
      "text": "so this looks uh reasonable to me",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 919,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 920,
      "text": "so at looks uh reasonable to me okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 921,
      "text": "so at looks uh reasonable to me okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 922,
      "text": "so at this point we have the gpt2 model we this point we have the gpt2 model we this point we have the gpt2 model we have some confidence that it's correctly have some confidence that it's correctly have some confidence that it's correctly implemented we've initialized it implemented we've initialized it implemented we've initialized it properly and we have a data loader properly and we have a data loader properly and we have a data loader that's iterating through data batches that's iterating through data batches that's iterating through data batches and we can train so now comes the fun",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 923,
      "text": "and we can train so now comes the fun and we can train so now comes the fun part I'd like us to speed up the part I'd like us to speed up the part I'd like us to speed up the training by a lot",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 924,
      "text": "so we're getting our training by a lot",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 925,
      "text": "so we're getting our training by a lot",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 926,
      "text": "so we're getting our money's worth with respect to the money's worth with respect to the money's worth with respect to the hardware that we are uh using here and hardware that we are uh using here and hardware that we are uh using here and uh we're going to speed up the training uh we're going to speed up the training uh we're going to speed up the training by quite a bit uh now you always want to by quite a bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 927,
      "text": "uh now you always want to by quite a bit uh now you always want to start with what Hardware do you have start with what Hardware do you have start with what Hardware do you have what does it offer and are you fully what does it offer and are you fully what does it offer and are you fully utilizing it so in my case if we go to utilizing it so in my case if we go to utilizing it so in my case if we go to Nvidia Nvidia Nvidia SMI we can see SMI we can see SMI we can see that I have eight gpus and each one of that I have eight gpus and each one of that I have eight gpus and each one of those gpus is an a100 sxm 80 gb",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 928,
      "text": "so this those gpus is an a100 sxm 80 gb",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 929,
      "text": "so this those gpus is an a100 sxm 80 gb",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 930,
      "text": "so this is the GPU that I have available to me is the GPU that I have available to me is the GPU that I have available to me in this box now when I look when I use in this box now when I look when I use in this box now when I look when I use um to spin up these kinds of Boxes by um to spin up these kinds of Boxes by um to spin up these kinds of Boxes by the way my favorite place to go to is the way my favorite place to go to is the way my favorite place to go to is Lambda Labs",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 931,
      "text": "um they do sponsor my Lambda Labs um they do sponsor my Lambda Labs um they do sponsor my development and that of my projects uh development and that of my projects uh development and that of my projects uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 932,
      "text": "but I this is my favorite place to go",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 933,
      "text": "but I this is my favorite place to go",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 934,
      "text": "but I this is my favorite place to go and this is where you can spin up one of and this is where you can spin up one of and this is where you can spin up one of these machines and you pay per hour and these machines and you pay per hour and these machines and you pay per hour",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 935,
      "text": "and it's very very simple",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 936,
      "text": "it's very very simple",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 937,
      "text": "it's very very simple",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 938,
      "text": "so I like to spin them up and then so I like to spin them up",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 939,
      "text": "and then so I like to spin them up and then connect vsod to it and that's how I connect vsod to it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 940,
      "text": "and that's how I connect vsod to it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 941,
      "text": "and that's how I develop now when we look at the A1 100s develop now when we look at the A1 100s develop now when we look at the A1 100s that are available here a100 80 GB sxm that are available here a100 80 GB sxm that are available here a100 80 GB sxm is the um GPU that I have here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 942,
      "text": "and we is the um GPU that I have here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 943,
      "text": "and we is the um GPU that I have here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 944,
      "text": "and we have a bunch of numbers here for um how have a bunch of numbers here for um how have a bunch of numbers here for um how many calculations you can expect out of many calculations you can expect out of many calculations you can expect out of this GPU so when I come over here this GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 945,
      "text": "so when I come over here this GPU so when I come over here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 946,
      "text": "and I break in right after here so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 947,
      "text": "and I break in right after here so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 948,
      "text": "and I break in right after here so python python python trity",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 949,
      "text": "so I'm breaking in right after we trity",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 950,
      "text": "so I'm breaking in right after we trity",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 951,
      "text": "so I'm breaking in right after we calculate the loit and calculate the loit and calculate the loit and laws and the interesting thing I'd like laws and the interesting thing I'd like laws and the interesting thing I'd like you to note is when I do lit.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 952,
      "text": "dtype this you to note is when I do lit.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 953,
      "text": "dtype this you to note is when I do lit.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 954,
      "text": "dtype this prints a torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 955,
      "text": "FL 32 so by default iny prints a torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 956,
      "text": "FL 32 so by default iny prints a torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 957,
      "text": "FL 32 so by default iny torch when you create tensors um and torch when you create tensors um and torch when you create tensors um and this is the case for all the activations this is the case for all the activations this is the case for all the activations and for the parameters of the network and for the parameters of the network and for the parameters of the network and so on by default everything is in and so on by default everything is in and so on by default everything is in float 32 that means that every single float 32 that means that every single float 32 that means that every single number activation or weight and so on is number activation or weight and so on is number activation or weight and so on is using a float representation that has 32 using a float representation that has 32 using a float representation that has 32 bits and uh that's actually quite a bit bits and uh that's actually quite a bit bits and uh that's actually quite a bit of memory and it turns out empirically of memory and it turns out empirically of memory and it turns out empirically that for deep learning as a that for deep learning as a that for deep learning as a computational workload this is way too computational workload this is way too computational workload this is way too much and deep learning and the training much and deep learning and the training much and deep learning and the training of these networks can tolerate of these networks can tolerate of these networks can tolerate significantly lower precisions um not significantly lower precisions um not significantly lower precisions um not all computational workflows can tolerate all computational workflows can tolerate all computational workflows can tolerate small Precision so for example um if we small Precision",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 958,
      "text": "so for example um if we small Precision so for example um if we go back to to the data sheet you'll see go back to to the data sheet you'll see go back to to the data sheet you'll see that actually these gpus support up to that actually these gpus support up to that actually these gpus support up to fp64 and this is quite useful I fp64 and this is quite useful I fp64 and this is quite useful I understand for a lot of um scientific understand for a lot of um scientific understand for a lot of um scientific Computing applications and there really Computing applications",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 959,
      "text": "and there really Computing applications",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 960,
      "text": "and there really need this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 961,
      "text": "uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 962,
      "text": "but we don't need that much need this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 963,
      "text": "uh but we don't need that much need this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 964,
      "text": "uh but we don't need that much Precision for deep learning training",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 965,
      "text": "So Precision for deep learning training So Precision for deep learning training",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 966,
      "text": "So currently we are here currently we are here currently we are here fp32 and with this code as it is right fp32 and with this code as it is right fp32 and with this code as it is right now we expect to get at at most 19.5 now we expect to get at at most 19.5 now we expect to get at at most 19.5 Tera flops of performance that means Tera flops of performance that means Tera flops of performance that means we're doing 19.5 trillion operations we're doing 19.5 trillion operations we're doing 19.5 trillion operations floating Point operations so this is floating Point operations so this is floating Point operations so this is floating Point multiply add most um most floating Point multiply add most um most floating Point multiply add most um most likely and so these are the floating likely and so these are the floating likely and so these are the floating Point operations Point operations Point operations uh now notice that if we are willing to uh now notice that if we are willing to uh now notice that if we are willing to go down in Precision so tf32 is a lower go down in Precision so tf32 is a lower go down in Precision so tf32 is a lower Precision format we're going to see in a Precision format we're going to see in a Precision format we're going to see in a second you can actually get an 8X second you can actually get an 8X second you can actually get an 8X Improvement here and if you're willing Improvement here and if you're willing Improvement here and if you're willing to go down to float 16 or B float 16 you to go down to float 16 or B float 16 you to go down to float 16 or B float 16 you can actually get time 16x performance can actually get time 16x performance can actually get time 16x performance all the way to 312 Tera flops you see all the way to 312 Tera flops you see all the way to 312 Tera flops you see here that Nvidia likes to site numbers here that Nvidia likes to site numbers here that Nvidia likes to site numbers that have an asterisk here this asterisk that have an asterisk here this asterisk that have an asterisk here this asterisk uh says with sparsity uh but we are not uh says with sparsity uh but we are not uh says with sparsity uh but we are not going to be using sparsity in R code and going to be using sparsity in R code and going to be using sparsity in R code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 967,
      "text": "and I don't know that this is very widely I don't know that this is very widely I don't know that this is very widely used in the industry right now so most used in the industry right now so most used in the industry right now so most people look at this number here uh people look at this number here uh people look at this number here uh without sparcity and you'll notice that without sparcity and you'll notice that without sparcity and you'll notice that we could have got even more here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 968,
      "text": "but we could have got even more here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 969,
      "text": "but we could have got even more here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 970,
      "text": "but this is int 8 and int 8 is used for this is int 8 and int 8 is used for this is int 8 and int 8 is used for inference not for training uh because inference not for training uh because inference not for training uh because int 8 has a um it basically has um int 8 has a um it basically has um int 8 has a um it basically has um uniform uniform uniform spacing um and uh we actually require a spacing um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 971,
      "text": "and uh we actually require a spacing um and uh we actually require a float so that we get a better match to float so that we get a better match to float so that we get a better match to the uh normal distributions that occur the uh normal distributions that occur the uh normal distributions that occur during training of neural networks where during training of neural networks where during training of neural networks where both activations and weights are both activations and weights are both activations and weights are distributed as a normal distribution and distributed as a normal distribution and distributed as a normal distribution and so uh floating points are really so uh floating points are really so uh floating points are really important to to match that uh important to to match that uh important to to match that uh representation so we're not typically representation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 972,
      "text": "so we're not typically representation so we're not typically using int 8 uh for training but we are using int 8 uh for training but we are using int 8 uh for training but we are using it for inference and if we bring using it for inference and if we bring using it for inference and if we bring down the Precision we can get a lot more down the Precision we can get a lot more down the Precision we can get a lot more Terra flops out of the tensor course",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 973,
      "text": "Terra flops out of the tensor course Terra flops out of the tensor course available in the gpus we'll talk about available in the gpus we'll talk about available in the gpus we'll talk about that in a second but in addition to that that in a second but in addition to that that in a second but in addition to that if all of these numbers have fewer bits if all of these numbers have fewer bits if all of these numbers have fewer bits of representation it's going to be much of representation it's going to be much of representation it's going to be much easier to move them around and that's easier to move them around and that's easier to move them around and that's where we start to get into the memory where we start to get into the memory where we start to get into the memory bandwidth and the memory of the model so bandwidth and the memory of the model so bandwidth and the memory of the model so not only do we have a finite capacity of not only do we have a finite capacity of not only do we have a finite capacity of the number of bits that our GPU can the number of bits that our GPU can the number of bits that our GPU can store but in addition to that there's a store but in addition to that there's a store but in addition to that there's a speed with which you can access this speed with which you can access this speed with which you can access this memory um and you have a certain memory memory um and you have a certain memory memory um and you have a certain memory bandwidth it's a very precious resource bandwidth it's a very precious resource bandwidth it's a very precious resource and in fact many of the deep learning uh and in fact many of the deep learning uh and in fact many of the deep learning uh work workloads for training are memory work workloads for training are memory work workloads for training are memory bound and what that means is actually bound and what that means is actually bound and what that means is actually that the tensor cores that do all these that the tensor cores that do all these that the tensor cores that do all these extremely fast multiplications most of extremely fast multiplications most of extremely fast multiplications most of the time they're waiting around they're the time they're waiting around they're the time they're waiting around they're idle um because we can't feed them with idle um because we can't feed them with idle um because we can't feed them with data fast enough we can't load the data data fast enough we can't load the data data fast enough we can't load the data fast enough from memory so typical fast enough from memory so typical fast enough from memory so typical utilizations of your Hardware if you're utilizations of your Hardware if you're utilizations of your Hardware if you're getting 60% uh utilization you're getting 60% uh utilization you're getting 60% uh utilization you're actually doing extremely well um so half actually doing extremely well um so half actually doing extremely well um so half of the time in a well-tuned application of the time in a well-tuned application of the time in a well-tuned application your tensor cores are not doing your tensor cores are not doing your tensor cores are not doing multiplies because the data is not multiplies because the data is not multiplies because the data is not available so the memory bandwidth here available so the memory bandwidth here available so the memory bandwidth here is extremely important as well and if we is extremely important as well and if we is extremely important as well and if we come down in the Precision for all the come down in the Precision for all the come down in the Precision for all the floats all the numbers weights and floats all the numbers weights and floats all the numbers weights and activations suddenly require less memory activations suddenly require less memory activations suddenly require less memory so we can store more and we can access so we can store more",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 974,
      "text": "and we can access so we can store more and we can access it faster so everything speeds up and it faster so everything speeds up and it faster so everything speeds up and it's amazing and now let's reap the it's amazing and now let's reap the it's amazing and now let's reap the benefits of it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 975,
      "text": "um and let's first look benefits of it um and let's first look benefits of it um and let's first look at the tensor float 32 at the tensor float 32 at the tensor float 32 format",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 976,
      "text": "okay so first of all what are format",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 977,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 978,
      "text": "so first of all what are format",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 979,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 980,
      "text": "so first of all what are tensor cores",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 981,
      "text": "well tensor course tensor tensor cores well tensor course tensor tensor cores",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 982,
      "text": "well tensor course tensor core is just an instruction in the a100 core is just an instruction in the a100 core is just an instruction in the a100 architecture",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 983,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 984,
      "text": "so so what it does is architecture right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 985,
      "text": "so so what it does is architecture right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 986,
      "text": "so so what it does is it does basically",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 987,
      "text": "a little 4x4 Matrix it does basically a little 4x4 Matrix it does basically a little 4x4 Matrix multiply",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 988,
      "text": "so uh this is just matrix multiply",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 989,
      "text": "so uh this is just matrix multiply",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 990,
      "text": "so uh this is just matrix multiplication here of 4x4 matrices and multiplication here of 4x4 matrices and multiplication here of 4x4 matrices and there are multiple configurations as to there are multiple configurations as to there are multiple configurations as to what Precision any of these matrices are what Precision any of these matrices are what Precision any of these matrices are it in what Precision the internal it in what Precision the internal it in what Precision the internal accumulate happens",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 991,
      "text": "and then what is the accumulate happens and then what is the accumulate happens and then what is the output Precision input precisions",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 992,
      "text": "Etc so output Precision input precisions Etc so output Precision input precisions Etc so there's a few switches but it's there's a few switches",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 993,
      "text": "but it's there's a few switches",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 994,
      "text": "but it's basically a 4x4 multiply",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 995,
      "text": "and then basically a 4x4 multiply",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 996,
      "text": "and then basically a 4x4 multiply",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 997,
      "text": "and then anytime we have any operations that anytime we have any operations that anytime we have any operations that require Magic multiplication uh they get require Magic multiplication uh they get require Magic multiplication uh they get broken up into these into this broken up into these into this broken up into these into this instruction of little 4x4 multiply and instruction of little 4x4 multiply and instruction of little 4x4 multiply and so everything gets broken up into this so everything gets broken up into this so everything gets broken up into this instruction because it's the fastest way instruction because it's the fastest way instruction because it's the fastest way to multiply matrices and it turns out to multiply matrices and it turns out to multiply matrices and it turns out that most of the computational work that that most of the computational work that that most of the computational work that we're doing up above uh all of it really we're doing up above uh all of it really we're doing up above uh all of it really is matrix multiplication most of the is matrix multiplication most of the is matrix multiplication most of the work computationally happens in the work computationally happens in the work computationally happens in the linear layers um linear linear Etc linear layers um linear linear Etc linear layers um linear linear",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 998,
      "text": "Etc there's a few things sandwiched in there's a few things sandwiched in there's a few things sandwiched in between so there's some additions in between so there's some additions in between so there's some additions in residuals there's some G nonlinearities residuals there's some G nonlinearities residuals there's some G nonlinearities there's some layer Norms Etc",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 999,
      "text": "but if you there's some layer Norms Etc but if you there's some layer Norms Etc",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1000,
      "text": "but if you just time them you'll see that these are just time them you'll see that these are just time them you'll see that these are nothing like basically the in nothing like basically the in nothing like basically the in Transformer is just a bunch of Matrix Transformer is just a bunch of Matrix Transformer is just a bunch of Matrix multiplications really um and especially multiplications really um and especially multiplications really um and especially at this small scale 124 million at this small scale 124 million at this small scale 124 million parameter model actually the biggest parameter model actually the biggest parameter model actually the biggest matrix multiplication by far is the matrix multiplication by far is the matrix multiplication by far is the classifier layer at the top that is a classifier layer at the top that is a classifier layer at the top that is a massive Matrix multiply of going from massive Matrix multiply of going from massive Matrix multiply of going from 768 to 768 to 768 to 50257 and that Matrix multiply dominates 50257 and that Matrix multiply dominates 50257 and that Matrix multiply dominates anything else that happens in that anything else that happens in that anything else that happens in that Network roughly speaking so it's Matrix Network roughly speaking so it's Matrix Network roughly speaking so it's Matrix multiplies that become a lot faster multiplies that become a lot faster multiplies that become a lot faster which are hidden inside our linear which are hidden inside our linear which are hidden inside our linear layers and they're accelerated through layers and they're accelerated through layers and they're accelerated through tensor course now the best reference I tensor course now the best reference I tensor course now the best reference I would say for tensor course is basically would say for tensor course is basically would say for tensor course is basically just go to the um a 100 architecture just go to the um a 100 architecture just go to the um a 100 architecture white paper",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1001,
      "text": "and then it's pretty white paper",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1002,
      "text": "and then it's pretty white paper",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1003,
      "text": "and then it's pretty detailed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1004,
      "text": "and but I think people it's detailed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1005,
      "text": "and but I think people it's detailed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1006,
      "text": "and but I think people it's like relatively readable mostly if you like relatively readable mostly if you like relatively readable mostly if you half understand what's happening um so half understand what's happening um so half understand what's happening um so figure 9 tensor float figure 9 tensor float figure 9 tensor float 32",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1007,
      "text": "so this is the explanation basically 32 so this is the explanation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1008,
      "text": "basically 32 so this is the explanation basically for tf32 and what happens here and you for tf32 and what happens here and you for tf32 and what happens here and you see that there's many configuration see that there's many configuration see that there's many configuration options here available so the input options here available so the input options here available so the input operands and what precisions are they in operands and what precisions are they in operands and what precisions are they in the accumulator and um what um basically the accumulator and um what um basically the accumulator and um what um basically the um the internal representation the um the internal representation the um the internal representation within the instruction when you do the within the instruction when you do the within the instruction when you do the accumulate of this matrix accumulate of this matrix accumulate of this matrix multiplication so the intermediate plus multiplication so the intermediate plus multiplication so the intermediate plus equals um of the intermediate little equals um of the intermediate little equals um of the intermediate little vector multiplies here that all happens vector multiplies here that all happens vector multiplies here that all happens in in in fp32 and then uh this is an aex fp32 and then uh this is an aex fp32 and then uh this is an aex improvement as I mentioned to the Ops improvement as I mentioned to the Ops improvement as I mentioned to the Ops that we get so tf32 specifically we're that we get so tf32 specifically we're that we get so tf32 specifically we're looking at this row here and the way looking at this row here and the way looking at this row here and the way this works this works this works is is is um normally fp32 has 32 bits um normally fp32 has 32 bits um normally fp32 has 32 bits tf32 is the exact same bits we have one tf32 is the exact same bits we have one tf32 is the exact same bits we have one sign bit we have eight exponent bits sign bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1009,
      "text": "we have eight exponent bits sign bit we have eight exponent bits except the mantisa bits get cropped in except the mantisa bits get cropped in except the mantisa bits get cropped in the float",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1010,
      "text": "and so basically um we end up the float",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1011,
      "text": "and so basically um we end up the float",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1012,
      "text": "and so basically um we end up with just 19 bits instead of 32 bits with just 19 bits instead of 32 bits with just 19 bits instead of 32 bits because the last 133 bits get truncated because the last 133 bits get truncated because the last 133 bits get truncated they get dropped um and all this is they get dropped um and all this is they get dropped um and all this is internal to the instruction so none of internal to the instruction so none of internal to the instruction so none of it is visible to anything in our pytorch it is visible to anything in our pytorch it is visible to anything in our pytorch uh none of our pytorch code will change uh none of our pytorch code will change uh none of our pytorch code will change all of the numbers will look identical all of the numbers will look identical all of the numbers will look identical it's just that when you call the tensor it's just that when you call the tensor it's just that when you call the tensor core um instruction internally in the core um instruction internally in the core um instruction internally in the hardware it will crop out these 13 bits hardware it will crop out these 13 bits hardware it will crop out these 13 bits and that allows it to uh calculate this and that allows it to uh calculate this and that allows it to uh calculate this little Matrix multiply significantly little Matrix multiply significantly little Matrix multiply significantly faster 8X faster now of course this faster 8X faster now of course this faster 8X faster now of course this speed up comes at a cost and the cost is speed up comes at a cost and the cost is speed up comes at a cost and the cost is that we are reducing the Precision our that we are reducing the Precision our that we are reducing the Precision our accumulate is still an fp32 our output accumulate is still an fp32 our output accumulate is still an fp32 our output is fp32 our inputs are fp32 but is fp32 our inputs are fp32 but is fp32 our inputs are fp32 but internally things get truncated in the internally things get truncated in the internally things get truncated in the operand to perform the operation faster operand to perform the operation faster operand to perform the operation faster and so our results are starting to be a",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1013,
      "text": "and so our results are starting to be a",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1014,
      "text": "and so our results are starting to be a bit more approximate but empirically bit more approximate but empirically bit more approximate but empirically when you actually train with this you when you actually train with this you when you actually train with this you basically can't tell the difference basically can't tell the difference basically can't tell the difference",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1015,
      "text": "so the reason I like tf32 is because if so the reason I like tf32 is because if so the reason I like tf32 is because if you can tolerate a little bit of a you can tolerate a little bit of a you can tolerate a little bit of a Precision fudge um then this is free Precision fudge",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1016,
      "text": "um then this is free Precision fudge um then this is free like none of your codes sees this it's like none of your codes sees this it's like none of your codes sees this it's fully internal to the operation and the fully internal to the operation and the fully internal to the operation and the operation to you just go 8X faster and operation to you just go 8X faster and operation to you just go 8X faster and it's a bit more approximate and so it's it's a bit more approximate",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1017,
      "text": "and so it's it's a bit more approximate",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1018,
      "text": "and so it's a pretty sweet spot I would say in a pretty sweet spot I would say in a pretty sweet spot I would say in optimization and uh let's see what that optimization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1019,
      "text": "and uh let's see what that optimization and uh let's see what that looks like first so I've set up our Cod looks like first so I've set up our Cod looks like first so I've set up our Cod to just time the uh iterations so import to just time the uh iterations so import to just time the uh iterations so import time I changed the hyper parameters so time I changed the hyper parameters so time I changed the hyper parameters so that we have something a bit more that that we have something a bit more that that we have something a bit more that reflects uh kind of workload that we reflects uh kind of workload that we reflects uh kind of workload that we want to run uh because we want to do a want to run uh because we want to do a want to run uh because we want to do a fairly large run at the end of this so fairly large run at the end of this so fairly large run at the end of this so let's use batch size 16 and let's now let's use batch size 16 and let's now let's use batch size 16 and let's now use the actual gpt2 um maximum sequence use the actual gpt2 um maximum sequence use the actual gpt2 um maximum sequence length of 10,24 length of 10,24 length of 10,24 tokens uh so this is the tokens uh so this is the tokens uh so this is the configuration and then for 50 iterations configuration and then for 50 iterations configuration and then for 50 iterations I'm just doing something very lazy here I'm just doing something very lazy here I'm just doing something very lazy here I'm doing time.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1020,
      "text": "time to get the current I'm doing time.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1021,
      "text": "time to get the current I'm doing time.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1022,
      "text": "time to get the current time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1023,
      "text": "and then this is the optimization time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1024,
      "text": "and then this is the optimization time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1025,
      "text": "and then this is the optimization Loop and now I want to time how long Loop and now I want to time how long Loop and now I want to time how long this takes now one issue with working this takes now one issue with working this takes now one issue with working with gpus is that as your with gpus is that as your with gpus is that as your CPU um when your CPU runs it's just CPU um when your CPU runs it's just CPU um when your CPU runs it's",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1026,
      "text": "just scheduling work on GPU it's ordering scheduling work on GPU it's ordering scheduling work on GPU it's ordering some work right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1027,
      "text": "and so it send a request some work",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1028,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1029,
      "text": "and so it send a request some work",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1030,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1031,
      "text": "and so it send a request",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1032,
      "text": "and then it continues running",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1033,
      "text": "and so we and then it continues running",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1034,
      "text": "and so we and then it continues running",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1035,
      "text": "and so we can actually it can happen sometimes can actually it can happen sometimes can actually it can happen sometimes that we sort of um speed through this that we sort of um speed through this that we sort of um speed through this and we queue up a lot of kernels to run and we queue up a lot of kernels to run and we queue up a lot of kernels to run on the GPU and then the CPU sort of like on the GPU and then the CPU sort of like on the GPU and then the CPU sort of like gets here and takes time at time but gets here and takes time at time but gets here and takes time at time but actually the GPU is still running actually the GPU is still running actually the GPU is still running because it takes it time to actually because it takes it time to actually because it takes it time to actually work through the work that was scheduled work through the work that was scheduled work through the work that was scheduled to run",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1036,
      "text": "and so you're just building up a to run",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1037,
      "text": "and so you're just building up a to run",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1038,
      "text": "and so you're just building up a queue for the GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1039,
      "text": "and so actually if you queue for the GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1040,
      "text": "and so actually if you queue for the GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1041,
      "text": "and so actually if you need to you want to wait toat data need to you want to wait toat data need to you want to wait toat data synchronize",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1042,
      "text": "and this will wait for the synchronize and this will wait for the synchronize and this will wait for the GPU to finish all the work that was GPU to finish all the work that was GPU to finish all the work that was scheduled to run up above here and then scheduled to run up above here and then scheduled to run up above here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1043,
      "text": "and then we can actually take the time so we can actually take the time so we can actually take the time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1044,
      "text": "so basically we're waiting for the GPU to basically we're waiting for the GPU to basically we're waiting for the GPU to stop this iteration take time and then stop this iteration take time and then stop this iteration take time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1045,
      "text": "and then we're going to just print it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1046,
      "text": "so we're going to just print it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1047,
      "text": "so we're going to just print it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1048,
      "text": "so so here I'm going to run the training so here I'm going to run the training so here I'm going to run the training Loop and here on the right I'm watching Loop and here on the right I'm watching Loop and here on the right I'm watching Nvidia SMI",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1049,
      "text": "so we start off at zero um Nvidia SMI",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1050,
      "text": "so we start off at zero um Nvidia SMI",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1051,
      "text": "so we start off at zero um we're not using the GPU and then by we're not using the GPU and then by we're not using the GPU and then by default P will use gpu0",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1052,
      "text": "so we see that default P will use gpu0 so we see that default P will use gpu0 so we see that it gets filled up",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1053,
      "text": "and we're using 35 GB it gets filled up and we're using 35 GB it gets filled up and we're using 35 GB out of 80 gabt out of 80 gabt out of 80 gabt available",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1054,
      "text": "and then here on the left we available",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1055,
      "text": "and then here on the left we available",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1056,
      "text": "and then here on the left we see that because we've cranked up the see that because we've cranked up the see that because we've cranked up the batch batch batch size now it's only 20 batches to do a size now it's only 20 batches to do a size now it's only 20 batches to do a single Epoch on our tiny Shakespeare single Epoch on our tiny Shakespeare single Epoch on our tiny Shakespeare",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1057,
      "text": "and we see that we're seeing roughly a",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1058,
      "text": "and we see that we're seeing roughly a",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1059,
      "text": "and we see that we're seeing roughly a th000 milliseconds per iteration here th000 milliseconds per iteration here th000 milliseconds per iteration here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1060,
      "text": "right right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1061,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1062,
      "text": "so the first iteration sometimes is so the first iteration sometimes is so the first iteration sometimes is slower and that's because pytorch might slower and that's because pytorch might slower and that's because pytorch might be doing a lot of initializations here be doing a lot of initializations here be doing a lot of initializations here on the very first iteration",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1063,
      "text": "and so it's on the very first iteration",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1064,
      "text": "and so it's on the very first iteration",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1065,
      "text": "and so it's probably initializing all these uh probably initializing all these uh probably initializing all these uh tensors and buffers to hold all the tensors and buffers to hold all the tensors and buffers to hold all the gradients",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1066,
      "text": "and I'm not 100% sure all the gradients",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1067,
      "text": "and I'm not 100% sure all the gradients",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1068,
      "text": "and I'm not 100% sure all the work that happens here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1069,
      "text": "but uh this could work that happens here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1070,
      "text": "but uh this could work that happens here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1071,
      "text": "but uh this could be a slower iteration when you're timing be a slower iteration when you're timing be a slower iteration when you're timing your logic you always want to be careful your logic you always want to be careful your logic you always want to be careful with that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1072,
      "text": "but basically we're seeing a with that but basically we're seeing a with that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1073,
      "text": "but basically we're seeing a th000 milliseconds per iteration th000 milliseconds per iteration th000 milliseconds per iteration um and so this will run for roughly 50 um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1074,
      "text": "and so this will run for roughly 50 um and so this will run for roughly 50 seconds as we have it right now so seconds as we have it right now so seconds as we have it right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1075,
      "text": "so that's our Baseline in flo 32 one more that's our Baseline in flo 32 one more that's our Baseline in flo 32 one more thing I wanted to mention is that if thing I wanted to mention is that if thing I wanted to mention is that if this doesn't fit into your GPU and this doesn't fit into your GPU and this doesn't fit into your GPU and you're getting out of memory errors then you're getting out of memory errors then you're getting out of memory errors then start decreasing your batch size until start decreasing your batch size until start decreasing your batch size until things fit so instead of 16 try eight or things fit so instead of 16 try eight or things fit so instead of 16 try eight or four or whatever you need to fit um the four or whatever you need to fit um the four or whatever you need to fit um the batch into your GPU and if you have a batch into your GPU and if you have a batch into your GPU and if you have a bigger GPU you can actually potentially bigger GPU you can actually potentially bigger GPU you can actually potentially get away with 32 and so on uh by default get away with 32 and so on uh by default get away with 32 and so on uh by default you want to basically max out has Max you want to basically max out has Max you want to basically max out has Max Max out the batch size that fits on your Max out the batch size that fits on your Max out the batch size that fits on your GPU and you want to keep it nice numbers GPU and you want to keep it nice numbers GPU and you want to keep it nice numbers so use numbers that have lots of powers so use numbers that have lots of powers so use numbers that have lots of powers of two in them so 16 is a good number 8 of two in them so 16 is a good number 8 of two in them so 16 is a good number 8 24 32 48",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1076,
      "text": "These are nice numbers but 24 32 48",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1077,
      "text": "These are nice numbers but 24 32 48",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1078,
      "text": "These are nice numbers but don't use something like 17 uh because don't use something like 17 uh because don't use something like 17 uh because that will run very inefficiently on a that will run very inefficiently on a that will run very inefficiently on a GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1079,
      "text": "uh and we're going to see that a bit GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1080,
      "text": "uh and we're going to see that a bit GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1081,
      "text": "uh and we're going to see that a bit later as well so for now let's just later as well so for now let's just later as well so for now let's just stick with stick with 16124 and uh the one thing that I added 16124 and uh the one thing that I added 16124 and uh the one thing that I added also here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1082,
      "text": "and I ran it again is I'm also here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1083,
      "text": "and I ran it again is I'm also here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1084,
      "text": "and I ran it again is I'm calculating a tokens per second calculating a tokens per second calculating a tokens per second throughput during training throughput during training throughput during training because we might end up changing the because we might end up changing the because we might end up changing the backat size around over time but tokens backat size around over time but tokens backat size around over time but tokens per second is the objective measure that per second is the objective measure that per second is the objective measure that we actually really care about how many we actually really care about how many we actually really care about how many tokens of data are we training on and tokens of data are we training on and tokens of data are we training on and what is the throughput of tokens that what is the throughput of tokens that what is the throughput of tokens that we're getting in our optimization so we're getting in our optimization so we're getting in our optimization so right now we're processing and training right now we're processing and training right now we're processing and training on 163,000 tokens per second roughly and on 163,000 tokens per second roughly and on 163,000 tokens per second roughly and that's a bit more objective that's a bit more objective that's a bit more objective metric",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1085,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1086,
      "text": "so let's now enable tf32 now metric",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1087,
      "text": "okay so let's now enable tf32 now metric",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1088,
      "text": "okay so let's now enable tf32 now luckily pytorch makes this fairly easy luckily pytorch makes this fairly easy luckily pytorch makes this fairly easy for us and uh to enable tf32 you just for us and uh to enable tf32 you just for us and uh to enable tf32 you just need to do a single line and is this and need to do a single line and is this and need to do a single line and is this and when we go to the py documentation here when we go to the py documentation here when we go to the py documentation here for this function basically this tells for this function basically this tells for this function basically this tells pych what kind of kernels to run and by pych what kind of kernels to run and by pych what kind of kernels to run and by default I believe it is highest highest default I believe it is highest highest default I believe it is highest highest Precision for mat M and that means that Precision for mat M and that means that Precision for mat M and that means that everything happens in float 32 just like everything happens in float 32 just like everything happens in float 32 just like it did before",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1089,
      "text": "but if we set it to high it did before",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1090,
      "text": "but if we set it to high it did before",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1091,
      "text": "but if we set it to high as we do right now Matrix as we do right now Matrix as we do right now Matrix multiplications will not use tensor flow multiplications will not use tensor flow multiplications will not use tensor flow 32 when it's 32 when it's 32 when it's available my GPU is a100",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1092,
      "text": "so it's an available my GPU is a100",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1093,
      "text": "so it's an available my GPU is a100",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1094,
      "text": "so it's an ampere series and therefore tf32 is ampere series and therefore tf32 is ampere series and therefore tf32 is available if you have an older GPU this available if you have an older GPU this available if you have an older GPU this might not be available for you but for might not be available for you but for might not be available for you but for my GPU it's available",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1095,
      "text": "and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1096,
      "text": "so what I my GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1097,
      "text": "it's available",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1098,
      "text": "and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1099,
      "text": "so what I my GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1100,
      "text": "it's available",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1101,
      "text": "and so what I expect P to do is that every single expect P to do is that every single expect P to do is that every single place where we see an nn. linear inside place where we see an nn. linear inside place where we see an nn. linear inside there there's a matrix multiplication there there's a matrix multiplication there there's a matrix multiplication",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1102,
      "text": "and I expect that matrix multiplication",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1103,
      "text": "and I expect that matrix multiplication",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1104,
      "text": "and I expect that matrix multiplication now to be um running on tensor course now to be um running on tensor course now to be um running on tensor course utilizing the TF 32%",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1105,
      "text": "so this is the single line of change so this is the single line of change that is I believe necessary and let's that is I believe necessary and let's that is I believe necessary and let's rerun this now we saw that um in terms rerun this now we saw that um in terms rerun this now we saw that um in terms of the throughput that is promised to us of the throughput that is promised to us of the throughput that is promised to us we're supposed to be getting 8X roughly we're supposed to be getting 8X roughly we're supposed to be getting 8X roughly so let's see what so let's see what so let's see what happens and that 8X came from here right happens and that 8X came from here right happens and that 8X came from here right um 8X and it also came from looking at um 8X and it also came from looking at um 8X and it also came from looking at it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1106,
      "text": "um here 156 T flops instead of of it um here 156 T flops instead of of it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1107,
      "text": "um here 156 T flops instead of of 19.5 okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1108,
      "text": "so what actually happened uh 19.5",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1109,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1110,
      "text": "so what actually happened uh 19.5",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1111,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1112,
      "text": "so what actually happened uh so we're seeing that our throughput so we're seeing that our throughput so we're seeing that our throughput roughly 3x not aex",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1113,
      "text": "so we are going we're roughly 3x not aex",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1114,
      "text": "so we are going we're roughly 3x not aex",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1115,
      "text": "so we are going we're from 1,000 milliseconds we're going down from 1,000 milliseconds we're going down from 1,000 milliseconds we're going down to 300 milliseconds and our throughput to 300 milliseconds and our throughput to 300 milliseconds and our throughput is now about 50,000 tokens per second",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1116,
      "text": "so is now about 50,000 tokens per second so is now about 50,000 tokens per second",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1117,
      "text": "so we have a roughly 3x instead of 8X so we have a roughly 3x instead of 8X so we have a roughly 3x instead of 8X so what happened and basically What's what happened and basically What's what happened and basically What's Happening Here is again a lot of these Happening Here is again a lot of these Happening Here is again a lot of these workloads are memory bound and so even workloads are memory bound and so even workloads are memory bound and so even though the though the though the tf32 offers in principle a lot faster tf32 offers in principle a lot faster tf32 offers in principle a lot faster throughput all of these numbers throughput all of these numbers throughput all of these numbers everywhere are still float 32s and it's everywhere are still float 32s",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1118,
      "text": "and it's everywhere are still float 32s",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1119,
      "text": "and it's float 32 numbers that are being shipped float 32 numbers that are being shipped float 32 numbers that are being shipped all over the place through the memory all over the place through the memory all over the place through the memory system and is just costing us way too system and is just costing us way too system and is just costing us way too much time to shuttle around all this much time to shuttle around all this much time to shuttle around all this data and so even though we've made the data and so even though we've made the data and so even though we've made the multiply itself much faster uh we are multiply itself much faster uh we are multiply itself much faster uh we are memory bound and we're not actually memory bound",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1120,
      "text": "and we're not actually memory bound and we're not actually seeing the full benefit uh that would seeing the full benefit uh that would seeing the full benefit uh that would come from uh this napkin math here uh come from uh this napkin math here uh come from uh this napkin math here uh that said we are getting one a 3X faster that said we are getting one a 3X faster that said we are getting one a 3X faster throughput and this is free um single throughput and this is free um single throughput and this is free um single line of code in P torch all your line of code in P torch all your line of code in P torch all your variables are still float 32 everywhere variables are still float 32 everywhere variables are still float 32 everywhere it just runs faster and it's slightly it just runs faster and it's slightly it just runs faster and it's slightly more approximate but we're not going to more approximate",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1121,
      "text": "but we're not going to more approximate",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1122,
      "text": "but we're not going to notice it basically uh so that's notice it basically uh so that's notice it basically",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1123,
      "text": "uh so that's tf32",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1124,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1125,
      "text": "so let's now continue so we've tf32",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1126,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1127,
      "text": "so let's now continue so we've tf32",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1128,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1129,
      "text": "so let's now continue so we've exercised this row and um we saw that we exercised this row",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1130,
      "text": "and um we saw that we exercised this row and um we saw that we can crop out some of the Precision can crop out some of the Precision can crop out some of the Precision inside the operation itself",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1131,
      "text": "but we saw inside the operation itself",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1132,
      "text": "but we saw inside the operation itself",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1133,
      "text": "but we saw that we're still memory bound we're that we're still memory bound we're that we're still memory bound",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1134,
      "text": "we're still moving around all these floats still moving around all these floats still moving around all these floats right otherwise",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1135,
      "text": "and we're paying that right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1136,
      "text": "otherwise and we're paying that right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1137,
      "text": "otherwise and we're paying that cost because of this so let's now cost because of this so let's now cost because of this so let's now decrease the amount of stuff that we're decrease the amount of stuff that we're decrease the amount of stuff that we're going to be moving around and we're going to be moving around and we're going to be moving around and we're going to do that by dropping down to B going to do that by dropping down to B going to do that by dropping down to B float 16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1138,
      "text": "so we're only going to be float 16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1139,
      "text": "so we're only going to be float 16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1140,
      "text": "so we're only going to be maintaining 16 bits per float",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1141,
      "text": "and we're maintaining 16 bits per float and we're maintaining 16 bits per float and we're going to use the B flat 16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1142,
      "text": "and I'll going to use the B flat 16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1143,
      "text": "and I'll going to use the B flat 16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1144,
      "text": "and I'll explain in a bit uh fp16 difference and explain in a bit uh fp16 difference and explain in a bit uh fp16 difference",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1145,
      "text": "and uh we're going to be in this row",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1146,
      "text": "so when uh we're going to be in this row",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1147,
      "text": "so when uh we're going to be in this row",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1148,
      "text": "so when we go back to the documentation here for we go back to the documentation here for we go back to the documentation here for the a the a the a 100 um we see here the precisions that 100 um we see here the precisions that 100 um we see here the precisions that are are available and this is the are are available and this is the are are available and this is the original fp32 the tf32 crops out the original fp32 the tf32 crops out the original fp32 the tf32 crops out the Precision and then here in Precision and then here in Precision and then here in bf16 you see that it is very similar to bf16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1149,
      "text": "you see that it is very similar to bf16 you see that it is very similar to tf32",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1150,
      "text": "but it's even more aggressive in tf32 but it's even more aggressive in tf32 but it's even more aggressive in cropping off of the Precision the cropping off of the Precision the cropping off of the Precision the mantisa of this float so the important mantisa of this float so the important mantisa of this float so the important thing with B float 16 is that the thing with B float 16 is that the thing with B float 16 is that the exponent bits and the sign bit of course exponent bits and the sign bit of course exponent bits and the sign bit of course remain unchanged so if you're familiar remain unchanged so if you're familiar remain unchanged so if you're familiar with your float numbers",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1151,
      "text": "and I think this with your float numbers and I think this with your float numbers",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1152,
      "text": "and I think this should should probably be an entire should should probably be an entire should should probably be an entire video by itself video by itself video by itself",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1153,
      "text": "the exponent sets the range that you can the exponent sets the range that you can the exponent sets the range that you can represent of your numbers and the represent of your numbers and the represent of your numbers and the Precision is how much",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1154,
      "text": "Precision you have Precision is how much",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1155,
      "text": "Precision you have Precision is how much",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1156,
      "text": "Precision you have for your numbers and so the range of for your numbers and so the range of for your numbers and so the range of numbers is identical but we can we have numbers is identical but we can we have numbers is identical but we can we have fewer possibilities within that range fewer possibilities within that range fewer possibilities within that range because we are truncating the Mena so we because we are truncating the Mena",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1157,
      "text": "so we because we are truncating the Mena so we have less Precision in that have less Precision in that have less Precision in that range what that means is that things are range what that means is that things are range what that means is that things are actually fairly nice because we have the actually fairly nice because we have the actually fairly nice because we have the original range of numbers that are original range of numbers that are original range of numbers that are representable in float",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1158,
      "text": "but we just have representable in float",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1159,
      "text": "but we just have representable in float",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1160,
      "text": "but we just have less Precision for it and the difference less Precision for it and the difference less Precision for it and the difference with fp16 is that they actually touch with fp16 is that they actually touch with fp16 is that they actually touch and change the range so fp16 cannot and change the range so fp16 cannot and change the range so fp16 cannot represent the full range of fp32 it has represent the full range of fp32 it has represent the full range of fp32 it has a reduced range and that's where you a reduced range",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1161,
      "text": "and that's where you a reduced range",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1162,
      "text": "and that's where you start to actually run into issues start to actually run into issues start to actually run into issues because now you need uh these gradient because now you need uh these gradient because now you need uh these gradient scalers and things like that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1163,
      "text": "and I'm not scalers and things like that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1164,
      "text": "and I'm not scalers and things like that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1165,
      "text": "and I'm not going to go into the detail of that in going to go into the detail of that in going to go into the detail of that in this video because that's a whole video this video because that's a whole video this video because that's a whole video by itself but fb16 actually historically by itself but fb16 actually historically by itself but fb16 actually historically came first that was available in the came first that was available in the came first that was available in the Volta series before Amper and so fp16 Volta series before Amper and so fp16 Volta series before Amper and so fp16 came first and everyone started to train came first and everyone started to train came first and everyone started to train in fp16 but everyone had to use all in fp16 but everyone had to use all in fp16 but everyone had to use all these gradient scaling operations which these gradient scaling operations which these gradient scaling operations which are kind of annoying",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1166,
      "text": "and it's an are kind of annoying",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1167,
      "text": "and it's an are kind of annoying",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1168,
      "text": "and it's an additional source of state and additional source of state and additional source of state and complexity and the reason for that was complexity and the reason for that was complexity and the reason for that was because the exponent range was reduced because the exponent range was reduced because the exponent range was reduced in fp16 so that's the i e fp16 spec and in fp16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1169,
      "text": "so that's the i e fp16 spec and in fp16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1170,
      "text": "so that's the i e fp16 spec",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1171,
      "text": "and then they came out with bf16 and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1172,
      "text": "the then they came out with bf16 and the then they came out with bf16 and the Ampere and they made it much simpler Ampere",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1173,
      "text": "and they made it much simpler Ampere",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1174,
      "text": "and they made it much simpler because we're just truncating manessa we because we're just truncating manessa we because we're just truncating manessa we have the exact same range and we do not have the exact same range and we do not have the exact same range and we do not need gradient scalers so everything is need gradient scalers so everything is need gradient scalers so everything is much much simpler now when we do use much much simpler now when we do use much much simpler now when we do use bf16 though we are impacting the numbers bf16 though we are impacting the numbers bf16 though we are impacting the numbers that we might be seeing in our pytorch that we might be seeing in our pytorch that we might be seeing in our pytorch code these this change is not just local code these this change is not just local code these this change is not just local to the operation itself so let's see how to the operation itself so let's see how to the operation itself so let's see how that works that works that works um there's some documentation here that um there's some documentation here that um there's some documentation here that so I think this is probably the best",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1175,
      "text": "so I think this is probably the best",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1176,
      "text": "so I think this is probably the best best page to explain how to use mixed best page to explain how to use mixed best page to explain how to use mixed Precision in pytorch um because there Precision in pytorch um because there Precision in pytorch um because there are many other tutorials and so on even are many other tutorials and so on even are many other tutorials and so on even within pitor documentation that are a within pitor documentation that are a within pitor documentation that are a lot more confusing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1177,
      "text": "and so I recommend lot more confusing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1178,
      "text": "and so I recommend lot more confusing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1179,
      "text": "and so I recommend specifically this one because there's specifically this one because there's specifically this one because there's five other copies that I would not five other copies that I would not five other copies that I would not recommend and then when we come recommend",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1180,
      "text": "and then when we come recommend",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1181,
      "text": "and then when we come here ignore everything about everything here ignore everything about everything here ignore everything about everything ignore everything about gradient ignore everything about gradient ignore everything about gradient scalers and only look at torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1182,
      "text": "scalers and only look at torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1183,
      "text": "scalers and only look at torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1184,
      "text": "AutoCast and basically also this comes AutoCast and basically also this comes AutoCast and basically also this comes to a single line of code at the end so to a single line of code at the end so to a single line of code at the end",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1185,
      "text": "so this is the context manager that we this is the context manager that we this is the context manager that we want and we want to use that in our want and we want to use that in our want and we want to use that in our Network when you click into the torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1186,
      "text": "Network when you click into the torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1187,
      "text": "Network when you click into the torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1188,
      "text": "AutoCast autocasting it has a few more AutoCast autocasting it has a few more AutoCast autocasting it has a few more uh a bit more guideline for you",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1189,
      "text": "so it's uh a bit more guideline for you",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1190,
      "text": "so it's uh a bit more guideline for you",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1191,
      "text": "so it's telling you do not call B flat 16 on any telling you do not call B flat 16 on any telling you do not call B flat 16 on any of your tensors just use AutoCast and of your tensors just use AutoCast and of your tensors just use AutoCast and only surround the uh forward pass of the only surround the uh forward pass of the only surround the uh forward pass of the model and the loss calculation and model and the loss calculation and model and the loss calculation and that's the only two things that you",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1192,
      "text": "that's the only two things that you",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1193,
      "text": "that's the only two things that you should be surrounding leave the backward should be surrounding leave the backward should be surrounding leave the backward and the optimizer step alone",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1194,
      "text": "so that's and the optimizer step alone so that's and the optimizer step alone",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1195,
      "text": "so that's the guidance that comes from the P team the guidance that comes from the P team the guidance that comes from the P team",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1196,
      "text": "so we're going to follow that guidance so we're going to follow that guidance",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1197,
      "text": "so we're going to follow that guidance and for us because the L calculation is and for us because the L calculation is and for us because the L calculation is inside of the model forward pass for us inside of the model forward pass for us inside of the model forward pass for us we are going to be doing we are going to be doing we are going to be doing this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1198,
      "text": "and then we don't want to be using this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1199,
      "text": "and then we don't want to be using this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1200,
      "text": "and then we don't want to be using torch Flo 16 because if we do that we torch Flo 16 because if we do that we torch Flo 16 because if we do that we need to start using gradient scalers as need to start using gradient scalers as need to start using gradient scalers as well so we are going to be using B float",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1201,
      "text": "well so we are going to be using B float",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1202,
      "text": "well so we are going to be using B float 16 this is only possible to do an ampere 16 this is only possible to do an ampere 16 this is only possible to do an ampere uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1203,
      "text": "but this means that the changes are uh but this means that the changes are uh but this means that the changes are extremely minimal like basically just extremely minimal like basically just extremely minimal like basically just this one line of this one line of this one line of code um let me first break code um let me first break code um let me first break in to here before we actually run this in to here before we actually run this in to here before we actually run this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1204,
      "text": "so right after logits I'd like to show so right after logits I'd like to show so right after logits I'd like to show you that different from the tf32 that we you that different from the tf32 that we you that different from the tf32 that we saw this is actually going to impact our saw this is actually going to impact our saw this is actually going to impact our tensors tensors tensors so this Lis tensor if we now look at so this Lis tensor if we now look at so this Lis tensor if we now look at this and we look at the dtype we this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1205,
      "text": "and we look at the dtype we this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1206,
      "text": "and we look at the dtype we suddenly see that this is now B float suddenly see that this is now B float suddenly see that this is now B float 16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1207,
      "text": "uh it's not float 32 anymore",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1208,
      "text": "so our 16 uh it's not float 32 anymore",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1209,
      "text": "so our 16 uh it's not float 32 anymore so our activations have been changed the activations have been changed the activations have been changed the activations tensor is now B FL 16 but activations tensor is now B FL 16 but activations tensor is now B FL 16 but not everything has changed so model.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1210,
      "text": "not everything has changed so model.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1211,
      "text": "not everything has changed so model.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1212,
      "text": "Transformer wte uh this is the weight uh token wte uh this is the weight uh token embedding table it has a weight inside embedding table it has a weight inside embedding table it has a weight inside it and the dtype of this weight this it and the dtype of this weight this it and the dtype of this weight this parameter is still torch float 32",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1213,
      "text": "so our parameter is still torch float 32",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1214,
      "text": "so our parameter is still torch float 32 so our parameters seem to still be in float 32 parameters seem to still be in float 32 parameters seem to still be in float 32 but our activations the loits are now in but our activations the loits are now in but our activations the loits are now in P 16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1215,
      "text": "so clearly this is why we get the P",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1216,
      "text": "16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1217,
      "text": "so clearly this is why we get the P",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1218,
      "text": "16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1219,
      "text": "so clearly this is why we get the mixed Precision some things pytorch is mixed Precision some things pytorch is mixed Precision some things pytorch is keeping inlow 32 some things pytorch is keeping inlow 32 some things pytorch is keeping inlow 32 some things pytorch is converting to lower Precision um and converting to lower Precision um and converting to lower Precision um and what gets converted at what point is not what gets converted at what point is not what gets converted at what point is not super clear I remember scrolling super clear",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1220,
      "text": "I remember scrolling super clear",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1221,
      "text": "I remember scrolling down is it here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1222,
      "text": "okay I can't find here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1223,
      "text": "okay I can't find it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1224,
      "text": "I I thought it was here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1225,
      "text": "okay there",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1226,
      "text": "we it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1227,
      "text": "I I thought it was here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1228,
      "text": "okay there",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1229,
      "text": "we it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1230,
      "text": "I I thought it was here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1231,
      "text": "okay there we go so there are a few docks on when go so there are a few docks on when go so there are a few docks on when you're using this AutoCast what gets you're using this AutoCast what gets you're using this AutoCast what gets converted to B FL 16 and and when so for converted to B FL 16 and and when so for converted to B FL 16 and and when so for example only these Matrix multiply like example only these Matrix multiply like example only these Matrix multiply like operations get converted to float 16 but operations get converted to float 16 but operations get converted to float 16 but a lot of operations remain in float 32 a lot of operations remain in float 32 a lot of operations remain in float 32 so in particular a lot of normalizations so in particular a lot of normalizations so in particular a lot of normalizations like layer norms and things like that like layer norms and things like that like layer norms and things like that not all of those layers might be not all of those layers might be not all of those layers might be converted um so only some layers converted um so only some layers converted um so only some layers selectively would be running B flat 16 selectively would be running B flat 16 selectively would be running B flat 16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1232,
      "text": "but things like softmax uh layer Norms but things like softmax uh layer Norms but things like softmax uh layer Norms uh log um log soft Max so loss function uh log um log soft Max so loss function uh log um log soft Max so loss function calculations a lot of those things might calculations a lot of those things might calculations a lot of those things might remain in float 32 because they are more remain in float 32 because they are more remain in float 32 because they are more susceptible to Precision changes major susceptible to Precision changes major susceptible to Precision changes major multiplies are fairly um multiplies are fairly um multiplies are fairly um robust to Precision changes uh so some robust to Precision changes uh so some robust to Precision changes uh so some parts of the network are um impacted parts of the network are um impacted parts of the network are um impacted more or less by the Precision more or less by the Precision more or less by the Precision change um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1233,
      "text": "so basically only some parts change um so basically only some parts change um so basically only some parts of the of the model are running in of the of the model are running in of the of the model are running in reduced Precision let's take it for a reduced Precision let's take it for a reduced Precision let's take it for a spin and let's actually see what kind of spin and let's actually see what kind of spin and let's actually see what kind of improvement we achieve here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1234,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1235,
      "text": "so we used to be 333 here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1236,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1237,
      "text": "so we used to be 333 milliseconds we're now 300 milliseconds we're now 300 milliseconds we're now 300 and we used to be somewhere around and we used to be somewhere around and we used to be somewhere around 50,000 tokens per second we're now at 55 50,000 tokens per second we're now at 55 50,000 tokens per second we're now at 55",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1238,
      "text": "so we're definitely running faster",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1239,
      "text": "but so we're definitely running faster",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1240,
      "text": "but so we're definitely running faster but maybe not a lot faster",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1241,
      "text": "and that's maybe not a lot faster and that's maybe not a lot faster and that's because there are still many many because there are still many many because there are still many many bottlenecks in our gbt2 we're just bottlenecks in our gbt2 we're just bottlenecks in our gbt2 we're just getting started but we have dropped down getting started but we have dropped down getting started but we have dropped down the precision as far as we can with my the precision as far as we can with my the precision as far as we can with my current GPU which is a100 we're using current GPU which is a100 we're using current GPU which is a100 we're using pytorch AutoCast unfortunately I don't pytorch AutoCast unfortunately I don't pytorch AutoCast unfortunately I don't actually exactly know what pytorch actually exactly know what pytorch actually exactly know what pytorch AutoCast do uh does I don't actually AutoCast do uh does I don't actually AutoCast do uh does I don't actually know exactly what's in B flat 16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1242,
      "text": "what's know exactly what's in B flat 16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1243,
      "text": "what's know exactly what's in B flat 16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1244,
      "text": "what's in float 32 in float 32 in float 32 we could go in and we could start to we could go in and we could start to we could go in and we could start to scrutinize it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1245,
      "text": "um but these are the kinds scrutinize it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1246,
      "text": "um but these are the kinds scrutinize it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1247,
      "text": "um but these are the kinds of rules that pytorch has internally and of rules that pytorch has internally and of rules that pytorch has internally and unfortunately they don't documented very unfortunately they don't documented very unfortunately they don't documented very well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1248,
      "text": "uh so we're not going to go into",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1249,
      "text": "well uh so we're not going to go into",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1250,
      "text": "well uh so we're not going to go into that into in too much detail but for now that into in too much detail but for now that into in too much detail but for now we are training in B flow 16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1251,
      "text": "we do not we are training in B flow 16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1252,
      "text": "we do not we are training in B flow 16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1253,
      "text": "we do not need a gradient scaler and the reason need a gradient scaler and the reason need a gradient scaler and the reason things are running faster is because um things are running faster is because um things are running faster is because um we are able to run tensor course in B FL we are able to run tensor course in B FL we are able to run tensor course in B FL 16 now that means we are in this row but 16 now that means we are in this row but 16 now that means we are in this row",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1254,
      "text": "but uh we are also paying in Precision for uh we are also paying in Precision for uh we are also paying in Precision for this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1255,
      "text": "uh so um we expect slightly less this uh so um we expect slightly less this uh so um we expect slightly less accurate results with respect to the accurate results with respect to the accurate results with respect to the original fp32 but empirically in many original fp32 but empirically in many original fp32 but empirically in many cases this is a worth it uh kind of cases this is a worth it uh kind of cases this is a worth it uh kind of tradeoff because it allows you to run tradeoff because it allows you to run tradeoff because it allows you to run faster and you could for example train faster and you could for example train faster and you could for example train longer and make up for the uh for that longer and make up for the uh for that longer and make up for the uh for that Precision decrease",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1256,
      "text": "so um that's b46 for Precision decrease",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1257,
      "text": "so um that's b46 for Precision decrease",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1258,
      "text": "so um that's b46 for now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1259,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1260,
      "text": "so as we can see we are now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1261,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1262,
      "text": "so as we can see we are now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1263,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1264,
      "text": "so as we can see we are currently at about 300 milliseconds uh currently at about 300 milliseconds uh currently at about 300 milliseconds uh per iteration and we're now going to per iteration and we're now going to per iteration and we're now going to reach for some really heavy weapons in reach for some really heavy weapons in reach for some really heavy weapons in the pie torch Arsenal and in particular the pie torch Arsenal and in particular the pie torch Arsenal and in particular we're going to introduce torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1265,
      "text": "compile we're going to introduce torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1266,
      "text": "compile we're going to introduce torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1267,
      "text": "compile so torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1268,
      "text": "compile is really quite so torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1269,
      "text": "compile is really quite so torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1270,
      "text": "compile is really quite incredible infrastructure from the incredible infrastructure from the incredible infrastructure from the pytorch team and it's basically a pytorch team and it's basically a pytorch team and it's basically a compiler for neural networks like it's compiler for neural networks like it's compiler for neural networks like it's almost like GCC for CN C++ code this is almost like GCC for CN C++ code this is almost like GCC for CN C++ code this is just this GCC of neural nuts so came out just this GCC of neural nuts so came out just this GCC of neural nuts so came out a while ago and extremely simple to use a while ago and extremely simple to use a while ago and extremely simple to use um the way to use torch compile is to do um the way to use torch compile is to do um the way to use torch compile is to do this it's a single line of code to this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1271,
      "text": "it's a single line of code to this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1272,
      "text": "it's a single line of code to compile your model and return it now compile your model and return it now compile your model and return it now this line of code will cost you this line of code will cost you this line of code will cost you compilation time but as you might guess compilation time but as you might guess compilation time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1273,
      "text": "but as you might guess it's going to make the code a lot faster it's going to make the code a lot faster it's going to make the code a lot faster so let's actually run that because this so let's actually run that because this so let's actually run that because this will take some time to run but currently will take some time to run but currently will take some time to run but currently remember we're at 300 milliseconds and remember we're at 300 milliseconds and remember we're at 300 milliseconds and we'll see what happens now while this is we'll see what happens now while this is we'll see what happens now while this is running I'd like to explain a little bit running I'd like to explain a little bit running I'd like to explain a little bit of what torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1274,
      "text": "compile does under the of what torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1275,
      "text": "compile does under the of what torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1276,
      "text": "compile does under the hood uh so feel free to read this page hood uh so feel free to read this page hood uh so feel free to read this page of P torch but basically there's no real of P torch but basically there's no real of P torch but basically there's no real good reason for you to not use torch good reason for you to not use torch good reason for you to not use torch compile in your pie torch I kind of feel compile in your pie torch I kind of feel compile in your pie torch I kind of feel like you should be using almost by like you should be using almost by like you should be using almost by default if you're not uh unless you're default if you're not uh unless you're default if you're not uh unless you're debugging",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1277,
      "text": "and you want your code to run debugging",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1278,
      "text": "and you want your code to run debugging",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1279,
      "text": "and you want your code to run really fast",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1280,
      "text": "and there's one line here in really fast",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1281,
      "text": "and there's one line here in really fast",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1282,
      "text": "and there's one line here in torch compile that I found that actually torch compile that I found that actually torch compile that I found that actually kind of like gets to why this is faster kind of like gets to why this is faster kind of like gets to why this is faster speed up mainly comes from reducing speed up mainly comes from reducing speed up mainly comes from reducing python overhead and GPU read wrs so let python overhead and GPU read wrs so let python overhead and GPU read wrs",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1283,
      "text": "so let me unpack that a little bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1284,
      "text": "um okay here me unpack that a little bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1285,
      "text": "um okay here me unpack that a little bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1286,
      "text": "um okay here we are okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1287,
      "text": "so we went from 300 we are",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1288,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1289,
      "text": "so we went from 300 we are",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1290,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1291,
      "text": "so we went from 300 milliseconds we're now running at 129 milliseconds we're now running at 129 milliseconds we're now running at 129 milliseconds so this is uh 300 129 about milliseconds so this is uh 300 129 about milliseconds so this is uh 300 129 about 2.3x Improvement from a single line of 2.3x Improvement from a single line of 2.3x Improvement from a single line of code in py torch uh so quite incredible code in py torch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1292,
      "text": "uh so quite incredible code in py torch uh so quite incredible",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1293,
      "text": "so what is happening what's happening so what is happening what's happening so what is happening what's happening under the hood well when you pass the under the hood well when you pass the under the hood well when you pass the model to torch model to torch model to torch compile what we have here in this NN compile what we have here in this NN compile what we have here in this NN module this is really just the module this is really just the module this is really just the algorithmic description of what we'd algorithmic description of what we'd algorithmic description of what we'd like to happen in our Network and torch like to happen in our Network and torch like to happen in our Network and torch compile will analyze the entire thing compile will analyze the entire thing compile will analyze the entire thing and it will look at what operations You' and it will look at what operations You' and it will look at what operations",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1294,
      "text": "You' like to use and with the benefit of like to use and with the benefit of like to use and with the benefit of knowing exactly what's going to happen knowing exactly what's going to happen knowing exactly what's going to happen it doesn't have to run in What's called it doesn't have to run in What's called it doesn't have to run in What's called the e mode it doesn't have to just kind the e mode it doesn't have to just kind the e mode it doesn't have to just kind of like go layer by layer like the of like go layer by layer like the of like go layer by layer like the python interpreter normally would start python interpreter normally would start python interpreter normally would start at the at the at the forward and the python interpreter will forward and the python interpreter will forward and the python interpreter will go okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1295,
      "text": "let's do this operation and then go",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1296,
      "text": "okay let's do this operation and then go",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1297,
      "text": "okay let's do this operation and then let's do that operation and it kind of let's do that operation and it kind of let's do that operation and it kind of materializes all the operations as it materializes all the operations as it materializes all the operations as it goes through uh so these um calculations goes through uh so these um calculations goes through uh so these um calculations are dispatched and run in this order and are dispatched and run in this order and are dispatched and run in this order and the python interpreter and this code the python interpreter and this code the python interpreter and this code doesn't know what kind of operations are doesn't know what kind of operations are doesn't know what kind of operations are going to happen later but torch compile going to happen later but torch compile going to happen later but torch compile sees your entire code at the same time sees your entire code at the same time sees your entire code at the same time and it's able to know what operations and it's able to know what operations and it's able to know what operations you intend to run and it will kind of you intend to run and it will kind of you intend to run and it will kind of optimize that process the first thing it optimize that process the first thing it optimize that process the first thing it will do is will it will take out the will do is will it will take out the will do is will it will take out the python interpreter from the forward pass python interpreter from the forward pass python interpreter from the forward pass entirely and it will kind of compile entirely and it will kind of compile entirely and it will kind of compile this entire neural net as a single this entire neural net as a single this entire neural net as a single object with no python interpreter object with no python interpreter object with no python interpreter involved so it knows exactly what's involved so it knows exactly what's involved so it knows exactly what's going to run and we'll just run that and going to run and we'll just run that and going to run and we'll just run that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1298,
      "text": "and it's all going to be running in it's all going to be running in it's all going to be running in efficient efficient efficient code uh the second thing that happens is code uh the second thing that happens is code uh the second thing that happens is uh this read write that they mentioned uh this read write that they mentioned uh this read write that they mentioned very briefly so a good example of that I very briefly so a good example of that I very briefly so a good example of that I think is the G nonlinearity that we've think is the G nonlinearity that we've think is the G nonlinearity that we've been looking at so here we use the n and been looking at so here we use the n and been looking at so here we use the n and G now this here is me uh basically just G now this here is me uh basically just G now this here is me uh basically just breaking up the inang Galu uh which you breaking up the inang Galu uh which you breaking up the inang Galu uh which you remember has this formula",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1299,
      "text": "so this here remember has this formula",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1300,
      "text": "so this here remember has this formula",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1301,
      "text": "so this here is the equivalent implementation to is the equivalent implementation to is the equivalent implementation to what's happening inside g algorithmic l",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1302,
      "text": "what's happening inside g algorithmic l",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1303,
      "text": "what's happening inside g algorithmic l",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1304,
      "text": "it's it's",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1305,
      "text": "it's identical Now by default if uh we just identical Now by default if uh we just identical Now by default if uh we just we using this instead of ending.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1306,
      "text": "G here we using this instead of ending.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1307,
      "text": "G here we using this instead of ending.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1308,
      "text": "G here what would happen without torch compile what would happen without torch compile what would happen without torch compile well the python interpreter would make well the python interpreter would make well the python interpreter would make its way here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1309,
      "text": "and then it would be okay its way here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1310,
      "text": "and then it would be okay its way here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1311,
      "text": "and then it would be okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1312,
      "text": "well there's an input well let me first",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1313,
      "text": "well there's an input well let me first",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1314,
      "text": "well there's an input well let me first let me raise this input to the third let me raise this input to the third let me raise this input to the third power",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1315,
      "text": "and it's going to dispatch a power and it's going to dispatch a power",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1316,
      "text": "and it's going to dispatch a kernel that takes your input and raises kernel that takes your input and raises kernel that takes your input and raises it to the third power and that kernel it to the third power and that kernel it to the third power and that kernel will run and when this kernel runs what will run and when this kernel runs what will run and when this kernel runs what ends up happening is this input is ends up happening is this input is ends up happening is this input is stored in the memory of the GPU so stored in the memory of the GPU so stored in the memory of the GPU so here's a helpful example of the layout here's a helpful example of the layout here's a helpful example of the layout of what's happening right you have your of what's happening",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1317,
      "text": "right you have your of what's happening right you have your CPU this is in every single computer CPU this is in every single computer CPU this is in every single computer there's a few cores in there",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1318,
      "text": "and you there's a few cores in there",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1319,
      "text": "and you there's a few cores in there",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1320,
      "text": "and you have your uh Ram uh your memory and the have your uh Ram uh your memory and the have your uh Ram uh your memory and the CPU can talk to the memory and this is CPU can talk to the memory and this is CPU can talk to the memory and this is all well known",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1321,
      "text": "but now we've added the all well known",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1322,
      "text": "but now we've added the all well known",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1323,
      "text": "but now we've added the GPU and the GPU is a slightly different GPU and the GPU is a slightly different GPU and the GPU is a slightly different architecture of course they can architecture of course they can architecture of course they can communicate and it's different in that communicate",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1324,
      "text": "and it's different in that communicate",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1325,
      "text": "and it's different in that it's got a lot more course than a CPU it's got a lot more course than a CPU it's got a lot more course than a CPU all of those cores are individually a all of those cores are individually a all of those cores are individually a lot simpler too but it also has memory lot simpler too but it also has memory lot simpler too but it also has memory right this high bandwidth memory",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1326,
      "text": "I'm right this high bandwidth memory",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1327,
      "text": "I'm right this high bandwidth memory",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1328,
      "text": "I'm sorry if I'm botching it hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1329,
      "text": "I don't sorry if I'm botching it hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1330,
      "text": "I don't sorry if I'm botching it hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1331,
      "text": "I don't even know what that stands for I'm just even know what that stands for I'm just even know what that stands for I'm just realizing that realizing that realizing that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1332,
      "text": "but uh this is the memory and it's very",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1333,
      "text": "but uh this is the memory and it's very",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1334,
      "text": "but uh this is the memory and it's very equivalent to uh RAM basically in the equivalent to uh RAM basically in the equivalent to uh RAM basically in the computer and what's happening is that computer and what's happening is that computer and what's happening is that input is living in the memory and when input is living in the memory and when input is living in the memory and when you do input you do input you do input cubed this has to travel to the GPU to cubed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1335,
      "text": "this has to travel to the GPU to cubed this has to travel to the GPU to the course and to all the caches and the course and to all the caches and the course and to all the caches and registers on the actual chip of this registers on the actual chip of this registers on the actual chip of this GPU and it has to calculate the all the GPU and it has to calculate the all the GPU and it has to calculate the all the elements to the third",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1336,
      "text": "and then it saves elements to the third",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1337,
      "text": "and then it saves elements to the third",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1338,
      "text": "and then it saves the result back to the memory and it's the result back to the memory and it's the result back to the memory and it's this uh travel time that actually causes this uh travel time that actually causes this uh travel time that actually causes a lot of issues so here remember this a lot of issues so here remember this a lot of issues so here remember this memory bandwidth we can communicate memory bandwidth we can communicate memory bandwidth we can communicate about 2 terabytes per second which is a about 2 terabytes per second which is a about 2 terabytes per second which is a lot",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1339,
      "text": "but also we have to Traverse this lot",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1340,
      "text": "but also we have to Traverse this lot",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1341,
      "text": "but also we have to Traverse this link and it's very slow so here on the link",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1342,
      "text": "and it's very slow so here on the link",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1343,
      "text": "and it's very slow so here on the GPU we're on chip and everything is GPU we're on chip and everything is GPU we're on chip and everything is super fast within the chip but going to super fast within the chip but going to super fast within the chip but going to the memory is extremely expensive takes the memory is extremely expensive takes the memory is extremely expensive takes extremely long amount of time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1344,
      "text": "and so we extremely long amount of time and so we extremely long amount of time and so we load the input do the calculations and load the input do the calculations and load the input do the calculations and load back the output and this round trip load back the output and this round trip load back the output and this round trip takes a lot of time takes a lot of time takes a lot of time and now right after we do that we and now right after we do that we and now right after we do that we multiply by this constant",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1345,
      "text": "so what multiply by this constant so what multiply by this constant so what happens then is we dispatch another happens then is we dispatch another happens then is we dispatch another kernel and then the result travels back kernel and then the result travels back kernel and then the result travels back all the elements get multiplied by a all the elements get multiplied by a all the elements get multiplied by a constant and then the results travel constant and then the results travel constant and then the results travel back to the memory and then we take the back to the memory and then we take the back to the memory and then we take the result and we add back input and so this result and we add back input and so this result and we add back input and so this entire thing again travels to the GPU entire thing again travels to the GPU entire thing again travels to the GPU adds the inputs and gets written back so adds the inputs and gets written back so adds the inputs and gets written back so we're making all these round trips from we're making all these round trips from we're making all these round trips from the memory to actually where the comput the memory to actually where the comput the memory to actually where the comput happens because all the tensor cores and happens because all the tensor cores and happens because all the tensor cores and alus and everything like that is all alus and everything like that is all alus and everything like that is all stored on the chip in the GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1346,
      "text": "so we're stored on the chip in the GPU so we're stored on the chip in the GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1347,
      "text": "so we're doing a ton of round trips and pytorch doing a ton of round trips and pytorch doing a ton of round trips and pytorch uh without using torch compile doesn't uh without using torch compile doesn't uh without using torch compile doesn't know to optimize this because it doesn't know to optimize this because it doesn't know to optimize this because it doesn't know what kind of operations you're know what kind of operations you're know what kind of operations you're running later you're just telling it running later you're just telling it running later you're just telling it raise the power to the third then do raise the power to the third then do raise the power to the third then do this then do that and it will just do this then do that and it will just do this then do that and it will just do that in that sequence but torch compile that in that sequence but torch compile that in that sequence but torch compile sees your entire code it will come here sees your entire code it will come here sees your entire code it will come here and it will realize wait all of these and it will realize wait all of these and it will realize wait all of these are elementwise operations and actually are elementwise operations and actually are elementwise operations and actually what I'm going to do is I'm going to do what I'm going to do is I'm going to do what I'm going to do is I'm going to do a single trip of input to the GPU then a single trip of input to the GPU then a single trip of input to the GPU then for every single element I'm going to do for every single element I'm going to do for every single element I'm going to do all of these operations while that all of these operations while that all of these operations while that memory is on the GPU or chunks of it memory is on the GPU or chunks of it memory is on the GPU or chunks of it rather and then I'm going to write back rather and then I'm going to write back rather and then I'm going to write back a single time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1348,
      "text": "so we're not going to have a single time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1349,
      "text": "so we're not going to have a single time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1350,
      "text": "so we're not going to have these round trips and that's one example these round trips",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1351,
      "text": "and that's one example these round trips",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1352,
      "text": "and that's one example of what's called kernel fusion and is a of what's called kernel fusion and is a of what's called kernel fusion and is a major way in which everything is sped up major way in which everything is sped up major way in which everything is sped up so basically if you have your benefit of so basically if you have your benefit of so basically if you have your benefit of onet and you know exactly what you're onet",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1353,
      "text": "and you know exactly what you're onet",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1354,
      "text": "and you know exactly what you're going to compute you can optimize your going to compute you can optimize your going to compute you can optimize your round trips to the memory and you're not round trips to the memory and you're not round trips to the memory and you're not going to pay the the memory bandwidth going to pay the the memory bandwidth going to pay the the memory bandwidth cost and that's fundamentally what makes cost and that's fundamentally what makes cost and that's fundamentally what makes some of these operations a lot faster some of these operations a lot faster some of these operations a lot faster and what they mean by read writes and what they mean by read writes and what they mean by read writes here so let me erase this because we are here so let me erase this because we are here so let me erase this because we are not using it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1355,
      "text": "and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1356,
      "text": "yeah we should be using not using it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1357,
      "text": "and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1358,
      "text": "yeah we should be using not using it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1359,
      "text": "and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1360,
      "text": "yeah we should be using torch compile and our code is now torch compile and our code is now torch compile and our code is now significantly faster and we're doing significantly faster and we're doing significantly faster and we're doing about about about 125,000 tokens per second",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1361,
      "text": "but we still 125,000 tokens per second",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1362,
      "text": "but we still 125,000 tokens per second",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1363,
      "text": "but we still have a long way to go before we move on have a long way to go before we move on have a long way to go before we move on I wanted to supplement the discussion a I wanted to supplement the discussion a I wanted to supplement the discussion a little bit with a few more figures uh little bit with a few more figures uh little bit with a few more figures uh because this is a complic topic",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1364,
      "text": "but it's because this is a complic topic",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1365,
      "text": "but it's because this is a complic topic",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1366,
      "text": "but it's worth understanding on a high level uh worth understanding on a high level uh worth understanding on a high level uh what's happening here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1367,
      "text": "and I could what's happening here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1368,
      "text": "and I could what's happening here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1369,
      "text": "and I could probably spend an entire video of like probably spend an entire video of like probably spend an entire video of like two hours on this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1370,
      "text": "but just the preview two hours on this but just the preview two hours on this but just the preview of that basically so this chip here that of that basically so this chip here that of that basically so this chip here that is uh the GPU this chip is where all the is uh the GPU this chip is where all the is uh the GPU this chip is where all the calculations happen mostly but this chip calculations happen mostly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1371,
      "text": "but this chip calculations happen mostly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1372,
      "text": "but this chip also does have some memory in it but also does have some memory in it but also does have some memory in it but",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1373,
      "text": "most of the memory by far is here in the most of the memory by far is here in the most of the memory by far is here in the high bandwidth memory hbm and is high bandwidth memory hbm and is high bandwidth memory hbm and is connected they're connected um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1374,
      "text": "but these connected they're connected um but these connected they're connected um but these are two separate chips basically are two separate chips basically are two separate chips basically now here this is a zoom in of kind of now here this is a zoom in of kind of now here this is a zoom in of kind of this cartoon diagram of a GPU and what this cartoon diagram of a GPU and what this cartoon diagram of a GPU and what we're seeing here is number one you see we're seeing here is number one you see we're seeing here is number one you see this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1375,
      "text": "hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1376,
      "text": "I I realize it's probably very this hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1377,
      "text": "I I realize it's probably very this hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1378,
      "text": "I I realize it's probably very small for you but on the sides here it small for you but on the sides here it small for you but on the sides here it says hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1379,
      "text": "and so that that's the links to says hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1380,
      "text": "and so that that's the links to says hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1381,
      "text": "and so that that's the links to the hbm now the hbm is again off chip on the hbm now the hbm is again off chip on the hbm now the hbm is again off chip on the chip there are a large number of the chip there are a large number of the chip there are a large number of these streaming these streaming these streaming multiprocessors uh every one of these is multiprocessors uh every one of these is multiprocessors uh every one of these is an SM there's 120 of them in total and an SM there's 120 of them in total and an SM there's 120 of them in total and this is where the a lot of the this is where the a lot of the this is where the a lot of the calculations happen and this is a zoom calculations happen and this is a zoom calculations happen and this is a zoom in of a single individual as it has in of a single individual as it has in of a single individual as it has these four quadrants and see for example these four quadrants and see for example these four quadrants and see for example tensor core this is where a lot of the tensor core this is where a lot of the tensor core this is where a lot of the Matrix multiply stuff happens but Matrix multiply stuff happens but Matrix multiply stuff happens but there's all these other units to do all there's all these other units to do all there's all these other units to do all different kinds of calculations for fp64 different kinds of calculations for fp64 different kinds of calculations for fp64 fp32 and for integers and so on now so fp32 and for integers and so on now so fp32 and for integers and so on now so we have all this uh logic here to do the we have all this uh logic here to do the we have all this uh logic here to do the calculations but in addition to that on calculations but in addition to that on calculations but in addition to that on the chip there is memory sprinkled the chip there is memory sprinkled the chip there is memory sprinkled throughout the chip so L2 cache is some throughout the chip so L2 cache is some throughout the chip so L2 cache is some amount of memory that lives on the chip amount of memory that lives on the chip amount of memory that lives on the chip and then on the SMS themselves there's and then on the SMS themselves there's and then on the SMS themselves there's L1 cache I realized it's probably very L1 cache I realized it's probably very L1 cache I realized it's probably very small for you but this blue bar is L1 small for you but this blue bar is L1 small for you but this blue bar is L1 and there's also registers um and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1382,
      "text": "so and there's also registers um and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1383,
      "text": "so and there's also registers um and so there is memory stored here but the way there is memory stored here but the way there is memory stored here but the way this memory is stored is very different this memory is stored is very different this memory is stored is very different from the way memory is stored in hbm uh from the way memory is stored in hbm uh from the way memory is stored in hbm uh this is a very different implementation this is a very different implementation this is a very different implementation uh using um just in terms of like what uh using um just in terms of like what uh using um just in terms of like what the Silicon looks like it's a very the Silicon looks like it's a very the Silicon looks like it's a very different different different implementation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1384,
      "text": "um so here you would implementation um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1385,
      "text": "so here you would implementation um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1386,
      "text": "so here you would using transistors and capacitors and using transistors and capacitors and using transistors and capacitors and here it's a very different here it's a very different here it's a very different implementation uh with SRAM and what implementation uh with SRAM and what implementation uh with SRAM and what that looks like but long story short is that looks like but long story short is that looks like but long story short is um there is um memory inside the chip um there is um memory inside the chip um there is um memory inside the chip but it's not a lot of memory that's the",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1387,
      "text": "but it's not a lot of memory that's the",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1388,
      "text": "but it's not a lot of memory that's the critical point",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1389,
      "text": "so this is some C this is critical point so this is some C this is critical point",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1390,
      "text": "so this is some C this is a example diagram of a slightly a example diagram of a slightly a example diagram of a slightly different GPU just like here where it different GPU just like here where it different GPU just like here where it shows that for example typical numbers shows that for example typical numbers shows that for example typical numbers for CPU Dam memory which is this thing for CPU Dam memory which is this thing for CPU Dam memory which is this thing here you might have one tab of this here you might have one tab of this here you might have one tab of this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1391,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1392,
      "text": "but it would be extremely right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1393,
      "text": "but it would be extremely right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1394,
      "text": "but it would be extremely expensive to access especially for a GPU expensive to access especially for a GPU expensive to access especially for a GPU you have to go through the CPU here now you have to go through the CPU here now you have to go through the CPU here now next we have the hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1395,
      "text": "so we have tens of next we have the hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1396,
      "text": "so we have tens of next we have the hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1397,
      "text": "so we have tens of gigabytes of hbm memory on a typical GPU gigabytes of hbm memory on a typical GPU gigabytes of hbm memory on a typical GPU here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1398,
      "text": "but it's as I mentioned very here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1399,
      "text": "but it's as I mentioned very here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1400,
      "text": "but it's as I mentioned very expensive to access and then on the chip expensive to access and then on the chip expensive to access and then on the chip itself everything is extremely fast itself everything is extremely fast itself everything is extremely fast within the chip but we only have couple within the chip but we only have couple within the chip but we only have couple 10 megabytes of memory collectively 10 megabytes of memory collectively 10 megabytes of memory collectively throughout the Chip",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1401,
      "text": "And so there's just throughout the Chip",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1402,
      "text": "And so there's just throughout the Chip",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1403,
      "text": "And so there's just not enough space because the memory is not enough space because the memory is not enough space because the memory is very expensive on the chip and so very expensive on the chip and so very expensive on the chip and so there's not a lot of it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1404,
      "text": "but it is there's not a lot of it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1405,
      "text": "but it is there's not a lot of it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1406,
      "text": "but it is lightning fast to access in relative lightning fast to access in relative lightning fast to access in relative terms",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1407,
      "text": "and so basically whenever we have terms and so basically whenever we have terms and so basically whenever we have these kernels um the more accurate these kernels um the more accurate these kernels um the more accurate picture of what's Happening Here is that picture of what's Happening Here is that picture of what's Happening Here is that we take these inputs which live by we take these inputs which live by we take these inputs which live by default on the global memory and now we default on the global memory and now we default on the global memory and now we need to perform some calculation so we need to perform some calculation so we need to perform some calculation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1408,
      "text": "so we start streaming the data from the um start streaming the data from the um start streaming the data from the um Global memory to the uh chip we perform Global memory to the uh chip we perform Global memory to the uh chip we perform the calculations on the chip and then the calculations on the chip and then the calculations on the chip and then stream it back and store it back to the stream it back and store it back to the stream it back and store it back to the global memory",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1409,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1410,
      "text": "and so if we are if global memory",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1411,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1412,
      "text": "and so if we are if global memory",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1413,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1414,
      "text": "and so if we are if we don't have torch compile we are we don't have torch compile we are",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1415,
      "text": "we don't have torch compile we are streaming the data through the chip streaming the data through the chip streaming the data through the chip doing the calculations and saving to the doing the calculations and saving to the doing the calculations and saving to the memory and we're doing those round trips memory and we're doing those round trips memory and we're doing those round trips many many many many many many times but uh if it's torch compiled then times but uh if it's torch compiled then times but uh if it's torch compiled then we start streaming the memory as before we start streaming the memory as before we start streaming the memory as before but then while we're on the chip we're but then while we're on the chip we're but then while we're on the chip we're we're we have a chunk of the uh data we're we have a chunk of the uh data we're we have a chunk of the uh data that we're trying to process so that that we're trying to process so that that we're trying to process so that chunk now lives on the chip while it's chunk now lives on the chip while it's chunk now lives on the chip while it's on the chip it's extremely fast to on the chip it's extremely fast to on the chip it's extremely fast to operate on so if we have kernel Fusion operate on so if we have kernel Fusion operate on so if we have kernel Fusion we can do all the operations right there we can do all the operations right there we can do all the operations right there in an element-wise fashion and those are in an element-wise fashion and those are in an element-wise fashion and those are very cheap",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1416,
      "text": "and then we do a single round very cheap",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1417,
      "text": "and then we do a single round very cheap",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1418,
      "text": "and then we do a single round trip back to the global memory so trip back to the global memory so trip back to the global memory so operator Fusion basically allows you to operator Fusion basically allows you to operator Fusion basically allows you to keep your chunk of data on the Chip And keep your chunk of data on the Chip And keep your chunk of data on the Chip And do lots of calculations on it before you do lots of calculations on it before you do lots of calculations on it before you write it back and that gives huge write it back",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1419,
      "text": "and that gives huge write it back",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1420,
      "text": "and that gives huge savings and that's why torch compile savings",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1421,
      "text": "and that's why torch compile savings",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1422,
      "text": "and that's why torch compile ends up being a lot faster or that's one ends up being a lot faster or that's one ends up being a lot faster or that's one of the major of the major of the major reasons uh so again just a very brief reasons uh so again just a very brief reasons uh so again just a very brief intro to the memory hierarchy and intro to the memory hierarchy and intro to the memory hierarchy and roughly what torch compile does for you roughly what torch compile does for you roughly what torch compile does for you now torch compile is amazing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1423,
      "text": "but there now torch compile is amazing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1424,
      "text": "but there now torch compile is amazing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1425,
      "text": "but there are operations torch compile will not are operations torch compile will not are operations torch compile will not find and an amazing example of that is find and an amazing example of that is find and an amazing example of that is Flash attention to which we turn next so Flash attention to which we turn next so Flash attention to which we turn next so flash attention comes from this paper flash attention comes from this paper flash attention comes from this paper from uh Stanford in from uh Stanford in from uh Stanford in 2022 and it's this incredible algorithm 2022",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1426,
      "text": "and it's this incredible algorithm 2022",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1427,
      "text": "and it's this incredible algorithm for performing attention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1428,
      "text": "so um and for performing attention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1429,
      "text": "so um and for performing attention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1430,
      "text": "so um and running it a lot faster so flash running it a lot faster so flash running it a lot faster so flash attention will come here and we will attention will come here and we will attention will come here and we will take out these four take out these four take out these four lines and Flash attention implements lines and Flash attention implements lines and Flash attention implements these four lines really really quickly these four lines really really quickly these four lines really really quickly and how does it do that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1431,
      "text": "well flash and how does it do that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1432,
      "text": "well flash and how does it do that well flash attention is a kernel Fusion operation attention is a kernel Fusion operation attention is a kernel Fusion operation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1433,
      "text": "so you see here we have um in this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1434,
      "text": "so you see here we have um in this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1435,
      "text": "so you see here we have um in this diagram they're showing P torch and you diagram they're showing P torch and you diagram they're showing P torch and you have these four operations uh they're have these four operations uh they're have these four operations uh they're including Dropout",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1436,
      "text": "but we are not using including Dropout but we are not using including Dropout but we are not using Dropout here so we just have these four Dropout here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1437,
      "text": "so we just have these four Dropout here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1438,
      "text": "so we just have these four lines of code here and instead of those lines of code here and instead of those lines of code here and instead of those we are fusing them into a single fused we are fusing them into a single fused we are fusing them into a single fused kernel of flash attention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1439,
      "text": "so it's an kernel of flash attention so it's an kernel of flash attention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1440,
      "text": "so it's an it's a it's a kernel Fusion algorithm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1441,
      "text": "it's a it's a kernel Fusion algorithm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1442,
      "text": "it's a it's a kernel Fusion algorithm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1443,
      "text": "but it's a kernel Fusion that torch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1444,
      "text": "but it's a kernel Fusion that torch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1445,
      "text": "but it's a kernel Fusion that torch compile cannot find compile cannot find compile cannot find and the reason that it cannot find it is and the reason that it cannot find it is and the reason that it cannot find it is that it um requires an algorithmic that it um requires an algorithmic that it um requires an algorithmic rewrite of how attention is actually rewrite of how attention is actually rewrite of how attention is actually implemented here in this case and what's implemented here in this case and what's implemented here in this case and what's remarkable about it is that uh flash remarkable about it is that uh flash remarkable about it is that uh flash attention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1446,
      "text": "actually if you just count the attention actually if you just count the attention actually if you just count the number of flops flash attention does number of flops flash attention does number of flops flash attention does more flops than this attention here but more flops than this attention here but more flops than this attention here but flash attention is actually flash attention is actually flash attention is actually significantly faster in fact they site significantly faster in fact they site significantly faster in fact they site 7. six times faster potentially and 7. six times faster potentially and 7. six times faster potentially and that's because it is very mindful of the that's because it is very mindful of the that's because it is very mindful of the memory hierarchy as I described it just memory hierarchy as I described it just memory hierarchy as I described it just now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1447,
      "text": "and so it's very mindful about now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1448,
      "text": "and so it's very mindful about now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1449,
      "text": "and so it's very mindful about what's in high bandwidth memory what's what's in high bandwidth memory what's what's in high bandwidth memory what's in the shared memory and it is very in the shared memory and it is very in the shared memory and it is very careful with how it orchestrates the careful with how it orchestrates the careful with how it orchestrates the computation such that we have fewer computation such that we have fewer computation such that we have fewer reads and writes to the high bandwidth reads and writes to the high bandwidth reads and writes to the high bandwidth memory and so even though we're doing memory and so even though we're doing memory and so even though we're doing more flops the expensive part is they more flops the expensive part is they more flops the expensive part is they load and store into hbm and that's what load and store into hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1450,
      "text": "and that's what load and store into hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1451,
      "text": "and that's what they avoid and so in particular they do they avoid and so in particular they do they avoid and so in particular they do not ever materialize this end byend not ever materialize this end byend not ever materialize this end byend attention Matrix this ATT here a flash attention Matrix this ATT here a flash attention Matrix this ATT here a flash attention is designed such that this attention is designed such that this attention is designed such that this Matrix never gets materialized at any Matrix never gets materialized at any Matrix never gets materialized at any point and it never gets read or written point and it never gets read or written point and it never gets read or written to the hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1452,
      "text": "and this is a very large to the hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1453,
      "text": "and this is a very large to the hbm",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1454,
      "text": "and this is a very large Matrix",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1455,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1456,
      "text": "so um because this is where Matrix right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1457,
      "text": "so um because this is where Matrix right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1458,
      "text": "so um because this is where all the queries and keys interact and all the queries and keys interact and all the queries and keys interact and we're sort of getting we're sort of getting we're sort of getting um for each head for each batch element um for each head for each batch element um for each head for each batch element we're getting a t BYT Matrix of we're getting a t BYT Matrix of we're getting a t BYT Matrix of attention which is a Million numbers attention which is a Million numbers attention which is a Million numbers even for a single head at a single batch even for a single head at a single batch even for a single head at a single batch index at like so so basically this is a index at like so so basically this is a index at like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1459,
      "text": "so so basically this is a ton of memory and and this is never ton of memory and and this is never ton of memory and and this is never materialized and the way that this is materialized and the way that this is materialized and the way that this is achieved is that basically the achieved is that basically the achieved is that basically the fundamental algorithmic rewrite here fundamental algorithmic rewrite here fundamental algorithmic rewrite here relies on this online softmax trick relies on this online softmax trick relies on this online softmax trick which was proposed previously and I'll which was proposed previously and I'll which was proposed previously and I'll show you the paper in a bit and the show you the paper in a bit and the show you the paper in a bit and the online softmax trick coming from a online softmax trick coming from a online softmax trick coming from a previous paper um shows how you can previous paper um shows how you can previous paper um shows how you can incrementally evaluate a soft Max incrementally evaluate a soft Max incrementally evaluate a soft Max without having to sort of realize all of without having to sort of realize all of without having to sort of realize all of the inputs to the softmax to do the the inputs to the softmax to do the the inputs to the softmax to do the normalization and you do that by having normalization and you do that by having normalization and you do that by having these intermediate variables M and L and these intermediate variables M and L and these intermediate variables M and L and there's an update to them that allows there's an update to them that allows there's an update to them that allows you to evaluate the softmax in an online you to evaluate the softmax in an online you to evaluate the softmax in an online manner um now flash attention actually manner um now flash attention actually manner um now flash attention actually so recently flash attention 2 came out so recently flash attention 2 came out so recently flash attention 2 came out as well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1460,
      "text": "so I have that paper up here as as well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1461,
      "text": "so I have that paper up here as as well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1462,
      "text": "so I have that paper up here as well uh that has additional gains to how well uh that has additional gains to how well uh that has additional gains to how it calculates flash attention and the it calculates flash attention and the it calculates flash attention and the original paper that this is based on original paper that this is based on original paper that this is based on basically is this online normalizer basically is this online normalizer basically is this online normalizer calculation for softmax and remarkably calculation for softmax and remarkably calculation for softmax and remarkably it came out of Nvidia and it came out of it came out of Nvidia",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1463,
      "text": "and it came out of it came out of Nvidia",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1464,
      "text": "and it came out of it like really early 2018 so this is 4 it like really",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1465,
      "text": "early 2018 so this is 4 it like really early 2018 so this is 4 years before flash attention years before flash attention years before flash attention and this paper says that we propose a and this paper says that we propose a and this paper says that we propose a way to compute the classical softmax way to compute the classical softmax way to compute the classical softmax with fewer memory accesses and with fewer memory accesses and with fewer memory accesses and hypothesize that this reduction in hypothesize that this reduction in hypothesize that this reduction in memory accesses should improve softmax memory accesses should improve softmax memory accesses should improve softmax performance on actual hardware and so performance on actual hardware and so performance on actual hardware and so they are extremely correct in this they are extremely correct in this they are extremely correct in this hypothesis but it's really fascinating hypothesis",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1466,
      "text": "but it's really fascinating hypothesis",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1467,
      "text": "but it's really fascinating to me that they're from Nvidia and that to me that they're from Nvidia and that to me that they're from Nvidia and that they had this realization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1468,
      "text": "but they they had this realization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1469,
      "text": "but they they had this realization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1470,
      "text": "but they didn't actually take it to the actual didn't actually take it to the actual didn't actually take it to the actual flash attention that had to come four flash attention that had to come four flash attention that had to come four years later from Stanford",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1471,
      "text": "so I don't years later from Stanford",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1472,
      "text": "so I don't years later from Stanford",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1473,
      "text": "so I don't fully understand the historical how this fully understand the historical how this fully understand the historical how this happened historically",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1474,
      "text": "um but they do happened historically um but they do happened historically um but they do basically propose this online update to basically propose this online update to basically propose this online update to the softmax uh right here and this is the softmax uh right here and this is the softmax uh right here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1475,
      "text": "and this is fundamentally what they reuse here to fundamentally what they reuse here to fundamentally what they reuse here to calculate the softmax in a streaming calculate the softmax in a streaming calculate the softmax in a streaming Manner",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1476,
      "text": "and then they realize they can Manner",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1477,
      "text": "and then they realize they can Manner",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1478,
      "text": "and then they realize they can actually fuse all the other operations actually fuse all the other operations actually fuse all the other operations with the online sofx calculation into a with the online sofx calculation into a with the online sofx calculation into a single fused kernel flash attention and single fused kernel flash attention and single fused kernel flash attention and that's what we are about to use so great",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1479,
      "text": "that's what we are about to use so great",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1480,
      "text": "that's what we are about to use so great example I think of being aware of um example I think of being aware of um example I think of being aware of um memory hierarchy the fact that flops memory hierarchy the fact that flops memory hierarchy the fact that flops don't matter uh the entire memory access don't matter uh the entire memory access don't matter uh the entire memory access pattern matters and that torch compile pattern matters and that torch compile pattern matters and that torch compile is amazing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1481,
      "text": "but there are many is amazing but there are many is amazing but there are many optimizations that are still available optimizations that are still available optimizations that are still available to us that potentially torch compile to us that potentially torch compile to us that potentially torch compile cannot find maybe maybe one day it could cannot find maybe maybe one day it could cannot find maybe maybe one day it could",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1482,
      "text": "but right now it seems like a lot to ask but right now it seems like a lot to ask but right now it seems like a lot to ask so here's what we're going to do we're so here's what we're going to do we're so here's what we're going to do we're going to use Flash attention and the way going to use Flash attention and the way going to use Flash attention and the way to do that basically in pytorch is we to do that basically in pytorch is we to do that basically in pytorch is we are going to comment out these four are going to comment out these four are going to comment out these four lines",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1483,
      "text": "and we're going to replace them lines",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1484,
      "text": "and we're going to replace them lines",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1485,
      "text": "and we're going to replace them with a single line",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1486,
      "text": "and here we are with a single line",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1487,
      "text": "and here we are with a single line",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1488,
      "text": "and here we are calling this compound operation in calling this compound operation in calling this compound operation in pytorch called scale that product pytorch called scale that product pytorch called scale that product attention and uh pytorch will call flash attention and uh pytorch will call flash attention and uh pytorch will call flash attention when you use it in this way attention when you use it in this way attention when you use it in this way I'm not actually 100% sure why torch I'm not actually 100% sure why torch I'm not actually 100% sure why torch compile doesn't realize that these four compile doesn't realize that these four compile doesn't realize that these four lines should just call flash attention lines should just call flash attention lines should just call flash attention in this exact way we have to do it again in this exact way we have to do it again in this exact way we have to do it again for it which in my opinion is a little for it which in my opinion is a little for it which in my opinion is a little bit odd but um here we are so you have bit odd",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1489,
      "text": "but um here we are so you have bit odd",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1490,
      "text": "but um here we are so you have to use this compound up and uh let's to use this compound up and uh let's to use this compound up and uh let's wait for a few moments before torch comp wait for a few moments before torch comp wait for a few moments before torch comp compile gets around to it and then let's compile gets around to it and then let's compile gets around to it and then let's remember that we achieved 6.05 661 I remember that we achieved 6.05 661 I remember that we achieved 6.05 661 I have it here that's the loss we were have it here that's the loss we were have it here that's the loss we were expecting to see and we took 130 expecting to see and we took 130 expecting to see and we took 130 milliseconds uh before this change so milliseconds uh before this change so milliseconds uh before this change so we're expecting to see the exact same we're expecting to see the exact same we're expecting to see the exact same result by iteration 49",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1491,
      "text": "but we expect to result by iteration 49",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1492,
      "text": "but we expect to result by iteration 49",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1493,
      "text": "but we expect to see faster runtime because Flash see faster runtime because Flash see faster runtime because Flash attention is just a an algorithmic attention is just a an algorithmic attention is just a an algorithmic rewrite and it's a faster kernel but it rewrite and it's a faster kernel but it rewrite and it's a faster kernel but it doesn't actually change any of the doesn't actually change any of the doesn't actually change any of the computation and we should have the exact computation and we should have the exact computation and we should have the exact same optimization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1494,
      "text": "so okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1495,
      "text": "so we're a lot same optimization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1496,
      "text": "so okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1497,
      "text": "so we're a lot same optimization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1498,
      "text": "so okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1499,
      "text": "so we're a lot faster we're at about 95 milliseconds faster we're at about 95 milliseconds faster we're at about 95 milliseconds and we achiev and we achiev and we achiev 6.58",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1500,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1501,
      "text": "so they're basically identical 6.58",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1502,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1503,
      "text": "so they're basically identical 6.58",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1504,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1505,
      "text": "so they're basically identical up to a floating Point fudge Factor",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1506,
      "text": "so up to a floating Point fudge Factor so up to a floating Point fudge Factor",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1507,
      "text": "so it's the identical computation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1508,
      "text": "but it's it's the identical computation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1509,
      "text": "but it's it's the identical computation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1510,
      "text": "but it's significantly faster going from 130 to significantly faster going from 130 to significantly faster going from 130 to roughly 90 roughly 90 roughly 90 96",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1511,
      "text": "and so this is um 96 divide 96 and so this is um 96 divide 96",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1512,
      "text": "and so this is um 96 divide 130ish so this is maybe 27 is% 130ish",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1513,
      "text": "so this is maybe 27 is% 130ish",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1514,
      "text": "so this is maybe 27 is% Improvement",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1515,
      "text": "um so uh really interesting Improvement",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1516,
      "text": "um so uh really interesting Improvement um so uh really interesting and that is Flash retention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1517,
      "text": "okay we are and that is Flash retention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1518,
      "text": "okay we are and that is Flash retention",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1519,
      "text": "okay we are now getting to one of my favorite now getting to one of my favorite now getting to one of my favorite optimizations",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1520,
      "text": "and it is simultaneously optimizations and it is simultaneously optimizations",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1521,
      "text": "and it is simultaneously the dumbest and the most brilliant the dumbest and the most brilliant the dumbest and the most brilliant optimization and it's always a little optimization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1522,
      "text": "and it's always a little optimization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1523,
      "text": "and it's always a little bit surprising to me um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1524,
      "text": "anyway so bit surprising to me um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1525,
      "text": "anyway so bit surprising to me um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1526,
      "text": "anyway",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1527,
      "text": "so basically I mentioned a few minutes ago basically I mentioned a few minutes ago basically I mentioned a few minutes ago that there are some numbers that are that there are some numbers that are that there are some numbers that are nice and some numbers that are ugly so nice and some numbers that are ugly so nice and some numbers that are ugly so 64 is a beautiful nice number 128 is 64 is a beautiful nice number 128 is 64 is a beautiful nice number 128 is even nicer 256 is beautiful what makes even nicer 256 is beautiful what makes even nicer 256 is beautiful what makes these numbers beautiful is that there these numbers beautiful is that there these numbers beautiful is that there are many powers of two inside them you are many powers of two inside them you are many powers of two inside them you can divide by two many times and uh can divide by two many times and uh can divide by two many times and uh examples of ugly numbers are like 13 and examples of ugly numbers are like 13 and examples of ugly numbers are like 13 and 17 and something like that prime numbers 17 and something like that prime numbers 17 and something like that prime numbers numbers that are not even and so on and numbers that are not even and so on and numbers that are not even and so on and so pretty much you always want to use so pretty much you always want to use so pretty much you always want to use nice numbers in all of your code that nice numbers in all of your code that nice numbers in all of your code that deals with neural networks or Cuda deals with neural networks or Cuda deals with neural networks or Cuda because everything in Cuda Works in sort because everything in Cuda Works in sort because everything in Cuda Works in sort of like powers of two and lots of of like powers of two and lots of of like powers of two and lots of kernels are written in terms of powers kernels are written in terms of powers kernels are written in terms of powers of Two",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1528,
      "text": "And there are lots of blocks of of Two",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1529,
      "text": "And there are lots of blocks of of Two",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1530,
      "text": "And there are lots of blocks of sizes 16 and uh 64 and so on so sizes 16 and uh 64 and so on so sizes 16 and uh 64 and so on so everything is written in those terms and everything is written in those terms and everything is written in those terms and you always have special case handling you always have special case handling you always have special case handling for all kinds of uh logic that U when for all kinds of uh logic that U when for all kinds of uh logic that U when your inputs are not made of nice numbers your inputs are not made of nice numbers your inputs are not made of nice numbers",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1531,
      "text": "so let's see what that looks like so let's see what that looks like so let's see what that looks like basically scan your code and look for basically scan your code and look for basically scan your code and look for ugly numbers is roughly theistic so ugly numbers is roughly theistic so ugly numbers is roughly theistic so three times is kind of ugly um I'm not three times is kind of ugly um I'm not three times is kind of ugly um I'm not 100% sure maybe this can be improved but",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1532,
      "text": "100% sure maybe this can be improved but 100% sure maybe this can be improved but this is uh this is ugly and not this is uh this is ugly and not this is uh this is ugly and not ideal um four times is nice",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1533,
      "text": "so that's uh ideal um four times is nice",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1534,
      "text": "so that's uh ideal um four times is nice",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1535,
      "text": "so that's uh that's nice",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1536,
      "text": "that's nice",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1537,
      "text": "that's nice 1024 is very nice that's a power of two 1024 is very nice that's a power of two 1024 is very nice that's a power of two 12 is a little bit suspicious um not too 12 is a little bit suspicious um not too 12 is a little bit suspicious um not too many powers of two 768 is great 50, 257 many powers of two 768 is great 50, 257 many powers of two 768 is great 50, 257 is a really really ugly number um it's is a really really ugly number um it's is a really really ugly number um it's first of all it's odd",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1538,
      "text": "so uh and there's first of all it's odd",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1539,
      "text": "so uh and there's first of all it's odd",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1540,
      "text": "so uh and there's no not too many powers of two in there",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1541,
      "text": "no not too many powers of two in there no not too many powers of two in there",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1542,
      "text": "so this is a very ugly number and it's so this is a very ugly number",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1543,
      "text": "and it's so this is a very ugly number",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1544,
      "text": "and it's highly suspicious",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1545,
      "text": "and then when we highly suspicious",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1546,
      "text": "and then when we highly suspicious and then when we scroll down all these numbers are nice scroll down all these numbers are nice scroll down all these numbers are nice",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1547,
      "text": "and then here we have mostly nice",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1548,
      "text": "and then here we have mostly nice",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1549,
      "text": "and then here we have mostly nice numbers except for 25 so in this numbers except for 25 so in this numbers except for 25 so in this configuration of gpt2 XL a number of configuration of gpt2 XL a number of configuration of gpt2 XL a number of heads is 25 uh that's a really ugly heads is 25 uh that's a really ugly heads is 25 uh that's a really ugly number that's an odd number and um number that's an odd number and um number that's an odd number and um actually this did cause a lot of actually this did cause a lot of actually this did cause a lot of headaches for us recently when we're headaches for us recently when we're headaches for us recently when we're trying to optimize some kernels uh to trying to optimize some kernels uh to trying to optimize some kernels uh to run this fast um and required a bunch of run this fast um and required a bunch of run this fast um and required a bunch of special case handling so basically these special case handling so basically these special case handling so basically these numbers are we have some ugly numbers numbers are we have some ugly numbers numbers are we have some ugly numbers and some of them are easier to fix than and some of them are easier to fix than and some of them are easier to fix than others and in particular the voap size others and in particular the voap size others and in particular the voap size being 50257 that's a very ugly number being 50257 that's a very ugly number being 50257 that's a very ugly number very suspicious",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1550,
      "text": "and we want to fix it very suspicious and we want to fix it very suspicious",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1551,
      "text": "and we want to fix it now when you when you fix these things now when you when you fix these things now when you when you fix these things uh one of the easy ways to do that is uh one of the easy ways to do that is uh one of the easy ways to do that is you basically um increase the number you basically um increase the number you basically um increase the number until it's the nearest power of two that until it's the nearest power of two that until it's the nearest power of two that you like so here's a much nicer number",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1552,
      "text": "you like so here's a much nicer number",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1553,
      "text": "you like so here's a much nicer number it's it's 50304 and why is that because 50304 can 50304 and why is that because 50304 can 50304 and why is that because 50304 can be divided by 8 or by 16 or by 32 be divided by 8 or by 16 or by 32 be divided by 8 or by 16 or by 32 64 it can even be divided by 128 I think 64 it can even be divided by 128 I think 64 it can even be divided by 128 I think",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1554,
      "text": "yeah",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1555,
      "text": "so it's a very nice number um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1556,
      "text": "so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1557,
      "text": "yeah",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1558,
      "text": "so it's a very nice number um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1559,
      "text": "so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1560,
      "text": "yeah",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1561,
      "text": "so it's a very nice number um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1562,
      "text": "so what we're going to do here is the GPT what we're going to do here is the GPT what we're going to do here is the GPT config and you see that we initialized B config",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1563,
      "text": "and you see that we initialized B config",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1564,
      "text": "and you see that we initialized B cap size to cap size to cap size to 50257 Let's override just 50257 Let's override just 50257 Let's override just that um element to be 50304 okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1565,
      "text": "so everything else stays",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1566,
      "text": "the 50304 okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1567,
      "text": "so everything else stays the same we're just increasing our same we're just increasing our same we're just increasing our vocabulary size so we're adding it's vocabulary size so we're adding it's vocabulary size so we're adding it's almost like we're adding fake tokens uh almost like we're adding fake tokens uh almost like we're adding fake tokens uh so that book up size has powers of two so that book up size has powers of two so that book up size has powers of two inside it now actually what I'm doing inside it now actually what I'm doing inside it now actually what I'm doing here by the way is I'm increasing the here by the way is I'm increasing the here by the way is I'm increasing the amount of computation that our network amount of computation that our network amount of computation that our network will be doing if you just count the the will be doing if you just count the the will be doing if you just count the the flops on like do the math of how many flops on like do the math of how many flops on like do the math of how many flops we're doing we're going to be flops we're doing we're going to be flops we're doing we're going to be doing more flops and we still have to doing more flops and we still have to doing more flops and we still have to think through whether this doesn't break think through whether this doesn't break think through whether this doesn't break anything",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1568,
      "text": "but if I just run this uh let's anything",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1569,
      "text": "but if I just run this uh let's anything",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1570,
      "text": "but if I just run this uh let's see what we get uh currently this ran in see what we get uh currently this ran in see what we get uh currently this ran in maybe maybe maybe 96.5 milliseconds per step I'm just kind 96.5 milliseconds per step I'm just kind 96.5 milliseconds per step I'm just kind of like eyeballing it and let's see what of like eyeballing it and let's see what of like eyeballing it and let's see what kind of a result we're going to kind of a result we're going to kind of a result we're going to get uh while this is compiling let's get uh while this is compiling let's get uh while this is compiling let's think through whether our code actually think through whether our code actually think through whether our code actually works okay when we increase the vocap works okay when we increase the vocap works okay when we increase the vocap size like this let's look at where vocap size like this let's look at where vocap size like this let's look at where vocap size is actually size is actually size is actually used",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1571,
      "text": "so we swing up to the inet and we used so we swing up to the inet and we used so we swing up to the inet",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1572,
      "text": "and we see that it's used inside the embedding see that it's used inside the embedding see that it's used inside the embedding table of course so all the way at the table of course",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1573,
      "text": "so all the way at the table of course",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1574,
      "text": "so all the way at the bottom of the Transformer and it's used bottom of the Transformer and it's used bottom of the Transformer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1575,
      "text": "and it's used at the classifier layer all the way at at the classifier layer all the way at at the classifier layer all the way at the top of the Transformer so in two the top of the Transformer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1576,
      "text": "so in two the top of the Transformer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1577,
      "text": "so in two places",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1578,
      "text": "and let's take a look",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1579,
      "text": "and we're places and let's take a look",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1580,
      "text": "and we're places and let's take a look",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1581,
      "text": "and we're running at 93 so 93 milliseconds instead running at 93 so 93 milliseconds instead running at 93 so 93 milliseconds instead of of 96.5",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1582,
      "text": "so we are seeing a roughly yeah 4% 96.5 so we are seeing a roughly yeah 4% 96.5 so we are seeing a roughly yeah 4% Improvement here uh by doing more Improvement here uh by doing more Improvement here uh by doing more calculations and the reason for this is calculations and the reason for this is calculations and the reason for this is we fixed we've made an ugly number into we fixed we've made an ugly number into we fixed we've made an ugly number into a nice number let's I'm going to come a nice number let's I'm going to come a nice number let's I'm going to come into the explanation for that a little into the explanation for that a little into the explanation for that a little bit again",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1583,
      "text": "but for now let's just bit again",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1584,
      "text": "but for now let's just bit again",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1585,
      "text": "but for now let's just convince ourselves that we're not convince ourselves that we're not convince ourselves that we're not breaking anything when we do this so breaking anything when we do this so breaking anything when we do this so first of all we've made the the wte the first of all we've made the the wte the first of all we've made the the wte the embedding table for the tokens we've embedding table for the tokens we've embedding table for the tokens we've made it larger it's almost like we made it larger it's almost like we made it larger it's almost like we introduced more tokens at the bottom and introduced more tokens at the bottom and introduced more tokens at the bottom and these tokens are never used because the these tokens are never used because the these tokens are never used because the gbt tokenizer only has tokens up to gbt tokenizer only has tokens up to gbt tokenizer only has tokens up to $50,000 $50,000 $50,000 256 and so we'll never index into the 256 and so we'll never index into the 256 and so we'll never index into the rows that we've added so we're wasting a rows that we've added so we're wasting a rows that we've added so we're wasting a little bit of space here by creating little bit of space here by creating little bit of space here by creating memory that's never going to be accessed memory that's never going to be accessed memory that's never going to be accessed never going to be used Etc now that's never going to be used Etc now that's never going to be used Etc now that's not fully correct because this wte not fully correct because this wte not fully correct because this wte weight ends up being shared and ends up weight ends up being shared and ends up weight ends up being shared and ends up being used in the classifier here at the being used in the classifier here at the being used in the classifier here at the end",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1586,
      "text": "so what is that doing to the end so what is that doing to the end so what is that doing to the classifier right here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1587,
      "text": "well what what classifier right here well what what classifier right here well what what that's doing is we're predicting that's doing is we're predicting that's doing is we're predicting additional Dimensions at the classifier additional Dimensions at the classifier additional Dimensions at the classifier now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1588,
      "text": "and we're predicting probabilities now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1589,
      "text": "and we're predicting probabilities now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1590,
      "text": "and we're predicting probabilities for tokens that will of course never be for tokens that will of course never be for tokens that will of course never be present in the training set um and so present in the training set um and so present in the training set um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1591,
      "text": "and so therefore the network has to learn that therefore the network has to learn that therefore the network has to learn that these probabilities uh have to be driven these probabilities uh have to be driven these probabilities uh have to be driven to zero and so the logits that the to zero",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1592,
      "text": "and so the logits that the to zero",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1593,
      "text": "and so the logits that the network produces have to drive those network produces have to drive those network produces have to drive those dimensions of the output to negative dimensions of the output to negative dimensions of the output to negative Infinity but it that's no different from Infinity but it that's no different from Infinity but it that's no different from all the other tokens that are already in all the other tokens that are already in all the other tokens that are already in our data set um or rather that are not our data set um or rather that are not our data set um or rather that are not in our data set so Shakespeare only in our data set so Shakespeare only in our data set so Shakespeare only probably uses let's say a th000 tokens probably uses let's say a th000 tokens probably uses let's say a th000 tokens out of 50,000 to 57 tokens so most of out of 50,000 to 57 tokens so most of out of 50,000 to 57 tokens so most of the tokens are already being driven to the tokens are already being driven to the tokens are already being driven to zero probability by the optimization we' zero probability by the optimization we' zero probability by the optimization we' just introduced a few more tokens now just introduced a few more tokens now just introduced a few more tokens now that in a similar manner will never be that in a similar manner will never be that in a similar manner will never be used and have to be driven to zero in used and have to be driven to zero in used and have to be driven to zero in probability um so functionally though probability um so functionally though probability um so functionally though nothing breaks we're using a bit more nothing breaks we're using a bit more nothing breaks we're using a bit more extra um memory but otherwise this is a extra um memory but otherwise this is a extra um memory but otherwise this is a harmless operation as far as I can tell harmless operation as far as I can tell harmless operation as far as I can tell but and we're adding calculation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1594,
      "text": "but",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1595,
      "text": "but and we're adding calculation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1596,
      "text": "but",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1597,
      "text": "but and we're adding calculation but it's running faster",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1598,
      "text": "and it's running it's running faster",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1599,
      "text": "and it's running it's running faster",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1600,
      "text": "and it's running faster because as I mentioned in Cuda so faster because as I mentioned in Cuda so faster because as I mentioned in Cuda so many kernels use uh block tiles and many kernels use uh block tiles and many kernels use uh block tiles and these block towels are usually nice these block towels are usually nice these block towels are usually nice numbers uh so powers of two so numbers uh so powers of two so numbers uh so powers of two so calculations are done in like chunks of calculations are done in like chunks of calculations are done in like chunks of 64 or chunks of 32 and when your um when 64 or chunks of 32 and when your um when 64 or chunks of 32 and when your um when your desired calculation doesn't neatly your desired calculation doesn't neatly your desired calculation doesn't neatly fit into those block tiles um there are fit into those block tiles um there are fit into those block tiles um there are all kinds of boundary kernels that can all kinds of boundary kernels that can all kinds of boundary kernels that can kick in to like do the last part so kick in to like do the last part so kick in to like do the last part so basically in a lot of kernels they will basically in a lot of kernels they will basically in a lot of kernels they will chunk at up your input and they will do chunk at up your input and they will do chunk at up your input and they will do the nice part first",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1601,
      "text": "and then they have a the nice part first",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1602,
      "text": "and then they have a the nice part first",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1603,
      "text": "and then they have a whole second second phase where they whole second second phase where they whole second second phase where they come back to any that like uh remains uh come back to any that like uh remains uh come back to any that like uh remains uh and then they process the remaining part",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1604,
      "text": "and then they process the remaining part",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1605,
      "text": "and then they process the remaining part and the kernels for that could be very and the kernels for that could be very and the kernels for that could be very inefficient and so you're basically um inefficient and so you're basically um inefficient",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1606,
      "text": "and so you're basically um spinning up all this extra compute and spinning up all this extra compute and spinning up all this extra compute and is extremely inefficient so you might as is extremely inefficient so you might as is extremely inefficient so you might as well pad your inputs and um make it fit well pad your inputs and um make it fit well pad your inputs and um make it fit nicely and usually that empiric lens up nicely and usually that empiric lens up nicely and usually that empiric lens up actually running faster um so this is actually running faster um so this is actually running faster um so this is another example of a 4% Improvement that another example of a 4% Improvement that another example of a 4% Improvement that we've added and this is something that we've added and this is something that we've added and this is something that also torch compile did not find for us also torch compile did not find for us also torch compile did not find for us you would hope that torch compile at you would hope that torch compile at you would hope that torch compile at some point could figure an optimization some point could figure an optimization some point could figure an optimization like this out",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1607,
      "text": "uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1608,
      "text": "but for now uh this is like this out uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1609,
      "text": "but for now uh this is like this out uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1610,
      "text": "but for now uh this is it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1611,
      "text": "and I also have to point out that it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1612,
      "text": "and I also have to point out that it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1613,
      "text": "and I also have to point out that we're using pytorch nightly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1614,
      "text": "so that's we're using pytorch nightly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1615,
      "text": "so that's we're using pytorch nightly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1616,
      "text": "so that's why we're only seeing 4% if you're using why we're only seeing 4% if you're using why we're only seeing 4% if you're using pytorch 2.3.1 or earlier you would pytorch 2.3.1 or earlier you would pytorch 2.3.1 or earlier you would actually see something like 30% actually see something like 30% actually see something like 30% Improvement just from this change from Improvement just from this change from Improvement just from this change from changing it to from 50,000 to 57 to changing it to from 50,000 to 57 to changing it to from 50,000 to 57 to 50304 so again one of my favorite 50304 so again one of my favorite 50304 so again one of my favorite examples also of having to understand examples also of having to understand examples also of having to understand the under the hood and how it all works the under the hood and how it all works the under the hood and how it all works and to know what kinds of things to and to know what kinds of things to and to know what kinds of things to Tinker with to push the performance of Tinker with to push the performance of Tinker with to push the performance of your code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1617,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1618,
      "text": "so at this point we have your code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1619,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1620,
      "text": "so at this point we have your code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1621,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1622,
      "text": "so at this point we have improved the performance by about 11x improved the performance by about 11x improved the performance by about 11x right because we started at about 1,000 right because we started at about 1,000 right because we started at about 1,000 milliseconds per step and we're now down milliseconds per step and we're now down milliseconds per step",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1623,
      "text": "and we're now down to like 93 milliseconds so that's uh to like 93 milliseconds so that's uh to like 93 milliseconds so that's uh quite good",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1624,
      "text": "and we're uh doing a much quite good",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1625,
      "text": "and we're uh doing a much quite good",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1626,
      "text": "and we're uh doing a much better job of utilizing our GPU better job of utilizing our GPU better job of utilizing our GPU resources",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1627,
      "text": "so I'm going to now turn to resources",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1628,
      "text": "so I'm going to now turn to resources",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1629,
      "text": "so I'm going to now turn to more algorithmic changes uh and more algorithmic changes uh and more algorithmic changes uh and improvements to the actual optimization improvements to the actual optimization improvements to the actual optimization itself and what we would like to do is itself and what we would like to do is itself and what we would like to do is we would like to follow the hyper we would like to follow the hyper we would like to follow the hyper parameters that are mentioned in the GP parameters that are mentioned in the GP parameters that are mentioned in the GP G2 or gpt2 gpt3 paper now sadly gpt2 is G2 or gpt2 gpt3 paper now sadly gpt2 is G2 or gpt2 gpt3 paper now sadly gpt2 is uh doesn't actually say too much it's uh doesn't actually say too much",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1630,
      "text": "it's uh doesn't actually say too much it's very nice of them that they released the very nice of them that they released the very nice of them that they released the model weights and the code but the paper model weights and the code but the paper model weights and the code but the paper itself is extremely vague as to the itself is extremely vague as to the itself is extremely vague as to the optimization details uh the code itself optimization details uh the code itself optimization details uh the code itself that they released as well the code that they released as well the code that they released as well the code we've been looking at this is just the we've been looking at this is just the we've been looking at this is just the inference code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1631,
      "text": "so there's no training inference code so there's no training inference code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1632,
      "text": "so there's no training code here and very few hyp parameters so code here and very few hyp parameters so code here and very few hyp parameters so this doesn't also tell us too much so this doesn't also tell us too much so this doesn't also tell us too much so for that we have to turn to the gpt3 for that we have to turn to the gpt3 for that we have to turn to the gpt3 paper and um in the depending of the paper and um in the depending of the paper and um in the depending of the gpt3 paper um they have a lot more hyper gpt3 paper",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1633,
      "text": "um they have a lot more hyper gpt3 paper um they have a lot more hyper parameters here for us to use and the parameters here for us to use and the parameters here for us to use and the gpt3 paper in general is a lot more gpt3 paper in general is a lot more gpt3 paper in general is a lot more detailed as to uh all of the you know detailed as to uh all of the you know detailed as to uh all of the you know small details that go into the model small details that go into the model small details that go into the model training but gpt3 U models were never training but gpt3 U models were never training but gpt3 U models were never released so gbt2",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1634,
      "text": "we have the weights but released so gbt2 we have the weights but released so gbt2 we have the weights but no details and gpt3 we have lots of no details and gpt3 we have lots of no details and gpt3 we have lots of details but no weights",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1635,
      "text": "so um but roughly details but no weights",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1636,
      "text": "so um but roughly details but no weights",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1637,
      "text": "so um but roughly speaking gpt2 and gpt3 architectures are speaking gpt2 and gpt3 architectures are speaking gpt2 and gpt3 architectures are very very similar and um basically there very very similar and um basically there very very similar and um basically there are very few changes the context length are very few changes the context length are very few changes the context length was expanded from 1024 to 2048 and was expanded from 1024 to 2048 and was expanded from 1024 to 2048 and that's kind of like the major change uh that's kind of like the major change uh that's kind of like the major change uh and some of the hyper parameters around and some of the hyper parameters around and some of the hyper parameters around the Transformer have changed but the Transformer have changed but the Transformer have changed but otherwise they're pretty much the same",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1638,
      "text": "otherwise they're pretty much the same",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1639,
      "text": "otherwise they're pretty much the same model it's just that gpt3 was trained model it's just that gpt3 was trained model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1640,
      "text": "it's just that gpt3 was trained for a lot longer on a bigger data set for a lot longer on a bigger data set for a lot longer on a bigger data set and uh has a lot more thorough",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1641,
      "text": "and uh has a lot more thorough",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1642,
      "text": "and uh has a lot more thorough evaluations uh and the gpt3 model is 175 evaluations uh and the gpt3 model is 175 evaluations uh and the gpt3 model is 175 billion instead of 1.6 billion um in the billion instead of 1.6 billion um in the billion instead of 1.6 billion um in the gpt2 so long story short",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1643,
      "text": "we're going to gpt2 so long story short we're going to gpt2 so long story short we're going to go to gp3 paper to follow along some the go to gp3 paper to follow along some the go to gp3 paper to follow along some the hyper parameters so to train all the hyper parameters so to train all the hyper parameters so to train all the versions of gpt3 we use atom with beta 1 versions of gpt3 we use atom with beta 1 versions of gpt3 we use atom with beta 1 beta 2 of9 and .95 so let's swing over beta 2 of9 and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1644,
      "text": ".95",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1645,
      "text": "so let's swing over beta 2 of9 and .95",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1646,
      "text": "so let's swing over here and make sure that the betas here and make sure that the betas here and make sure that the betas parameter which you can see here parameter which you can see here parameter which you can see here defaults to 0.9 and defaults to 0.9 and defaults to 0.9 and 999 is actually set to 0.9 and 999 is actually set to 0.9 and 999 is actually set to 0.9 and .95 and then the Epsilon parameter uh .95 and then the Epsilon parameter uh .95 and then the Epsilon parameter uh you can see is the default is 1 in8 and you can see is the default is 1 in8 and you can see is the default is 1 in8",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1647,
      "text": "and this is also one in8 let's just uh put this is also",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1648,
      "text": "one in8 let's just uh put this is also one in8 let's just uh put it in so that works it in so that works it in so that works expit uh now next up they say we clip expit uh now next up they say we clip expit uh now next up they say we clip the gra Global Norm of the gradient at the gra Global Norm of the gradient at the gra Global Norm of the gradient at 1.0 so what this is referring to is that 1.0 so what this is referring to is that 1.0 so what this is referring to is that once we calculate the gradients right once we calculate the gradients right once we calculate the gradients right after l. backward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1649,
      "text": "um we basically have after l. backward um we basically have after l. backward um we basically have the gradients at all the parameter the gradients at all the parameter the gradients at all the parameter tensors and what people like to do is tensors and what people like to do is tensors and what people like to do is basically uh clip them to have some kind basically uh clip them to have some kind basically uh clip them to have some kind of a maximum Norm so in pytor this is of a maximum Norm so in pytor this is of a maximum Norm so in pytor this is fairly easy to do uh it's one line of fairly easy to do uh it's one line of fairly easy to do uh it's one line of code here that we have to insert right code here that we have to insert right code here that we have to insert right after we calcul Cal the gradients and after we calcul Cal the gradients and after we calcul Cal the gradients and what this utility function is doing is what this utility function is doing is what this utility function is doing is um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1650,
      "text": "it's calculating the global Norm of um it's calculating the global Norm of um it's calculating the global Norm of the parameters so every single par um the parameters so every single par um the parameters so every single par um gradient on all the parameters you gradient on all the parameters you gradient on all the parameters you square it and you add it all up and you square it and you add it all up and you square it and you add it all up and you take a big square root of that and take a big square root of that and take a big square root of that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1651,
      "text": "and that's the norm of the parameter V that's the norm of the parameter V that's the norm of the parameter V Vector basically it's the it's the Vector basically it's the it's the Vector basically it's the it's the length of it if you if you'd like to length of it if you if you'd like to length of it if you if you'd like to look at it that way and we are basically look at it that way and we are basically look at it that way and we are basically making sure that its length is no more making sure that its length is no more making sure that its length is no more than 1.0 and we're going to clip it than 1.0 and we're going to clip it than 1.0 and we're going to clip it and the reason that people like to use and the reason that people like to use and the reason that people like to use this is that uh sometimes you can get this is that uh sometimes you can get this is that uh sometimes you can get unlucky during your optimization maybe unlucky during your optimization maybe unlucky during your optimization maybe it's a bad data batch or something like it's a bad data batch or something like it's a bad data batch or something like that and if you get very unlucky in the that and if you get very unlucky in the that and if you get very unlucky in the batch you might get really high loss and batch you might get really high loss and batch you might get really high loss and really high loss could lead to a really really high loss could lead to a really really high loss could lead to a really high gradient and this could basically high gradient and this could basically high gradient and this could basically uh shock your model and shock the uh shock your model and shock the uh shock your model and shock the optimization so people like to use a optimization so people like to use a optimization so people like to use a gradient Norm clipping uh to prevent the gradient Norm clipping uh to prevent the gradient Norm clipping uh to prevent the model from um basically getting too big model from um basically getting too big model from um basically getting too big of shocks in terms of the gradient of shocks in terms of the gradient of shocks in terms of the gradient magnet ude and uh the upper bound it in magnet ude and uh the upper bound it in magnet ude and uh the upper bound it in this way it's a bit of a hacky solution this way it's a bit of a hacky solution this way it's a bit of a hacky solution it's about like a patch on top of like it's about like a patch on top of like it's about like a patch on top of like deeper issues uh but uh people still do deeper issues uh but uh people still do deeper issues uh but uh people still do it fairly frequently now the clip grad it fairly frequently now the clip grad it fairly frequently now the clip grad Norm Returns the norm of the gradient Norm Returns the norm of the gradient Norm Returns the norm of the gradient which I like to always visualize uh which I like to always visualize uh which I like to always visualize uh because um it is useful information and because um it is useful information and because um it is useful information and sometimes you can look at the norm of sometimes you can look at the norm of sometimes you can look at the norm of the gradient and if it's well behaved the gradient and if it's well behaved the gradient and if it's well behaved things are good if it's climbing things things are good if it's climbing things things are good if it's climbing things are bad and they're destabilizing during are bad and they're destabilizing during are bad and they're destabilizing during training sometimes you could get a spike training sometimes you could get a spike training sometimes you could get a spike in the norm and that means there's some in the norm and that means there's some in the norm and that means there's some kind of an issue or an instability so kind of an issue or an instability so kind of an issue or an instability so the norm here will be a the norm here will be a the norm here will be a norm uh and let's do a uh 4f or norm uh and let's do a uh 4f or norm uh and let's do a uh 4f or something like something like something like that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1652,
      "text": "and I believe this is just a float that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1653,
      "text": "and I believe this is just a float that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1654,
      "text": "and I believe this is just a float",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1655,
      "text": "and so we should be able to uh print",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1656,
      "text": "and so we should be able to uh print",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1657,
      "text": "and so we should be able to uh print that uh so that's Global gradient that uh so that's Global gradient that uh so that's Global gradient clipping now they go into the details of clipping now they go into the details of clipping now they go into the details of the learning rate uh scheduler so they the learning rate uh scheduler so they the learning rate uh scheduler so they don't just use a fixed learning rate don't just use a fixed learning rate don't just use a fixed learning rate like we do here for 3 E4",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1658,
      "text": "but there's like we do here for 3 E4",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1659,
      "text": "but there's like we do here for 3 E4",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1660,
      "text": "but there's actually basically a cosine DK learning actually basically a cosine DK learning actually basically a cosine DK learning rate schedule um it's got a warm-up and rate schedule um it's got a warm-up and rate schedule um it's got a warm-up and it's got a cosine DEC to 10% over some it's got a cosine DEC to 10% over some it's got a cosine DEC to 10% over some Horizon Horizon Horizon um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1661,
      "text": "and so we're going to implement uh um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1662,
      "text": "and so we're going to implement uh um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1663,
      "text": "and so we're going to implement uh this in a second",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1664,
      "text": "I just like to see Norm this in a second I just like to see Norm this in a second I just like to see Norm printed here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1665,
      "text": "okay there we go so what printed here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1666,
      "text": "okay there we go so what printed here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1667,
      "text": "okay there we go so what happened here is the norm is actually happened here is the norm is actually happened here is the norm is actually really high in the beginning 30 or so really high in the beginning 30 or so really high in the beginning 30 or so and you see that as we continue training and you see that as we continue training and you see that as we continue training it kind of like it kind of like it kind of like stabilizes um at values below one um and stabilizes um at values below one um and stabilizes um at values below one",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1668,
      "text": "um and this is not that crazy uncommon for the this is not that crazy uncommon for the this is not that crazy uncommon for the norm to be high in the very first few norm to be high in the very first few norm to be high in the very first few stages basically What's Happening Here stages basically What's Happening Here stages basically What's Happening Here is the model is completely random and so is the model is completely random and so is the model is completely random",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1669,
      "text": "and so there's a ton of learning happening very there's a ton of learning happening very there's a ton of learning happening very early in the network but that learning early in the network but that learning early in the network but that learning is kind of like um you know it's mostly is kind of like um you know it's mostly is kind of like um you know it's mostly learning the biases of the output tokens learning the biases of the output tokens learning the biases of the output tokens and so it's a bit of an unstable time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1670,
      "text": "uh and so it's a bit of an unstable time uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1671,
      "text": "and so it's a bit of an unstable time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1672,
      "text": "uh but the network usually stabilizes in a but the network usually stabilizes in a but the network usually stabilizes in a very few iterations so this looks very very few iterations",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1673,
      "text": "so this looks very very few iterations so this looks very relatively reasonable to me except relatively reasonable to me except relatively reasonable to me except usually I would expect this looks a usually I would expect this looks a usually I would expect this looks a little bit funky that we go from 28 to 6 little bit funky that we go from 28 to 6 little bit funky that we go from 28 to 6 to 2 and then to 10 um it's not to 2 and then to 10 um it's not to 2 and then to 10 um it's not completely insane",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1674,
      "text": "but it's just kind of completely insane",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1675,
      "text": "but it's just kind of completely insane",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1676,
      "text": "but it's just kind of a little bit a little bit a little bit funky",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1677,
      "text": "um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1678,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1679,
      "text": "so let's now get to the funky",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1680,
      "text": "um okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1681,
      "text": "so let's now get to the funky",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1682,
      "text": "um okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1683,
      "text": "so let's now get to the learning rate schuer so the learning learning rate schuer so the learning learning rate schuer so the learning rate schedule that's used here in gpt3 rate schedule that's used here in gpt3 rate schedule that's used here in gpt3 is what's called a cosine Decay learning is what's called a cosine Decay learning is what's called a cosine Decay learning schedule with warmup and the way this schedule with warmup and the way this schedule with warmup and the way this looks is that the learning rate is looks is that the learning rate is looks is that the learning rate is basically starts right at around zero basically starts right at around zero basically starts right at around zero linearly rank s up over some amount of linearly rank s up over some amount of linearly rank s up over some amount of time and then comes down with this time and then comes down with this time and then comes down with this cosine sort of form and comes down to cosine sort of form and comes down to cosine sort of form and comes down to some kind of a minimum learning rate some kind of a minimum learning rate some kind of a minimum learning rate that's up to you so here the minimum that's up to you so here the minimum that's up to you so here the minimum learning rate is zero",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1684,
      "text": "but uh here in the learning rate is zero",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1685,
      "text": "but uh here in the learning rate is zero",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1686,
      "text": "but uh here in the paper they said that they use cosine paper they said that they use cosine paper they said that they use cosine Decay for learning rate down to 10% of Decay for learning rate down to 10% of Decay for learning rate down to 10% of its value over the first 260 billion its value over the first 260 billion its value over the first 260 billion tokens and then training continues 10% tokens and then training continues 10% tokens and then training continues 10% after and there's a linear warmup over after and there's a linear warmup over after and there's a linear warmup over the first 375 million tokens so that's the first 375 million tokens so that's the first 375 million tokens so that's about the learn R",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1687,
      "text": "so let's now implement about the learn R so let's now implement about the learn R",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1688,
      "text": "so let's now implement this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1689,
      "text": "uh so I already implemented it here this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1690,
      "text": "uh so I already implemented it here this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1691,
      "text": "uh so I already implemented it here and the way this works is let me scroll and the way this works is let me scroll and the way this works is let me scroll down first here I changed our training down first here I changed our training down first here I changed our training Loop a little bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1692,
      "text": "so this was a 4i in Loop a little bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1693,
      "text": "so this was a 4i in Loop a little bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1694,
      "text": "so this was a 4i in Max steps I just change it to step now Max steps I just change it to step now Max steps I just change it to step now so that we have the notion of a step is so that we have the notion of a step is so that we have the notion of a step is a single optimization step in the in the a single optimization step in the in the a single optimization step in the in the for Loop",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1695,
      "text": "and then here I get the LR for for Loop and then here I get the LR for for Loop and then here I get the LR for this step of the optimization using a this step of the optimization using a this step of the optimization using a new function I call get LR",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1696,
      "text": "and then in new function I call get LR",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1697,
      "text": "and then in new function I call get LR",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1698,
      "text": "and then in pytorch to set the learning rate I think pytorch to set the learning rate I think pytorch to set the learning rate I think this is is the way to set the learning this is is the way to set the learning this is is the way to set the learning rate it's a little bit gnarly um because rate it's a little bit gnarly um because rate it's a little bit gnarly um because you have to basically there's a notion you have to basically there's a notion you have to basically there's a notion of different par parameter groups that of different par parameter groups that of different par parameter groups that could exist in the optimizer and so you could exist in the optimizer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1699,
      "text": "and so you could exist in the optimizer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1700,
      "text": "and so you actually have to iterate over them even actually have to iterate over them even actually have to iterate over them even though we currently have a single param though we currently have a single param though we currently have a single param group only um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1701,
      "text": "and you have to set the LR group only um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1702,
      "text": "and you have to set the LR group only um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1703,
      "text": "and you have to set the LR in this for Loop kind of style is is my in this for Loop kind of style is is my in this for Loop kind of style is is my impression right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1704,
      "text": "so we have this impression right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1705,
      "text": "so we have this impression right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1706,
      "text": "so we have this look of LR we set the learning rate and look of LR we set the learning rate and look of LR we set the learning rate and then on the bottom",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1707,
      "text": "I'm also printing it then on the bottom I'm also printing it then on the bottom I'm also printing it uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1708,
      "text": "so that's all the changes I made to uh so that's all the changes I made to uh so that's all the changes I made to this Loop and then of course the get LR this Loop and then of course the get LR this Loop and then of course the get LR is my scheduler now it's worth pointing is my scheduler now it's worth pointing is my scheduler",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1709,
      "text": "now it's worth pointing out that pytorch actually has learning out that pytorch actually has learning out that pytorch actually has learning rate schedulers and you can use them and rate schedulers and you can use them and rate schedulers and you can use them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1710,
      "text": "and I believe there's a cosine learning rate I believe there's a cosine learning rate I believe there's a cosine learning rate schedule in pytorch I just don't really schedule in pytorch I just don't really schedule in pytorch I just don't really love using that code because honestly love using that code because honestly love using that code because honestly it's like five lines of code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1711,
      "text": "and I fully it's like five lines of code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1712,
      "text": "and I fully it's like five lines of code and I fully understand what's happening inside these understand what's happening inside these understand what's happening inside these lines so I don't love to use lines",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1713,
      "text": "so I don't love to use lines",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1714,
      "text": "so I don't love to use abstractions where they're kind of in abstractions where they're kind of in abstractions where they're kind of in screwable",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1715,
      "text": "and then I don't know what screwable",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1716,
      "text": "and then I don't know what screwable",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1717,
      "text": "and then I don't know what they're doing so personal style",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1718,
      "text": "so the they're doing so personal style",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1719,
      "text": "so the they're doing so personal style so the max learning rate here is let's say 3 E4 max learning rate here is let's say 3 E4 max learning rate here is let's say 3 E4",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1720,
      "text": "but we're going to see that in gpt3",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1721,
      "text": "but we're going to see that in gpt3",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1722,
      "text": "but we're going to see that in gpt3 here they have a table of what the here they have a table of what the here they have a table of what the maximum learning rate is for every model maximum learning rate is for every model maximum learning rate is for every model size",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1723,
      "text": "so um for for this one basically 12 size so um for for this one basically 12 size so um for for this one basically 12 12 layer 768 gpt3 so the gpt3 small is 12 layer 768 gpt3 so the gpt3 small is 12 layer 768 gpt3 so the gpt3 small is roughly like a GPT roughly like a GPT roughly like a GPT 2124m we see that here they use a 2124m we see that here they use a 2124m we see that here they use a learning rate of 6 E4 so we could learning rate of 6 E4 so we could learning rate of 6 E4 so we could actually go higher um in fact we may actually go higher um in fact we may actually go higher um in fact we may want to try to follow that and just set want to try to follow that and just set want to try to follow that and just set the max LR here at six the max LR here at six the max LR here at six uh then the that's the maximum learning",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1724,
      "text": "uh then the that's the maximum learning uh then the that's the maximum learning rate the minum learning rate is uh 10% rate the minum learning rate is uh 10% rate the minum learning rate is uh 10% of that per description in the paper of that per description in the paper of that per description in the paper some number of steps that we're going to some number of steps that we're going to some number of steps that we're going to warm up over and then the maximum steps warm up over and then the maximum steps warm up over and then the maximum steps of the optimization which I now use also of the optimization which I now use also of the optimization which I now use also in the for Loop down here and then you in the for Loop down here and then you in the for Loop down here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1725,
      "text": "and then you can go over this code if you like it's can go over this code if you like it's can go over this code if you like it's not U",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1726,
      "text": "it's not terribly inside Flor not U",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1727,
      "text": "it's not terribly inside Flor not U",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1728,
      "text": "it's not terribly inside Flor interesting",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1729,
      "text": "I'm just uh modulating based interesting I'm just uh modulating based interesting I'm just uh modulating based on the iteration number which learning on the iteration number which learning on the iteration number which learning rate uh there should be so this is the rate uh there should be so this is the rate uh there should be so this is the warm-up region um warm-up region um warm-up region",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1730,
      "text": "um this is the region after the this is the region after the this is the region after the optimization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1731,
      "text": "and then this is the region optimization and then this is the region optimization and then this is the region sort of in between and this is where I sort of in between",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1732,
      "text": "and this is where I sort of in between",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1733,
      "text": "and this is where I calculate the cosine learning rate calculate the cosine learning rate calculate the cosine learning rate schedule and you can step through this schedule and you can step through this schedule and you can step through this in detail if you'd like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1734,
      "text": "uh but this is in detail if you'd like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1735,
      "text": "uh but this is in detail if you'd like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1736,
      "text": "uh but this is basically implementing this basically implementing this basically implementing this curve and I ran this already and this is curve",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1737,
      "text": "and I ran this already",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1738,
      "text": "and this is curve",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1739,
      "text": "and I ran this already",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1740,
      "text": "and this is what that looks what that looks what that looks like um so when we now run we start at like um so when we now run we start at like um so when we now run we start at um some very low number now note that we um some very low number now note that we um some very low number now note that we don't start exactly at zero because that don't start exactly at zero because that don't start exactly at zero because that would be not useful to update with a would be not useful to update with a would be not useful to update with a learning rate of zero that's why there's learning rate of zero that's why there's learning rate of zero that's why there's an it+ one so that on the zeroth an it+ one so that on the zeroth an it+ one so that on the zeroth iteration we are not using exactly zero iteration we are not using exactly zero iteration we are not using exactly zero we're using something very very low then we're using something very very low then we're using something very very low",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1741,
      "text": "then we linearly warm up to maximum learning we linearly warm up to maximum learning we linearly warm up to maximum learning rate which in this case was 34 when I rate which in this case was 34 when I rate which in this case was 34 when I ran it but now would be 6 E4",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1742,
      "text": "and then it ran it but now would be 6 E4",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1743,
      "text": "and then it ran it but now would be 6 E4",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1744,
      "text": "and then it starts to decay all the way down to um 3 starts to decay all the way down to um 3 starts to decay all the way down to um 3 E5 which was at the time 10% of the E5 which was at the time 10% of the E5 which was at the time 10% of the original learning rate now one thing we original learning rate now one thing we original learning rate now one thing we are not following exactly is that they are not following exactly is that they are not following exactly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1745,
      "text": "is that they mentioned that um mentioned that um mentioned that um let me see if I can find it let me see if I can find it let me see if I can find it again we're not exactly following what again we're not exactly following what again we're not exactly following what they did they did they did because uh they mentioned that their because uh they mentioned that their because uh they mentioned that their training Horizon is 300 billion tokens training Horizon is 300 billion tokens training Horizon is 300 billion tokens and they come down to 10% of the initial and they come down to 10% of the initial and they come down to 10% of the initial learning rate of at 260 billion and then learning rate of at 260 billion and then learning rate of at 260 billion",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1746,
      "text": "and then they train after 260 with 10% so they train after 260 with 10%",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1747,
      "text": "so they train after 260 with 10% so basically their Decay time is less than basically their Decay time is less than basically their Decay time is less than the max steps time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1748,
      "text": "whereas for us the max steps time whereas for us the max steps time whereas for us they're exactly equal so it's not they're exactly equal",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1749,
      "text": "so it's not they're exactly equal",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1750,
      "text": "so it's not exactly faithful",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1751,
      "text": "but it's um it's an exactly faithful",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1752,
      "text": "but it's um it's an exactly faithful",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1753,
      "text": "but it's um it's",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1754,
      "text": "an okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1755,
      "text": "um this is okay for us and for our okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1756,
      "text": "um this is okay for us and for our okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1757,
      "text": "um this is okay for us and for our purposes right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1758,
      "text": "and um we're just purposes right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1759,
      "text": "and um we're just purposes right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1760,
      "text": "and um we're just going to use this ourselves",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1761,
      "text": "I don't going to use this ourselves",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1762,
      "text": "I don't going to use this ourselves",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1763,
      "text": "I don't think it makes too too big of a think it makes too too big of a think it makes too too big of a difference honestly I should point out difference honestly I should point out difference honestly I should point out that what learning rate schedule you use that what learning rate schedule you use that what learning rate schedule you use is totally up to you there's many is totally up to you there's many is totally up to you there's many different types um coign learning rate different types um coign learning rate different types um coign learning rate has been popularized a lot by gpt2 and has been popularized a lot by gpt2 and has been popularized a lot by gpt2 and gpt3 but people have come up with all gpt3 but people have come up with all gpt3 but people have come up with all kinds of uh other learning rate kinds of uh other learning rate kinds of uh other learning rate schedules um and this is kind of like an schedules um and this is kind of like an schedules um and this is kind of like an active area of uh research as to which active area of uh research as to which active area of uh research as to which one is the most effective at train these one is the most effective at train these one is the most effective at train these networks",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1764,
      "text": "okay next up the paper talks networks okay next up the paper talks networks okay next up the paper talks about the gradual batch size increase so about the gradual batch size increase so about the gradual batch size increase so there's a ramp on the batch size that is there's a ramp on the batch size that is there's a ramp on the batch size that is linear and you start with very small linear",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1765,
      "text": "and you start with very small linear and you start with very small batch size and you ramp up to a big batch size and you ramp up to a big batch size and you ramp up to a big batch size over time uh we're going to batch size over time uh we're going to batch size over time uh we're going to actually skip this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1766,
      "text": "and we're not going actually skip this and we're not going actually skip this and we're not going to work with it and the reason I don't to work with it and the reason I don't to work with it and the reason I don't love to use it is that it complicates a love to use it is that it complicates a love to use it is that it complicates a lot of the arithmetic because you are lot of the arithmetic because you are lot of the arithmetic because you are changing the number of tokens that changing the number of tokens that changing the number of tokens that you're processing at every single step you're processing at every single step you're processing at every single step of the optimization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1767,
      "text": "and I like to keep of the optimization and I like to keep of the optimization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1768,
      "text": "and I like to keep that math very very simple also my that math very very simple also my that math very very simple also my understanding is that that this is not understanding is that that this is not understanding is that that this is not like a major um Improvement and also my like a major um Improvement and also my like a major um Improvement and also my understanding is that this is not like understanding is that this is not like understanding is that this is not like an algorithmic optimization Improvement an algorithmic optimization Improvement an algorithmic optimization Improvement",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1769,
      "text": "it's more of a systems and speed it's more of a systems and speed it's more of a systems and speed Improvement and roughly speaking this is Improvement and roughly speaking this is Improvement and roughly speaking this is because uh in the early stages of the because uh in the early stages of the because uh in the early stages of the optimization uh again the model is in a optimization uh again the model is in a optimization uh again the model is in a very atypical setting and mostly what very atypical setting and mostly what very atypical setting and mostly what you're learning is that um you're mostly you're learning is that um you're mostly you're learning is that um you're mostly learning to ignore the tokens uh that learning to ignore the tokens uh that learning to ignore the tokens uh that don't come up in your training set very don't come up in your training set very don't come up in your training set very often you're learning very simple biases often you're learning very simple biases often you're learning very simple biases and and that kind of a thing and so and and that kind of a thing and so and and that kind of a thing and so every single example that you put every single example that you put every single example that you put through your network is basically just through your network is basically just through your network is basically just telling you use these tokens and don't telling you use these tokens and don't telling you use these tokens and don't use these tokens and so the gradients use these tokens and so the gradients use these tokens and so the gradients from every single example are actually from every single example are actually from every single example are actually extremely highly correlated they all extremely highly correlated they all extremely highly correlated they all look roughly the same in the in the OR look roughly the same in the in the OR look roughly the same in the in the OR original parts of the optimization original parts of the optimization original parts of the optimization because they're all just telling you because they're all just telling you because they're all just telling you that these tokens don't appear and these that these tokens don't appear and these that these tokens don't appear and these tokens do appear and so because the tokens do appear and so because the tokens do appear and so because the gradients are all very similar and gradients are all very similar and gradients are all very similar and they're highly correlated then why are they're highly correlated then why are they're highly correlated then why are you doing batch sizes of like Millions you doing batch sizes of like Millions you doing batch sizes of like Millions when if you do a batch size of 32k when if you do a batch size of 32k when if you do a batch size of 32k you're basically getting the exact same you're basically getting the exact same you're basically getting the exact same gradient early on in the training and gradient early on in the training and gradient early on in the training and then later in the optimization once then later in the optimization once then later in the optimization once you've learned all the simple stuff you've learned all the simple stuff you've learned all the simple stuff that's where the actual work starts and that's where the actual work starts and that's where the actual work starts and that's where the gradients become more that's where the gradients become more that's where the gradients become more decorrelated per examples and that's decorrelated per examples and that's decorrelated per examples and that's where they actually offer you sort of where they actually offer you sort of where they actually offer you sort of statistical power in some sense um so statistical power in some sense um so statistical power in some sense",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1770,
      "text": "um so we're going to skip this just because it we're going to skip this just because it we're going to skip this just because it kind of complicates things and we're kind of complicates things and we're kind of complicates things and we're going to go going to go going to go to uh data are sampled without to uh data are sampled without to uh data are sampled without replacement during training um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1771,
      "text": "so until replacement during training um so until replacement during training um so until an Epoch boundary is reached so without an Epoch boundary is reached so without an Epoch boundary is reached so without replacement means that they're not replacement means that they're not replacement means that they're not sampling from some fixed pool and then sampling from some fixed pool and then sampling from some fixed pool and then uh take a sequence train on it but then uh take a sequence train on it but then uh take a sequence train on it but then also like return the sequence to the also like return the sequence to the also like return the sequence to the pool they are exhausting a pool so when pool they are exhausting a pool so when pool they are exhausting a pool so when they draw a sequence it's it's gone they draw a sequence it's it's gone they draw a sequence it's it's gone until the next Epoch of training uh so until the next Epoch of training uh so until the next Epoch of training uh so we're already doing that because our we're already doing that because our we're already doing that because our data loader um iterates over chunks of data loader um iterates over chunks of data loader um iterates over chunks of data so there's no replacement they data so there's no replacement they data so there's no replacement they don't become eligible to be drawn again don't become eligible to be drawn again don't become eligible to be drawn again until the next P so we're basically until the next P",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1772,
      "text": "so we're basically until the next P",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1773,
      "text": "so we're basically already doing already doing already doing that um all models use a weight decay of that um all models use a weight decay of that um all models use a weight decay of 0.1 to provide a small amount of 0.1 to provide a small amount of 0.1 to provide a small amount of regularization so let's Implement a regularization so let's Implement a regularization so let's Implement a weight Decay and you see here that I've weight Decay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1774,
      "text": "and you see here that I've weight Decay and you see here that I've already kind of made the changes and in already kind of made the changes and in already kind of made the changes and in particular instead of creating the particular instead of creating the particular instead of creating the optimizer right here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1775,
      "text": "um I I'm creating a optimizer right here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1776,
      "text": "um I I'm creating a optimizer right here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1777,
      "text": "um I I'm creating a new configure optimizers function inside new configure optimizers function inside new configure optimizers function inside the model and I'm passing in some of the the model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1778,
      "text": "and I'm passing in some of the the model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1779,
      "text": "and I'm passing in some of the hyper parameters instead so let's look hyper parameters instead so let's look hyper parameters instead so let's look at the configure optimizers which is at the configure optimizers which is at the configure optimizers which is supposed to return the optimizer object",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1780,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1781,
      "text": "so it looks complicated but object",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1782,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1783,
      "text": "so it looks complicated",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1784,
      "text": "but it's actually really simple",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1785,
      "text": "and it's it's actually really simple",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1786,
      "text": "and it's it's actually really simple",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1787,
      "text": "and it's just um we're just being very careful just um we're just being very careful just um we're just being very careful and there's a few settings here to go and there's a few settings here to go and there's a few settings here to go through the most important thing with through the most important thing with through the most important thing with respect to this line is that you see respect to this line",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1788,
      "text": "is that you see respect to this line is that you see there's a weight Decay parameter here there's a weight Decay parameter here there's a weight Decay parameter here and I'm passing that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1789,
      "text": "and I'm passing that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1790,
      "text": "and I'm passing that into um well I'm passing that into into um well I'm passing that into into um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1791,
      "text": "well I'm passing that into something called optim groups that something called optim groups that something called optim groups that eventually ends up going into the addom eventually ends up going into the addom eventually ends up going into the addom W Optimizer um and the weight Decay W Optimizer um and the weight Decay W Optimizer um and the weight Decay that's by default used in Addam W here that's by default used in Addam W here that's by default used in Addam W here is 0.01",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1792,
      "text": "so it's it's u 10 times lower is 0.01",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1793,
      "text": "so it's it's u 10 times lower is 0.01",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1794,
      "text": "so it's it's u 10 times lower than what's used in gpt3 paper here um than what's used in gpt3 paper here um than what's used in gpt3 paper here um so the weight dek basically ends up so the weight dek basically ends up so the weight dek basically ends up making its way into the ADD and W making its way into the ADD and W making its way into the ADD and W through the optimizer groups now what through the optimizer groups now what through the optimizer groups now what else is going on here in this uh else is going on here in this uh else is going on here in this uh function so the two things that are function so the two things that are function so the two things that are happening here that are important is happening here that are important is happening here that are important",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1795,
      "text": "is that I'm splitting up the parameters that I'm splitting up the parameters that I'm splitting up the parameters into those that should be weight decayed into those that should be weight decayed into those that should be weight decayed and those that should not be weight and those that should not be weight and those that should not be weight decayed so in particular it is common to decayed so in particular it is common to decayed so in particular it is common to not weight decay uh biases and any other not weight decay uh biases and any other not weight decay uh biases and any other sort of one-dimensional tensors so the sort of one-dimensional tensors so the sort of one-dimensional tensors so the one-dimensional tensors are in the no one-dimensional tensors are in the no one-dimensional tensors are in the no Decay prams and these are also things Decay prams and these are also things Decay prams and these are also things like uh layer Norm scales and biases it like uh layer Norm scales and biases it like uh layer Norm scales and biases it doesn't really make sense to weight doesn't really make sense to weight doesn't really make sense to weight Decay those you mostly want to weight Decay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1796,
      "text": "those",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1797,
      "text": "you mostly want to weight Decay those you mostly want to weight Decay uh the weights that participate in Decay uh the weights that participate in Decay uh the weights that participate in Matrix multiplications and you want to Matrix multiplications and you want to Matrix multiplications and you want to potentially weight Decay the potentially weight Decay the potentially weight Decay the embeddings and uh We've covered in embeddings and uh We've covered in embeddings and uh We've covered in previous video why it makes sense to previous video why it makes sense to previous video why it makes sense to Decay the weights because you can sort Decay the weights because you can sort Decay the weights because you can sort of the it as a regularization because of the it as a regularization because of the it as a regularization because when you're pulling down all the weights when you're pulling down all the weights when you're pulling down all the weights you're forcing the optimization to use you're forcing the optimization to use you're forcing the optimization to use more of the weights um and you're not more of the weights um and you're not more of the weights",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1798,
      "text": "um and you're not allowing any one of the weights allowing any one of the weights allowing any one of the weights individually to be way too large um individually to be way too large um individually to be way too large um you're forcing you're forcing the you're forcing you're forcing the you're forcing you're forcing the network to kind of like distribute the network to kind of like distribute the network to kind of like distribute the work across more channels because work across more channels because work across more channels because there's sort of like a pull of gravity there's sort of like a pull of gravity there's sort of like a pull of gravity on the weights on the weights on the weights themselves um so that's why we are themselves um so that's why we are themselves um so that's why we are separating it in those ways here we're separating it in those ways here we're separating it in those ways here we're only decaying the embeddings and the only decaying the embeddings and the only decaying the embeddings and the mmal participating ways mmal participating ways mmal participating ways uh we're printing the number of uh uh we're printing the number of uh uh we're printing the number of uh parameters that we decaying and not most parameters that we decaying and not most parameters that we decaying and not most of the parameters will be decayed and of the parameters will be decayed and of the parameters will be decayed and then one more thing that we're doing then one more thing that we're doing then one more thing that we're doing here is I'm doing another optimization here is I'm doing another optimization here is I'm doing another optimization here and previous add and W did not have here and previous add and W did not have here and previous add and W did not have this option but later parts of pytorch this option but later parts of pytorch this option but later parts of pytorch introduced it and that's why I'm introduced it and that's why I'm introduced it and that's why I'm guarding it with an inspect do signature guarding it with an inspect do signature guarding it with an inspect do signature which is basically checking if this which is basically checking if this which is basically checking if this fused um quar is present inside atom W fused um quar is present inside atom W fused um quar is present inside atom W",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1799,
      "text": "and then if it is present I'm going to",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1800,
      "text": "and then if it is present I'm going to",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1801,
      "text": "and then if it is present I'm going to end up using it and passing it in here end up using it and passing it in here end up using it and passing it in here because some earlier versions do not because some earlier versions do not because some earlier versions do not have fused equals so here's adamw fused have fused equals so here's adamw fused have fused equals so here's adamw fused equals it did not used to exist and it equals it did not used to exist and it equals it did not used to exist and it was added later",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1802,
      "text": "and there's some docks was added later and there's some docks was added later and there's some docks here for what's happening and basically here for what's happening and basically here for what's happening",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1803,
      "text": "and basically they say that by default they do not use they say that by default they do not use they say that by default they do not use fused because it is relatively new and fused because it is relatively new and fused because it is relatively new and we want to give it sufficient big time we want to give it sufficient big time we want to give it sufficient big time so by default they don't use fused but so by default they don't use fused but so by default they don't use fused but fused is a lot faster when it is fused is a lot faster when it is fused is a lot faster when it is available and when you're running on available",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1804,
      "text": "and when you're running on available",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1805,
      "text": "and when you're running on Cuda and what that does is in instead of Cuda and what that does is in instead of Cuda and what that does is in instead of iterating in a for Loop over all the iterating in a for Loop over all the iterating in a for Loop over all the parameter tensors and updating them that parameter tensors and updating them that parameter tensors and updating them that would launch a lot of kernels right and would launch a lot of kernels right and would launch a lot of kernels",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1806,
      "text": "right and so a fused just means that it's a um all so a fused just means that it's a um all so a fused just means that it's a um all those kernels are fused into a single those kernels are fused into a single those kernels are fused into a single kernel you get rid of a lot of overhead kernel you get rid of a lot of overhead kernel you get rid of a lot of overhead and you a single time on all the and you a single time on all the and you a single time on all the parameters call a uh kernel that updates parameters call a uh kernel that updates parameters call a uh kernel that updates them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1807,
      "text": "and so it's just basically a kernel them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1808,
      "text": "and so it's just basically a kernel them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1809,
      "text": "and so it's just basically a kernel Fusion for the atom W update instead of Fusion for the atom W update instead of Fusion for the atom W update instead of iterating over all the iterating over all the iterating over all the tensors so that's the configure tensors so that's the configure tensors so that's the configure optimizers function that I like to use optimizers function that I like to use optimizers function that I like to use",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1810,
      "text": "and we can rerun and we're not going to and we can rerun and we're not going to",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1811,
      "text": "and we can rerun and we're not going to see any major differences from what we see any major differences from what we see any major differences from what we saw before but we are going to see some saw before but we are going to see some saw before but we are going to see some prints uh coming from here so let's just prints uh coming from here so let's just prints uh coming from here so let's just take a look at what they look take a look at what they look take a look at what they look like so we see that number of Decay like so we see that number of Decay like so we see that number of Decay tensors is 50",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1812,
      "text": "and it's most of the tensors is 50",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1813,
      "text": "and it's most of the tensors is 50",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1814,
      "text": "and it's most of the parameters and number of non- deay parameters and number of non- deay parameters and number of non- deay tensors is 98 and these are the biases tensors is 98 and these are the biases tensors is 98 and these are the biases and the layer Norm parameters mostly and and the layer Norm parameters mostly and and the layer Norm parameters mostly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1815,
      "text": "and that's there's only 100,000 of those so that's there's only 100,000 of those",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1816,
      "text": "so that's there's only 100,000 of those so most of it is decayed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1817,
      "text": "and then we are most of it is decayed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1818,
      "text": "and then we are most of it is decayed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1819,
      "text": "and then we are using the fused implementation of ATM W using the fused implementation of ATM W using the fused implementation of ATM W which will be a lot faster so if you which will be a lot faster so if you which will be a lot faster so if you have it available I would advise you to have it available I would advise you to have it available I would advise you to use it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1820,
      "text": "I'm not actually 100% sure why use it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1821,
      "text": "I'm not actually 100% sure why use it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1822,
      "text": "I'm not actually 100% sure why they don't default to it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1823,
      "text": "it seems fairly they don't default to it it seems fairly they don't default to it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1824,
      "text": "it seems fairly benign and benign and benign and harmless and also because we are using harmless and also because we are using harmless and also because we are using the fused implementation I think this is the fused implementation I think this is the fused implementation I think this is why we have dropped um notice that the why we have dropped um notice that the why we have dropped um notice that the running time used to be 93 milliseconds running time used to be 93 milliseconds running time used to be 93 milliseconds per step and we're now down to 90 per step and we're now down to 90 per step",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1825,
      "text": "and we're now down to 90 milliseconds per step because of using milliseconds per step because of using milliseconds per step because of using the fused atom W Optimizer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1826,
      "text": "so in a the fused atom W Optimizer so in a the fused atom W Optimizer so in a single commit here we are introducing single commit here we are introducing single commit here we are introducing fused atom getting improvements on the fused atom getting improvements on the fused atom getting improvements on the time and we're adding or changing the time and we're adding or changing the time and we're adding or changing the weight Decay but we're only weight weight Decay but we're only weight weight Decay but we're only weight decaying the two dimensional parameters decaying the two dimensional parameters decaying the two dimensional parameters the embeddings and the matrices that the embeddings and the matrices that the embeddings and the matrices that participate in linear so that is this participate in linear so that is this participate in linear so that is this and we can take this out and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1827,
      "text": "uh yeah",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1828,
      "text": "and we can take this out",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1829,
      "text": "and uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1830,
      "text": "yeah",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1831,
      "text": "and we can take this out",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1832,
      "text": "and uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1833,
      "text": "yeah",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1834,
      "text": "that is it for this line one more quick that is it for this line one more quick that is it for this line one more quick note before we continue here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1835,
      "text": "I just want note before we continue here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1836,
      "text": "I just want note before we continue here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1837,
      "text": "I just want to point out that the relationship to point out that the relationship to point out that the relationship between weight Decay learning rate batch between weight Decay learning rate batch between weight Decay learning rate batch size the atom parameters beta 1 beta 2 size the atom parameters beta 1 beta 2 size the atom parameters beta 1 beta 2 the Epsilon and so on these are very the Epsilon and so on these are very the Epsilon and so on these are very complicated uh mathematical complicated uh mathematical complicated uh mathematical relationships in the optimization relationships in the optimization relationships in the optimization literature and um for the most part I'm literature and um for the most part I'm literature and um for the most part I'm in this video I'm just trying to copy in this video I'm just trying to copy in this video I'm just trying to copy paste the settings that open AI used but paste the settings that open AI used but paste the settings that open AI used",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1838,
      "text": "but this is a complicated topic uh quite this is a complicated topic uh quite this is a complicated topic uh quite deep",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1839,
      "text": "and um yeah in this video I just deep",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1840,
      "text": "and um yeah in this video I just deep",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1841,
      "text": "and um yeah in this video I just want to copy the parameters because it's want to copy the parameters because it's want to copy the parameters because it's a whole different video to really talk a whole different video to really talk a whole different video to really talk about that in detail and give it a about that in detail and give it a about that in detail and give it a proper Justice instead of just high proper Justice instead of just high proper Justice instead of just high level level level intuitions uh now the next thing that I intuitions uh now the next thing that I intuitions uh now the next thing that I want to move on to is that uh this want to move on to is that uh this want to move on to is that uh this paragraph here by the way we're going to paragraph here by the way we're going to paragraph here by the way we're going to turn back around to when we improve our turn back around to when we improve our turn back around to when we improve our data loader for now I want to swing back data loader for now I want to swing back data loader for now I want to swing back around around around to this table where you will notice that um for table where you will notice that um for different models we of course have different models we of course have different models we of course have different U hyper parameters for the different U hyper parameters for the different U hyper parameters for the Transformer that dictate the size of the Transformer that dictate the size of the Transformer that dictate the size of the Transformer Network we also have a Transformer Network we also have a Transformer Network we also have a different learning rate so we're seeing different learning rate so we're seeing different learning rate",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1842,
      "text": "so we're seeing the pattern that the bigger networks are the pattern that the bigger networks are the pattern that the bigger networks are trained with slightly lower learning trained with slightly lower learning trained with slightly lower learning rates and we also see this batch size rates and we also see this batch size rates and we also see this batch size where in in the small networks they use where in in the small networks they use where in in the small networks they use a smaller batch size and in the bigger a smaller batch size and in the bigger a smaller batch size and in the bigger networks they use a bigger batch size networks they use a bigger batch size networks they use a bigger batch size now the problem with for us is we can't now the problem with for us is we can't now the problem with for us is we can't just use 0.5 million batch size because just use 0.5 million batch size because just use 0.5 million batch size because uh if I just try to come in here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1843,
      "text": "and I uh if I just try to come in here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1844,
      "text": "and I uh if I just try to come in here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1845,
      "text": "and I try to set uh this uh B where is my try to set uh this uh B where is my try to set uh this uh B where is my b b b um b equals where where do I call the DAT equals where where do I call the DAT okay b equal 16 if I try to set um okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1846,
      "text": "b equal 16 if I try to set um okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1847,
      "text": "b equal 16 if I try to set um well well we have to be careful it's not",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1848,
      "text": "well well we have to be careful it's not",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1849,
      "text": "well well we have to be careful it's not 0.5 million because this is the badge 0.5 million because this is the badge 0.5 million because this is the badge size in the number of tokens every size in the number of tokens every size in the number of tokens every single one of our rows is24 tokens so single one of our rows is24 tokens so single one of our rows is24 tokens so 0.5 E6 1 million divide 1024 this would 0.5 E6 1 million divide 1024 this would 0.5 E6 1 million divide 1024 this would need about a need about a need about a 488 match size so the problem is I can't 488 match size",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1850,
      "text": "so the problem is I can't 488 match size so the problem is I can't come in here and set this to 488 uh come in here and set this to 488 uh come in here and set this to 488 uh because my GPU would explode um this because my GPU would explode um this because my GPU would explode um this would not fit for sure",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1851,
      "text": "and so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1852,
      "text": "but we would not fit for sure",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1853,
      "text": "and so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1854,
      "text": "but we would not fit for sure",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1855,
      "text": "and so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1856,
      "text": "but we still want to use this batch size still want to use this batch size still want to use this batch size because again as I mentioned the batch because again as I mentioned the batch because again as I mentioned the batch size is correlated with all the other size is correlated with all the other size is correlated with all the other optimization hyper parameters and the optimization hyper parameters and the optimization hyper parameters and the learning rates and so on so we want to learning rates and so on so we want to learning rates and so on so we want to have a faithful representation of all have a faithful representation of all have a faithful representation of all the hyper parameters and therefore we the hyper parameters and therefore we the hyper parameters and therefore we need to uh use a bat size of .5 million need to uh use a bat size of .5 million need to uh use a bat size of .5 million roughly but the question is how do we roughly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1857,
      "text": "but the question is how do we roughly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1858,
      "text": "but the question is how do we use .5 million if we only have a small use .5 million if we only have a small use .5 million if we only have a small GPU well for that we need to use what's GPU well for that we need to use what's GPU well for that we need to use what's called gradient accumulation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1859,
      "text": "uh so we're called gradient accumulation uh so we're called gradient accumulation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1860,
      "text": "uh so we're going to turn to that next",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1861,
      "text": "and it allows going to turn to that next",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1862,
      "text": "and it allows going to turn to that next",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1863,
      "text": "and it allows us to simulate in a Serial way any us to simulate in a Serial way any us to simulate in a Serial way any arbitrary batch size that we set and so arbitrary batch size that we set and so arbitrary batch size that we set and so we can do a batch size of .5 million we we can do a batch size of .5 million",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1864,
      "text": "we we can do a batch size of .5 million we just have to run longer and we have to just have to run longer and we have to just have to run longer and we have to process multiple sequences and basically process multiple sequences and basically process multiple sequences and basically add up all the gradients from them to add up all the gradients from them to add up all the gradients from them to simulate a batch size of .5 million so simulate a batch size of .5 million so simulate a batch size of .5 million",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1865,
      "text": "so let's turn to that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1866,
      "text": "next",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1867,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1868,
      "text": "so I let's turn to that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1869,
      "text": "next",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1870,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1871,
      "text": "so I let's turn to that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1872,
      "text": "next",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1873,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1874,
      "text": "so I started the implementation right here started the implementation right here started the implementation right here just by adding these lines of code and just by adding these lines of code and just by adding these lines of code and basically what I did is first I set the basically what I did is first I set the basically what I did is first I set the total batch size that we desire so this total batch size that we desire so this total batch size that we desire",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1875,
      "text": "so this is exactly .5 million",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1876,
      "text": "and I used a nice is exactly .5 million",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1877,
      "text": "and I used a nice is exactly .5 million",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1878,
      "text": "and I used a nice number a power of two uh because 2 to number a power of two uh because 2 to number a power of two uh because 2 to the 19 is 524 288 so it's roughly .5 the 19 is 524 288",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1879,
      "text": "so it's roughly .5",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1880,
      "text": "the 19 is 524 288 so it's roughly .5 million",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1881,
      "text": "it's a nice number now our micro million",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1882,
      "text": "it's a nice number now our micro million",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1883,
      "text": "it's a nice number now our micro batch size as we call it now is 16 so batch size as we call it now is 16 so batch size as we call it now is 16",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1884,
      "text": "so this is going to be we still have B BYT this is going to be we still have B BYT this is going to be we still have B BYT in the SE that go into the Transformer in the SE that go into the Transformer in the SE that go into the Transformer and do forward backward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1885,
      "text": "but we're not and do forward backward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1886,
      "text": "but we're not and do forward backward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1887,
      "text": "but we're not going to do an update",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1888,
      "text": "right we're going going to do an update",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1889,
      "text": "right we're going going to do an update",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1890,
      "text": "right we're going to do many forward backwards we're going to do many forward backwards we're going to do many forward backwards we're going to and those gradients are all going to to and those gradients are all going to to and those gradients are all going to plus equals on the parameter gradients plus equals on the parameter gradients plus equals on the parameter gradients they're all going to add up so we're they're all going to add up so we're they're all going to add up so we're going to do forward backward grad akum going to do forward backward grad akum going to do forward backward grad akum steps number of times",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1891,
      "text": "and then we're steps number of times and then we're steps number of times",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1892,
      "text": "and then we're going to do a single update once all going to do a single update once all going to do a single update once all that is that is that is accumulated so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1893,
      "text": "in particular our micro accumulated so in particular our micro accumulated so in particular our micro batch size is just now controlling how batch size is just now controlling how batch size is just now controlling how many tokens how many rows we're many tokens how many rows we're many tokens how many rows we're processing in a single go over a forward processing in a single go over a forward processing in a single go over a forward backward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1894,
      "text": "so um here we are doing 16 * backward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1895,
      "text": "so um here we are doing 16 * backward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1896,
      "text": "so um here we are doing 16 * 124 we're doing 16 124 we're doing 16 124 we're doing 16 384 um tokens per forward backward and 384 um tokens per forward backward and 384 um tokens per forward backward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1897,
      "text": "and we are supposed to be doing 2 to the 19 we are supposed to be doing 2 to the 19 we are supposed to be doing 2 to the 19 whoops what am I doing 2 to the whoops what am I doing 2 to the whoops what am I doing 2 to the 19 in total",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1898,
      "text": "so the grat Aon will be 32 uh so therefore gr AUM here will work 32 uh so therefore gr AUM here will work out to 32 and we have to do 32 forward out to 32 and we have to do 32 forward out to 32 and we have to do 32 forward backward um and then a single update now backward um and then a single update now backward um and then a single update now we see that we have about 100 we see that we have about 100 we see that we have about 100 milliseconds for a singer forward milliseconds for a singer forward milliseconds for a singer forward backward so doing 32 of them will be backward so doing 32 of them will be backward so doing 32 of them will be will make every step roughly 3 seconds will make every step roughly 3 seconds will make every step roughly 3 seconds just napkin",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1899,
      "text": "just napkin just napkin math",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1900,
      "text": "so that's grum steps",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1901,
      "text": "but now we math",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1902,
      "text": "so that's grum steps",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1903,
      "text": "but now we math",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1904,
      "text": "so that's grum steps",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1905,
      "text": "but now we actually have to Implement that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1906,
      "text": "so we're actually have to Implement that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1907,
      "text": "so we're actually have to Implement that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1908,
      "text": "so we're going to swing over to our training Loop going to swing over to our training Loop going to swing over to our training Loop because now this part because now this part because now this part here and this part here the forward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1909,
      "text": "and here and this part here the forward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1910,
      "text": "and here and this part here the forward and the backward we have to now repeat this the backward we have to now repeat this the backward we have to now repeat this 32 times before we do everything else 32 times before we do everything else 32 times before we do everything else that follows so let's uh see how we can that follows so let's uh see how we can that follows so let's uh see how we can Implement that so let's come over here Implement that so let's come over here Implement that so let's come over here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1911,
      "text": "and actually we do have to load a new",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1912,
      "text": "and actually we do have to load a new",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1913,
      "text": "and actually we do have to load a new batch every single time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1914,
      "text": "so let me move batch every single time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1915,
      "text": "so let me move batch every single time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1916,
      "text": "so let me move that over here and now this is where we that over here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1917,
      "text": "and now this is where we that over here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1918,
      "text": "and now this is where we have the inner loop so for micro step in have the inner loop so for micro step in have the inner loop so for micro step in range graum range graum range graum steps we do this and remember that l. steps we do this and remember that l. steps we do this and remember that l. backward always deposits gradients so backward always deposits gradients so backward always deposits gradients so we're doing inside losta",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1919,
      "text": "backward we're doing inside losta backward we're doing inside losta backward there's always a plus equals on the there's always a plus equals on the there's always a plus equals on the gradients so in every single L of gradients so in every single L of gradients so in every single L of backward gradients will add up on the backward gradients will add up on the backward gradients will add up on the gradient gradient gradient tensors um so we lost that backward and tensors um so we lost that backward and tensors um so we lost that backward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1920,
      "text": "and then we get all the gradients over there then we get all the gradients over there then we get all the gradients over there and then we normalize and everything",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1921,
      "text": "and then we normalize and everything",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1922,
      "text": "and then we normalize and everything else should just follow um so we're very else should just follow um so we're very else should just follow um so we're very close",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1923,
      "text": "but actually there's like subtle close",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1924,
      "text": "but actually there's like subtle close",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1925,
      "text": "but actually there's like subtle and deep issue here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1926,
      "text": "and this is actually and deep issue here and this is actually and deep issue here and this is actually incorrect so invite",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1927,
      "text": "I invite you to incorrect so invite I invite you to incorrect so invite I invite you to think about why this is not yet think about why this is not yet think about why this is not yet sufficient um and uh let me fix it then sufficient um and uh let me fix it then sufficient",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1928,
      "text": "um and uh let me fix it then",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1929,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1930,
      "text": "so I brought back the jupyter",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1931,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1932,
      "text": "so I brought back the jupyter",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1933,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1934,
      "text": "so I brought back the jupyter notebook so we can think about this notebook so we can think about this notebook so we can think about this carefully in a simple toy setting and carefully in a simple toy setting and carefully in a simple toy setting and see what's happening so let's create a see what's happening so let's create a see what's happening so let's create a very simple neural nut that takes a 16 very simple neural nut that takes a 16 very simple neural nut that takes a 16 Vector of 16 numbers and returns a Vector of 16 numbers and returns a Vector of 16 numbers and returns a single single single number",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1935,
      "text": "and then here I'm creating some number",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1936,
      "text": "and then here I'm creating some number",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1937,
      "text": "and then here I'm creating some random uh examples X and some targets uh random uh examples X and some targets uh random uh examples X and some targets uh y Y",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1938,
      "text": "and then we are using the mean y Y",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1939,
      "text": "and then we are using the mean y Y",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1940,
      "text": "and then we are using the mean squared loss uh here to calculate the squared loss uh here to calculate the squared loss uh here to calculate the loss so basically what this is is four loss so basically what this is is four loss so basically what this is is four individual examples and we're just doing individual examples and we're just doing individual examples and we're just doing Simple regression with the mean squared Simple regression with the mean squared Simple regression with the mean squared loss over those four loss over those four loss over those four examples now when we calculate the loss examples now when we calculate the loss examples now when we calculate the loss and we lost that backward and look at and we lost that backward and look at and we lost that backward and look at the gradient this is the gradient that the gradient this is the gradient that the gradient this is the gradient",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1941,
      "text": "that we",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1942,
      "text": "we",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1943,
      "text": "we achieve now the loss objective here achieve now the loss objective here achieve now the loss objective here notice that in MSE loss the default for notice that in MSE loss the default for notice that in MSE loss the default for the loss function is reduction is mean the loss function is reduction is mean the loss function is reduction is mean",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1944,
      "text": "so we're we're calculating the average",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1945,
      "text": "so we're we're calculating the average",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1946,
      "text": "so we're we're calculating the average mean loss um the the mean loss here over mean loss um the the mean loss here over mean loss um the the mean loss here over the four examples so this is the exact the four examples",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1947,
      "text": "so this is the exact the four examples",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1948,
      "text": "so this is the exact loss objective and this is the average loss objective and this is the average loss objective and this is the average the one over four because there are four the one over four because there are four the one over four because there are four independent examples here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1949,
      "text": "and then we independent examples here and then we independent examples here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1950,
      "text": "and then we have the four examples and their mean have the four examples and their mean have the four examples and their mean squared error the squared error and then squared error the squared error and then squared error the squared error and then this makes it the mean squared error so this makes it the mean squared error so this makes it the mean squared error so therefore uh we are we calculate the therefore uh we are we calculate the therefore uh we are we calculate the squared error",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1951,
      "text": "and then we normalize it squared error and then we normalize it squared error and then we normalize it to make it the mean over the examples to make it the mean over the examples to make it the mean over the examples and there's four examples here so now and there's four examples here so now and there's four examples here so now when we come to the gradient when we come to the gradient when we come to the gradient accumulation version of it this uh this accumulation version of it this uh this accumulation version of it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1952,
      "text": "this uh this here is the gradient accumulation here is the gradient accumulation here is the gradient accumulation version of it where we have grad acum version of it where we have grad acum version of it where we have grad acum steps of four",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1953,
      "text": "and I reset the gradient steps of four and I reset the gradient steps of four",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1954,
      "text": "and I reset the gradient we've grum steps of four and now I'm we've grum steps of four and now I'm we've grum steps of four",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1955,
      "text": "and now I'm evaluating all the examples individually evaluating all the examples individually evaluating all the examples individually instead and calling L that backward on instead and calling L that backward on instead and calling L that backward on them many times and then we're looking them many times and then we're looking them many times",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1956,
      "text": "and then we're looking at the gradient that we achieve from at the gradient that we achieve from at the gradient that we achieve from that so basically now we forward our that so basically now we forward our that so basically now we forward our function calculate the exact same loss function calculate the exact same loss function calculate the exact same loss do a backward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1957,
      "text": "and we do that four times do a backward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1958,
      "text": "and we do that four times do a backward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1959,
      "text": "and we do that four times and when we look at the gradient uh and when we look at the gradient uh and when we look at the gradient uh you'll notice that the gradients don't you'll notice that the gradients don't you'll notice that the gradients don't match",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1960,
      "text": "so here we uh did a single batch match",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1961,
      "text": "so here we uh did a single batch match",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1962,
      "text": "so here we uh did a single batch of four and here we did uh four gradient of four and here we did uh four gradient of four and here we did uh four gradient accumulation steps of batch size one and accumulation steps of batch size one and accumulation steps of batch size one and the gradients are not the same and the gradients are not the same and the gradients are not the same and basically the the reason that they're basically the the reason that they're basically the the reason that they're not the same is exactly because this not the same is exactly because this not the same is exactly because this mean squared error gets lost this one mean squared error gets lost this one mean squared error gets lost this one quarter in this loss gets lost because quarter in this loss gets lost because quarter in this loss gets lost because what happens here is the loss of what happens here is the loss of what happens here is the loss of objective for every one of the loops is objective for every one of the loops is objective for every one of the loops is just a mean squ error um which in this just a mean squ error um which in this just a mean squ error um which in this case because there's only a single case because there's only a single case because there's only a single example is just this term here so that example is just this term here so that example is just this term here so that was the loss in the zeroth eration same was the loss in the zeroth eration same was the loss in the zeroth eration same in the first third and so on and then in the first third and so on and then in the first third and so on and then when you do the loss.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1963,
      "text": "backward we're when you do the loss.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1964,
      "text": "backward we're when you do the loss.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1965,
      "text": "backward we're accumulating gradients and what happens accumulating gradients and what happens accumulating gradients and what happens is that accumulation in the gradient is is that accumulation in the gradient is is that accumulation in the gradient is basically equivalent to doing a sum in basically equivalent to doing a sum in basically equivalent to doing a sum in the the loss so our loss actually here is this loss so our loss actually here is this loss so our loss actually here is this without the factor of one quarter without the factor of one quarter without the factor of one quarter outside of it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1966,
      "text": "so we're missing the outside of it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1967,
      "text": "so we're missing the outside of it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1968,
      "text": "so we're missing the normalizer and therefore our gradients normalizer and therefore our gradients normalizer and therefore our gradients are off",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1969,
      "text": "and so the way to fix this or are off",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1970,
      "text": "and so the way to fix this or are off",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1971,
      "text": "and so the way to fix this or one of them is basically we can actually one of them is basically we can actually one of them is basically we can actually come here and we can say loss equals come here and we can say loss equals come here and we can say loss equals loss divide loss divide loss divide 4 and what happens now is that we're 4 and what happens now is that we're 4 and what happens now is that we're introducing we're we're scaling our loss introducing we're we're scaling our loss introducing we're we're scaling our loss we're introducing a one quarter in front we're introducing a one quarter in front we're introducing a one quarter in front of all of these places so all the individual losses are places so all the individual losses are now scaled by one quarter",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1972,
      "text": "and and then now scaled by one quarter and and then now scaled by one quarter",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1973,
      "text": "and and then when we backward all of these accumulate when we backward all of these accumulate when we backward all of these accumulate with a sum",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1974,
      "text": "but now there's a one quarter with a sum",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1975,
      "text": "but now there's a one quarter with a sum",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1976,
      "text": "but now there's a one quarter inside every one of these components and inside every one of these components and inside every one of these components and now our losses will be now our losses will be now our losses will be equivalent so when I run this you see equivalent",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1977,
      "text": "so when I run this you see equivalent",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1978,
      "text": "so when I run this you see that the U gradients are now identical that the U gradients are now identical that the U gradients are now identical so long story short with this simple so long story short with this simple so long story short with this simple example uh when you step through it you example uh when you step through it you example uh when you step through it you can see that basically the reason that can see that basically the reason that can see that basically the reason that this is not correct is because in the this is not correct is because in the this is not correct is because in the same way as here in the MSE loss the same way as here in the MSE loss the same way as here in the MSE loss the loss that we're calculating here in the loss that we're calculating here in the loss that we're calculating here in the model is using a reduction of mean as model is using a reduction of mean as model is using a reduction of mean as well uh so where's the loss after that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1979,
      "text": "well uh so where's the loss after that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1980,
      "text": "well uh so where's the loss after that cross cross cross entropy and by default the reduction uh entropy and by default the reduction uh entropy and by default the reduction uh here in Cross entropy is also I don't here in Cross entropy is also I don't here in Cross entropy is also I don't know why they don't show it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1981,
      "text": "but it's the know why they don't show it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1982,
      "text": "but it's the know why they don't show it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1983,
      "text": "but it's the mean",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1984,
      "text": "uh the mean uh loss at all the B mean uh the mean uh loss at all the B mean uh the mean uh loss at all the B BYT elements BYT elements BYT elements",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1985,
      "text": "right so there's a reduction by mean in right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1986,
      "text": "so there's a reduction by mean",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1987,
      "text": "in right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1988,
      "text": "so there's a reduction by mean in there",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1989,
      "text": "and if we're just doing this there and if we're just doing this there and if we're just doing this gradient accumulation here we're missing gradient accumulation here we're missing gradient accumulation here we're missing that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1990,
      "text": "and so the way to fix this is to that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1991,
      "text": "and so the way to fix this is to that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1992,
      "text": "and so the way to fix this is to simply compensate for the number of simply compensate for the number of simply compensate for the number of gradient accumulation steps and we can gradient accumulation steps and we can gradient accumulation steps and we can in the same way divide this loss so in in the same way divide this loss so in in the same way divide this loss so in particular here the number of steps that particular here the number of steps that particular here the number of steps that we're doing is loss equals loss divide we're doing is loss equals loss divide we're doing is loss equals loss divide gradient accumulation steps so even uh gradient accumulation steps so even uh gradient accumulation steps so even uh co-pilot s gets the modification but in co-pilot s gets the modification but in co-pilot s gets the modification but in the same way exactly we are scaling down the same way exactly we are scaling down the same way exactly we are scaling down the loss so that when we do loss that the loss so that when we do loss that the loss so that when we do loss that backward which basically corresponds to backward which basically corresponds to backward which basically corresponds to a sum in the objective we are summing up a sum in the objective we are summing up a sum in the objective we are summing up the already the already the already normalized um loss and and therefore normalized um loss and and therefore normalized um loss and and therefore when we sum up the losses divided by when we sum up the losses divided by when we sum up the losses divided by grum steps we are recovering the grum steps we are recovering the grum steps we are recovering the additional normalizer uh and so now additional normalizer uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1993,
      "text": "and so now additional normalizer uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1994,
      "text": "and so now these two will be now this will be these two will be now this will be these two will be now this will be equivalent to the original uh sort of equivalent to the original uh sort of equivalent to the original uh sort of optimization because the gradient will optimization because the gradient will optimization because the gradient will come out the same",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1995,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1996,
      "text": "so I had to do a come out the same",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1997,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1998,
      "text": "so I had to do a come out the same",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 1999,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2000,
      "text": "so I had to do a few more touch-ups",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2001,
      "text": "and I launched few more touch-ups and I launched few more touch-ups and I launched launched the optimization here so in launched the optimization here so in launched the optimization here so in particular one thing we want to do particular one thing we want to do particular one thing we want to do because we want to print things nicely because we want to print things nicely because we want to print things nicely is well first of all we need to create is well first of all we need to create is well first of all we need to create like an accumulator over the loss we like an accumulator over the loss we like an accumulator over the loss we can't just print the loss because we'd can't just print the loss because we'd can't just print the loss because we'd be printing only the final loss at the be printing only the final loss at the be printing only the final loss at the final micro step so instead we have loss final micro step so instead we have loss final micro step so instead we have loss ofon which I initialize at zero and then ofon which I initialize at zero and then ofon which I initialize at zero",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2002,
      "text": "and then I accumulate a uh the loss into it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2003,
      "text": "and I accumulate a uh the loss into it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2004,
      "text": "and I accumulate a uh the loss into it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2005,
      "text": "and I'm using detach so that um uh I'm I'm using detach so that um uh I'm I'm using detach so that um uh I'm detaching the tensor uh from the graph detaching the tensor uh from the graph detaching the tensor uh from the graph",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2006,
      "text": "and I'm just trying to keep track of the",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2007,
      "text": "and I'm just trying to keep track of the",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2008,
      "text": "and I'm just trying to keep track of the values so I'm making these Leaf nodes values",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2009,
      "text": "so I'm making these Leaf nodes values so I'm making these Leaf nodes when I add them so that's lakum",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2010,
      "text": "and then when I add them so that's lakum",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2011,
      "text": "and then when I add them so that's lakum",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2012,
      "text": "and then we're printing that here instead of loss we're printing that here instead of loss we're printing that here instead of loss and then in addition to that I had to and then in addition to that I had to and then in addition to that I had to account for the grum steps inside the account for the grum steps inside the account for the grum steps inside the tokens processed because now the tokens tokens processed because now the tokens tokens processed because now the tokens processed per step is B * T * gradient processed per step is B * T * gradient processed per step is B * T * gradient accumulation so long story short here we accumulation so long story short here we accumulation so long story short here we have the optimization it looks uh have the optimization it looks uh have the optimization it looks uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2013,
      "text": "reasonable right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2014,
      "text": "we're starting at a reasonable right we're starting at a reasonable right we're starting at a good spot we calculated the grum steps good spot we calculated the grum steps good spot we calculated the grum steps to be to be to be 32 and uh we're getting about 3 seconds 32 and uh we're getting about 3 seconds 32 and uh we're getting about 3 seconds here here here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2015,
      "text": "right right um um um and so this looks pretty good now if and so this looks pretty good now if and so this looks pretty good now if you'd like to verify that uh your you'd like to verify that uh your you'd like to verify that uh your optimization and the implementation here optimization and the implementation here optimization and the implementation here is correct and your working on a side is correct and your working on a side is correct and your working on a side well now because we have the total patch well now because we have the total patch well now because we have the total patch size and the gradient accumulation steps size and the gradient accumulation steps size and the gradient accumulation steps our setting of B is purely a performance our setting of B is purely a performance our setting of B is purely a performance optimization kind of setting so if you optimization kind of setting so if you optimization kind of setting so if you have a big GPU you can actually increase have a big GPU you can actually increase have a big GPU you can actually increase this to 32 and you'll probably go a bit this to 32 and you'll probably go a bit this to 32 and you'll probably go a bit faster if you have a very small GPU you faster if you have a very small GPU you faster if you have a very small GPU you can try eight or four but in any case can try eight or four but in any case can try eight or four but in any case you should be getting the exact same you should be getting the exact same you should be getting the exact same optimization and the same answers up to optimization and the same answers up to optimization and the same answers up to like a floating Point error because the like a floating Point error because the like a floating Point error because the gradient accumulation kicks in and um gradient accumulation kicks in and um gradient accumulation kicks in and um and can um handle everything serially as and can um handle everything serially as and can um handle everything serially as an an an Neary so uh that's it for gradient Neary",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2016,
      "text": "so uh that's it for gradient Neary",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2017,
      "text": "so uh that's it for gradient accumulation I think",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2018,
      "text": "okay so now is the accumulation I think",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2019,
      "text": "okay so now is the accumulation I think",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2020,
      "text": "okay so now is the time to bring out the heavy weapons uh time to bring out the heavy weapons uh time to bring out the heavy weapons uh you've noticed that so far we've only you've noticed that so far we've only you've noticed that so far we've only been using a single GPU for training but been using a single GPU for training but been using a single GPU for training",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2021,
      "text": "but actually I am paying for eight gpus here actually I am paying for eight gpus here actually I am paying for eight gpus here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2022,
      "text": "and so uh we should be putting all of",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2023,
      "text": "and so uh we should be putting all of",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2024,
      "text": "and so uh we should be putting all of them to work and in particular they are them to work and in particular they are them to work and in particular they are going to collaborate and uh you know going to collaborate and uh you know going to collaborate and uh you know optimize over tokens at the same time optimize over tokens at the same time optimize over tokens at the same time and communicate so that um uh they're and communicate so that um uh they're and communicate so that um uh they're all kind of collaborating on the all kind of collaborating on the all kind of collaborating on the optimization for this we are going to be optimization for this we are going to be optimization for this we are going to be using the distributed data parallel from using the distributed data parallel from using the distributed data parallel from pytorch there's also a legacy data pytorch there's also a legacy data pytorch there's also a legacy data parallel which I recommend you not use parallel which I recommend you not use parallel which I recommend you not use and that's kind of like you know Legacy and that's kind of like you know Legacy and that's kind of like you know Legacy distributed data parallel Works in a distributed data parallel Works in a distributed data parallel Works in a very simple way we have eight gpus so very simple way we have eight gpus so very simple way we have eight gpus so we're going to uh launch eight processes we're going to uh launch eight processes we're going to uh launch eight processes and each process is going to be assigned and each process is going to be assigned and each process is going to be assigned to GPU and for each process the training to GPU and for each process the training to GPU and for each process the training Loop and everything we've worked on so Loop and everything we've worked on so Loop and everything we've worked on so far is going to look pretty much the far is going to look pretty much the far is going to look pretty much the same H GPU as far as it's concerned is same H GPU as far as it's concerned is same H GPU as far as it's concerned is just working on exactly what we've built just working on exactly what we've built just working on exactly what we've built so far but now Secret L there's eight of so far",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2025,
      "text": "but now Secret L there's eight of so far",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2026,
      "text": "but now Secret L there's eight of them and they're all going to be them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2027,
      "text": "and they're all going to be them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2028,
      "text": "and they're all going to be processing slightly different parts of processing slightly different parts of processing slightly different parts of the data and we're going to add one more the data",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2029,
      "text": "and we're going to add one more the data and we're going to add one more part where once they all calculate their part where once they all calculate their part where once they all calculate their gradients there's one more part where we gradients there's one more part where we gradients there's one more part where we do a average of those do a average of those do a average of those gradients",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2030,
      "text": "and so that's how they're gradients",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2031,
      "text": "and so that's how they're gradients",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2032,
      "text": "and so that's how they're going to be collaborating on uh the going to be collaborating on uh the going to be collaborating on uh the computational workload here so to use computational workload here so to use computational workload here so to use all eight of them we're not going to be all eight of them we're not going to be all eight of them we're not going to be launching our script anymore with just launching our script anymore with just launching our script anymore with just um pytorch train um pytorch train um pytorch train gbt2 piy we're going to be running it gbt2 piy",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2033,
      "text": "we're going to be running it gbt2 piy",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2034,
      "text": "we're going to be running it with a special command called torrun in with a special command called torrun in with a special command called torrun in pytorch we'll see that in a bit and pytorch we'll see that in a bit and pytorch we'll see that in a bit and torrun uh when it runs our python script torrun uh when it runs our python script torrun uh when it runs our python script we'll actually make sure to run eight we'll actually make sure to run eight we'll actually make sure to run eight eight of them in parallel and it creates eight of them in parallel and it creates eight of them in parallel and it creates these environmental variables where each these environmental variables where each these environmental variables where each of these processes can look up which uh of these processes can look up which uh of these processes can look up which uh basically which one of the processes it basically which one of the processes it basically which one of the processes it is so for example torron will set rank is so for example torron will set rank is so for example torron will set rank local Rank and World size environmental local Rank and World size environmental local Rank and World size environmental variables and so this is a bad way to variables and so this is a bad way to variables and so this is a bad way to detect whether uh DDP is running so if detect whether uh DDP is running so if detect whether uh DDP is running so if we're using torch run if DDP is we're using torch run if DDP is we're using torch run if DDP is running then uh we have to make sure running",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2035,
      "text": "then",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2036,
      "text": "uh we have to make sure running then uh we have to make sure that K is available because I don't know that K is available because I don't know that K is available because I don't know that you can run this on CPU anymore or that you can run this on CPU anymore or that you can run this on CPU anymore or that that makes sense to do um this is that that makes sense to do um this is that that makes sense to do um this is some um setup code here the important some um setup code here the important some um setup code here the important part is that there's a world size which part is that there's a world size which part is that there's a world size which for us will be eight that's the total for us will be eight that's the total for us will be eight that's the total number of processes running there's a number of processes running there's a number of processes running there's a rank which is um each process will rank which is um each process will rank which is um each process will basically run the ex exact same code at basically run the ex exact same code at basically run the ex exact same code at the exact same time roughly but all the the exact same time roughly but all the the exact same time roughly but all the process the only difference between process the only difference between process the only difference between these processes is that they all have a these processes is that they all have a these processes is that they all have a different dtp rank so the um gpu0 will different dtp rank so the um gpu0 will different dtp rank so the um gpu0 will have DDP rank of zero GPU 1 will have uh have DDP rank of zero GPU 1 will have uh have DDP rank of zero GPU 1 will have uh rank of one Etc",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2037,
      "text": "so otherwise they're all rank of one Etc",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2038,
      "text": "so otherwise they're all rank of one Etc",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2039,
      "text": "so otherwise they're all running the exact same script it's just running the exact same script it's just running the exact same script it's just that DDP rank will be a slightly that DDP rank will be a slightly that DDP rank will be a slightly different integer and that is the way different integer and that is the way different integer and that is the way for us to coordinate that they don't for for us to coordinate that they don't for for us to coordinate that they don't for example run on the same data we want to example run on the same data we want to example run on the same data we want to we want them to run on different parts we want them to run on different parts we want them to run on different parts of the data and so on of the data and so on of the data and so on now local rank is something that is only now local rank is something that is only now local rank is something that is only used in a multi- node setting we only used in a multi- node setting we only used in a multi- node setting we only have a single node with ag gpus and so have a single node with ag gpus and so have a single node with ag gpus and so local rank is the rank of the GPU on a local rank is the rank of the GPU on a local rank is the rank of the GPU on a single node so from 0 to seven as an single node so from 0 to seven as an single node so from 0 to seven as an example but for us we're mostly going to example but for us we're mostly going to example but for us we're mostly going to be running on a single box so the things be running on a single box so the things be running on a single box so the things we care about are Rank and World size we care about are Rank and World size we care about are Rank and World size this is eight and this will be whatever this is eight and this will be whatever this is eight and this will be whatever it is depending on the GPU uh that uh it is depending on the GPU uh that uh it is depending on the GPU uh that uh that this particular instantiation of that this particular instantiation of that this particular instantiation of the script runs on the script runs on the script runs on now here we make sure that according to now here we make sure that according to now here we make sure that according to the local rank we are setting the device the local rank we are setting the device the local rank we are setting the device to be Cuda colon and colon indicates to be Cuda colon and colon indicates to be Cuda colon and colon indicates which GPU to use if there are more than which GPU to use if there are more than which GPU to use if there are more than one gpus so depending on the local rank one gpus so depending on the local rank one gpus so depending on the local rank of this process it's going to use just of this process it's going to use just of this process it's going to use just the appropriate GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2040,
      "text": "so there's no the appropriate GPU so there's no the appropriate GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2041,
      "text": "so there's no collisions on which GPU is being used by collisions on which GPU is being used by collisions on which GPU is being used by which which which process and finally there's a Boolean process and finally there's a Boolean process and finally there's a Boolean variable that I like to create which is variable that I like to create which is variable that I like to create which is the DDP rank",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2042,
      "text": "equ equal Z so the master the DDP rank equ equal Z",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2043,
      "text": "so the master the DDP rank equ equal Z so the master process is arbitrarily process number process is arbitrarily process number process is arbitrarily process number zero and it does a lot of the printing zero",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2044,
      "text": "and it does a lot of the printing zero and it does a lot of the printing logging checkpointing Etc and the other logging checkpointing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2045,
      "text": "Etc and the other logging checkpointing Etc and the other processes are thought of mostly as a processes are thought of mostly as a processes are thought of mostly as a compute processes that are assisting and compute processes that are assisting and compute processes that are assisting and so Master process zero will have some so Master process zero will have some so Master process zero will have some additional work to do all the other additional work to do all the other additional work to do all the other processes will uh will mostly just be processes will uh will mostly just be processes will uh will mostly just be doing forward doing forward doing forward backwards and if we're not using DDP and backwards and if we're not using DDP and backwards and if we're not using DDP and none of these variables are set we none of these variables are set we none of these variables are set we revert back to single GPU training so revert back to single GPU training so revert back to single GPU training so that means that we only have rank zero that means that we only have rank zero that means that we only have rank zero the world size is just one uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2046,
      "text": "and and we the world size is just one uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2047,
      "text": "and and we the world size is just one uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2048,
      "text": "and and we are the master process and we try to are the master process and we try to are the master process and we try to autodetect the device and this is world autodetect the device and this is world autodetect the device and this is world as as as normal",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2049,
      "text": "so so far all we've done is we've normal",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2050,
      "text": "so so far all we've done is we've normal",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2051,
      "text": "so so far all we've done is we've initialized initialized initialized DDP and uh in the case where we're DDP and uh in the case where we're DDP and uh in the case where we're running with torrun which we'll see in a running with torrun which we'll see in a running with torrun which we'll see in a bit there's going to be eight copies bit there's going to be eight copies bit there's going to be eight copies running in parallel each one of them running in parallel each one of them running in parallel each one of them will have a different Rank and now we will have a different Rank and now we will have a different Rank and now we have to make sure that everything have to make sure that everything have to make sure that everything happens uh correctly afterwards",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2052,
      "text": "so the happens uh correctly afterwards so the happens uh correctly afterwards so the tricky thing with running multiple tricky thing with running multiple tricky thing with running multiple processes is you always have to imagine processes is you always have to imagine processes is you always have to imagine that there's going to be eight processes that there's going to be eight processes that there's going to be eight processes running in parallel",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2053,
      "text": "so as you read the running in parallel so as you read the running in parallel so as you read the code now you have to imagine there's code now you have to imagine there's code now you have to imagine there's eight you know eight python interpreters eight you know eight python interpreters eight you know eight python interpreters running down these lines of code and the running down these lines of code and the running down these lines of code and the only difference between them is that only difference between them is that only difference between them is that they have a different DDP rank so they they have a different DDP rank",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2054,
      "text": "so they they have a different DDP rank so they all come here they all pick the exact all come here they all pick the exact all come here they all pick the exact same seed they all make all of these same seed they all make all of these same seed they all make all of these calculations completely unaware of the calculations completely unaware of the calculations completely unaware of the other copies running roughly speaking other copies running roughly speaking other copies running roughly speaking",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2055,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2056,
      "text": "so they all make the exact same right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2057,
      "text": "so they all make the exact same right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2058,
      "text": "so they all make the exact same calculations and now we have to adjust calculations and now we have to adjust calculations and now we have to adjust these calculations to take into account these calculations to take into account these calculations to take into account that there's actually like a certain that there's actually like a certain that there's actually like a certain world size and certain ranks so in world size and certain ranks so in world size and certain ranks so in particular these micro batches and particular these micro batches and particular these micro batches and sequence lengths these are all just per sequence lengths these are all just per sequence lengths these are all just per GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2059,
      "text": "right so now there's going to be num GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2060,
      "text": "right so now there's going to be num GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2061,
      "text": "right so now there's going to be num processes of them running in parallel so processes of them running in parallel so processes of them running in parallel",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2062,
      "text": "so we have to adjust this right because the we have to adjust this right because the we have to adjust this right because the grum steps now is going to be total B grum steps now is going to be total B grum steps now is going to be total B size divide B * T time U DDP R size divide B * T time U DDP R size divide B * T time U DDP R size because each um process will will size because each um process will will size because each um process will will do B * T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2063,
      "text": "and there's this many of do B * T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2064,
      "text": "and there's this many of do B * T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2065,
      "text": "and there's this many of them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2066,
      "text": "and so in addition to that we we them and so in addition to that we we them and so in addition to that we we want to make sure that this fits nicely want to make sure that this fits nicely want to make sure that this fits nicely into total batch size which for us it into total batch size which for us it into total batch size which for us it will because 16 * 124 * 8 8 gpus is will because 16 * 124 * 8 8 gpus is will because 16 * 124 * 8 8 gpus is 131 uh K and so 131 uh K and so 131 uh K and so 524288 this means that our gratum will 524288 this means that our gratum will 524288 this means that our gratum will be four with the current settings right be four with the current settings right be four with the current settings",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2067,
      "text": "right so there's going to be 16 * 124 process so there's going to be 16 * 124 process so there's going to be 16 * 124 process on each GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2068,
      "text": "and then there's a GP pus so on each GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2069,
      "text": "and then there's a GP pus so on each GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2070,
      "text": "and then there's a GP pus",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2071,
      "text": "so we're going to be doing we're going to be doing we're going to be doing 131,000 tokens in a single forward 131,000 tokens in a single forward 131,000 tokens in a single forward backward on the 8 backward on the 8 backward on the 8 gpus so we want to make sure that this gpus so we want to make sure that this gpus so we want to make sure that this fits nicely so that we can derive a nice fits nicely so that we can derive a nice fits nicely so that we can derive a nice gradient accumulation gradient accumulation gradient accumulation steps and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2072,
      "text": "uh yeah let's just adjust the steps",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2073,
      "text": "and uh yeah",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2074,
      "text": "let's just adjust the steps and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2075,
      "text": "uh yeah let's just adjust the comments here times uh DDP World size comments here times uh DDP World size comments here times uh DDP World size",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2076,
      "text": "okay so each GPU calculates this now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2077,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2078,
      "text": "so each GPU calculates this now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2079,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2080,
      "text": "so each GPU calculates this now this is where we start to get run into this is where we start to get run into this is where we start to get run into issues",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2081,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2082,
      "text": "so we are each process is issues",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2083,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2084,
      "text": "so we are each process is issues",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2085,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2086,
      "text": "so we are each process is going to come by a print",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2087,
      "text": "and they're all going to come by a print",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2088,
      "text": "and they're all going to come by a print",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2089,
      "text": "and they're all going to print so we're going to have going to print so we're going to have going to print so we're going to have eight copies of these prints so one way eight copies of these prints so one way eight copies of these prints so one way to deal with this is exactly this master to deal with this is exactly this master to deal with this is exactly this master process variable that we have so if process variable that we have so if process variable that we have so if Master process then guard this and Master process then guard this and Master process then guard this and that's just so that we just print this a that's just so that we just print this a that's just so that we just print this a single time because otherwise all the single time because otherwise all the single time because otherwise all the processes would have computed the exact processes would have computed the exact processes would have computed the exact same variables and there's no need to same variables and there's no need to same variables and there's no need to print this eight print this eight print this eight times um before getting into the data times um before getting into the data times um before getting into the data loader and we're going to have to loader and we're going to have to loader and we're going to have to refactor it obviously maybe at this refactor it obviously maybe at this refactor it obviously maybe at this point is uh we should do some prints and point is uh we should do some prints and point is uh we should do some prints and uh just take it out for a spin and exit uh just take it out for a spin and exit uh just take it out for a spin and exit at this point so import at this point so import at this point so import sis and S start exit and print IM GPU um DDP GPU um DDP rank IM GPU DDP Rank and that um rank IM GPU DDP Rank and that um rank IM GPU DDP Rank and that um print print print by",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2090,
      "text": "so uh so now let's try to run this by so uh so now let's try to run this by so uh so now let's try to run this and just see how this works so let's and just see how this works so let's and just see how this works so let's take it for a spin just so we see what take it for a spin just so we see what take it for a spin just so we see what it looks like so normally we use to it looks like so normally we use to it looks like so normally we use to launch python train gpd2 P like this now launch python train gpd2 P like this now launch python train gpd2 P like this now we're going to run with torch run and we're going to run with torch run and we're going to run with torch run and this is what it looks like so torch run this is what it looks like so torch run this is what it looks like so torch run Standalone number of processes for Standalone number of processes for Standalone number of processes for example is eight for us because we have example is eight for us because we have example is eight for us because we have eight gpus uh and then change of2 Pi",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2091,
      "text": "so eight gpus uh and then change of2 Pi so eight gpus uh and then change of2 Pi",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2092,
      "text": "so this is what the command would look like this is what the command would look like this is what the command would look like and torch run again we'll run eight of and torch run again we'll run eight of and torch run again we'll run eight of these so let's just see what happens so these so let's just see what happens so these so let's just see what happens so first first first it gets a little busy so there's a lot it gets a little busy",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2093,
      "text": "so there's a lot it gets a little busy",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2094,
      "text": "so there's a lot going on here so first of all there's going on here so first of all there's going on here so first of all there's some warnings from distributed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2095,
      "text": "and I some warnings from distributed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2096,
      "text": "and I some warnings from distributed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2097,
      "text": "and I don't actually know that these mean don't actually know that these mean don't actually know that these mean anything I think this is just like the anything I think this is just like the anything I think this is just like the code is setting up and the processes are code is setting up and the processes are code is setting up and the processes are coming online",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2098,
      "text": "and we're seeing some coming online",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2099,
      "text": "and we're seeing some coming online",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2100,
      "text": "and we're seeing some preliminary failure to collect while the preliminary failure to collect while the preliminary failure to collect while the processes come up I'm not 100% sure processes come up",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2101,
      "text": "I'm not 100% sure processes come up",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2102,
      "text": "I'm not 100% sure about that but we start to then get into about that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2103,
      "text": "but we start to then get into about that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2104,
      "text": "but we start to then get into actual prints actual prints actual prints so all the processes went down",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2105,
      "text": "and then so all the processes went down and then so all the processes went down and then the first print actually comes from the first print actually comes from the first print actually comes from process 5 uh just by chance and then it process 5 uh just by chance and then it process 5 uh just by chance and then it printed so process 5 basically got here printed so process 5 basically got here printed so process 5 basically got here first it said I'm process on GPU 5 buy first it said I'm process on GPU 5 buy first it said I'm process on GPU 5 buy and then this these prints come from the and then this these prints come from the and then this these prints come from the master master master process so process 5 just finished first process so process 5 just finished first process so process 5 just finished first for whatever reason it just depends on for whatever reason it just depends on for whatever reason it just depends on how the operating system scheduled the how the operating system scheduled the how the operating system scheduled the processes to run uh then gpu0 ended then processes to run uh then gpu0 ended then processes to run uh then gpu0 ended then GPU 3 and two and then uh probably GPU 3 and two and then uh probably GPU 3 and two and then uh probably process 5 or something like that has uh process 5 or something like that has uh process 5 or something like that has uh exited and and DDP really doesn't like exited and and DDP really doesn't like exited and and DDP really doesn't like that because we didn't properly dispose that because we didn't properly dispose that because we didn't properly dispose of uh the multi-gpus um setting and so of uh the multi-gpus um setting and so of uh the multi-gpus um setting and so process group has not been destroyed process group has not been destroyed process group has not been destroyed before we destruct uh so it really before we destruct uh so it really before we destruct uh so it really doesn't like that and in an actual doesn't like that and in an actual doesn't like that and in an actual application we would want to call application we would want to call application we would want to call destroy process group uh so that we destroy process group uh so that we destroy process group uh so that we clean up DDP properly and so it doesn't clean up DDP properly and so it doesn't clean up DDP properly and so it doesn't like that too much and then the rest of like that too much",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2106,
      "text": "and then the rest of like that too much",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2107,
      "text": "and then the rest of the gpus finish and that's it so the gpus finish",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2108,
      "text": "and that's it so the gpus finish",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2109,
      "text": "and that's it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2110,
      "text": "so basically we can't guarantee when these basically we can't guarantee when these basically we can't guarantee when these processes are running it's totally processes are running it's totally processes are running it's totally",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2111,
      "text": "but they are running in parallel we",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2112,
      "text": "but they are running in parallel we",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2113,
      "text": "but they are running in parallel we don't want them to be printing um and don't want them to be printing um and don't want them to be printing um and next up let's erase next up let's erase next up let's erase this next up we want to make sure that this next up we want to make sure that this next up we want to make sure that when we create data loader light we need when we create data loader light we need when we create data loader light we need to now make it aware of this to now make it aware of this to now make it aware of this multi-process um setting because we multi-process um setting because we multi-process um setting because we don't want all the processes to be don't want all the processes to be don't want all the processes to be loading the exact same data we want loading the exact same data we want loading the exact same data we want every process to get its own chunk of every process to get its own chunk of every process to get its own chunk of data so that they're all working on data so that they're all working on data so that they're all working on different parts of the data set of different parts of the data set of different parts of the data set of course",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2114,
      "text": "so let's adjust that so one course so let's adjust that so one course so let's adjust that so one particular particularly simple and a particular particularly simple and a particular particularly simple and a naive way to do this is we have to make naive way to do this is we have to make naive way to do this is we have to make sure that we pass in the rank and the sure that we pass in the rank and the sure that we pass in the rank and the size to the data size to the data size to the data loader and then when we come up here we loader and then when we come up here we loader and then when we come up here we see that we now take Rank and processes see that we now take Rank and processes see that we now take Rank and processes and we save them now the current and we save them now the current and we save them now the current position will not be zero uh because position will not be zero uh because position will not be zero uh because what we want is we want to stride out what we want is we want to stride out what we want is we want to stride out all the processes so one way to do this all the processes so one way to do this all the processes so one way to do this is we basically take S.B times salt.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2115,
      "text": "T is we basically take S.B times salt.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2116,
      "text": "T is we basically take S.B times salt.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2117,
      "text": "T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2118,
      "text": "and then multiply it by the process and then multiply it by the process and then multiply it by the process rank so proc process rank 0 will start rank so proc process rank 0 will start rank so proc process rank 0 will start at zero but process rank one now starts at zero but process rank one now starts at zero but process rank one now starts at B * T process rank two is starts at 2 at B * T process rank two is starts at 2 at B * T process rank two is starts at 2 * B",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2119,
      "text": "* D Etc so that is the * B * D Etc",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2120,
      "text": "so that is the * B * D Etc so that is the initialization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2121,
      "text": "now we still they still initialization now we still they still initialization now we still they still do this identically but now when we do this identically but now when we do this identically but now when we advance we don't Advance by B *",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2122,
      "text": "T we advance we don't Advance by B *",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2123,
      "text": "T we advance we don't Advance by B *",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2124,
      "text": "T we advance by B",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2125,
      "text": "* T times number of advance by B * T times number of advance by B * T times number of processes",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2126,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2127,
      "text": "so basically um the processes",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2128,
      "text": "right so basically um the processes",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2129,
      "text": "right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2130,
      "text": "so basically um the total number of tokens that we're um total number of tokens that we're um total number of tokens that we're um consuming is B * T * number processes consuming is B * T * number processes consuming is B * T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2131,
      "text": "* number processes and they all go off to a different Rank and they all go off to a different Rank and they all go off to a different Rank and the position has to advance by the and the position has to advance by the and the position has to advance by the entire entire entire chunk and then here B",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2132,
      "text": "* T time uh s. num chunk and then here B",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2133,
      "text": "* T time uh s. num chunk and then here B",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2134,
      "text": "* T time uh s. num processes + one would be to exceed processes + one would be to exceed processes + one would be to exceed number of tokens then we're going to number of tokens then we're going to number of tokens then we're going to Loop and when we Loop we want to of Loop and when we Loop we want to of Loop and when we Loop we want to of course Loop in the exact same way so we course Loop in the exact same way so we course Loop in the exact same way so we sort of like reset back uh so this is sort of like reset back uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2135,
      "text": "so this is sort of like reset back uh so this is the simplest change that I can uh find the simplest change that I can uh find the simplest change that I can uh find for kind of a very simple distributed for kind of a very simple distributed for kind of a very simple distributed data Lo light",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2136,
      "text": "and um you can notice that data Lo light",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2137,
      "text": "and um you can notice that data Lo light",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2138,
      "text": "and um you can notice that if process rank is zero and non if process rank is zero and non if process rank is zero and non processes is one then uh the whole thing processes is one then uh the whole thing processes is one then uh the whole thing will be identical to what we had before will be identical to what we had before will be identical to what we had before",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2139,
      "text": "but now we can have actually multiple",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2140,
      "text": "but now we can have actually multiple",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2141,
      "text": "but now we can have actually multiple processes uh running and this should processes uh running and this should processes uh running",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2142,
      "text": "and this should work work work fine um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2143,
      "text": "so that's the data loader",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2144,
      "text": "okay fine",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2145,
      "text": "um so that's the data loader",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2146,
      "text": "okay fine",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2147,
      "text": "um so that's the data loader",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2148,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2149,
      "text": "so next up once they've all initialized so next up once they've all initialized so next up once they've all initialized the data loader they come here and they the data loader they come here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2150,
      "text": "and they the data loader they come here and they all create a GPT model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2151,
      "text": "uh so we create all create a GPT model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2152,
      "text": "uh so we create all create a GPT model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2153,
      "text": "uh so we create eight GPT models on eight processes but eight GPT models on eight processes but eight GPT models on eight processes but because the seeds are fixed here they because the seeds are fixed here they because the seeds are fixed here they all create the same identical model they all create the same identical model they all create the same identical model they all move it to the device of their Rank all move it to the device of their Rank all move it to the device of their Rank and they all compile the model and and they all compile the model and and they all compile the model and because the models are identical there because the models are identical there because the models are identical there are eight identical compilations are eight identical compilations are eight identical compilations happening in parallel but that's okay happening in parallel but that's okay happening in parallel but that's okay now none of this uh changes because that now none of this uh changes because that now none of this uh changes because that is on a per step basis and we're is on a per step basis and we're is on a per step basis and we're currently working kind of within step currently working kind of within step currently working kind of within step because we need to um just uh all the because we need to um just uh all the because we need to um just uh all the all the changes we're making are kind of all the changes we're making are kind of all the changes we're making are kind of like a within step like a within step like a within step changes now the important thing here is changes now the important thing here is changes now the important thing here is when we construct the M model we when we construct the M model we when we construct the M model we actually have a bit of work to to do actually have a bit of work to to do actually have a bit of work to to do here get loits is deprecated so uh here get loits is deprecated so uh here get loits is deprecated so uh create create create model we need to actually wrap the model model we need to actually wrap the model model we need to actually wrap the model into the distributed data parallel into the distributed data parallel into the distributed data parallel container",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2154,
      "text": "so um this is how we wrap the container",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2155,
      "text": "so um this is how we wrap the container",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2156,
      "text": "so um this is how we wrap the model into the DDP container and these model into the DDP container and these model into the DDP container and these are the docs for DDP and they're quite are the docs for DDP",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2157,
      "text": "and they're quite are the docs for DDP",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2158,
      "text": "and they're quite extensive",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2159,
      "text": "and there's a lot of caveats extensive and there's a lot of caveats extensive and there's a lot of caveats and a lot of things to be careful with and a lot of things to be careful with and a lot of things to be careful with because everything complexifies times 10 because everything complexifies times 10 because everything complexifies times 10 when multiple processes are involved but when multiple processes are involved but when multiple processes are involved but roughly speaking this device IDs I roughly speaking this device IDs I roughly speaking this device IDs I believe has to be passed in now believe has to be passed in now believe has to be passed in now unfortunately the docs for what device unfortunately the docs for what device unfortunately the docs for what device IDs is is is extremely unclear uh so IDs is is is extremely unclear uh so IDs is is is extremely unclear uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2160,
      "text": "so when you actually like come here this when you actually like come here this when you actually like come here this comment for what device IDs is is comment for what device IDs is is comment for what device IDs is is roughly roughly roughly nonsensical um but I'm pretty sure it's nonsensical",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2161,
      "text": "um but I'm pretty sure it's nonsensical um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2162,
      "text": "but I'm pretty sure it's supposed to be the DDP local rank so not supposed to be the DDP local rank so not supposed to be the DDP local rank so not the DDP rank the local rank uh so this the DDP rank the local rank uh so this the DDP rank the local rank",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2163,
      "text": "uh so this is what you pass in here this wraps the is what you pass in here this wraps the is what you pass in here this wraps the model and in particular what DDP does model and in particular what DDP does model and in particular what DDP does for you is in a forward pass it actually for you is in a forward pass it actually for you is in a forward pass it actually behaves identically",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2164,
      "text": "so um my behaves identically",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2165,
      "text": "so um my behaves identically",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2166,
      "text": "so um my understanding of it is nothing should be understanding of it is nothing should be understanding of it is nothing should be changed in the forward pass but in the changed in the forward pass but in the changed in the forward pass but in the backward pass as you are doing the backward pass as you are doing the backward pass as you are doing the backward pass um in the simpl setting backward pass um in the simpl setting backward pass um in the simpl setting once the backp passes over on each once the backp passes over on each once the backp passes over on each independent GPU each independent GPU has independent GPU each independent GPU has independent GPU each independent GPU has the gradient for all the parameters and the gradient for all the parameters and the gradient for all the parameters and what DDP does for you is once the what DDP does for you is once the what DDP does for you is once the backward pass is over it will call backward pass is over it will call backward pass is over it will call what's called all reduce and it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2167,
      "text": "what's called all reduce and it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2168,
      "text": "what's called all reduce and it basically does an average across all the basically does an average across all the basically does an average across all the uh ranks of their gradients and and then uh ranks of their gradients and and then uh ranks of their gradients and and then it will deposit that average on every it will deposit that average on every it will deposit that average on every single rank so every sing Single rank single rank so every sing Single rank single rank so every sing Single rank will end up with the average on it and will end up with the average on it and will end up with the average on it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2169,
      "text": "and so basically that's the communication it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2170,
      "text": "so basically that's the communication it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2171,
      "text": "so basically that's the communication it just synchronizes and averages the just synchronizes and averages the just synchronizes and averages the gradients and that's what DDP offers you gradients and that's what DDP offers you gradients and that's what DDP offers you now DDP actually is a little bit more um now DDP actually is a little bit more um now DDP actually is a little bit more um it is a little bit more involved than it is a little bit more involved than it is a little bit more involved than that because as you are doing the that because as you are doing the that because as you are doing the backward pass through the layers of the backward pass through the layers of the backward pass through the layers of the Transformer it actually can dispatch Transformer it actually can dispatch Transformer it actually can dispatch Communications for the gradient while Communications for the gradient while Communications for the gradient while the backward pass is still happening so the backward pass is still happening so the backward pass is still happening so there's overlap of the uh communication there's overlap of the uh communication there's overlap of the uh communication of the gradient and the synchronization of the gradient and the synchronization of the gradient and the synchronization of them and uh the backward pass and uh of them and uh the backward pass and uh of them and uh the backward pass and uh this is just more efficient and um uh to this is just more efficient and um uh to this is just more efficient and um uh to do it that way",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2172,
      "text": "so that's what DDP does do it that way",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2173,
      "text": "so that's what DDP does do it that way",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2174,
      "text": "so that's what DDP does for you um forward is unchanged and for you um forward is unchanged and for you um forward is unchanged and backward is mostly unchanged and we're backward is mostly unchanged and we're backward is mostly unchanged and we're tacking on this average as we'll see in tacking on this average as we'll see in tacking on this average as we'll see in a bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2175,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2176,
      "text": "so now let's go to the uh a bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2177,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2178,
      "text": "so now let's go to the uh a bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2179,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2180,
      "text": "so now let's go to the uh optimization nothing here changes let's optimization nothing here changes let's optimization nothing here changes let's go to the optimization here the inner go to the optimization here the inner go to the optimization here the inner loop and think through the loop and think through the loop and think through the synchronization of uh these gradients in synchronization of uh these gradients in synchronization of uh these gradients in the DP so basically by default what the DP so basically by default what the DP so basically by default what happens as I mentioned is when you do l. happens as I mentioned is when you do l. happens as I mentioned is when you do l. backward here it will do the backward backward here it will do the backward backward here it will do the backward pass and then it will synchronize the pass and then it will synchronize the pass and then it will synchronize the gradients um the problem here is because gradients um the problem here is because gradients um the problem here is because of the gradient accumulation steps Loop of the gradient accumulation steps Loop of the gradient accumulation steps Loop here we don't actually want to do the here we don't actually want to do the here we don't actually want to do the synchronization after every single La synchronization after every single La synchronization after every single La step backward because we are just step backward because we are just step backward because we are just depositing gradients and we're doing depositing gradients and we're doing depositing gradients and we're doing that serially and we just want them that serially",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2181,
      "text": "and we just want them that serially",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2182,
      "text": "and we just want them adding up and we don't want to adding up and we don't want to adding up and we don't want to synchronize every single time that would synchronize every single time that would synchronize every single time that would be extremely wasteful",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2183,
      "text": "so basically we be extremely wasteful",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2184,
      "text": "so basically we be extremely wasteful",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2185,
      "text": "so basically we want to add them up and then on the the want to add them up and then on the the want to add them up and then on the the very last",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2186,
      "text": "uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2187,
      "text": "it's only on the very last very last uh it's only on the very last very last uh it's only on the very last step when micro when micro step becomes step when micro when micro step becomes step when micro when micro step becomes gratak steps minus one only at that last gratak steps minus one only at that last gratak steps minus one only at that last step do we want to actually do the step do we want to actually do the step do we want to actually do the alberu uh to average up the gradients so alberu uh to average up the gradients so alberu uh to average up the gradients so to do that we come here and um the to do that we come here and um the to do that we come here and um the official sanctioned way by the way is to official sanctioned way by the way is to official sanctioned way by the way is to do this no sync context manager so do this no sync context manager so do this no sync context manager so pytorch says this is a context manager pytorch says this is a context manager pytorch says this is a context manager to disable gradient synchronization to disable gradient synchronization to disable gradient synchronization across DDP processes So within this across DDP processes So within this across DDP processes So within this context gradient will be context gradient will be context gradient will be accumulated",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2188,
      "text": "and basically when you do no accumulated and basically when you do no accumulated and basically when you do no sync there will be no communication so sync there will be no communication so sync there will be no communication so they are telling us to do with DDP",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2189,
      "text": "no they are telling us to do with DDP",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2190,
      "text": "no they are telling us to do with DDP no sync uh do the gradient accumulation sync uh do the gradient accumulation sync uh do the gradient accumulation accumulate grats and then they are accumulate grats and then they are accumulate grats and then they are asking us to do DDP again with another asking us to do DDP again with another asking us to do DDP again with another input and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2191,
      "text": "that backward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2192,
      "text": "and I just input and that backward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2193,
      "text": "and I just input and that backward",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2194,
      "text": "and I just really don't love this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2195,
      "text": "I I just really really don't love this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2196,
      "text": "I I just really really don't love this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2197,
      "text": "I I just really don't like it uh the fact that you have don't like it uh the fact that you have don't like it uh the fact that you have to copy paste your code here and use a to copy paste your code here and use a to copy paste your code here and use a context manager and this is just super context manager and this is just super context manager and this is just super ugly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2198,
      "text": "so when I went to this source code ugly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2199,
      "text": "so when I went to this source code ugly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2200,
      "text": "so when I went to this source code here you can see that when you enter here you can see that when you enter here you can see that when you enter you simply toggle this variable this you simply toggle this variable this you simply toggle this variable this require backward grat sync and this is require backward grat sync and this is require backward grat sync and this is uh being toggled around and changed and uh being toggled around and changed and uh being toggled around and changed and this is the variable that basically uh this is the variable that basically uh this is the variable that basically uh if you step through it is being toggled if you step through it is being toggled if you step through it is being toggled to determine if the gradient is going to to determine if the gradient is going to to determine if the gradient is going to be synchronized",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2201,
      "text": "so I actually just kind be synchronized",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2202,
      "text": "so I actually just kind be synchronized",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2203,
      "text": "so I actually just kind of like to use that directly uh so of like to use that directly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2204,
      "text": "uh so of like to use that directly uh so instead what I like to do is the instead what I like to do is the instead what I like to do is the following right here before the L back following right here before the L back following right here before the L back backward if we are using the DDP then um backward if we are using the DDP then um backward if we are using the DDP then um then basically we only want to then basically we only want to then basically we only want to synchronize we only want this variable synchronize we only want this variable synchronize we only want this variable to be true when it is the final to be true when it is the final to be true when it is the final iteration in all the other iterations iteration in all the other iterations iteration in all the other iterations inside the micr steps we want to be inside the micr steps we want to be inside the micr steps we want to be false",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2205,
      "text": "so I just toggle it like this so false",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2206,
      "text": "so I just toggle it like this so false",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2207,
      "text": "so I just toggle it like this so required backward graph sync should only required backward graph sync should only required backward graph sync should only turn on when the micro step is the last turn on when the micro step is the last turn on when the micro step is the last step",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2208,
      "text": "and so I'm toggling this variable step",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2209,
      "text": "and so I'm toggling this variable step",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2210,
      "text": "and so I'm toggling this variable directly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2211,
      "text": "and I hope that that impacts directly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2212,
      "text": "and I hope that that impacts directly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2213,
      "text": "and I hope that that impacts last St backwards last St backwards last St backwards and this is a naughty thing to do and this is a naughty thing to do",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2214,
      "text": "and this is a naughty thing to do because you know they could probably because you know they could probably because you know they could probably change the DDP and this variable will go change the DDP and this variable will go change the DDP and this variable will go away",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2215,
      "text": "but for now I believe this this away",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2216,
      "text": "but for now I believe this this away",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2217,
      "text": "but for now I believe this this works and it allows me to avoid the use works and it allows me to avoid the use works and it allows me to avoid the use of context managers and code duplication of context managers and code duplication of context managers and code duplication I'm just toggling the variable and then I'm just toggling the variable and then I'm just toggling the variable and then Lop backward will not synchronize most Lop backward will not synchronize most Lop backward will not synchronize most of the steps and it will synchronize the of the steps and it will synchronize the of the steps and it will synchronize the very last step and so once this is over very last step and so once this is over very last step and so once this is over uh and we come out every single um rank",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2218,
      "text": "uh and we come out every single um rank",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2219,
      "text": "uh and we come out every single um rank will suddenly magically have the average will suddenly magically have the average will suddenly magically have the average of all the gradients that were stored on of all the gradients that were stored on of all the gradients that were stored on all the ranks so now we have to think all the ranks so now we have to think all the ranks so now we have to think through whether that is what we want and through whether that is what we want and through whether that is what we want and also um if this suffices and whether how also um if this suffices and whether how also um if this suffices and whether how it works with the loss and what is loss it works with the loss and what is loss it works with the loss and what is loss AUM",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2220,
      "text": "so let's think through through that AUM so let's think through through that AUM so let's think through through that now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2221,
      "text": "and the problem I'm getting at is now and the problem I'm getting at is now and the problem I'm getting at is that we've averaged the gradients which that we've averaged the gradients which that we've averaged the gradients which is great but the loss AUM has not been is great but the loss AUM has not been is great but the loss AUM has not been impacted yet and the",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2222,
      "text": "and this is outside impacted yet and the",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2223,
      "text": "and this is outside impacted yet and the",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2224,
      "text": "and this is outside of the DDP container so that is not of the DDP container so that is not of the DDP container",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2225,
      "text": "so that is not being averaged um and so here when when being averaged um and so here when when being averaged um and so here when when we are printing Los AUM well presumably we are printing Los AUM well presumably we are printing Los AUM well presumably we're only going to be printing on the we're only going to be printing on the we're only going to be printing on the master process uh rank zero",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2226,
      "text": "and it's master process uh rank zero",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2227,
      "text": "and it's master process uh rank zero",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2228,
      "text": "and it's just going to be printing the losses just going to be printing the losses just going to be printing the losses that it saw on its process but instead that it saw on its process but instead that it saw on its process but instead we want it to print the loss over all we want it to print the loss over all we want it to print the loss over all the processes and the average of that the processes and the average of that the processes and the average of that loss because we did average of gradients loss because we did average of gradients loss because we did average of gradients so we want the average of loss as well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2229,
      "text": "so we want the average of loss as well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2230,
      "text": "so we want the average of loss as well so simply here after this uh this is the so simply here after this uh this is the so simply here after this uh this is the code that I've used in the past um and code that I've used in the past um and code that I've used in the past um and instead of LF we want instead of LF",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2231,
      "text": "we want instead of LF we want Lum",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2232,
      "text": "so if Lum so if Lum so if DDP again then this is a p torch DDP again then this is a p torch DDP again then this is a p torch distributed I import it where do I distributed I import it where do I distributed I import it where do I import import import it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2233,
      "text": "uh oh gosh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2234,
      "text": "so this file is starting it uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2235,
      "text": "oh gosh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2236,
      "text": "so this file is starting it uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2237,
      "text": "oh gosh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2238,
      "text": "so this file is starting to get out of control",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2239,
      "text": "huh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2240,
      "text": "so if uh so to get out of control",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2241,
      "text": "huh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2242,
      "text": "so if uh so to get out of control",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2243,
      "text": "huh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2244,
      "text": "so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2245,
      "text": "if uh so import torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2246,
      "text": "distributed as dist import torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2247,
      "text": "distributed as dist import torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2248,
      "text": "distributed as dist so dist.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2249,
      "text": "so dist.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2250,
      "text": "so dist.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2251,
      "text": "ALU and we're doing the average on Lum ALU and we're doing the average on Lum ALU and we're doing the average on Lum and so this lakum tensor exists on all and so this lakum tensor exists on all and so this lakum tensor exists on all the ranks when we call all use of the ranks when we call all use of the ranks when we call all use of average it creates the average of those average it creates the average of those average it creates the average of those numbers and it deposits that average on numbers and it deposits that average on numbers and it deposits that average on all the ranks so all the ranks after all the ranks so all the ranks after all the ranks so all the ranks after this um call will now contain L AUM uh this um call will now contain L AUM uh this um call will now contain L AUM uh averaged up and so when we print here on averaged up and so when we print here on averaged up",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2252,
      "text": "and so when we print here on the master process the L AUM is the master process the L AUM is the master process the L AUM is identical in all the other ranks as well identical in all the other ranks as well identical in all the other ranks as well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2253,
      "text": "so here if Master process so here if Master process so here if Master process oops we want to print like this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2254,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2255,
      "text": "and oops we want to print like this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2256,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2257,
      "text": "and oops we want to print like this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2258,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2259,
      "text": "and finally we have to be careful because finally we have to be careful because finally we have to be careful because we're not processing even more tokens so we're not processing even more tokens so we're not processing even more tokens so times DDP World size times DDP World size times DDP World size that's number of tokens that we've that's number of tokens that we've that's number of tokens that we've processed up processed up processed up above and everything else should be fine",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2260,
      "text": "uh and everything else should be fine uh the only other thing to be careful with the only other thing to be careful with the only other thing to be careful with is as I mentioned you want to destroy is as I mentioned you want to destroy is as I mentioned you want to destroy the process group so that we are nice to the process group so that we are nice to the process group so that we are nice to nickel and it's not going to uh to uh to nickel",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2261,
      "text": "and it's not going to uh to uh to nickel",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2262,
      "text": "and it's not going to uh to uh to DDP",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2263,
      "text": "and it's not going to complain to us DDP",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2264,
      "text": "and it's not going to complain to us DDP",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2265,
      "text": "and it's not going to complain to us uh when we exit uh when we exit uh when we exit here so that should be it let's try to here so that should be it let's try to here so that should be it let's try to take it for a spin",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2266,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2267,
      "text": "so I launched take it for a spin",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2268,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2269,
      "text": "so I launched take it for a spin",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2270,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2271,
      "text": "so I launched the script and it should be uh printing the script and it should be uh printing the script and it should be uh printing here imminently we're now training with here imminently we're now training with here imminently we're now training with 8 gpus at the same time so the gradient 8 gpus at the same time so the gradient 8 gpus at the same time so the gradient accumulation steps is not 32 it is now accumulation steps is not 32 it is now accumulation steps is not 32 it is now divide 8",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2272,
      "text": "and it's just four uh so um divide 8",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2273,
      "text": "and it's just four uh so um divide 8",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2274,
      "text": "and it's just four",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2275,
      "text": "uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2276,
      "text": "so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2277,
      "text": "um otherwise this is what the optimization otherwise this is what the optimization otherwise this is what the optimization now looks like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2278,
      "text": "and wow we're going now looks like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2279,
      "text": "and wow we're going now looks like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2280,
      "text": "and wow we're going really fast",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2281,
      "text": "so we're processing 1.5 really fast",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2282,
      "text": "so we're processing 1.5 really fast",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2283,
      "text": "so we're processing 1.5 million tokens uh per second now so million tokens uh per second now so million tokens uh per second now so these are some serious numbers and the these are some serious numbers and the these are some serious numbers and the tiny shakespare data set is so tiny that tiny shakespare data set is so tiny that tiny shakespare data set is so tiny that we're just doing like so many Epoch over we're just doing like so many Epoch over we're just doing like so many Epoch over it most likely",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2284,
      "text": "but this is roughly what it most likely",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2285,
      "text": "but this is roughly what it most likely",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2286,
      "text": "but this is roughly what looks like um one thing that I had to looks like um one thing that I had to looks like um one thing that I had to fix by the way is that this was model.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2287,
      "text": "fix by the way is that this was model.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2288,
      "text": "fix by the way is that this was model.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2289,
      "text": "configure optimizers which Now doesn't configure optimizers which Now doesn't configure optimizers which Now doesn't work because model now is a DDP model so work because model now is a DDP model so work because model now is a DDP model so instead this has to become raw instead this has to become raw instead this has to become raw model.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2290,
      "text": "configure optimizers where raw model.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2291,
      "text": "configure optimizers where raw model.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2292,
      "text": "configure optimizers where raw model is something I create here so model is something I create here so model is something I create here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2293,
      "text": "so right after I wrap the model into DDP uh right after I wrap the model into DDP uh right after I wrap the model into DDP uh I have to create the raw model which in I have to create the raw model which in I have to create the raw model which in the case of DDP is a model.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2294,
      "text": "module is the case of DDP is a model.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2295,
      "text": "module is the case of DDP is a model.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2296,
      "text": "module is where it stores the raw and then module where it stores the raw and then module where it stores the raw and then module of gpt2 as we have it which contains the of gpt2 as we have it which contains the of gpt2 as we have it which contains the uh configure optimizers function that we uh configure optimizers function that we uh configure optimizers function that we want to call so that's one thing that I want to call so that's one thing that I want to call so that's one thing that I have to fix otherwise this seems to run have to fix otherwise this seems to run have to fix otherwise this seems to run now one thing you'll notice is that when now one thing you'll notice is that when now one thing you'll notice is that when you actually compare this run and the you actually compare this run and the you actually compare this run and the numbers in it to the just running a numbers in it to the just running a numbers in it to the just running a single GPU you'll notice that this is single GPU you'll notice that this is single GPU you'll notice that this is single GPU run with 32 gratum the single GPU run with 32 gratum the single GPU run with 32 gratum the numbers won't exactly match numbers won't exactly match numbers won't exactly match up and uh that's kind of a boring reason up and uh that's kind of a boring reason up and uh that's kind of a boring reason for why that happens uh the reason for for why that happens uh the reason for for why that happens uh the reason for that is that in the data loader we're that is that in the data loader we're that is that in the data loader we're basically just iterating through batches basically just iterating through batches basically just iterating through batches and slightly different way because now and slightly different way because now and slightly different way because now we're looking for an entire page of data we're looking for an entire page of data we're looking for an entire page of data and if that page uh for all the gpus if and if that page uh for all the gpus if and if that page uh for all the gpus if that chunk exceeds the number of tokens that chunk exceeds the number of tokens that chunk exceeds the number of tokens we just Loop and so actually the single",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2297,
      "text": "we just Loop and so actually the single we just Loop and so actually the single GPU and the H GPU process will end up um GPU and the H GPU process will end up um GPU and the H GPU process will end up um resetting in a slightly different Manner resetting in a slightly different Manner resetting in a slightly different Manner and so our batches are slightly and so our batches are slightly and so our batches are slightly different",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2298,
      "text": "and so we get slightly different",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2299,
      "text": "and so we get slightly different",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2300,
      "text": "and so we get slightly different numbers but one way to different numbers but one way to different numbers but one way to convince yourself that this is okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2301,
      "text": "it convince yourself that this is okay it convince yourself that this is okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2302,
      "text": "it just make the total batch size much just make the total batch size much just make the total batch size much smaller and the b and a t and then um smaller and the b and a t and then um smaller and the b and a t and then um so I think I used uh 4 * 124 * 8",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2303,
      "text": "so I",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2304,
      "text": "so I think I used uh 4 * 124 * 8",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2305,
      "text": "so I",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2306,
      "text": "so I think I used uh 4 * 124 * 8",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2307,
      "text": "so I used 32768 as a total patch size and used 32768 as a total patch size and used 32768 as a total patch size",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2308,
      "text": "and then um so I made sure that the single then um so I made sure that the single then um so I made sure that the single GPU will do eight creting accumulation GPU will do eight creting accumulation GPU will do eight creting accumulation steps and then the multi-gpu and then steps and then the multi-gpu and then steps and then the multi-gpu",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2309,
      "text": "and then you're reducing the boundary effects of you're reducing the boundary effects of you're reducing the boundary effects of the data loader and you'll see that the the data loader and you'll see that the the data loader and you'll see that the numbers match up so long story short numbers match up so long story short numbers match up so long story short we're now going really really fast",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2310,
      "text": "the",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2311,
      "text": "we're now going really really fast",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2312,
      "text": "the",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2313,
      "text": "we're now going really really fast the optimization is mostly consistent with optimization is mostly consistent with optimization is mostly consistent with gpt2 and three hyper parameters and uh gpt2 and three hyper parameters and uh gpt2 and three hyper parameters and uh we have outgrown our tiny Shakespeare we have outgrown our tiny Shakespeare we have outgrown our tiny Shakespeare file and we want to upgrade it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2314,
      "text": "so let's file and we want to upgrade it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2315,
      "text": "so let's file and we want to upgrade it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2316,
      "text": "so let's move to next to that next so let's now move to next to that next so let's now move to next to that next so let's now take a look at what data sets were used take a look at what data sets were used take a look at what data sets were used by gpt2 and gpt3",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2317,
      "text": "so gbt2 used this web by gpt2 and gpt3 so gbt2 used this web by gpt2 and gpt3 so gbt2 used this web Text data set that was never released um Text data set that was never released um Text data set that was never released um there's an attempt at reproducing it there's an attempt at reproducing it there's an attempt at reproducing it called open web text uh so basically called open web text uh so basically called open web text uh so basically roughly speaking what they say here in roughly speaking what they say here in roughly speaking what they say here in the paper is that they scraped all the paper is that they scraped all the paper is that they scraped all outbound links from Reddit and then uh outbound links from Reddit and then uh outbound links from Reddit and then uh with at least three Karma and that was with at least three Karma and that was with at least three Karma and that was kind of like their starting point and kind of like their starting point and kind of like their starting point and they collected all the web P all the web they collected all the web P all the web they collected all the web P all the web pages and all the text in them and so pages and all the text in them and so pages and all the text in them and so this was 45 million links and this ended this was 45 million links and this ended this was 45 million links and this ended up being 40 GB of text",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2318,
      "text": "so uh so that's up being 40 GB of text",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2319,
      "text": "so uh so that's up being 40 GB of text",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2320,
      "text": "so uh so that's roughly what gpt2 says about its data roughly what gpt2 says about its data roughly what gpt2 says about its data set so it's basically outbound links set so it's basically outbound links set so it's basically outbound links from Reddit now when we go over to gpt3 from Reddit now when we go over to gpt3 from Reddit now when we go over to gpt3 there's a training data set section and there's a training data set section and there's a training data set section and that's where they start to talk about um that's where they start to talk about um that's where they start to talk about um common coll which is a lot more uh used common coll which is a lot more uh used common coll which is a lot more uh used actually I think even gpt2 talked about actually I think even gpt2 talked about actually I think even gpt2 talked about common coll um but basically it's not a common coll um but basically it's not a common coll um but basically it's not a very high quality data set all by itself very high quality data set all by itself very high quality data set all by itself because it is extremely noisy this is a because it is extremely noisy this is a because it is extremely noisy this is a completely random subset of the internet completely random subset of the internet completely random subset of the internet and it's much worse than you think",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2321,
      "text": "so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2322,
      "text": "and it's much worse than you think so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2323,
      "text": "and it's much worse than you think so people go into Great Lengths to filter people go into Great Lengths to filter people go into Great Lengths to filter common craw because there's good stuff common craw because there's good stuff common craw because there's good stuff in it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2324,
      "text": "but most of it is just like ad in it but most of it is just like ad in it but most of it is just like ad spam random tables and numbers and stock spam random tables and numbers and stock spam random tables and numbers and stock tickers and uh it's just total mess tickers and uh it's just total mess tickers and uh it's just total mess",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2325,
      "text": "so that's why people like to train on so that's why people like to train on so that's why people like to train on these data mixtures that they curate and these data mixtures that they curate and these data mixtures that they curate and uh are careful with so a large chunk of uh are careful with so a large chunk of uh are careful with so a large chunk of these data mixtures typically will be these data mixtures typically will be these data mixtures typically will be common C like for example 50% of the common C like for example 50% of the common C like for example 50% of the tokens will be comic but then here in tokens will be comic but then here in tokens will be comic but then here in gpt3 they're also using web text to from gpt3 they're also using web text to from gpt3 they're also using web text to from before so that's Reddit outbound",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2326,
      "text": "but before so that's Reddit outbound",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2327,
      "text": "but before so that's Reddit outbound",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2328,
      "text": "but they're also adding for example books they're also adding for example books they're also adding for example books and they're adding Wikipedia there's and they're adding Wikipedia there's and they're adding Wikipedia there's many other things you can decide to add many other things you can decide to add many other things you can decide to add now this data set for gpt3 was also now this data set for gpt3 was also now this data set for gpt3 was also never released so today some of the data never released so today some of the data never released so today some of the data sets that I'm familiar with that are sets that I'm familiar with that are sets that I'm familiar with that are quite good and would be representative quite good and would be representative quite good and would be representative of something along these lines are of something along these lines are of something along these lines are number one the red pajama data set or number one the red pajama data set or number one the red pajama data set or more specifically for example the slim more specifically for example the slim more specifically for example the slim pajama subset of the red pajama data set pajama subset of the red pajama data set pajama subset of the red pajama data set which is a cleaned and D duplicated which is a cleaned and D duplicated which is a cleaned and D duplicated version of it and just to give you a version of it and just to give you a version of it and just to give you a sense again it's a bunch of common crawl sense again it's a bunch of common crawl sense again it's a bunch of common crawl um C4 which is also as far as I know um C4 which is also as far as I know um C4 which is also as far as I know more common craw but processed more common craw but processed more common craw but processed differently",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2329,
      "text": "and then we have GitHub differently",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2330,
      "text": "and then we have GitHub differently",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2331,
      "text": "and then we have GitHub books archive Wikipedia stack exchange books archive Wikipedia stack exchange books archive Wikipedia stack exchange these are the kinds of data sets that these are the kinds of data sets that these are the kinds of data sets that would go into these data mixtures now would go into these data mixtures now would go into these data mixtures now specifically the one that I like that specifically the one that I like that specifically the one that I like that came out recently is called Fine web came out recently is called Fine web came out recently is called Fine web data set uh so this is an attempt to data set uh so this is an attempt to data set uh so this is an attempt to basically collect really high quality basically collect really high quality basically collect really high quality common coll data and filter it in this common coll data and filter it in this common coll data and filter it in this case to 15 trillion tokens and then in case to 15 trillion tokens and then in case to 15 trillion tokens and then in addition to that more recently addition to that more recently addition to that more recently huggingface released this fine web edu huggingface released this fine web edu huggingface released this fine web edu subset which is 1.3 trillion of subset which is 1.3 trillion of subset which is 1.3 trillion of educational and 5.4 trillion of high educational and 5.4 trillion of high educational and 5.4 trillion of high educational content so basically they're educational content so basically they're educational content so basically they're trying to filter common C to very high trying to filter common C to very high trying to filter common C to very high quality educational subsets and uh this quality educational subsets and uh this quality educational subsets and uh this is the one that we will use there's a is the one that we will use there's a is the one that we will use there's a long uh web page here on fine web and long uh web page here on fine web and long uh web page here on fine web and they go into a ton of detail about how they go into a ton of detail about how they go into a ton of detail about how they process the data which is really they process the data which is really they process the data which is really fascinating reading by the way and I fascinating reading by the way and I fascinating reading by the way",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2332,
      "text": "and I would definitely recommend if you're would definitely recommend if you're would definitely recommend if you're interested into Data mixtures and so on interested into Data mixtures and so on interested into Data mixtures and so on and how data gets processed at these and how data gets processed at these and how data gets processed at these scales a look at this uh page and more scales a look at this uh page and more scales a look at this uh page and more specifically we'll be working with the specifically we'll be working with the specifically we'll be working with the fine web",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2333,
      "text": "edu I think and it's basically fine web",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2334,
      "text": "edu I think and it's basically fine web",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2335,
      "text": "edu I think and it's basically educational content from the educational content from the educational content from the internet uh they show that training on internet uh they show that training on internet uh they show that training on educational content in in their metrics educational content in in their metrics educational content in in their metrics um uh works really really",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2336,
      "text": "well and we're um uh works really really",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2337,
      "text": "well and we're um uh works really really",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2338,
      "text": "well and we're going to use this sample 10 billion going to use this sample 10 billion going to use this sample 10 billion tokens subsample of it because we're not tokens subsample of it because we're not tokens subsample of it because we're not going to be training on trillions of going to be training on trillions of going to be training on trillions of tokens uh we're just going to train on tokens uh we're just going to train on tokens uh we're just going to train on uh 10 billion sample of the fine web edu uh 10 billion sample of the fine web edu uh 10 billion sample of the fine web edu because empirically in my previous few because empirically in my previous few because empirically in my previous few experiments this actually suffices to experiments this actually suffices to experiments this actually suffices to really get close to gpt2 Performance and really get close to gpt2 Performance and really get close to gpt2 Performance",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2339,
      "text": "and it's um simple enough to work with",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2340,
      "text": "and it's um simple enough to work with and it's um simple enough to work with and so let's work with the sample 10 uh BT",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2341,
      "text": "so let's work with the sample 10 uh BT so let's work with the sample 10 uh BT so our goal will be to download it so our goal will be to download it so our goal will be to download it process it and make sure that our data process it and make sure that our data process it and make sure that our data loader can work with it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2342,
      "text": "so let's get to loader can work with it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2343,
      "text": "so let's get to loader can work with it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2344,
      "text": "so let's get to that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2345,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2346,
      "text": "so I introduced another um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2347,
      "text": "that okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2348,
      "text": "so I introduced another um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2349,
      "text": "that okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2350,
      "text": "so I introduced another um file here that will basically download file here that will basically download file here that will basically download Fine web edu from huging face data sets Fine web edu from huging face data sets Fine web edu from huging face data sets it will pre-process and pre- tokenize it will pre-process and pre- tokenize it will pre-process and pre- tokenize all of the data and it will save data all of the data and it will save data all of the data and it will save data shards to a uh folder on um local disk shards to a uh folder on um local disk shards to a uh folder on um local disk and so while this is running uh just and so while this is running uh just and so while this is running uh just wanted to briefly mention that you can wanted to briefly mention that you can wanted to briefly mention that you can kind of look through the data set viewer kind of look through the data set viewer kind of look through the data set viewer here just to get a sense of what's in here just to get a sense of what's in here just to get a sense of what's in here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2351,
      "text": "and it's kind of interesting",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2352,
      "text": "I mean here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2353,
      "text": "and it's kind of interesting",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2354,
      "text": "I mean here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2355,
      "text": "and it's kind of interesting",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2356,
      "text": "I mean it's a it basically looks like it's it's a it basically looks like it's it's a it basically looks like it's working fairly well like it's talking working fairly well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2357,
      "text": "like it's talking working fairly well like it's talking about nuclear energy in France it's about nuclear energy in France",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2358,
      "text": "it's about nuclear energy in France it's talking talking talking about Mexican about Mexican about Mexican America some mac PJs",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2359,
      "text": "Etc",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2360,
      "text": "so actually it America some mac PJs",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2361,
      "text": "Etc",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2362,
      "text": "so actually it America some mac PJs",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2363,
      "text": "Etc so actually it seems like their filters are working seems like their filters are working seems like their filters are working pretty well uh the filters here by the pretty well uh the filters here by the pretty well uh the filters here by the way were applied automatically using um way were applied automatically using um way were applied automatically using um llama 370b I believe",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2364,
      "text": "and so uh basically llama 370b I believe and so uh basically llama 370b I believe",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2365,
      "text": "and so uh basically llms are judging which content is llms are judging which content is llms are judging which content is educational and that ends up making it educational and that ends up making it educational and that ends up making it through the filter uh so that's pretty through the filter uh so that's pretty through the filter uh so that's pretty cool now in terms of the script itself cool now in terms of the script itself cool now in terms of the script itself I'm not going to go through the full I'm not going to go through the full I'm not going to go through the full script because it's not as interesting script because it's not as interesting script because it's not as interesting and not as llm Centric",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2366,
      "text": "but when you run and not as llm Centric",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2367,
      "text": "but when you run and not as llm Centric",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2368,
      "text": "but when you run this basically number one we're going to this basically number one we're going to this basically number one we're going to load the data set uh which this is all load the data set uh which this is all load the data set uh which this is all huging face code running this you're huging face code running this you're huging face code running this you're going to need to uh pip install data going to need to uh pip install data going to need to uh pip install data sets",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2369,
      "text": "um so it's downloading the data set sets um so it's downloading the data set sets um so it's downloading the data set then it is tokenizing all of the then it is tokenizing all of the then it is tokenizing all of the documents inside this data set now when documents inside this data set now when documents inside this data set now when we tokenize the documents you'll notice we tokenize the documents you'll notice we tokenize the documents you'll notice that um to tokenize a single document uh that um to tokenize a single document uh that um to tokenize a single document uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2370,
      "text": "we first we first we first start the tokens with the end of text start the tokens with the end of text start the tokens with the end of text token and this is a special token in the token and this is a special token in the token and this is a special token in the gpt2 tokenizer as you know so gpt2 tokenizer as you know so gpt2 tokenizer as you know so 50256 is the ID of the end of text and 50256 is the ID of the end of text and 50256 is the ID of the end of text and this is what begins a document even this is what begins a document even this is what begins a document even though it's called end of text",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2371,
      "text": "but this though it's called end of text",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2372,
      "text": "but this though it's called end of text but this is uh the first token that begins a is uh the first token that begins a is uh the first token that begins a document then we extend with all of the document then we extend with all of the document then we extend with all of the tokens of that document then we create a tokens of that document then we create a tokens of that document then we create a numpy array out of that we make sure numpy array out of that we make sure numpy array out of that we make sure that all the tokens are between that all the tokens are between that all the tokens are between",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2373,
      "text": "oh okay let me debug this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2374,
      "text": "oh okay let me debug this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2375,
      "text": "oh okay let me debug this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2376,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2377,
      "text": "so apologies for that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2378,
      "text": "uh it just okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2379,
      "text": "so apologies for that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2380,
      "text": "uh it just okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2381,
      "text": "so apologies for that uh it just had to do with me using a float division had to do with me using a float division had to do with me using a float division in Python it must be integer division so in Python it must be integer division so in Python it must be integer division so that this is an INT and everything is that this is an INT and everything is that this is an INT and everything is nice",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2382,
      "text": "um okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2383,
      "text": "but basically the nice um okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2384,
      "text": "but basically the nice um okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2385,
      "text": "but basically the tokenization here is relatively tokenization here is relatively tokenization here is relatively straightforward returns tokens in mp.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2386,
      "text": "straightforward returns tokens in mp.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2387,
      "text": "straightforward returns tokens in mp.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2388,
      "text": "un6 uh we're using .16 to save a little un6 uh we're using .16 to save a little un6 uh we're using .16 to save a little bit of space because 2 to the 16us 1 is bit of space because 2 to the 16us 1 is bit of space because 2 to the 16us 1 is 65,000 so the gpt2 max token ID is well 65,000",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2389,
      "text": "so the gpt2 max token ID is well 65,000 so the gpt2 max token ID is well below that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2390,
      "text": "and then here there's a bunch below that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2391,
      "text": "and then here there's a bunch below that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2392,
      "text": "and then here there's a bunch of multiprocessing code and it's of multiprocessing code and it's of multiprocessing code and it's honestly not that exciting",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2393,
      "text": "so I'm not honestly not that exciting",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2394,
      "text": "so I'm not honestly not that exciting",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2395,
      "text": "so I'm not going to step through it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2396,
      "text": "but we're going to step through it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2397,
      "text": "but we're going to step through it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2398,
      "text": "but we're loading the data set we're tokenizing it loading the data set we're tokenizing it loading the data set we're tokenizing it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2399,
      "text": "and we're saving everything to shards and we're saving everything to shards and we're saving everything to shards and the shards are numpy files uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2400,
      "text": "so and the shards are numpy files uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2401,
      "text": "so and the shards are numpy files uh so just storing a numpy array and uh which just storing a numpy array and uh which just storing a numpy array and uh which is very very similar to torch is very very similar to torch is very very similar to torch tensors and the first Shard 0000 is a tensors and the first Shard 0000 is a tensors and the first Shard 0000 is a Val a validation Shard and all the other Val a validation Shard and all the other Val a validation Shard and all the other shards are uh training shards and as I shards are uh training shards and as I shards are uh training shards and as I mentioned they all have 100 million mentioned they all have 100 million mentioned they all have 100 million tokens in them exactly um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2402,
      "text": "and and that tokens in them exactly um and and that tokens in them exactly um and and that just makes it easier to work with as to just makes it easier to work with as to just makes it easier to work with as to Shard the files because if we just have Shard the files because if we just have Shard the files because if we just have a single massive file sometimes they can a single massive file sometimes they can a single massive file sometimes they can be hard to work with on the disk and so be hard to work with on the disk and so be hard to work with on the disk and so sharting it is just kind of um nicer sharting it is just kind of um nicer sharting it is just kind of um nicer from that from that from that perspective",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2403,
      "text": "and uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2404,
      "text": "yeah",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2405,
      "text": "so we'll just perspective",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2406,
      "text": "and uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2407,
      "text": "yeah",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2408,
      "text": "so we'll just perspective",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2409,
      "text": "and uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2410,
      "text": "yeah",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2411,
      "text": "so we'll just let this run this will be probably um let this run this will be probably um let this run this will be probably um 30ish minutes",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2412,
      "text": "or so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2413,
      "text": "and then we're going 30ish minutes or so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2414,
      "text": "and then we're going 30ish minutes or so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2415,
      "text": "and then we're going to come back to actually train on this to come back to actually train on this to come back to actually train on this data and we're going to be actually data",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2416,
      "text": "and we're going to be actually data",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2417,
      "text": "and we're going to be actually doing some legit pre-training in this doing some legit pre-training in this doing some legit pre-training in this case this is a good data set we're doing case this is a good data set we're doing case this is a good data set we're doing lots of tokens per second we have 8 gpus lots of tokens per second we have 8 gpus lots of tokens per second we have 8 gpus the code is ready and so we're actually the code is ready and so we're actually the code is ready and so we're actually going to be doing a serious training run going to be doing a serious training run going to be doing a serious training run so let's get P it back in a bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2418,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2419,
      "text": "so so let's get P it back in a bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2420,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2421,
      "text": "so so let's get P it back in a bit",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2422,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2423,
      "text": "so we're back",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2424,
      "text": "so uh if we LS edu fine web we're back",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2425,
      "text": "so uh if we LS edu fine web we're back",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2426,
      "text": "so uh if we LS edu fine web we see that there's now 100 charts in it we see that there's now 100 charts in it we see that there's now 100 charts in it um and that makes sense because each um and that makes sense because each um and that makes sense because each chart is 100 million tokens so 100 chart is 100 million tokens so 100 chart is 100 million tokens so 100 charts of that is 10 billion tokens in charts of that is 10 billion tokens in charts of that is 10 billion tokens in total now swinging over to the main file total now swinging over to the main file total now swinging over to the main file I made some adjustments to our data I made some adjustments to our data I made some adjustments to our data loader again and that's because we're loader again and that's because we're loader again and that's because we're not running with uh Shakespeare anymore not running with uh Shakespeare anymore not running with uh Shakespeare anymore we want to use the fine web shards and we want to use the fine web shards and we want to use the fine web shards",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2427,
      "text": "and so you'll see some code here that so you'll see some code here that so you'll see some code here that additionally basically can load these additionally basically can load these additionally basically can load these shards uh we load the um un6 numpy file shards uh we load the um un6 numpy file shards uh we load the um un6 numpy file we convert it to a torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2428,
      "text": "long tensor we convert it to a torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2429,
      "text": "long tensor we convert it to a torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2430,
      "text": "long tensor which is what a lot of the layers up top which is what a lot of the layers up top which is what a lot of the layers up top expect by default",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2431,
      "text": "and then here we're expect by default",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2432,
      "text": "and then here we're expect by default",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2433,
      "text": "and then here we're just enumerating all the shards I also just enumerating all the shards I also just enumerating all the shards I also added a split to data load of light so added a split to data load of light so added a split to data load of light so we can uh load the split train but also we can uh load the split train but also we can uh load the split train but also the split",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2434,
      "text": "Val uh the zero the split Val uh the zero the split Val uh the zero split and then we can load the shards split",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2435,
      "text": "and then we can load the shards split",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2436,
      "text": "and then we can load the shards",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2437,
      "text": "and then here we also have not just the",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2438,
      "text": "and then here we also have not just the",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2439,
      "text": "and then here we also have not just the current position now but also the current position now but also the current position now but also the current Shard",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2440,
      "text": "so we have a position current Shard",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2441,
      "text": "so we have a position current Shard",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2442,
      "text": "so we have a position inside A Shard",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2443,
      "text": "and then when we uh run inside A Shard",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2444,
      "text": "and then when we uh run inside A Shard",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2445,
      "text": "and then when we uh run out of tokens in A Single Shard we first out of tokens in A Single Shard",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2446,
      "text": "we first out of tokens in A Single Shard we first Advance",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2447,
      "text": "The Shard and loop if we need to Advance The Shard and loop if we need to Advance The Shard and loop if we need to",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2448,
      "text": "and then we get the tokens and readjust",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2449,
      "text": "and then we get the tokens and readjust",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2450,
      "text": "and then we get the tokens and readjust the position so this data loader will the position so this data loader will the position so this data loader will now iterate all the shards as well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2451,
      "text": "so I now iterate all the shards as well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2452,
      "text": "so I now iterate all the shards as well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2453,
      "text": "so I Chang that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2454,
      "text": "and then the other thing that Chang that and then the other thing that Chang that and then the other thing that I did while uh the data was processing I did while uh the data was processing I did while uh the data was processing is our train loader now has split train is our train loader now has split train is our train loader now has split train of course and down here I set up some I of course and down here I set up some I of course and down here I set up some I set up some numbers set up some numbers set up some numbers",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2455,
      "text": "so we are doing 2 to the so we are doing 2 to the so we are doing 2 to the 9 uh tokens per uh per um per step and 9 uh tokens per uh per um per step and 9 uh tokens per uh per um per step and we want to do roughly 10 billion tokens we want to do roughly 10 billion tokens we want to do roughly 10 billion tokens um because that's how many unique tokens um because that's how many unique tokens um because that's how many unique tokens we have so if we did 10 billion tokens we have so if we did 10 billion tokens we have so if we did 10 billion tokens then divide that by 29 we see that this then divide that by 29 we see that this then divide that by 29 we see that this is 1973 steps so that's where that's is 1973 steps so that's where that's is 1973 steps so that's where that's from and then the GPT three paper says from and then the GPT three paper says from and then the GPT three paper says that they warm up the learning rate over that they warm up the learning rate over that they warm up the learning rate over 375 million tokens",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2456,
      "text": "so I came here and 375 million tokens so I came here and 375 million tokens",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2457,
      "text": "so I came here and 375 E6 tokens divide uh 2 to the 375 E6 tokens divide uh 2 to the 375 E6 tokens divide uh 2 to the 19 is 715 steps so that's why warm-up 19 is 715 steps so that's why warm-up 19 is 715 steps so that's why warm-up steps is set to 715",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2458,
      "text": "so this will exactly steps is set to 715 so this will exactly steps is set to 715",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2459,
      "text": "so this will exactly match um the warm-up schedule that gpt3 match um the warm-up schedule that gpt3 match um the warm-up schedule that gpt3 used and I think 715 by the way is very used",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2460,
      "text": "and I think 715 by the way is very used",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2461,
      "text": "and I think 715 by the way is very uh mild and this could be made uh mild and this could be made uh mild and this could be made significantly more aggressive probably significantly more aggressive probably significantly more aggressive probably even like 100 is good enough um even like 100 is good enough um even like 100 is good enough",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2462,
      "text": "um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2463,
      "text": "but it's okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2464,
      "text": "let's leave it for now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2465,
      "text": "so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2466,
      "text": "but it's okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2467,
      "text": "let's leave it for now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2468,
      "text": "so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2469,
      "text": "but it's okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2470,
      "text": "let's leave it for now so that we have the exact hyper parameters that we have the exact hyper parameters that we have the exact hyper parameters of gpt3",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2471,
      "text": "so I fix that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2472,
      "text": "and then um that's of gpt3",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2473,
      "text": "so I fix that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2474,
      "text": "and then um that's of gpt3",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2475,
      "text": "so I fix that and then um that's pretty much",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2476,
      "text": "it we can we can run so we pretty much",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2477,
      "text": "it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2478,
      "text": "we can we can run so we pretty much",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2479,
      "text": "it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2480,
      "text": "we can we can run so we have our script have our script have our script here and we can here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2481,
      "text": "and we can here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2482,
      "text": "and we can launch and actually sorry let me do one launch and actually sorry let me do one launch and actually sorry let me do one more thing excuse thing excuse me for my GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2483,
      "text": "I can actually fit more me for my GPU I can actually fit more me for my GPU I can actually fit more batch size and I believe I can fat I can batch size",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2484,
      "text": "and I believe I can fat I can batch size",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2485,
      "text": "and I believe I can fat I can fit 60 4 on my GPU as a micro bash size fit 60 4 on my GPU as a micro bash size fit 60 4 on my GPU as a micro bash size so let me try that I could be misremembering but that that I could be misremembering but that means 64 * 124 per GPU and then we have means 64 * 124 per GPU and then we have means 64 * 124 per GPU and then we have a gpus so that means we would not even a gpus",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2486,
      "text": "so that means we would not even a gpus",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2487,
      "text": "so that means we would not even be doing gradient accumulation if this be doing gradient accumulation if this be doing gradient accumulation if this fits because uh this just multi fits because uh this just multi fits because uh this just multi multiplies out to uh the full total bat multiplies out to uh the full total bat multiplies out to uh the full total bat size so no gradient size so no gradient size so no gradient accumulation and that would run pretty accumulation and that would run pretty accumulation and that would run pretty quickly if that fits let's go let's go I mean if this works let's go let's go",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2488,
      "text": "I mean if this works then this is basically a serious",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2489,
      "text": "then this is basically a serious",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2490,
      "text": "then this is basically a serious pre-training run um we're not logging pre-training run um we're not logging pre-training run um we're not logging we're not evaluating the validation we're not evaluating the validation we're not evaluating the validation split we're not running any evaluations split we're not running any evaluations split we're not running any evaluations",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2491,
      "text": "yet so it's not we haven't crossed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2492,
      "text": "our yet",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2493,
      "text": "so it's not we haven't crossed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2494,
      "text": "our yet",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2495,
      "text": "so it's not we haven't crossed our te's and dotted our eyes",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2496,
      "text": "but uh if we te's and dotted our eyes",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2497,
      "text": "but uh if we te's and dotted our eyes",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2498,
      "text": "but uh if we let this run for a while we're going to let this run for a while we're going to let this run for a while we're going to actually get a pretty good model and the actually get a pretty good model and the actually get a pretty good model and the model that might even be on par with or model that might even be on par with or model that might even be on par with or better than gpt2 124 M",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2499,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2500,
      "text": "so it looks better than gpt2 124 M",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2501,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2502,
      "text": "so it looks better than gpt2 124 M",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2503,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2504,
      "text": "so it looks like everything is going great",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2505,
      "text": "we're like everything is going great",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2506,
      "text": "we're like everything is going great we're processing 1.5 million tokens per processing 1.5 million tokens per processing 1.5 million tokens per second uh everything here looks good second uh everything here looks good second uh everything here looks good we're doing 330 milliseconds per we're doing 330 milliseconds per we're doing 330 milliseconds per iteration and we have to do a total iteration and we have to do a total iteration and we have to do a total of uh where are we printing that 1973 so of uh where are we printing that 1973 so of uh where are we printing that 1973 so 19073 times 0.33 19073 times 0.33 19073 times 0.33 is this many seconds this many minutes is this many seconds this many minutes is this many seconds this many minutes",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2507,
      "text": "so this will run for 1.7 so this will run for 1.7 so this will run for 1.7 hours uh so one and a half hour run uh hours uh so one and a half hour run uh hours uh so one and a half hour run uh like this and uh we don't even have to like this and uh we don't even have to like this and uh we don't even have to use gradient accumulation which is nice use gradient accumulation which is nice use gradient accumulation which is nice and you might not have that luxury in and you might not have that luxury in and you might not have that luxury in your GPU in that case just start your GPU in that case just start your GPU in that case just start decreasing the batch size until things decreasing the batch size until things decreasing the batch size until things fit but keep it to nice fit but keep it to nice fit but keep it to nice numbers um so that's pretty exciting numbers um so that's pretty exciting numbers um so that's pretty exciting we're currently warming up the learning we're currently warming up the learning we're currently warming up the learning rate",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2508,
      "text": "so you see that it's still very low rate",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2509,
      "text": "so you see that it's still very low rate",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2510,
      "text": "so you see that it's still very low one4",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2511,
      "text": "so this will ramp up over the next one4",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2512,
      "text": "so this will ramp up over the next one4",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2513,
      "text": "so this will ramp up over the next few steps all the way to 6 e few steps all the way to 6 e few steps all the way to 6 e",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2514,
      "text": "Nega uh 4 Nega uh 4 Nega uh 4 here very cool",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2515,
      "text": "so now what I'd like to here very cool",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2516,
      "text": "so now what I'd like to here very cool",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2517,
      "text": "so now what I'd like to do is uh let's cross the T and do our do is uh let's cross the T and do our do is uh let's cross the T and do our eyes let's evaluate on the validation eyes let's evaluate on the validation eyes let's evaluate on the validation split and let's try to figure out how we split and let's try to figure out how we split and let's try to figure out how we can run evals how we can do logging how can run evals how we can do logging how can run evals how we can do logging how we can visualize our losses and all the we can visualize our losses and all the we can visualize our losses and all the good stuff",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2518,
      "text": "so let's get to that before good stuff",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2519,
      "text": "so let's get to that before good stuff",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2520,
      "text": "so let's get to that before we actually do the run",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2521,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2522,
      "text": "so I've we actually do the run",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2523,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2524,
      "text": "so I've we actually do the run",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2525,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2526,
      "text": "so I've adjusted the code so that we're adjusted the code so that we're adjusted the code so that we're evaluating on the validation split so evaluating on the validation split so evaluating on the validation split so creating the Val loader just by passing creating the Val loader just by passing creating the Val loader just by passing in Split equals Val that will basically in Split equals Val that will basically in Split equals Val that will basically create a data loader just for the uh create a data loader just for the uh create a data loader just for the uh validation validation validation Shard um the other thing I did is in the Shard um the other thing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2527,
      "text": "I did",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2528,
      "text": "is in the Shard um the other thing I did is in the data loader I introduced a new function data loader I introduced a new function data loader I introduced a new function reset which is called at init and it reset which is called at init and it reset which is called at init and it basically resets the data loader and basically resets the data loader and basically resets the data loader and that is very useful because when we come that is very useful because when we come that is very useful because when we come to the main training Loop now so this is to the main training Loop now so this is to the main training Loop now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2529,
      "text": "so this is the code that I've added and basically the code that I've added and basically the code that I've added and basically every 100th iteration including the every 100th iteration including the every 100th iteration including the zeroth iteration we put the model into zeroth iteration we put the model into zeroth iteration we put the model into evaluation mode we reset the Val loader evaluation mode we reset the Val loader evaluation mode we reset the Val loader and then um no gradients involved we're",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2530,
      "text": "and then um no gradients involved we're",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2531,
      "text": "and then um no gradients involved we're going to going to going to basically accumulate the gradients over basically accumulate the gradients over basically accumulate the gradients over say 20 steps and then average it all up say 20 steps and then average it all up say 20 steps and then average it all up and print out the validation loss and so and print out the validation loss and so and print out the validation loss",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2532,
      "text": "and so that basically is the exact same logic that basically is the exact same logic that basically is the exact same logic as the training Loop roughly but there's as the training Loop roughly but there's as the training Loop roughly but there's no loss that backward it's only no loss that backward it's only no loss that backward it's only inference we're just measuring the loss inference we're just measuring the loss inference we're just measuring the loss we're adding it up everything else we're adding it up everything else we're adding it up everything else otherwise applies and is exactly as otherwise applies and is exactly as otherwise applies and is exactly as we've seen it before and so this will we've seen it before and so this will we've seen it before and so this will print the validation laws print the validation laws print the validation laws um every 100th iteration including on um every 100th iteration including on um every 100th iteration including on the very first the very first the very first iteration uh so that's nice that will iteration uh so that's nice that will iteration uh so that's nice that will tell us some amount some a little bit tell us some amount some a little bit tell us some amount some a little bit about how much we're overfitting that about how much we're overfitting that about how much we're overfitting that said like uh we have roughly Infinity said like uh we have roughly Infinity said like uh we have roughly Infinity data so we're mostly expecting our train data so we're mostly expecting our train data",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2533,
      "text": "so we're mostly expecting our train and Val loss to be about the same",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2534,
      "text": "but and Val loss to be about the same",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2535,
      "text": "but and Val loss to be about the same but the other reason I'm kind of interested the other reason I'm kind of interested the other reason I'm kind of interested in this is because we can take the GPT in this is because we can take the GPT in this is because we can take the GPT 2124m as openi released it we can 2124m as openi released it we can 2124m as openi released it we can initialize from it and we can basically initialize from it and we can basically initialize from it and we can basically see what kind of loss it achieves on the see what kind of loss it achieves on the see",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2536,
      "text": "what kind of loss it achieves on the validation loss as well and that gives validation loss as well and that gives validation loss as well and that gives us kind of an indication as to uh how us kind of an indication as to uh how us kind of an indication as to uh how much that model would generalize to 124 much that model would generalize to 124 much that model would generalize to 124 M",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2537,
      "text": "but it's not an sorry to fine web edu",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2538,
      "text": "M",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2539,
      "text": "but it's not an sorry to fine web edu",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2540,
      "text": "M",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2541,
      "text": "but it's not an sorry to fine web edu validation split that said it's not a validation split that said it's not a validation split that said it's not a super fair comparison to gpt2 because it super fair comparison to gpt2 because it super fair comparison to gpt2 because it was trained on a very different data was trained on a very different data was trained on a very different data distribution",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2542,
      "text": "but it's still kind of like distribution",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2543,
      "text": "but it's still kind of like distribution",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2544,
      "text": "but it's still kind of like an interesting data point and in any an interesting data point and in any an interesting data point and in any case you would always want to have a case you would always want to have a case you would always want to have a validation split in a training run like validation split in a training run like validation split in a training run like this so that you can make sure that you this so that you can make sure that you this so that you can make sure that you are not um overfitting and this is are not um overfitting",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2545,
      "text": "and this is are not um overfitting",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2546,
      "text": "and this is especially a concern if we were to make especially a concern if we were to make especially a concern if we were to make more Epoch in our training data um so more Epoch in our training data um so more Epoch in our training data",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2547,
      "text": "um so for example right now we're just doing a for example right now we're just doing a for example right now we're just doing a single Epoch but if we get to a point single Epoch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2548,
      "text": "but if we get to a point single Epoch but if we get to a point where we want to train on 10 epochs or where we want to train on 10 epochs or where we want to train on 10 epochs or something like that we would be really something like that we would be really something like that we would be really careful with maybe we are memorizing careful with maybe we are memorizing careful with maybe we are memorizing that data too much if we have a big that data too much if we have a big that data too much if we have a big enough model and our validation split enough model and our validation split enough model and our validation split would be one way to tell whether that is would be one way to tell whether that is would be one way to tell whether that is happening okay and in addition to that happening okay and in addition to that happening okay and in addition to that if you remember at bottom of our script if you remember at bottom of our script if you remember at bottom of our script we had all of this orphaned code for we had all of this orphaned code for we had all of this orphaned code for sampling from way back when so I deleted sampling from way back when so I deleted sampling from way back when so I deleted that code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2549,
      "text": "and I moved it up um to here that code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2550,
      "text": "and I moved it up um to here that code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2551,
      "text": "and I moved it up um to here so once in a while we simply value so once in a while we simply value so once in a while we simply value validation validation once in a while we sample we generate once in a while we sample we generate once in a while we sample we generate samples and then uh we do that only samples and then uh we do that only samples",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2552,
      "text": "and then uh we do that only every 100 steps and we train on every every 100 steps and we train on every every 100 steps and we train on every single step",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2553,
      "text": "so that's how I have a single step so that's how I have a single step",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2554,
      "text": "so that's how I have a structure right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2555,
      "text": "and I've been structure right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2556,
      "text": "and I've been structure right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2557,
      "text": "and I've been running this for 10,000 iterations so running this for 10,000 iterations so running this for 10,000 iterations so here are some samples on neration here are some samples on neration here are some samples on neration 1,000 1,000 1,000",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2558,
      "text": "um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2559,
      "text": "hello",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2560,
      "text": "I'm a language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2561,
      "text": "and I'm um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2562,
      "text": "hello I'm a language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2563,
      "text": "and I'm um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2564,
      "text": "hello I'm a language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2565,
      "text": "and I'm not able to get more not able to get more not able to get more creative I'm a language model and creative",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2566,
      "text": "I'm a language model and creative",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2567,
      "text": "I'm a language model and languages file you're learning about languages file you're learning about languages file you're learning about here is or is the beginning of a here is or is the beginning of a here is or is the beginning of a computer computer computer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2568,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2569,
      "text": "so this is all like pretty uh this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2570,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2571,
      "text": "so this is all like pretty uh this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2572,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2573,
      "text": "so this is all like pretty uh this is still a garble",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2574,
      "text": "uh but we're only at is still a garble uh but we're only at is still a garble uh but we're only at ration 1,000 and we've only just barely ration 1,000 and we've only just barely ration 1,000 and we've only just barely reached maximum learning rate uh so this reached maximum learning rate uh so this reached maximum learning rate uh so this is still learning uh we're about to get is still learning uh we're about to get is still learning uh we're about to get some more samples coming up in 1,00",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2575,
      "text": "okay 1,00",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2576,
      "text": "okay um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2577,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2578,
      "text": "this is you know the model is um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2579,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2580,
      "text": "this is you know the model is um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2581,
      "text": "okay this is you know the model is still is still a young baby",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2582,
      "text": "okay so uh still is still a young baby",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2583,
      "text": "okay so uh still is still a young baby",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2584,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2585,
      "text": "so uh basically all of this sampling code that basically all of this sampling code that basically all of this sampling code that I've put here everything should be I've put here everything should be I've put here everything should be familiar with to you and came from familiar with to you and came from familiar with to you and came from before the only thing that I did is I before the only thing that I did is I before the only thing that I did is I created a generator object in pytorch so created a generator object in pytorch so created a generator object in pytorch so that I have a direct control over the that I have a direct control over the that I have a direct control over the sampling of the random numbers don't sampling of the random numbers don't sampling of the random numbers don't because I don't want to impact the RNG because I don't want to impact the RNG because I don't want to impact the RNG state of the random number generator state of the random number generator state of the random number generator that is the global one used for training that is the global one used for training that is the global one used for training I want this to be completely outside of I want this to be completely outside of I want this to be completely outside of the training Loop",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2586,
      "text": "and so I'm using a the training Loop and so I'm using a the training Loop and so I'm using a special sampling RNG",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2587,
      "text": "and then I make special sampling RNG",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2588,
      "text": "and then I make special sampling RNG",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2589,
      "text": "and then I make sure to seed it that every single rank sure to seed it that every single rank sure to seed it that every single rank has a different seed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2590,
      "text": "and then I pass in has a different seed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2591,
      "text": "and then I pass in has a different seed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2592,
      "text": "and then I pass in here where we sort of consumer in the here where we sort of consumer in the here where we sort of consumer in the numbers in multinomial where the numbers in multinomial where the numbers in multinomial where the sampling happens I make sure to pass in sampling happens I make sure to pass in sampling happens I make sure to pass in the generator object there otherwise the generator object there otherwise the generator object there otherwise this is identical",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2593,
      "text": "uh now the other thing this is identical uh now the other thing this is identical uh now the other thing is um you'll notice that we're running a is um you'll notice that we're running a is um you'll notice that we're running a bit slower that's because I actually had bit slower that's because I actually had bit slower that's because I actually had to disable torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2594,
      "text": "compile to get this to to disable torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2595,
      "text": "compile to get this to to disable torch.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2596,
      "text": "compile to get this to sample and um so we're running a bit sample",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2597,
      "text": "and um so we're running a bit sample",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2598,
      "text": "and um so we're running a bit slower so for some reason it works with slower so for some reason it works with slower so for some reason it works with no torch compile",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2599,
      "text": "but when I torch no torch compile but when I torch no torch compile but when I torch compile my model I get a really scary compile my model I get a really scary compile my model I get a really scary error from pytorch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2600,
      "text": "and I have no idea error from pytorch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2601,
      "text": "and I have no idea error from pytorch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2602,
      "text": "and I have no idea how to resolve it right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2603,
      "text": "so probably how to resolve it right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2604,
      "text": "so probably how to resolve it right now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2605,
      "text": "so probably by the time you see this code released by the time you see this code released by the time you see this code released or something like that maybe it's fixed or something like that maybe it's fixed or something like that maybe it's fixed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2606,
      "text": "but for now I'm just going to do end",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2607,
      "text": "but for now I'm just going to do end but for now I'm just going to do end false",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2608,
      "text": "um and I'm going to bring back false",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2609,
      "text": "um and I'm going to bring back false",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2610,
      "text": "um and I'm going to bring back toor compile and you're not going to get toor compile and you're not going to get toor compile and you're not going to get samples",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2611,
      "text": "and I I think I'll fix this samples",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2612,
      "text": "and I I think I'll fix this samples",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2613,
      "text": "and I I think I'll fix this later uh by the way um I will be later uh by the way um I will be later uh by the way um I will be releasing all this code and actually releasing all this code and actually releasing all this code and actually I've been very careful about making get I've been very careful about making get I've been very careful about making get commits every time we add something and commits every time we add something and commits every time we add something",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2614,
      "text": "and so I'm going to release the entire repo",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2615,
      "text": "so I'm going to release the entire repo",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2616,
      "text": "so I'm going to release the entire repo that starts completely from scratch all that starts completely from scratch all that starts completely from scratch all the way to uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2617,
      "text": "now and after this as well the way to uh now and after this as well the way to uh now and after this as well and so everything should be exactly and so everything should be exactly and so everything should be exactly documented in the git commit history um documented in the git commit history um documented in the git commit history um um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2618,
      "text": "and so I think that will be nice",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2619,
      "text": "so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2620,
      "text": "um and so I think that will be nice",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2621,
      "text": "so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2622,
      "text": "um and so I think that will be nice",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2623,
      "text": "so hopefully by the time you go to GitHub hopefully by the time you go to GitHub hopefully by the time you go to GitHub uh this is removed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2624,
      "text": "and it's working",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2625,
      "text": "and uh this is removed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2626,
      "text": "and it's working",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2627,
      "text": "and uh this is removed",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2628,
      "text": "and it's working",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2629,
      "text": "and I will have fixed the bug",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2630,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2631,
      "text": "so I have I will have fixed the bug",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2632,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2633,
      "text": "so I have I will have fixed the bug",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2634,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2635,
      "text": "so I have the optimization running here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2636,
      "text": "and it's the optimization running here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2637,
      "text": "and it's the optimization running here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2638,
      "text": "and it's stepping",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2639,
      "text": "and we're on step 6,000 or so stepping",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2640,
      "text": "and we're on step 6,000 or so stepping and we're on step 6,000 or so",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2641,
      "text": "so we're about 30% through training now so we're about 30% through training now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2642,
      "text": "so we're about 30% through training now while this is training I would like to while this is training I would like to while this is training I would like to introduce one evaluation that we're introduce one evaluation that we're introduce one evaluation that we're going to use to supplement the going to use to supplement the going to use to supplement the validation set",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2643,
      "text": "and that is the H swag validation set",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2644,
      "text": "and that is the H swag validation set",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2645,
      "text": "and that is the H swag eval so hos swag comes from this paper eval so hos swag comes from this paper eval so hos swag comes from this paper back in 2019 so it's a 5-year-old eval back in 2019 so it's a 5-year-old eval back in 2019 so it's a 5-year-old eval now and the way H swag works is there is now and the way H swag works is there is now and the way H swag works is there is basically a sentence completion data set basically a sentence completion data set basically a sentence completion data set so it's a multiple choice for every one",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2646,
      "text": "so it's a multiple choice for every one so it's a multiple choice for every one of these questions we have uh basically of these questions we have uh basically of these questions we have uh basically a shared context like a woman is outside a shared context like a woman is outside a shared context like a woman is outside with a bucket and a dog the dog is with a bucket and a dog the dog is with a bucket and a dog the dog is running around trying to avoid bath she running around trying to avoid bath she running around trying to avoid bath she a Rises the bucket off with soap and a Rises the bucket off with soap and a Rises the bucket off with soap and blow dry the dog's head B uses a hose to blow dry the dog's head B uses a hose to blow dry the dog's head B uses a hose to keep it from getting soapy C gets the keep it from getting soapy C gets the keep it from getting soapy C gets the dog wet and it runs away again or D gets dog wet",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2647,
      "text": "and it runs away again or D gets dog wet and it runs away again or D gets into a bathtub with the dog into a bathtub with the dog into a bathtub with the dog",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2648,
      "text": "and so basically the idea is that these and so basically the idea is that these and so basically the idea is that these multiple choice are constructed so that multiple choice are constructed so that multiple choice are constructed so that one of them is a natural continuation of one of them is a natural continuation of one of them is a natural continuation of the um sentence and the others are the um sentence and the others are the um sentence and the others are not and uh the others might not make not and uh the others might not make not and uh the others might not make sense like uses the host to keep it from sense like uses the host to keep it from sense like uses the host to keep it from getting soaped that makes no sense and getting soaped that makes no sense and getting soaped that makes no sense",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2649,
      "text": "and so what happens is that models that are so what happens is that models that are so what happens is that models that are not trained very well are not able to not trained very well are not able to not trained very well are not able to tell these apart but models that have a tell these apart but models that have a tell these apart but models that have a lot of World Knowledge and can tell uh lot of World Knowledge and can tell uh lot of World Knowledge and can tell uh which um and can tell a lot about the which um and can tell a lot about the which um and can tell a lot about the world will be able to create these world will be able to create these world will be able to create these completions and these sentences are completions and these sentences are completions and these sentences are sourced from activity net and from Wiki sourced from activity net and from Wiki sourced from activity net and from Wiki how and at the bottom of the uh how and at the bottom of the uh how and at the bottom of the uh paper there's kind of like a cool chart paper there's kind of like a cool chart paper there's kind of like a cool chart of the kinds of domains in Wiki house",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2650,
      "text": "so of the kinds of domains in Wiki house so of the kinds of domains in Wiki house so there's a lot of sentences from there's a lot of sentences from there's a lot of sentences from computers and electronics and Homes and computers and electronics and Homes and computers and electronics and Homes and Garden and it has kind of a broad Garden and it has kind of a broad Garden and it has kind of a broad coverage of the kinds of things you need coverage of the kinds of things you need coverage of the kinds of things you need to know about the world in order to find to know about the world in order to find to know about the world in order to find the most likely completion and um the the most likely completion and um the the most likely completion and um the identity of that of that completion one identity of that of that completion one identity of that of that completion one more thing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2651,
      "text": "that's kind of interesting more thing that's kind of interesting more thing that's kind of interesting about H swag is the way it was about H swag",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2652,
      "text": "is the way it was about H swag is the way it was constructed is that the incorrect um constructed is that the incorrect um constructed is that the incorrect um options are deliberately um options are deliberately um options are deliberately um adversarially sourced so they're not adversarially sourced so they're not adversarially sourced so they're not just random sentences they're actually just random sentences they're actually just random sentences they're actually sentences generated by language models sentences generated by language models sentences generated by language models and they're generated in such a way that and they're generated in such a way that and they're generated in such a way that language models basically find them language models basically find them language models basically find them difficult but humans find them easy and difficult but humans find them easy and difficult but humans find them easy",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2653,
      "text": "and so they mentioned that humans have a 95% so they mentioned that humans have a 95% so they mentioned that humans have a 95% accuracy on this set but at the time the accuracy on this set but at the time the accuracy on this set but at the time the state-of-the-art language models had state-of-the-art language models had state-of-the-art language models had only 48%",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2654,
      "text": "and so at the time this was a only 48% and so at the time this was a only 48% and so at the time this was a good Benchmark now you can read the good Benchmark now you can read the good Benchmark now you can read the details of this paper to to learn more details of this paper to to learn more details of this paper to to learn more um the thing to point out though is that um the thing to point out though is that um the thing to point out though is that this is 5 years ago and since then what this is 5 years ago and since then what this is 5 years ago and since then what happened to H swag is that it's been happened to H swag is that it's been happened to H swag is that it's been totally just uh totally just uh totally just uh um solved",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2655,
      "text": "and so now the language models um solved",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2656,
      "text": "and so now the language models um solved",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2657,
      "text": "and so now the language models here are 96%",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2658,
      "text": "so basically the 4% the here are 96%",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2659,
      "text": "so basically the 4% the here are 96%",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2660,
      "text": "so basically the 4% the last 4% is probably errors in the data last 4% is probably errors in the data last 4% is probably errors in the data set or the questions are really really set or the questions are really really set or the questions are really really hard",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2661,
      "text": "and so basically this data set is hard",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2662,
      "text": "and so basically this data set is hard",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2663,
      "text": "and so basically this data set is kind of crushed with respect to language kind of crushed with respect to language kind of crushed with respect to language models but back then the best language models",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2664,
      "text": "but back then the best language models but back then the best language model was only at about 50% uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2665,
      "text": "but this model was only at about 50% uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2666,
      "text": "but this model was only at about 50% uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2667,
      "text": "but this is how far things got but still the the is how far things got",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2668,
      "text": "but still the the is how far things got",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2669,
      "text": "but still the the reason people like H swag",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2670,
      "text": "and it's not reason people like H swag",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2671,
      "text": "and it's not reason people like H swag",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2672,
      "text": "and it's not used by the way in gpt2 but in gpt3 used by the way in gpt2 but in gpt3 used by the way in gpt2 but in gpt3 there is H swag eval and lots of people there is H swag eval and lots of people there is H swag eval and lots of people use H use H use H swag",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2673,
      "text": "and so for gpt3 we have results swag and so for gpt3 we have results swag",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2674,
      "text": "and so for gpt3 we have results here here that are cited so we know",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2675,
      "text": "what percent that are cited so we know what percent that are cited",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2676,
      "text": "so we know what percent accuracies gpt3 um attains at all these accuracies gpt3 um attains at all these accuracies gpt3 um attains at all these different model checkpoints for H swag different model checkpoints for H swag different model checkpoints for H swag eval and the reason people like it is eval and the reason people like it is eval and the reason people like it is because H swag is a smooth eval and it because H swag is a smooth eval and it because H swag is a smooth eval and it is an eval that offers quote unquote is an eval that offers quote unquote is an eval that offers quote unquote early signal uh so early signal means early signal uh so early signal means early signal uh so early signal means that even small language models are that even small language models are that even small language models are going to start at the random chance of going to start at the random chance of going to start at the random chance of 25% but they're going to slowly improve 25% but they're going to slowly improve 25% but they're going to slowly improve and you're going to see 25 26 27 Etc and and you're going to see 25 26 27 Etc and and you're going to see 25 26 27 Etc and uh you can see slow Improvement even uh you can see slow Improvement even uh you can see slow Improvement even when the models are very small and it's when the models are very small and it's when the models are very small and it's very early",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2677,
      "text": "so it's smooth",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2678,
      "text": "it has early very early so it's smooth",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2679,
      "text": "it has early very early so it's smooth",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2680,
      "text": "it has early signal and um it's been around for a signal",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2681,
      "text": "and um it's been around for a signal",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2682,
      "text": "and um it's been around for a long time so that's why people kind of long time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2683,
      "text": "so that's why people kind of long time",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2684,
      "text": "so that's why people kind of like this like this like this eval uh now the way that we're going to eval uh now the way that we're going to eval uh now the way that we're going to evaluate this is as evaluate this is as evaluate this is as follows as I mentioned we have a shared follows as I mentioned we have a shared follows as I mentioned we have a shared context and this is kind of like a context and this is kind of like a context and this is kind of like a multiple choice task but instead of multiple choice task but instead of multiple choice task but",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2685,
      "text": "instead of giving the model a multiple choice giving the model a multiple choice giving the model a multiple choice question and asking it for A B C or D uh question and asking it for A B C or D uh question and asking it for A B C or D uh we can't do that because these models we can't do that because these models we can't do that because these models when they are so small as we are seeing when they are so small as we are seeing when they are so small as we are seeing here the models can't actually do here the models can't actually do here the models can't actually do multiple choice they don't understand multiple choice they don't understand multiple choice they don't understand the concept of associating a label to the concept of associating a label to the concept of associating a label to one of the options of multiple choice uh one of the options of multiple choice uh one of the options of multiple choice uh they don't understand that so we have to they don't understand that so we have to they don't understand that so we have to give it to them in a native form and the give it to them in a native form and the give it to them in a native form and the native form is a token completion so native form is a token completion so native form is a token completion so here's what we do we construct a batch here's what we do we construct a batch here's what we do we construct a batch of four rows and uh T tokens whatever of four rows and uh T tokens whatever of four rows and uh T tokens whatever that t happens to be then the shared that t happens to be then the shared that t happens to be then the shared context that is basically the context context that is basically the context context that is basically the context for the for choices the tokens of that for the for choices the tokens of that for the for choices the tokens of that are shared across all of the rows and are shared across all of the rows and are shared across all of the rows and then we have the four options",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2686,
      "text": "so we kind then we have the four options",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2687,
      "text": "so we kind then we have the four options",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2688,
      "text": "so we kind of like lay them out and then only one of like lay them out and then only one of like lay them out and then only one of the options is correct in this case of the options is correct in this case of the options is correct in this case label three option three and so um this label three option three and so um this label three option three and so um this is the correct option and option one two is the correct option and option one two is the correct option and option one two and for are and for are and for are incorrect now these options might be of incorrect now these options might be of incorrect now these options might be of different lengths",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2689,
      "text": "so what we do is we different lengths so what we do is we different lengths",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2690,
      "text": "so what we do is we sort of like take the longest length and sort of like take the longest length and sort of like take the longest length and that's the size of the batch B BYT and that's the size of the batch B BYT and that's the size of the batch B BYT and then some of these uh here are going to then some of these uh here are going to then some of these uh here are going to be pded Dimensions so they're going to be pded Dimensions so they're going to be pded Dimensions so they're going to be unused and so we need the tokens we be unused and so we need the tokens we be unused and so we need the tokens we need the correct label and we need a need the correct label",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2691,
      "text": "and we need a need the correct label",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2692,
      "text": "and we need a mask that tells us which tokens are mask that tells us which tokens are mask that tells us which tokens are active and the mask is then zero for active and the mask is then zero for active and the mask is then zero for these uh padded areas so that's how we these uh padded areas so that's how we these uh padded areas so that's how we construct these batches and then in construct these batches and then in construct these batches and then in order to get the language model to order to get the language model to order to get the language model to predict A B C or D the way this works is predict A B C or D the way this works is predict A B C or D the way this works is basically we're just going to look at basically we're just going to look at basically we're just going to look at the tokens their probabilities",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2693,
      "text": "and we're the tokens their probabilities and we're the tokens their probabilities and we're going to pick the option that gets the going to pick the option that gets the going to pick the option that gets the lowest or the highest average lowest or the highest average lowest or the highest average probability for the token so for the probability for the token so for the probability for the token so for the tokens because that is the most likely tokens because that is the most likely tokens because that is the most likely completion according to the language completion according to the language completion according to the language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2694,
      "text": "so we're just going to look at the model so we're just going to look at the model so we're just going to look at the um probabilities here and average them um probabilities here and average them um probabilities here and average them up across the options and pick the one up across the options and pick the one up across the options and pick the one with the highest probability roughly with the highest probability roughly with the highest probability roughly speaking so this is how we're going to speaking so this is how we're going to speaking so this is how we're going to do H swag",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2695,
      "text": "do H swag",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2696,
      "text": "do H swag um and this is I believe also",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2697,
      "text": "how uh um and this is I believe also how uh um and this is I believe also how uh gpt3 did it um this is how gpt3 did it gpt3 did it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2698,
      "text": "um this is how gpt3 did it gpt3 did it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2699,
      "text": "um this is how gpt3 did it as far as I know",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2700,
      "text": "but you should note as far as I know",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2701,
      "text": "but you should note as far as I know",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2702,
      "text": "but you should note that some of the other evals where you that some of the other evals where you that some of the other evals where you might see H swag may not do it this way might see H swag may not do it this way might see H swag may not do it this way they may do it in a multiple choice they may do it in a multiple choice they may do it in a multiple choice format where you sort of uh give the the format where you sort of uh give the the format where you sort of uh give the the context a single time and then the four context a single time and then the four context a single time and then the four completions and so the model is able to completions and so the model is able to completions and so the model is able to see all the four options before it picks see all the four options before it picks see all the four options before it picks the best possible option and that's the best possible option and that's the best possible option and that's actually an easier task for a model actually an easier task for a model actually an easier task for a model because you get to see the other options because you get to see the other options because you get to see the other options when you're picking your choice",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2703,
      "text": "um but when you're picking your choice um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2704,
      "text": "but when you're picking your choice um but unfortunately models at our size can't unfortunately models at our size can't unfortunately models at our size can't do that only models at a bigger size are do that only models at a bigger size are do that only models at a bigger size are able to do that and so our models are able to do that and so our models are able to do that and so our models are actually slightly handicapped in this actually slightly handicapped in this actually slightly handicapped in this way that they are not going to see the way that they are not going to see the way that they are not going to see the other options they're only going to see other options they're only going to see other options they're only going to see one option at a time and they just have one option at a time and they just have one option at a time and they just have to assign probabilities and the correct to assign probabilities and the correct to assign probabilities and the correct option has to win out in this metric all option has to win out in this metric all option has to win out in this metric all right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2705,
      "text": "so let's now implement this very right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2706,
      "text": "so let's now implement this very right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2707,
      "text": "so let's now implement this very briefly and incorporate it into our briefly and incorporate it into our briefly and incorporate it into our script",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2708,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2709,
      "text": "so what I've done here is script",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2710,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2711,
      "text": "so what I've done here is script",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2712,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2713,
      "text": "so what I've done here is I've introduced a new file called hell I've introduced a new file called hell I've introduced a new file called hell swag.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2714,
      "text": "py",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2715,
      "text": "that you can take a look into swag.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2716,
      "text": "py",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2717,
      "text": "that you can take a look into swag.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2718,
      "text": "py that you can take a look into",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2719,
      "text": "and I'm not going to to step through all",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2720,
      "text": "and I'm not going to to step through all",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2721,
      "text": "and I'm not going to to step through all of it because uh this is not exactly of it because uh this is not exactly of it because uh this is not exactly like deep code deep code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2722,
      "text": "it's kind of like deep code deep code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2723,
      "text": "it's kind of like deep code deep code",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2724,
      "text": "it's kind of like a little bit tedious honestly like a little bit tedious honestly like a little bit tedious honestly because what's happening is I'm because what's happening is I'm because what's happening is I'm downloading hsac from GitHub and I'm downloading hsac from GitHub and I'm downloading hsac from GitHub and I'm rendering all of its examples and there rendering all of its examples and there rendering all of its examples and there are a total of 10,000 examples I am are a total of 10,000 examples I am are a total of 10,000 examples I am rendering them into this format um and rendering them into this format um and rendering them into this format um and so here at the end of this render so here at the end of this render so here at the end of this render example function you can see that I'm example function you can see that I'm example function you can see that I'm returning the returning the returning the tokens uh the tokens of this um 4xt tokens uh the tokens of this um 4xt tokens uh the tokens of this um 4xt uh array of Tokens The Mask which tells uh array of Tokens The Mask which tells uh array of Tokens The Mask which tells us which parts are the options and us which parts are the options and us which parts are the options and everything else is zero and the label everything else is zero and the label everything else is zero and the label that is the correct label and so that that is the correct label and so that that is the correct label and so that allows us to then iterate the examples allows us to then iterate the examples allows us to then iterate the examples and render them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2725,
      "text": "and I have an evaluate and render them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2726,
      "text": "and I have an evaluate and render them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2727,
      "text": "and I have an evaluate function here which can load a um gpt2 function here which can load a um gpt2 function here which can load a um gpt2 from huging face and it runs the eval from huging face and it runs the eval from huging face and it runs the eval here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2728,
      "text": "um and it basically just calculates here um and it basically just calculates here um and it basically just calculates uh just as I described it predicts the uh just as I described it predicts the uh just as I described it predicts the option that has the lowest or the option that has the lowest or the option that has the lowest or the highest prob ility and the way to do highest prob ility and the way to do highest prob ility and the way to do that actually is we can basically that actually is we can basically that actually is we can basically evaluate the cross entropy loss",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2729,
      "text": "so we're evaluate the cross entropy loss so we're evaluate the cross entropy loss",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2730,
      "text": "so we're basically evaluating the loss of basically evaluating the loss of basically evaluating the loss of predicting the next token in a sequence predicting the next token in a sequence predicting the next token in a sequence and then we're looking at the row",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2731,
      "text": "that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2732,
      "text": "and then we're looking at the row that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2733,
      "text": "and then we're looking at the row that has the lowest average loss and that's has the lowest average loss and that's has the lowest average loss and that's the uh option that we pick as the the uh option that we pick as the the uh option that we pick as the prediction",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2734,
      "text": "and then we do some stats and prediction",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2735,
      "text": "and then we do some stats and prediction",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2736,
      "text": "and then we do some stats and prints and stuff like that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2737,
      "text": "so that is a prints and stuff like that so that is a prints and stuff like that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2738,
      "text": "so that is a way to evaluate L swag now if you go up way to evaluate L swag now if you go up way to evaluate L swag now if you go up here I'm showing that for GPT 2124m if here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2739,
      "text": "I'm showing that for GPT 2124m if here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2740,
      "text": "I'm showing that for GPT 2124m if you run this script you're going to see you run this script you're going to see you run this script you're going to see that H swag gets that H swag gets that H swag gets 29.5% um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2741,
      "text": "so that's the performance we 29.5% um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2742,
      "text": "so that's the performance we 29.5% um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2743,
      "text": "so that's the performance we get here now remember that random Chan get here now remember that random Chan get here now remember that random Chan is 25% so we haven't gone too far and is 25% so we haven't gone too far and is 25% so we haven't gone too far and gpt2 XL which is the biggest the gpt2 gpt2 XL which is the biggest the gpt2 gpt2 XL which is the biggest the gpt2 gets all the way up to 49% roughly so uh gets all the way up to 49% roughly so uh gets all the way up to 49% roughly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2744,
      "text": "so uh these are pretty low values considering these are pretty low values considering these are pretty low values considering that today's state-ofthe-art is more that today's state-ofthe-art is more that today's state-ofthe-art is more like 95%",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2745,
      "text": "uh so these are definitely like 95% uh so these are definitely like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2746,
      "text": "95% uh so these are definitely older models by now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2747,
      "text": "and then there's one older models by now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2748,
      "text": "and then there's one older models by now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2749,
      "text": "and then there's one more thing called Uther harness which is more thing called Uther harness which is more thing called Uther harness which is a very piece of infrastructure for a very piece of infrastructure for a very piece of infrastructure for running evals for language models and running evals for language models and running evals for language models and they get slightly different numbers and they get slightly different numbers and they get slightly different numbers",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2750,
      "text": "and I'm not 100% sure what the discrepancy I'm not 100% sure what the discrepancy I'm not 100% sure what the discrepancy is for these um it could be that they is for these um it could be that they is for these um it could be that they actually do the multiple choice uh actually do the multiple choice uh actually do the multiple choice uh instead of just the completions and that instead of just the completions and that instead of just the completions and that could be the um uh the discrepancy but could be the um uh the discrepancy but could be the um uh the discrepancy",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2751,
      "text": "but I'm not 100% sure about that i' have to I'm not 100% sure about that i' have to I'm not 100% sure about that i' have to take a look but for now our script take a look",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2752,
      "text": "but for now our script take a look but for now our script reports 2955 and so that is the number reports 2955 and so that is the number reports 2955 and so that is the number that we'd like to beat if we are that we'd like to beat if we are that we'd like to beat if we are training a GPD 2124m from scratch and training a GPD 2124m from scratch and training a GPD 2124m from scratch and ourselves um ourselves um ourselves um so now I'm going to go into actually",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2753,
      "text": "so now I'm going to go into actually",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2754,
      "text": "so now I'm going to go into actually incorporating this eval into our main incorporating this eval into our main incorporating this eval into our main training script and um and basically training script and um and basically training script and um and basically because we want to evaluate it in a because we want to evaluate it in a because we want to evaluate it in a periodic manner so that we can track H periodic manner so that we can track H periodic manner so that we can track H swag and how it evolves over time and swag and how it evolves over time and swag and how it evolves over time and see when when and if we cross uh this see when when and if we cross uh this see when when and if we cross uh this 2955 um sort of region so let's now walk 2955 um sort of region",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2755,
      "text": "so let's now walk 2955 um sort of region so let's now walk through some of the changes to train through some of the changes to train through some of the changes to train gpt2 thatp the first thing I did here is gpt2 thatp the first thing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2756,
      "text": "I did here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2757,
      "text": "is gpt2 thatp the first thing I did here is I actually made use compile optional I actually made use compile optional I actually made use compile optional kind of and I disabled it by default and kind of",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2758,
      "text": "and I disabled it by default and kind of and I disabled it by default and the problem with that is the problem the problem with that is the problem the problem with that is the problem with compile is that unfortunately it with compile is that unfortunately it with compile is that unfortunately it does make our code faster but it does make our code faster but it does make our code faster",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2759,
      "text": "but it actually breaks the evaluation code and actually breaks the evaluation code and actually breaks the evaluation code and the sampling code it gives me a very the sampling code it gives me a very the sampling code it gives me a very gnarly message",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2760,
      "text": "and I don't know why so gnarly message",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2761,
      "text": "and I don't know why so gnarly message",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2762,
      "text": "and I don't know why so hopefully by the time you get to the hopefully by the time you get to the hopefully by the time you get to the codebase when I put it up on GitHub uh codebase when I put it up on GitHub uh codebase when I put it up on GitHub uh we're going to fix that by then",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2763,
      "text": "but for we're going to fix that by then",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2764,
      "text": "but for we're going to fix that by then",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2765,
      "text": "but for now I'm running without torch compile now I'm running without torch compile",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2766,
      "text": "now I'm running without torch compile which is why you see this be a bit which is why you see this be a bit which is why you see this be a bit slower so we're running without torch slower",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2767,
      "text": "so we're running without torch slower",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2768,
      "text": "so we're running without torch compile I also create cre a log compile I also create cre a log compile I also create cre a log directory log where we can place our directory log where we can place our directory log where we can place our log.txt which will record the train loss log.txt which will record the train loss log.txt which will record the train loss validation loss and the H swag validation loss and the H swag validation loss and the H swag accuracies so a very simple text file accuracies so a very simple text file accuracies so a very simple text file and we're going to uh open for writing and we're going to uh open for writing and we're going to uh open for writing so that it sort of starts empty and then so that it sort of starts empty and then so that it sort of starts empty and then we're going to append to we're going to append to we're going to append to it I created a simple variable that um it I created a simple variable that um it I created a simple variable that um helps tell us when we have a last step helps tell us when we have a last step helps tell us when we have a last step and then basically periodically inside and then basically periodically inside and then basically periodically inside this Loop every 250th iteration or at this Loop every 250th iteration or at this Loop every 250th iteration or at the last step we're going to evaluate the last step we're going to evaluate the last step we're going to evaluate the validation loss and then every 250th the validation loss and then every 250th the validation loss and then every 250th iteration um we are going to evaluate H iteration um we are going to evaluate H iteration um we are going to evaluate H swag but only if we are not using swag but only if we are not using swag but only if we are not using compile because compile breaks it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2769,
      "text": "so I'm compile because compile breaks it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2770,
      "text": "so I'm compile because compile breaks it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2771,
      "text": "so I'm going to come back to this code for going to come back to this code for going to come back to this code for evaluating H swag in a second and then evaluating H swag in a second and then evaluating H swag in a second",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2772,
      "text": "and then every 250th iteration as well we're also every 250th iteration as well we're also every 250th iteration as well we're also going to sample from the model and so going to sample from the model and so going to sample from the model and so you should recognize this as our ancient you should recognize this as our ancient you should recognize this as our ancient code from way back when we started the code from way back when we started the code from way back when we started the video and we're just sampling from the video and we're just sampling from the video and we're just sampling from the model model model and then finally here um these are if and then finally here um these are if and then finally here um these are if we're not after we validate sample and we're not after we validate sample and we're not after we validate sample and evaluate hell swag we actually do a evaluate hell swag we actually do a evaluate hell swag we actually do a training step here and so this is one training step here and so this is one training step here and so this is one step of uh training and you should be step of uh training and you should be step of uh training and you should be pretty familiar with all of what this pretty familiar with all of what this pretty familiar with all of what this does and at the end here once we get our does and at the end here once we get our does and at the end here once we get our training laws we write it to the file so training laws we write it to the file so training laws we write it to the file so the only thing that changed that I the only thing that changed that I the only thing that changed that I really added is this entire section for really added is this entire section for really added is this entire section for H swag eval and the way this works is H swag eval and the way this works is H swag eval and the way this works is I'm trying to get all the gpus to I'm trying to get all the gpus to I'm trying to get all the gpus to collaborate on the H swag",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2773,
      "text": "and so we're collaborate on the H swag",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2774,
      "text": "and so we're collaborate on the H swag",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2775,
      "text": "and so we're iterating all the examples and then each iterating all the examples and then each iterating all the examples and then each process only picks the examples that process only picks the examples that process only picks the examples that assigned to it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2776,
      "text": "so we sort of take I and assigned to it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2777,
      "text": "so we sort of take I and assigned to it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2778,
      "text": "so we sort of take I and moded by the world size and we have to moded by the world size and we have to moded by the world size and we have to make it equal to rank",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2779,
      "text": "otherwise we make it equal to rank otherwise we make it equal to rank otherwise we continue and then we render an example continue and then we render an example continue and then we render an example put it on the GPU we get the low jits put it on the GPU we get the low jits put it on the GPU we get the low jits",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2780,
      "text": "then I create a helper function that then I create a helper function that then I create a helper function that helps us basically predict the option helps us basically predict the option helps us basically predict the option with the lowest loss so this comes here with the lowest loss so this comes here with the lowest loss so this comes here the prediction and then if it's correct the prediction and then if it's correct the prediction and then if it's correct we sort of keep count and then if we sort of keep count and then if we sort of keep count and then if multiple processes were collaborating on multiple processes were collaborating on multiple processes were collaborating on all this then we need to synchronize all this then we need to synchronize all this then we need to synchronize their stats and so the way one way to do their stats and so the way one way to do their stats and so the way one way to do that is to package up our statistics that is to package up our statistics that is to package up our statistics here into tensors which we can then call here into tensors which we can then call here into tensors which we can then call this.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2781,
      "text": "alberon and this.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2782,
      "text": "alberon and this.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2783,
      "text": "alberon and sum",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2784,
      "text": "and then here we sort of um unwrap sum",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2785,
      "text": "and then here we sort of um unwrap sum",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2786,
      "text": "and then here we sort of um unwrap them from tensors so that we just have them from tensors so that we just have them from tensors so that we just have ins and then here the master process ins and then here the master process ins and then here the master process will print and log the hellis swag will print and log the hellis swag will print and log the hellis swag accuracy accuracy accuracy",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2787,
      "text": "so that's kind of the that's kind of it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2788,
      "text": "so that's kind of the that's kind of it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2789,
      "text": "so that's kind of the that's kind of it and that's what I'm running right here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2790,
      "text": "and that's what I'm running right here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2791,
      "text": "and that's what I'm running right here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2792,
      "text": "so you see this optimization here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2793,
      "text": "and uh so you see this optimization here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2794,
      "text": "and uh so you see this optimization here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2795,
      "text": "and uh we just had a generation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2796,
      "text": "and this is we just had a generation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2797,
      "text": "and this is we just had a generation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2798,
      "text": "and this is Step 10,000 out of about 20,000 right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2799,
      "text": "so Step 10,000 out of about 20,000 right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2800,
      "text": "so Step 10,000 out of about 20,000 right",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2801,
      "text": "so we are halfway done and these are the we are halfway done and these are the we are halfway done and these are the kinds of samples that uh we are getting kinds of samples that uh we are getting kinds of samples that uh we are getting at this stage so let's take a look hello at this stage",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2802,
      "text": "so let's take a look hello at this stage so let's take a look",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2803,
      "text": "hello I'm a language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2804,
      "text": "so I'd like to use I'm a language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2805,
      "text": "so I'd like to use I'm a language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2806,
      "text": "so I'd like to use it to generate some kinds of output it to generate some kinds of output it to generate some kinds of output",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2807,
      "text": "hello I'm a language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2808,
      "text": "and I'm a",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2809,
      "text": "hello I'm a language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2810,
      "text": "and I'm a",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2811,
      "text": "hello I'm a language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2812,
      "text": "and I'm a developer for a lot of developer for a lot of developer for a lot of companies Al language companies Al language companies Al language model uh let's see if I can find fun model uh let's see if I can find fun model uh let's see if I can find fun",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2813,
      "text": "one",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2814,
      "text": "um I don't know you can go through this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2815,
      "text": "um I don't know you can go through this yourself but certainly the predictions yourself but certainly the predictions yourself but certainly the predictions are getting less and less random uh it are getting less and less random uh it are getting less and less random uh it seems like the model is a little bit seems like the model is a little bit seems like the model is a little bit more self-aware and using language uh more self-aware and using language uh more self-aware and using language uh that is a bit that is a bit that is a bit more uh specific to it being language more uh specific to it being language more uh specific to it being language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2816,
      "text": "hello I'm a language model and model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2817,
      "text": "hello I'm a language model and model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2818,
      "text": "hello I'm a language model and like how the language is used to like how the language is used to like how the language is used to communicate I'm a language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2819,
      "text": "and I'm communicate I'm a language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2820,
      "text": "and I'm communicate I'm a language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2821,
      "text": "and I'm going to be speaking English and German going to be speaking English and German going to be speaking English and German",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2822,
      "text": "okay I don't know so let's just wait",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2823,
      "text": "okay I don't know so let's just wait",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2824,
      "text": "okay I don't know so let's just wait until this optimization finishes and uh until this optimization finishes and uh until this optimization finishes and uh we'll see what kind of samples we get we'll see what kind of samples we get we'll see what kind of samples we get",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2825,
      "text": "and we're also going to look at the",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2826,
      "text": "and we're also going to look at the",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2827,
      "text": "and we're also going to look at the train Val and the hway accuracy and see train Val and the hway accuracy and see train Val and the hway accuracy and see how we're doing with respect to how we're doing with respect to how we're doing with respect to gpt2",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2828,
      "text": "okay good morning so focusing For a gpt2 okay good morning so focusing For a gpt2",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2829,
      "text": "okay good morning so focusing For a Moment On The jupyter Notebook here on Moment On The jupyter Notebook here on Moment On The jupyter Notebook here on the right I created a new cell that the right I created a new cell that the right I created a new cell that basically allows us to visualize the the basically allows us to visualize the the basically allows us to visualize the the train Val and Hela and um the hel score train Val and Hela and um the hel score train Val and Hela and um the hel score and you can step through this it and you can step through this it and you can step through this it basically like parses the log file that basically like parses the log file that basically like parses the log file that we are writing and um a lot of this is we are writing and um a lot of this is we are writing and um a lot of this is just like boring ma plot lip code but just like boring ma plot lip code but just like boring ma plot lip code but basically this is what our optimization basically this is what our optimization basically this is what our optimization looks like looks like looks like so we ran for 19,731 billion tokens which is whoops",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2830,
      "text": "oh 19,731 billion tokens which is whoops",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2831,
      "text": "oh my gosh which is one Epoch of the sample my gosh which is one Epoch of the sample my gosh which is one Epoch of the sample 10B of webd on the left we have the loss 10B of webd on the left we have the loss 10B of webd on the left we have the loss and the in blue we have the training and the in blue we have the training and the in blue we have the training loss in Orange we have the validation loss in Orange we have the validation loss in Orange we have the validation loss and red as a horizontal line we loss and red as a horizontal line we loss and red as a horizontal line we have the opening IG gpt2 124 M model have the opening IG gpt2 124 M model have the opening IG gpt2 124 M model checkpoint when it's just evaluated on checkpoint when it's just evaluated on checkpoint when it's just evaluated on the validation set of um of this fine the validation set of um of this fine the validation set of um of this fine web edu",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2832,
      "text": "uh so you can see that we are web edu",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2833,
      "text": "uh so you can see that we are web edu",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2834,
      "text": "uh so you can see that we are surpassing this orange is below the red surpassing this orange is below the red surpassing this orange is below the red so we're surpassing the validation set so we're surpassing the validation set so we're surpassing the validation set of this data set",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2835,
      "text": "and like I mentioned of this data set",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2836,
      "text": "and like I mentioned of this data set",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2837,
      "text": "and like I mentioned the data set distribution is very the data set distribution is very the data set distribution is very different from what gpt2 trained on so different from what gpt2 trained on so different from what gpt2 trained on so this is not an exactly fair comparison this is not an exactly fair comparison this is not an exactly fair comparison but it's a good cross check uh to uh to but it's a good cross check uh to uh to but it's a good cross check uh to uh to look at now we would ideally like look at now we would ideally like look at now we would ideally like something that is withheld and something that is withheld and something that is withheld and comparable and somewhat standard um and comparable and somewhat standard um and comparable and somewhat standard um and so for us that is helis swag and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2838,
      "text": "so on so for us that is helis swag and so on so for us that is helis swag",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2839,
      "text": "and so on here we see the H swag progress we made here we see the H swag progress we made here we see the H swag progress we made from 25% all the way here in red we see from 25% all the way here in red we see from 25% all the way here in red we see the open gpt2 124 M model in red",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2840,
      "text": "so it the open gpt2 124 M model in red",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2841,
      "text": "so it the open gpt2 124 M model in red so it achieves this h bag here and the the achieves this h bag here and the the achieves this h bag here and the the gpt3 model 124 M which was trained on gpt3 model 124 M which was trained on gpt3 model 124 M which was trained on 300 billion tokens achieves green so 300 billion tokens achieves green so 300 billion tokens achieves green so that's over here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2842,
      "text": "so you see that we that's over here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2843,
      "text": "so you see that we that's over here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2844,
      "text": "so you see that we basically surpassed the gbt2 24m uh basically surpassed the gbt2 24m uh basically surpassed the gbt2 24m uh model right here uh which is uh really model right here uh which is uh really model right here uh which is uh really nice now interestingly we were able to nice now interestingly we were able to nice now interestingly we were able to do so with only training on 10 billion do so with only training on 10 billion do so with only training on 10 billion tokens while gpt2 was trained on 100 tokens while gpt2 was trained on 100 tokens while gpt2 was trained on 100 billion tokens so uh for some reason we billion tokens so uh for some reason we billion tokens",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2845,
      "text": "so uh for some reason we were able to get away with significantly were able to get away with significantly were able to get away with significantly fewer tokens for training there are many fewer tokens for training there are many fewer tokens for training there are many possibilities to as to why we could possibilities to as to why we could possibilities to as to why we could match or surpass this accuracy um with match or surpass this accuracy um with match or surpass this accuracy um with only 10 million training so number one only 10 million training so number one only 10 million training so number one um it could be that opening gbt2 was um it could be that opening gbt2 was um it could be that opening gbt2 was trained on a much wider data trained on a much wider data trained on a much wider data distribution so in particular fine web distribution so in particular fine web distribution so in particular fine web edu is all English",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2846,
      "text": "it's not multilingual edu is all English",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2847,
      "text": "it's not multilingual edu is all English",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2848,
      "text": "it's not multilingual and there's not that much math and code and there's not that much math and code and there's not that much math and code um and so math and code and multilingual um and so math and code and multilingual um and so math and code and multilingual could have been stealing capacity from could have been stealing capacity from could have been stealing capacity from the original gpt2 model and um basically the original gpt2 model and um basically the original gpt2 model and um basically that could be partially the reason why that could be partially the reason why that could be partially the reason why uh this is not working out there's many uh this is not working out there's many uh this is not working out there's many other reasons um so for example the H other reasons um so for example the H other reasons um so for example the H swag eval is fairly old uh maybe 5 years swag eval is fairly old uh maybe 5 years swag eval is fairly old uh maybe 5 years",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2849,
      "text": "or so it is possible that aspects of H",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2850,
      "text": "or so it is possible that aspects of H",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2851,
      "text": "or so it is possible that aspects of H swag in some way or even identically swag in some way or even identically swag in some way or even identically have made it into the training",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2852,
      "text": "Set uh of have made it into the training Set uh of have made it into the training Set uh of fine web we don't know for sure but if fine web we don't know for sure but if fine web we don't know for sure but if that was the case then we are basically that was the case then we are basically that was the case then we are basically looking at the training curve instead of looking at the training curve instead of looking at the training curve instead of the validation curve so long story short the validation curve so long story short the validation curve so long story short this is not a perfect eval and there's this is not a perfect eval and there's this is not a perfect eval and there's some caveats here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2853,
      "text": "uh but at least we some caveats here uh but at least we some caveats here uh but at least we have some confidence that that we're not have some confidence that that we're not have some confidence that that we're not doing something completely wrong and doing something completely wrong and doing something completely wrong and um and uh it's probably the case that um and uh it's probably the case that um and uh it's probably the case that when people try to create these data when people try to create these data when people try to create these data sets they try to make sure that test sets they try to make sure that test sets they try to make sure that test sets that are very common are not part sets that are very common are not part sets that are very common are not part of the training set for example uh when of the training set for example uh when of the training set for example uh when hugging face created the fine web BDU hugging face created the fine web BDU hugging face created the fine web BDU they use H swag as an eval",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2854,
      "text": "so I would they use H swag as an eval so I would they use H swag as an eval so I would hope that they make sure that they D hope that they make sure that they D hope that they make sure that they D duplicate",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2855,
      "text": "and that there's no hella swag duplicate and that there's no hella swag duplicate and that there's no hella swag in the training set but we can't be sure in the training set but we can't be sure in the training set but we can't be sure uh the other thing I wanted to address uh the other thing I wanted to address uh the other thing I wanted to address briefly is look at this loss curve this briefly is look at this loss curve this briefly is look at this loss curve this looks really this looks really wrong looks really this looks really wrong looks really this looks really wrong here I don't actually know 100% what here I don't actually know 100% what here I don't actually know 100% what this is and I suspect it's because the this is",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2856,
      "text": "and I suspect it's because the this is",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2857,
      "text": "and I suspect it's because the uh 10 billion sample of fine web edu was uh 10 billion sample of fine web edu was uh 10 billion sample of fine web edu was not properly shuffled um and there's not properly shuffled um and there's not properly shuffled um and there's some issue here uh with the data that I some issue here uh with the data that I some issue here uh with the data that I don't fully understand yet",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2858,
      "text": "and there's don't fully understand yet and there's don't fully understand yet and there's some weird periodicity to it um and some weird periodicity to it um and some weird periodicity to it um and because we are in a very lazy way sort because we are in a very lazy way sort because we are in a very lazy way sort of serializing all the tokens and just of serializing all the tokens and just of serializing all the tokens and just iterating all them from scratch without iterating all them from scratch without iterating all them from scratch without doing any permutation or any random doing any permutation or any random doing any permutation or any random sampling ourselves",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2859,
      "text": "I think we're sampling ourselves I think we're sampling ourselves I think we're inheriting some of the ordering that inheriting some of the ordering that inheriting some of the ordering that they have in the data set",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2860,
      "text": "so uh this is they have in the data set",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2861,
      "text": "so uh this is they have in the data set",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2862,
      "text": "so uh this is not ideal but",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2863,
      "text": "hopefully by the time you not ideal but hopefully by the time you not ideal but hopefully by the time you get to this repo uh some of these things get to this repo uh some of these things get to this repo uh some of these things by the way will hopefully be fixed and I by the way will hopefully be fixed and I by the way will hopefully be fixed and I will release this build n GPT repo and will release this build n GPT repo and will release this build n GPT repo and right now it looks a little ugly and right now it looks a little ugly and right now it looks a little ugly and preliminary",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2864,
      "text": "uh so hopefully by the time preliminary uh so hopefully by the time preliminary uh so hopefully by the time you get here it's nicer but down here you get here",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2865,
      "text": "it's nicer but down here you get here it's nicer but down here I'm going to show aada",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2866,
      "text": "and I'm going to I'm going to show aada",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2867,
      "text": "and I'm going to I'm going to show aada",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2868,
      "text": "and I'm going to talk about about some of the things that talk about about some of the things that talk about about some of the things that happened after the video",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2869,
      "text": "and I expect happened after the video",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2870,
      "text": "and I expect happened after the video",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2871,
      "text": "and I expect that we will have fixed uh the small that we will have fixed uh the small that we will have fixed uh the small issue uh but for now basically this issue uh but for now basically this issue uh but for now basically this shows that uh our training is not uh shows that uh our training is not uh shows that uh our training is not uh completely wrong and it shows that uh completely wrong and it shows that uh completely wrong and it shows that uh we're able to surpass the accuracy with we're able to surpass the accuracy with we're able to surpass the accuracy with only 10x the token budget um and only 10x the token budget um and only 10x the token budget",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2872,
      "text": "um and possibly it could be also that the data possibly it could be also that the data possibly it could be also that the data set may have improved so uh the original set may have improved so uh the original set may have improved so uh the original uh gpt2 data set was web text",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2873,
      "text": "it's uh gpt2 data set was web text",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2874,
      "text": "it's uh gpt2 data set was web text it's possible that not a lot of care and possible that not a lot of care and possible that not a lot of care and attention went into the data set this attention went into the data set this attention went into the data set this was very early in llms whereas now was very early in llms whereas now was very early in llms whereas now there's a lot more scrutiny on good there's a lot more scrutiny on good there's a lot more scrutiny on good practices around uh D duplication practices around uh D duplication practices around uh D duplication filtering uh quality filtering and so on filtering uh quality filtering and so on filtering uh quality filtering and so on and it's possible that the data that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2875,
      "text": "and it's possible that the data that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2876,
      "text": "and it's possible that the data that we're training on is just of higher we're training on is just of higher we're training on is just of higher quality per token and that could be quality per token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2877,
      "text": "and that could be quality per token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2878,
      "text": "and that could be giving us a boost as well so a number of giving us a boost as well so a number of giving us a boost as well so a number of cave has to think about but for now uh cave has to think about but for now uh cave has to think about but for now uh we're pretty happy with this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2879,
      "text": "um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2880,
      "text": "and yeah we're pretty happy with this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2881,
      "text": "um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2882,
      "text": "and yeah we're pretty happy with this um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2883,
      "text": "and yeah now the next thing I was interested in now the next thing I was interested in now the next thing I was interested in is as you see it's a morning now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2884,
      "text": "so is as you see it's a morning now",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2885,
      "text": "so is as you see it's a morning now so there was an overnight",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2886,
      "text": "and I wanted to there was an overnight",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2887,
      "text": "and I wanted to there was an overnight",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2888,
      "text": "and I wanted to basically see how far I could push the basically see how far I could push the basically see how far I could push the result so uh to do an overnight run I result so uh to do an overnight run I result so uh to do an overnight run I basically did instead of one Epoch which basically did instead of one Epoch which basically did instead of one Epoch which took roughly two hours I just did a took roughly two hours",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2889,
      "text": "I just did a took roughly two hours I just did a times four so that that would take eight times four so that that would take eight times four so that that would take eight hours while I was sleeping",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2890,
      "text": "and so we did hours while I was sleeping",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2891,
      "text": "and so we did hours while I was sleeping",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2892,
      "text": "and so we did four Epoch or roughly 40 billion uh four Epoch or roughly 40 billion uh four Epoch or roughly 40 billion uh tokens of training and I was trying to tokens of training",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2893,
      "text": "and I was trying to tokens of training",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2894,
      "text": "and I was trying to see how far we could get um and so this see how far we could get um and so this see how far we could get um and so this was the only change",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2895,
      "text": "and I reran the was the only change",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2896,
      "text": "and I reran the was the only change",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2897,
      "text": "and I reran the script and when I point uh and read the script and when I point uh and read the script and when I point uh and read the log file at uh at the 40b uh this is log file at uh at the 40b uh this is log file at uh at the 40b uh this is what the curve look what the curve look what the curve look like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2898,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2899,
      "text": "so to narrate this number one like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2900,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2901,
      "text": "so to narrate this number one like",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2902,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2903,
      "text": "so to narrate this number one we are seeing this issue here here with we are seeing this issue here here with we are seeing this issue here here with the periodicity through the different the periodicity through the different the periodicity through the different Epoch and something really weird with Epoch and something really weird with Epoch and something really weird with the fine web edu data set and that is to the fine web edu data set and that is to the fine web edu data set and that is to be determined uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2904,
      "text": "but otherwise we are be determined uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2905,
      "text": "but otherwise we are be determined uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2906,
      "text": "but otherwise we are seeing that the H swag actually went up seeing that the H swag actually went up seeing that the H swag actually went up by a lot",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2907,
      "text": "and we almost we almost made it by a lot",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2908,
      "text": "and we almost we almost made it by a lot",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2909,
      "text": "and we almost we almost made it uh to the GPT 324m accuracy uh up here uh to the GPT 324m accuracy uh up here uh to the GPT 324m accuracy uh up here uh but not quite so uh it's too bad that uh but not quite so uh it's too bad that uh but not quite so uh it's too bad that I didn't sleep slightly longer",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2910,
      "text": "um and uh I didn't sleep slightly longer um and uh I didn't sleep slightly longer um and uh I think if this was an uh five Epoch run I think if this was an uh five Epoch run I think if this was an uh five Epoch run we may have gotten here now one thing to we may have gotten here now one thing to we may have gotten here now one thing to point out is that if you're doing multi point out is that if you're doing multi point out is that if you're doing multi Epoch runs uh we're not actually being Epoch runs",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2911,
      "text": "uh we're not actually being Epoch runs uh we're not actually being very careful in our data loader and very careful in our data loader and very careful in our data loader and we're not um I this data loader goes we're not um I this data loader goes we're not um I this data loader goes through the data in exactly the same through the data in exactly the same through the data in exactly the same format and exactly the same order and format and exactly the same order and format and exactly the same order and this is kind of suboptimal and you would this is kind of suboptimal and you would this is kind of suboptimal and you would want to look into extensions where you want to look into extensions where you want to look into extensions where you actually permute the data uh randomly actually permute the data uh randomly actually permute the data uh randomly you permute the documents around in you permute the documents around in you permute the documents around in Every Single Shard on every single new Every Single Shard on every single new Every Single Shard on every single new Epoch um and po even permute the Epoch um and po even permute the Epoch um and po even permute the shards and that would go a long way into shards and that would go a long way into shards and that would go a long way into decreasing the pricity and it's also decreasing the pricity and it's also decreasing the pricity and it's also better for the optimization so that better for the optimization so that better for the optimization so that you're not seeing things ident in the you're not seeing things ident in the you're not seeing things ident in the identical format and you're introducing identical format and you're introducing identical format and you're introducing some of the some uh Randomness in how some of the some uh Randomness in how some of the some uh Randomness in how the documents follow each other because the documents follow each other because the documents follow each other because you have to remember that in every you have to remember that in every you have to remember that in every single row these documents follow each single row these documents follow each single row these documents follow each other and then there's the end of text other",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2912,
      "text": "and then there's the end of text other",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2913,
      "text": "and then there's the end of text token",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2914,
      "text": "and then the next document",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2915,
      "text": "so the token and then the next document so the token and then the next document so the documents are currently glued together documents are currently glued together documents are currently glued together in the exact same identical manner but in the exact same identical manner but in the exact same identical manner but we actually want to break break up the we actually want to break break up the we actually want to break break up the documents and shuffle them around documents and shuffle them around documents and shuffle them around because the order of the documents because the order of the documents because the order of the documents shouldn't matter and they shouldn't um shouldn't matter and they shouldn't um shouldn't matter and they shouldn't um basically we want to break up that basically we want to break up that basically we want to break up that dependence because it's a kind of a dependence because it's a kind of a dependence because it's a kind of a spous correlation and so our data lad is spous correlation and so our data lad is spous correlation and so our data lad is not currently doing that and that's one not currently doing that and that's one not currently doing that and that's one Improvement uh you could think of Improvement uh you could think of Improvement uh you could think of making um the other thing to point out making um the other thing to point out making um the other thing to point out is we're almost matching gpt3 accuracy is we're almost matching gpt3 accuracy is we're almost matching gpt3 accuracy with only 40 billion tokens gpt3 trained with only 40 billion tokens gpt3 trained with only 40 billion tokens gpt3 trained on 300 billion tokens so again we're on 300 billion tokens so again we're on 300 billion tokens so again we're seeing about a 10x um Improvement here seeing about a 10x um Improvement here seeing about a 10x um Improvement here with respect to learning efficiency uh with respect to learning efficiency uh with respect to learning efficiency uh the other thing I wanted to",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2916,
      "text": "and I don't the other thing I wanted to and I don't the other thing I wanted to and I don't actually know exactly what to attribute actually know exactly what to attribute actually know exactly what to attribute this to other than some of the things this to other than some of the things this to other than some of the things that I already mentioned previously for that I already mentioned previously for that I already mentioned previously for the previous run uh the other thing I the previous run uh the other thing I the previous run uh the other thing I wanted to briefly mention is uh the max wanted to briefly mention is uh the max wanted to briefly mention is uh the max LR here I saw some people already play LR here I saw some people already play LR here I saw some people already play with this a little bit in a previous with this a little bit in a previous with this a little bit in a previous related repository um and it turns out related repository um and it turns out related repository um and it turns out that you can actually almost like three that you can actually almost like three that you can actually almost like three xas",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2917,
      "text": "so it's possible that the maximum xas so it's possible that the maximum xas so it's possible that the maximum learning rate can be a lot higher and learning rate can be a lot higher and learning rate can be a lot higher and for some reason the gpt3 hyper for some reason the gpt3 hyper for some reason the gpt3 hyper parameters that we are inheriting are parameters that we are inheriting are parameters that we are inheriting are actually extremely conservative and you actually extremely conservative and you actually extremely conservative and you can actually get away with a Higher can actually get away with a Higher can actually get away with a Higher Learning rate and it would train faster Learning rate and it would train faster Learning rate and it would train faster so a lot of these hyper parameters um so a lot of these hyper parameters um so a lot of these hyper parameters um are quite tunable and feel free to play are quite tunable and feel free to play are quite tunable and feel free to play with them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2918,
      "text": "and they're probably not set with them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2919,
      "text": "and they're probably not set with them",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2920,
      "text": "and they're probably not set precisely correctly and um it's possible precisely correctly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2921,
      "text": "and um it's possible precisely correctly",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2922,
      "text": "and um it's possible that you can get away with doing this that you can get away with doing this that you can get away with doing this basically and if you wanted to exactly basically and if you wanted to exactly basically and if you wanted to exactly be faithful to gpt3 you would also want be faithful to gpt3 you would also want be faithful to gpt3 you would also want to make the following difference you'd to make the following difference you'd to make the following difference you'd want to come here and the sequence want to come here and the sequence want to come here and the sequence length of gpt3 is 2x it's 20 48 instead length of gpt3 is 2x it's 20 48 instead length of gpt3 is 2x it's 20 48 instead of 1,24",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2923,
      "text": "so you would come here change of 1,24",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2924,
      "text": "so you would come here change of 1,24",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2925,
      "text": "so you would come here change this to 248 for T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2926,
      "text": "and then if you want this to 248 for T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2927,
      "text": "and then if you want this to 248 for T",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2928,
      "text": "and then if you want the exact same number of tokens uh half the exact same number of tokens uh half the exact same number of tokens uh half a million per iteration or per step you a million per iteration or per step you a million per iteration or per step you want to then decrease this to 32 so they want to then decrease this to 32 so they want to then decrease this to 32 so they still multiply to half a mil so that still multiply to half a mil so that still multiply to half a mil so that would give your model sequence length would give your model sequence length would give your model sequence length equal to that of gpt3 and in that case equal to that of gpt3 and in that case equal to that of gpt3 and in that case basically the basically the basically the um the models would be roughly identical um the models would be roughly identical um the models would be roughly identical as far as I'm as far as I'm aware as far as I'm as far as I'm aware as far as I'm as far as I'm aware because again gpt2 and gpt3 are very because again gpt2 and gpt3 are very because again gpt2 and gpt3 are very very similar models now we can also look very similar models now we can also look very similar models now we can also look at some of the samples here from the at some of the samples here from the at some of the samples here from the model that was trained overnight",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2929,
      "text": "so this model that was trained overnight",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2930,
      "text": "so this model that was trained overnight",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2931,
      "text": "so this is is the optimization",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2932,
      "text": "and you see that here the optimization and you see that here the optimization and you see that here we stepped all the way to we stepped all the way to we stepped all the way to 76290 also or so and these are the hos 76290 also or so and these are the hos 76290 also or so and these are the hos",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2933,
      "text": "mag we achieved was 33.2 4 and these are mag we achieved was 33.2 4 and these are mag we achieved was 33.2 4 and these are some of the samples from the model and some of the samples from the model and some of the samples from the model and you can see that if you read through you can see that if you read through you can see that if you read through this and pause the video briefly you can this and pause the video briefly you can this and pause the video briefly you can see that they are a lot more coherent uh see that they are a lot more coherent uh see that they are a lot more coherent uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2934,
      "text": "so so so um and they're actually addressing the um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2935,
      "text": "and they're actually addressing the um",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2936,
      "text": "and they're actually addressing the fact that it's a language model almost fact that it's a language model almost fact that it's a language model almost so uh hello I'm a language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2937,
      "text": "and I so uh hello I'm a language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2938,
      "text": "and I so uh hello I'm a language model",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2939,
      "text": "and I try to be as accurate as try to be as accurate as try to be as accurate as possible um I'm a language model not a possible um I'm a language model not a possible um I'm a language model not a programming programming programming language",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2940,
      "text": "I know how to communicate uh I language",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2941,
      "text": "I know how to communicate uh I language",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2942,
      "text": "I know how to communicate uh I use use use Python",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2943,
      "text": "Python",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2944,
      "text": "Python um I don't know if you pause this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2945,
      "text": "and um I don't know if you pause this",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2946,
      "text": "and um I don't know if you pause this and look at it and then compare it to the look at it and then compare it to the look at it and then compare it to the one to the model that was only trained one to the model that was only trained one to the model that was only trained for 10 billion uh you will see that for 10 billion uh you will see that for 10 billion uh you will see that these are a lot more coherent and you these are a lot more coherent and you these are a lot more coherent and you can play with this uh can play with this uh can play with this uh yourself one more thing I added to The yourself one more thing I added to The yourself one more thing",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2947,
      "text": "I added to The Code by the way is this chunk of code Code by the way is this chunk of code Code by the way is this chunk of code here so basically right after we here so basically right after we here so basically right after we evaluate the validation loss if we are evaluate the validation loss if we are evaluate the validation loss if we are the master process in addition to the master process in addition to the master process in addition to logging the validation loss every 5,000 logging the validation loss every 5,000 logging the validation loss every 5,000 steps we're also going to save the steps we're also going to save the steps we're also going to save the checkpoint which is really just the checkpoint which is really just the checkpoint which is really just the state dictionary of the model and so state dictionary of the model and so state dictionary of the model and so checkpointing is nice just because uh checkpointing is nice just because uh checkpointing is nice just because uh you can save the model and later you can you can save the model and later you can you can save the model and later you can uh use it in some way if you wanted to uh use it in some way if you wanted to uh use it in some way if you wanted to resume the optimiz ation then in resume the optimiz ation then in resume the optimiz ation then in addition to saving the model we have to addition to saving the model we have to addition to saving the model we have to also save the optimizer State dict also save the optimizer State dict also save the optimizer State dict because remember that the optimizer has because remember that the optimizer has because remember that the optimizer has a few additional buffers because of adom a few additional buffers because of adom a few additional buffers because of adom",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2948,
      "text": "so it's got the m and V",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2949,
      "text": "and uh you need so it's got the m and V",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2950,
      "text": "and uh you need so it's got the m and V",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2951,
      "text": "and uh you need to also resume the optimizer properly to also resume the optimizer properly to also resume the optimizer properly you have to be careful with your RNG",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2952,
      "text": "you have to be careful with your RNG",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2953,
      "text": "you have to be careful with your RNG seeds uh random number generators and so seeds uh random number generators and so seeds uh random number generators and",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2954,
      "text": "so on so if you wanted to exactly be able on so if you wanted to exactly be able on so if you wanted to exactly be able to resume optimization you have to think to resume optimization you have to think to resume optimization you have to think through the state of the of the training through the state of the of the training through the state of the of the training process",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2955,
      "text": "but if you just want to save the process but if you just want to save the process but if you just want to save the model this is how you would do it and model this is how you would do it and model this is how you would do it and one one nice reason why you might want one one nice reason why you might want one one nice reason why you might want to do this is because you may want to to do this is because you may want to to do this is because you may want to evaluate the model a lot more carefully evaluate the model a lot more carefully evaluate the model a lot more carefully",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2956,
      "text": "so here we are only kind of like winging so here we are only kind of like winging so here we are only kind of like winging the hell swag eval",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2957,
      "text": "but you may want to the hell swag eval",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2958,
      "text": "but you may want to the hell swag eval",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2959,
      "text": "but you may want to use something um nicer like for example use something um nicer like for example use something um nicer like for example the Luther uh Luther evaluation hardness the Luther uh Luther evaluation hardness the Luther uh Luther evaluation hardness evaluation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2960,
      "text": "hardness hardness",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2961,
      "text": "um so this evaluation hardness hardness",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2962,
      "text": "um so this evaluation hardness hardness um so this is a way to also evaluate language is a way to also evaluate language is a way to also evaluate language models and um so it's possible that um models",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2963,
      "text": "and um so it's possible that um models",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2964,
      "text": "and um so it's possible that um you may want to use basically different you may want to use basically different you may want to use basically different infrastructure to more thoroughly infrastructure to more thoroughly infrastructure to more thoroughly evaluate the models on different um evaluate the models on different um evaluate the models on different um evaluations and compare it to the evaluations and compare it to the evaluations and compare it to the opening gbt2 model on many other um opening gbt2 model on many other um opening gbt2 model on many other um tasks like for example that involve math tasks like for example that involve math tasks like for example that involve math code or different languages and so on so code or different languages and so on so code or different languages and so on so this is a nice functionality to have as this is a nice functionality to have as this is a nice functionality to have as well",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2965,
      "text": "well well um and then the other thing I wanted to um and then the other thing I wanted to um and then the other thing I wanted to mention is that everything we've built mention is that everything we've built mention is that everything we've built here this is only the pre-training step here this is only the pre-training step here this is only the pre-training step",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2966,
      "text": "so um the GPT here is a it dreams so um the GPT here is a it dreams so um the GPT here is a it dreams documents it just predicts the next to documents it just predicts the next to documents it just predicts the next to you can't talk to it like you can talk you can't talk to it like you can talk you can't talk to it like you can talk to chat GPT uh chat GPT if you wanted to to chat GPT uh chat GPT if you wanted to to chat GPT uh chat GPT if you wanted to talk to the model we have to fine-tune talk to the model we have to fine-tune talk to the model we have to fine-tune it into the chat format and it's not it into the chat format and it's not it into the chat format and it's not actually like that complicated if you're actually like that complicated if you're actually like that complicated if you're looking at supervised fine-tuning or sft looking at supervised fine-tuning or sft looking at supervised fine-tuning or sft really what that means is we're just really what that means is we're just really what that means is we're just swapping out a data set into a data set swapping out a data set into a data set swapping out a data set into a data set that is a lot more conversational and that is a lot more conversational and that is a lot more conversational and there's a user assistant user assistant there's a user assistant user assistant there's a user assistant user assistant kind of structure and we just fine-tune kind of structure and we just fine-tune kind of structure and we just fine-tune on it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2967,
      "text": "and then we um we basically fill on it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2968,
      "text": "and then we um we basically fill on it",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2969,
      "text": "and then we um we basically fill in the user tokens and we sample the in the user tokens and we sample the in the user tokens and we sample the assistant tokens it's not a lot more assistant tokens it's not a lot more assistant tokens it's not a lot more deeper than that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2970,
      "text": "uh but basically we deeper than that uh but basically we deeper than that uh but basically we swap out the data set and continue swap out the data set and continue swap out the data set and continue training uh but for now we're going to training uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2971,
      "text": "but for now we're going to training uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2972,
      "text": "but for now we're going to stop at uh pre-training one more thing stop at uh pre-training one more thing stop at uh pre-training one more thing that I wanted to briefly show you is that I wanted to briefly show you is that I wanted to briefly show you is that of course what we've built up today that of course what we've built up today that of course what we've built up today was building towards nanog GPT which is was building towards nanog GPT which is was building towards nanog GPT which is this repository from earlier uh but also this repository from earlier uh but also this repository from earlier uh but also there's actually another nanog GPT there's actually another nanog GPT there's actually another nanog GPT implementation and it's hiding in a more implementation and it's hiding in a more implementation and it's hiding in a more recent project that I've been working on recent project that I've been working on recent project that I've been working on called llm Doc and lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2973,
      "text": "C is a pure Cuda called llm Doc and lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2974,
      "text": "C is a pure Cuda called llm Doc and lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2975,
      "text": "C is a pure Cuda implementation of gpt2 or gpt3 training implementation of gpt2 or gpt3 training implementation of gpt2 or gpt3 training and it just directly uses uh Cuda and is and it just directly uses uh Cuda and is and it just directly uses uh Cuda and is written as Cuda now the nanog gbt here written as Cuda now the nanog gbt here written as Cuda now the nanog gbt here acts as reference code in pytorch to the acts as reference code in pytorch to the acts as reference code in pytorch to the C implementation so we're trying to C implementation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2976,
      "text": "so we're trying to C implementation so we're trying to exactly match up the two",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2977,
      "text": "but we're exactly match up the two but we're exactly match up the two",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2978,
      "text": "but we're hoping that the C Cuda is faster and of hoping that the C Cuda is faster and of hoping that the C Cuda is faster and of course currently that seems to be the course currently that seems to be the course currently that seems to be the case um because it is a direct optimized case um because it is a direct optimized case um because it is a direct optimized implementation so train gpt2 Pi in LL implementation so train gpt2 Pi in LL implementation so train gpt2 Pi in LL M.C is basically the nanog GPT and when M.C is basically the nanog GPT and when M.C is basically the nanog GPT and when you scroll through this file you'll find you scroll through this file you'll find you scroll through this file you'll find a lot of things that very much look like a lot of things that very much look like a lot of things that very much look like um things that we've built up in this um things that we've built up in this um things that we've built up in this lecture",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2979,
      "text": "and then when you look at train lecture and then when you look at train lecture and then when you look at train gpt2 docu uh this is the C Cuda gpt2 docu uh this is the C Cuda gpt2 docu uh this is the C Cuda implementation so there's a lot of MPI implementation so there's a lot of MPI implementation so there's a lot of MPI nickel GPU Cuda nickel GPU Cuda nickel GPU Cuda cc++ and you have to be familiar with cc++",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2980,
      "text": "and you have to be familiar with cc++",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2981,
      "text": "and you have to be familiar with that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2982,
      "text": "but uh um when this is built up we that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2983,
      "text": "but uh um when this is built up we that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2984,
      "text": "but uh um when this is built up we can actually run the two side by side can actually run the two side by side can actually run the two side by side and they're going to produce the exact and they're going to produce the exact and they're going to produce the exact same results but lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2985,
      "text": "C actually runs same results but lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2986,
      "text": "C actually runs same results but lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2987,
      "text": "C actually runs faster so let's see that so on the left faster so let's see that so on the left faster so let's see that so on the left I have pytorch a nanog GPT looking thing I have pytorch a nanog GPT looking thing I have pytorch a nanog GPT looking thing on the right I have the llmc call and on the right I have the llmc call and on the right I have the llmc call",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2988,
      "text": "and here I'm going to launch the here I'm going to launch the here I'm going to launch the two both of these are going to be two both of these are going to be two both of these are going to be running on a single GPU",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2989,
      "text": "and here I'm running on a single GPU and here I'm running on a single GPU and here I'm putting the lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2990,
      "text": "C on GPU 1 and this one putting the lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2991,
      "text": "C on GPU 1 and this one putting the lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2992,
      "text": "C on GPU 1 and this one will grab uh gpu0 by default and will grab uh gpu0 by default and will grab uh gpu0 by default and then we can see here that lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2993,
      "text": "c then we can see here that lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2994,
      "text": "c then we can see here that lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2995,
      "text": "c compiled and then allocate space and compiled and then allocate space and compiled and then allocate space and it's it's stepping so stepping so stepping so basically uh meanwhile P torch is still basically uh meanwhile P torch is still basically uh meanwhile P torch is still compiling because torch compile is a bit compiling because torch compile is a bit compiling because torch compile is a bit slower here than the lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2996,
      "text": "C nbcc Cuda slower here than the lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2997,
      "text": "C nbcc Cuda slower here than the lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2998,
      "text": "C nbcc Cuda compile and so this program has already compile and so this program has already compile and so this program has already started running",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 2999,
      "text": "and uh we're still started running",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3000,
      "text": "and uh we're still started running",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3001,
      "text": "and uh we're still waiting here for torch compile now of waiting here for torch compile now of waiting here for torch compile now of course uh this is a very specific course",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3002,
      "text": "uh this is a very specific course uh this is a very specific implementation to gpt2 and 3 a pytorch implementation to gpt2 and 3 a pytorch implementation to gpt2 and 3 a pytorch is a very general neural network is a very general neural network is a very general neural network framework so they're not exactly framework so they're not exactly framework so they're not exactly comparable but if you're only interested comparable but if you're only interested comparable but if you're only interested in training gpt2 and 3 lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3003,
      "text": "C is very in training gpt2 and 3 lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3004,
      "text": "C is very in training gpt2 and 3 lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3005,
      "text": "C is very fast it takes less space it's faster to fast it takes less space it's faster to fast it takes less space it's faster to start and it's faster per start",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3006,
      "text": "and it's faster per start",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3007,
      "text": "and it's faster per step",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3008,
      "text": "and so P started to Stepping here step",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3009,
      "text": "and so P started to Stepping here step",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3010,
      "text": "and so P started to Stepping here and as you can see we're running at and as you can see we're running at and as you can see we're running at about 223,000 tokens per second here and about 223,000 tokens per second here and about 223,000 tokens per second here and about 185,000 tokens per second here um about 185,000 tokens per second here um about 185,000 tokens per second here um so quite a bit slower",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3011,
      "text": "but I don't have so quite a bit slower",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3012,
      "text": "but I don't have so quite a bit slower",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3013,
      "text": "but I don't have full confidence that I exactly squeezed full confidence that I exactly squeezed full confidence that I exactly squeezed out all the juice from the pytorch out all the juice from the pytorch out all the juice from the pytorch implementation but the important thing implementation but the important thing implementation but the important thing here is notice that if I Aline up the here is notice that if I Aline up the here is notice that if I Aline up the steps you will see that the losses and steps you will see that the losses and steps you will see that the losses and Norms that are printed between these two Norms that are printed between these two Norms that are printed between these two are are are identical so on the left we have the pie identical so on the left we have the pie identical so on the left we have the pie torch and on the right this C torch and on the right this C torch and on the right this C implementation",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3014,
      "text": "and they're the same implementation and they're the same implementation and they're the same except this one runs faster uh so that's except this one runs faster uh so that's except this one runs faster uh so that's kind of I wanted to show you also kind of I wanted to show you also kind of I wanted to show you also briefly lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3015,
      "text": "C and this is a parallel briefly lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3016,
      "text": "C and this is a parallel briefly lm.",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3017,
      "text": "C and this is a parallel implementation and it's also something implementation and it's also something implementation and it's also something that you may want to uh play with or that you may want to uh play with or that you may want to uh play with or look at and um it's kind of interesting look at and um it's kind of interesting look at and um it's kind of interesting",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3018,
      "text": "okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3019,
      "text": "so at this point I should probably okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3020,
      "text": "so at this point I should probably okay",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3021,
      "text": "so at this point I should probably start wrapping up the video because I start wrapping up the video because I start wrapping up the video because I think it's getting way longer than I think it's getting way longer than I think it's getting way longer than I anticipated",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3022,
      "text": "uh but we did Cover a lot of anticipated uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3023,
      "text": "but we did Cover a lot of anticipated uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3024,
      "text": "but we did Cover a lot of ground and we built everything from ground and we built everything from ground and we built everything from scratch so as a brief summary we were scratch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3025,
      "text": "so as a brief summary we were scratch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3026,
      "text": "so as a brief summary we were looking at the gpt2 and GPT 3 looking at the gpt2 and GPT 3 looking at the gpt2 and GPT 3 papers we were looking at how you set up papers we were looking at how you set up papers we were looking at how you set up these training runs uh and all the these training runs uh and all the these training runs uh and all the considerations involved we wrote considerations involved we wrote considerations involved we wrote everything from scratch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3027,
      "text": "and then we saw everything from scratch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3028,
      "text": "and then we saw everything from scratch",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3029,
      "text": "and then we saw that over the duration of either a that over the duration of either a that over the duration of either a 2-hour training run or an overnight run 2-hour training run or an overnight run 2-hour training run or an overnight run we can actually match the 124 million we can actually match the 124 million we can actually match the 124 million parameter checkpoints of gbt2 and gpt3 parameter checkpoints of gbt2 and gpt3 parameter checkpoints of gbt2 and gpt3 uh to a very large extent uh to a very large extent uh to a very large extent um in principle the code that we wrote um in principle the code that we wrote um in principle the code that we wrote would be able to train even bigger would be able to train even bigger would be able to train even bigger models if you have the patients or the models if you have the patients or the models if you have the patients or the Computing resources",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3030,
      "text": "uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3031,
      "text": "and so you could Computing resources uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3032,
      "text": "and so you could Computing resources uh",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3033,
      "text": "and so you could potentially think about training some of potentially think about training some of potentially think about training some of the bigger checkpoints as well um there the bigger checkpoints as well um there the bigger checkpoints as well um there are a few remaining issues to address are a few remaining issues to address are a few remaining issues to address what's happening with the loss here what's happening with the loss here what's happening with the loss here which I suspect has to do with the fine which I suspect has to do with the fine which I suspect has to do with the fine web edu data sampling uh why can't we web edu data sampling uh why can't we web edu data sampling",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3034,
      "text": "uh why can't we turn on Torch compile uh it currently turn on Torch compile uh it currently turn on Torch compile uh it currently breaks generation and H swag",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3035,
      "text": "what's up breaks generation and H swag what's up breaks generation and H swag",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3036,
      "text": "what's up with that in the data loader we should with that in the data loader we should with that in the data loader we should probably be permuting our data when we probably be permuting our data when we probably be permuting our data when we reach boundaries so there's a few more reach boundaries so there's a few more reach boundaries so there's a few more issues like that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3037,
      "text": "and I expect to be issues like that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3038,
      "text": "and I expect to be issues like that",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3039,
      "text": "and I expect to be documenting some of those over time in documenting some of those over time in documenting some of those over time in the uh build n GPT repository here which the uh build n GPT repository here which the uh build n GPT repository here which I'm going to be releasing with this I'm going to be releasing with this I'm going to be releasing with this video if you have any questions or like video if you have any questions or like video if you have any questions or like to talk about anything that we covered to talk about anything that we covered to talk about anything that we covered please go to discussions tab",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3040,
      "text": "uh so we please go to discussions tab",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3041,
      "text": "uh so we please go to discussions tab",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3042,
      "text": "uh so we can talk here uh or please go to issues can talk here uh or please go to issues can talk here uh or please go to issues or pull request pull requests um or pull request pull requests um or pull request pull requests um depending on what you'd like to depending on what you'd like to depending on what you'd like to contribute or also have a look at the uh contribute or also have a look at the uh contribute or also have a look at the uh Zero to Hero Discord and uh I'm going to Zero to Hero Discord and uh I'm going to Zero to Hero Discord and uh I'm going to be hanging out here on N GPT be hanging out here on N GPT be hanging out here on N GPT um otherwise for now I'm pretty happy um otherwise for now I'm pretty happy um otherwise for now I'm pretty happy about where we got um and I hope you about where we got um and I hope you about where we got um and I hope you enjoyed the video",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3043,
      "text": "and I will see you enjoyed the video",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3044,
      "text": "and I will see you enjoyed the video",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    },
    {
      "id": 3045,
      "text": "and I will see you later",
      "start_time": "00:00:02.350",
      "end_time": "04:01:27.800"
    }
  ]
}