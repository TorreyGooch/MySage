{
  "video_id": "zduSFxRajkE",
  "sentences": [
    {
      "id": 1,
      "text": "hi everyone so in this video I'd like us hi everyone so in this video I'd like us to cover the process of tokenization in to cover the process of tokenization in to cover the process of tokenization in large language models now you see here large language models now you see here large language models now you see here that I have a set face and that's that I have a set face",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 2,
      "text": "and that's that I have a set face",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 3,
      "text": "and that's because uh tokenization is my least because uh tokenization is my least because uh tokenization is my least favorite part of working with large favorite part of working with large favorite part of working with large language models but unfortunately it is language models",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 4,
      "text": "but unfortunately it is language models",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 5,
      "text": "but unfortunately it is necessary to understand in some detail necessary to understand in some detail necessary to understand in some detail because it it is fairly hairy gnarly and because it it is fairly hairy gnarly and because it it is fairly hairy gnarly",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 6,
      "text": "and there's a lot of hidden foot guns to be there's a lot of hidden foot guns to be there's a lot of hidden foot guns to be aware of and a lot of oddness with large aware of and a lot of oddness with large aware of and a lot of oddness with large language models typically traces back to language models typically traces back to language models typically traces back to tokenization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 7,
      "text": "so what is tokenization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 8,
      "text": "so what is tokenization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 9,
      "text": "so what is tokenization now in my previous video tokenization now in my previous video tokenization now in my previous video Let's Build GPT from scratch uh we Let's Build GPT from scratch uh we Let's Build GPT from scratch uh we actually already did tokenization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 10,
      "text": "but we actually already did tokenization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 11,
      "text": "but we actually already did tokenization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 12,
      "text": "but we did a very naive simple version of did a very naive simple version of did a very naive simple version of tokenization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 13,
      "text": "so when you go to the tokenization so when you go to the tokenization so when you go to the Google colab for that video uh you see Google colab for that video uh you see Google colab for that video uh you see here that we loaded our training set and here that we loaded our training set and here that we loaded our training set and our training set was this uh Shakespeare our training set was this uh Shakespeare our training set was this uh Shakespeare uh data set now in the beginning the uh data set now in the beginning the uh data set now in the beginning the Shakespeare data set is just a large Shakespeare data set is just a large Shakespeare data set is just a large string in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 14,
      "text": "it's just text and so string in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 15,
      "text": "it's just text and so string in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 16,
      "text": "it's just text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 17,
      "text": "and so the question is how do we plug text into the question is how do we plug text into the question is how do we plug text into large language models and in this case large language models and in this case large language models and in this case here we created a vocabulary of 65 here we created a vocabulary of 65 here we created a vocabulary of 65 possible characters that we saw occur in possible characters that we saw occur in possible characters that we saw occur in this string these were the possible this string these were the possible this string these were the possible characters and we saw that there are 65 characters and we saw that there are 65 characters and we saw that there are 65 of them",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 18,
      "text": "and then we created a a lookup of them",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 19,
      "text": "and then we created a a lookup of them",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 20,
      "text": "and then we created a a lookup table for converting from every possible table for converting from every possible table for converting from every possible character a little string piece into a character a little string piece into a character a little string piece into a token an token an token an integer so here for example we tokenized integer so here for example we tokenized integer so here for example we tokenized the string High there and we received the string High there and we received the string High there and we received this sequence of this sequence of this sequence of tokens and here we took the first 1,000 tokens and here we took the first 1,000 tokens and here we took the first 1,000 characters of our data set and we characters of our data set and we characters of our data set and we encoded it into tokens and because it is encoded it into tokens and because it is encoded it into tokens and because it is this is character level we received this is character level we received this is character level we received 1,000 tokens in a sequence so token 18 1,000 tokens in a sequence so token 18 1,000 tokens in a sequence so token 18 47 47 47",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 21,
      "text": "Etc now later we saw that the way we Etc now later we saw that the way we Etc now later we saw that the way we plug these tokens into the language plug these tokens into the language plug these tokens into the language model is by using an embedding model is by using an embedding model is by using an embedding table and so basically if we have 65 table and so basically if we have 65 table and so basically if we have 65 possible tokens then this embedding possible tokens then this embedding possible tokens then this embedding table is going to have 65 rows and table is going to have 65 rows and table is going to have 65 rows and roughly speaking we're taking the roughly speaking we're taking the roughly speaking we're taking the integer associated with every single integer associated with every single integer associated with every single sing Le token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 22,
      "text": "we're using that as a sing Le token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 23,
      "text": "we're using that as a sing Le token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 24,
      "text": "we're using that as a lookup into this table and we're lookup into this table and we're lookup into this table and we're plucking out the corresponding row and plucking out the corresponding row and plucking out the corresponding row and this row is a uh is trainable parameters this row is a uh is trainable parameters this row is a uh is trainable parameters that we're going to train using back that we're going to train using back that we're going to train using back propagation and this is the vector that propagation and this is the vector that propagation and this is the vector that then feeds into the Transformer um and then feeds into the Transformer um and then feeds into the Transformer um and that's how the Transformer Ser of that's how the Transformer Ser of that's how the Transformer Ser of perceives every single perceives every single perceives every single",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 25,
      "text": "token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 26,
      "text": "so here we had a very naive token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 27,
      "text": "so here we had a very naive token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 28,
      "text": "so here we had a very naive tokenization process that was a tokenization process that was a tokenization process that was a character level tokenizer but in character level tokenizer but in character level tokenizer but in practice in state-ofthe-art uh language practice in state-ofthe-art uh language practice in state-ofthe-art uh language models people use a lot more complicated models people use a lot more complicated models people use a lot more complicated schemes unfortunately schemes unfortunately schemes unfortunately uh for constructing these uh token uh for constructing these uh token uh for constructing these uh token vocabularies so we're not dealing on the vocabularies so we're not dealing on the vocabularies so we're not dealing on the Character level we're dealing on chunk Character level we're dealing on chunk Character level we're dealing on chunk level and the way these um character level and the way these um character level and the way these um character chunks are constructed is using chunks are constructed is using chunks are constructed is using algorithms such as for example the bik algorithms such as for example the bik algorithms such as for example the bik pair in coding algorithm which we're pair in coding algorithm which we're pair in coding algorithm which we're going to go into in detail um and cover going to go into in detail um and cover going to go into in detail um and cover in this video I'd like to briefly show in this video I'd like to briefly show in this video I'd like to briefly show you the paper that introduced a bite you the paper that introduced a bite you the paper that introduced a bite level encoding as a mechanism for level encoding as a mechanism for level encoding as a mechanism for tokenization in the context of large tokenization in the context of large tokenization in the context of large language models and I would say that language models",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 29,
      "text": "and I would say that language models",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 30,
      "text": "and I would say that that's probably the gpt2 paper and if that's probably the gpt2 paper and if that's probably the gpt2 paper and if you scroll down here to the section you scroll down here to the section you scroll down here to the section input representation this is where they input representation this is where they input representation this is where they cover tokenization the kinds of cover tokenization the kinds of cover tokenization the kinds of properties that you'd like the properties that you'd like the properties that you'd like the tokenization to have and they conclude tokenization to have and they conclude tokenization to have and they conclude here that they're going to have a here that they're going to have a here that they're going to have a tokenizer where you have a vocabulary of tokenizer where you have a vocabulary of tokenizer where you have a vocabulary of 50,2 57 possible 50,2 57 possible 50,2 57 possible tokens and the context size is going to tokens and the context size is going to tokens and the context size is going to be 1,24 tokens so in the in in the be 1,24 tokens so in the in in the be 1,24 tokens so in the in in the attention layer of the Transformer attention layer of the Transformer attention layer of the Transformer neural network neural network neural network every single token is attending to the every single token is attending to the every single token is attending to the previous tokens in the sequence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 31,
      "text": "and it's previous tokens in the sequence and it's previous tokens in the sequence and it's going to see up to 1,24 tokens so tokens going to see up to 1,24 tokens so tokens going to see up to 1,24 tokens so tokens are this like fundamental unit um the are this like fundamental unit um the are this like fundamental unit um the atom of uh large language models if you atom of uh large language models if you atom of uh large language models if you will and everything is in units of will and everything is in units of will and everything is in units of tokens everything is about tokens and tokens everything is about tokens and tokens everything is about tokens and tokenization is the process for tokenization is the process for tokenization is the process for translating strings or text into translating strings or text into translating strings or text into sequences of tokens and uh vice versa sequences of tokens and uh vice versa sequences of tokens and uh vice versa when you go into the Llama 2 paper as when you go into the Llama 2 paper as when you go into the Llama 2 paper",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 32,
      "text": "as well I can show you that when you search well I can show you that when you search well I can show you that when you search token you're going to get get 63 hits um token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 33,
      "text": "you're going to get get 63 hits um token you're going to get get 63 hits um and that's because tokens are again",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 34,
      "text": "and that's because tokens are again",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 35,
      "text": "and that's because tokens are again pervasive so here they mentioned that pervasive so here they mentioned that pervasive so here they mentioned that they trained on two trillion tokens of they trained on two trillion tokens of they trained on two trillion tokens of data and so data and so data",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 36,
      "text": "and so on so we're going to build our own on so we're going to build our own on so we're going to build our own tokenizer luckily the bite be encoding tokenizer luckily the bite be encoding tokenizer luckily the bite be encoding algorithm is not uh that super algorithm is not uh that super algorithm is not uh that super complicated and we can build it from complicated and we can build it from complicated and we can build it from scratch ourselves and we'll see exactly scratch ourselves and we'll see exactly scratch ourselves and we'll see exactly how this works before we dive into code how this works before we dive into code how this works before we dive into code I'd like to give you a brief Taste of I'd like to give you a brief Taste of I'd like to give you a brief Taste of some of the complexities that come from some of the complexities that come from some of the complexities that come from the tokenization because I just want to the tokenization because I just want to the tokenization because I just want to make sure that we motivate it make sure that we motivate it make sure that we motivate it sufficiently for why we are doing all sufficiently for why we are doing all sufficiently for why we are doing all this and why this is so gross so this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 37,
      "text": "and why this is so gross",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 38,
      "text": "so this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 39,
      "text": "and why this is so gross so tokenization is at the heart of a lot of tokenization is at the heart of a lot of tokenization is at the heart of a lot of weirdness in large language models and I weirdness in large language models and I weirdness in large language models and I would advise that you do not brush it would advise that you do not brush it would advise that you do not brush it off a lot of the issues that may look off a lot of the issues that may look off a lot of the issues that may look like just issues with the new network like just issues with the new network like just issues with the new network architecture or the large language model architecture or the large language model architecture or the large language model itself are actually issues with the itself are actually issues with the itself are actually issues with the tokenization and fundamentally Trace uh tokenization and fundamentally Trace uh tokenization and fundamentally Trace uh back to it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 40,
      "text": "so if you've noticed any back to it so if you've noticed any back to it so if you've noticed any issues with large language models can't issues with large language models can't issues with large language models can't you know not able to do spelling tasks you know not able to do spelling tasks you know not able to do spelling tasks very easily that's usually due to very easily that's usually due to very easily that's usually due to tokenization simple string processing tokenization simple string processing tokenization simple string processing can be difficult for the large language can be difficult for the large language can be difficult for the large language model to perform model to perform model to perform natively uh non-english languages can natively uh non-english languages can natively uh non-english languages can work much worse and to a large extent work much worse and to a large extent work much worse and to a large extent this is due to this is due to this is due to tokenization sometimes llms are bad at tokenization sometimes llms are bad at tokenization sometimes llms are bad at simple arithmetic also can trace be simple arithmetic also can trace be simple arithmetic also can trace be traced to traced to traced to tokenization uh gbt2 specifically would tokenization uh gbt2 specifically would tokenization uh gbt2 specifically would have had quite a bit more issues with have had quite a bit more issues with have had quite a bit more issues with python than uh future versions of it due python than uh future versions of it due python than uh future versions of it due to tokenization there's a lot of other to tokenization there's a lot of other to tokenization there's a lot of other issues maybe you've seen weird warnings issues maybe you've seen weird warnings issues maybe you've seen weird warnings about a trailing whites space this is a about a trailing whites space this is a about a trailing whites space this is a tokenization issue um tokenization issue um tokenization issue um if you had asked GPT earlier about solid if you had asked GPT earlier about solid if you had asked GPT earlier about solid gold Magikarp and what it is you would gold Magikarp and what it is you would gold Magikarp and what it is you would see the llm go totally crazy and it see the llm go totally crazy and it see the llm go totally crazy and it would start going off about a completely would start going off about a completely would start going off about a completely unrelated tangent topic maybe you've unrelated tangent topic maybe you've unrelated tangent topic maybe you've been told to use yl over Json in been told to use yl over Json in been told to use yl over Json in structure data all of that has to do structure data all of that has to do structure data all of that has to do with tokenization so basically with tokenization so basically with tokenization so basically tokenization is at the heart of many tokenization is at the heart of many tokenization is at the heart of many issues I will look back around to these issues I will look back around to these issues I will look back around to these at the end of the video but for now let at the end of the video but for now let at the end of the video but for now let me just um skip over it a little bit and me just um skip over it a little bit and me just um skip over it a little bit and let's go to this web app um the Tik let's go to this web app um the Tik let's go to this web app um the Tik tokenizer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 41,
      "text": "bell.app so I have it loaded tokenizer bell.app so I have it loaded tokenizer bell.app so I have it loaded here and what I like about this web app here and what I like about this web app here and what I like about this web app is that tokenization is running a sort is that tokenization is running a sort is that tokenization is running a sort of live in your browser in JavaScript so of live in your browser in JavaScript so of live in your browser in JavaScript so you can just type here stuff",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 42,
      "text": "hello world you can just type here stuff",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 43,
      "text": "hello world you can just type here stuff hello world and the whole string and the whole string and the whole string rokenes so here what we see on uh the rokenes so here what we see on uh the rokenes so here what we see on uh the left is a string that you put in on the left is a string that you put in on the left is a string that you put in on the right we're currently using the gpt2 right we're currently using the gpt2 right we're currently using the gpt2 tokenizer we see that this string that I tokenizer we see that this string that I tokenizer we see that this string that I pasted here is currently tokenizing into pasted here is currently tokenizing into pasted here is currently tokenizing into 300 tokens and here they are sort of uh 300 tokens and here they are sort of uh 300 tokens and here they are sort of uh shown explicitly in different colors for shown explicitly in different colors for shown explicitly in different colors for every single",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 44,
      "text": "token so for example uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 45,
      "text": "every single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 46,
      "text": "so for example uh every single token so for example uh this word tokenization became two tokens this word tokenization became two tokens this word tokenization became two tokens the token the token the token 3,642 and 1,634 the token um space is is token 318 1,634 the token um space is is token 318 so be careful on the bottom you can show so be careful on the bottom you can show so be careful on the bottom you can show white space and keep in mind that there white space and keep in mind that there white space and keep in mind that there are spaces and uh sln new line are spaces and uh sln new line are spaces and uh sln new line characters in here but you can hide them characters in here but you can hide them characters in here but you can hide them for for for clarity the token space at is token 379 clarity the token space at is token 379 clarity the token space at is token 379 the to the Token space the is 262 Etc so the to the Token space the is 262 Etc so the to the Token space the is 262 Etc so you notice here that the space is part you notice here that the space is part you notice here that the space is part of that uh token of that uh token of that uh token chunk now so this is kind of like how chunk now so this is kind of like how chunk now so this is kind of like how our English sentence broke up and that our English sentence broke up and that our English sentence broke up and that seems all well and good now now here I seems all well and good now now here I seems all well and good now now here I put in some arithmetic",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 47,
      "text": "so we see that uh put in some arithmetic",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 48,
      "text": "so we see that uh put in some arithmetic",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 49,
      "text": "so we see that uh the token 127 Plus and then token six the token 127 Plus and then token six the token 127 Plus and then token six space 6 followed by 77 so what's space 6 followed by 77",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 50,
      "text": "so what's space 6 followed by 77",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 51,
      "text": "so what's happening here is that 127 is feeding in happening here is that 127 is feeding in happening here is that 127 is feeding in as a single token into the large as a single token into the large as a single token into the large language model but the um number 677 language model but the um number 677 language model but the um number 677 will actually feed in as two separate will actually feed in as two separate will actually feed in as two separate tokens and so the large language model tokens and so the large language model tokens and so the large language model has to sort of um take account of that has to sort of um take account of that has to sort of um take account of that and process it correctly in its Network and process it correctly in its Network and process it correctly in its Network and see here 804 will be broken up into and see here 804 will be broken up into and see here 804 will be broken up into two tokens and it's is all completely two tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 52,
      "text": "and it's is all completely two tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 53,
      "text": "and it's is all completely arbitrary",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 54,
      "text": "and here I have another arbitrary",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 55,
      "text": "and here I have another arbitrary",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 56,
      "text": "and here I have another example of four-digit numbers and they example of four-digit numbers and they example of four-digit numbers and they break up in a way that they break up and break up in a way that they break up and break up in a way that they break up",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 57,
      "text": "and it's totally arbitrary sometimes you",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 58,
      "text": "it's totally arbitrary sometimes you",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 59,
      "text": "it's totally arbitrary sometimes you have um multiple digits single token have um multiple digits single token have um multiple digits",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 60,
      "text": "single token sometimes you have individual digits as sometimes you have individual digits as sometimes you have individual digits as many tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 61,
      "text": "and it's all kind of pretty many tokens and it's all kind of pretty many tokens and it's all kind of pretty arbitrary and coming out of the arbitrary and coming out of the arbitrary and coming out of the tokenizer here's another example we have tokenizer here's another example we have tokenizer here's another example we have the string egg and you see here that the string egg and you see here that the string egg and you see here that this became two this became two this became two tokens but for some reason when I say I tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 62,
      "text": "but for some reason when I say I tokens but for some reason when I say I have an egg you see when it's a space have an egg you see when it's a space have an egg you see when it's a space egg it's two token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 63,
      "text": "it's sorry it's a egg it's two token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 64,
      "text": "it's sorry it's a egg it's two token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 65,
      "text": "it's sorry it's a single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 66,
      "text": "so just egg by itself in single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 67,
      "text": "so just egg by itself in single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 68,
      "text": "so just egg by itself in the beginning of a sentence is two the beginning of a sentence is two the beginning of a sentence is two tokens but here as a space egg is tokens but here as a space egg is tokens but here as a space egg is suddenly a single token uh for the exact suddenly a single token uh for the exact suddenly a single token uh for the exact same string okay here lowercase egg same string",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 69,
      "text": "okay here lowercase egg same string",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 70,
      "text": "okay here lowercase egg turns out to be a single token and in turns out to be a single token and in turns out to be a single token and in particular notice that the color is particular notice that the color is particular notice that the color is different so this is a different token different",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 71,
      "text": "so this is a different token different",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 72,
      "text": "so this is a different token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 73,
      "text": "so this is case sensitive and of course so this is case sensitive and of course so this is case sensitive and of course a capital egg would also be different a capital egg would also be different a capital egg would also be different tokens and again um this would be two tokens and again um this would be two tokens and again um this would be two tokens arbitrarily so so for the same tokens arbitrarily so so for the same tokens arbitrarily so so for the same concept egg depending on if it's in the concept egg depending on if it's in the concept egg depending on if it's in the beginning of a sentence at the end of a beginning of a sentence at the end of a beginning of a sentence at the end of a sentence lowercase uppercase or mixed sentence lowercase uppercase or mixed sentence lowercase uppercase or mixed all this will be uh basically very all this will be uh basically very all this will be uh basically very different tokens and different IDs and different tokens and different IDs and different tokens and different IDs and the language model has to learn from raw the language model has to learn from raw the language model has to learn from raw data from all the internet text that data from all the internet text that data from all the internet text that it's going to be training on that these it's going to be training on that these it's going to be training on that these are actually all the exact same concept are actually all the exact same concept are actually all the exact same concept and it has to sort of group them in the",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 74,
      "text": "and it has to sort of group them in the",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 75,
      "text": "and it has to sort of group them in the parameters of the neural network and parameters of the neural network and parameters of the neural network and understand just based on the data understand just based on the data",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 76,
      "text": "understand just based on the data patterns that these are all very similar patterns that these are all very similar patterns that these are all very similar but maybe not almost exactly similar",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 77,
      "text": "but but maybe not almost exactly similar",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 78,
      "text": "but but maybe not almost exactly similar but but very very similar but very very similar but very very similar um after the EG demonstration here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 79,
      "text": "I um after the EG demonstration here I um after the EG demonstration here I have um an introduction from open a eyes have um an introduction from open a eyes have um an introduction from open a eyes chbt in Korean so manaso Pang uh Etc uh chbt in Korean so manaso Pang uh Etc uh chbt in Korean so manaso Pang uh Etc uh so this is in Korean and the reason I so this is in Korean and the reason I so this is in Korean and the reason I put this here is because you'll notice put this here is because you'll notice put this here is because you'll notice that um non-english languages work that um non-english languages work that um non-english languages work slightly worse in Chachi part of this is slightly worse in Chachi part of this is slightly worse in Chachi part of this is because of course the training data set because of course the training data set because of course the training data set for Chachi is much larger for English for Chachi is much larger for English for Chachi is much larger for English and for everything else",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 80,
      "text": "but the same is and for everything else but the same is and for everything else but the same is true not just for the large language true not just for the large language true not just for the large language model itself but also for the tokenizer model itself but also for the tokenizer model itself but also for the tokenizer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 81,
      "text": "so when we train the tokenizer we're so when we train the tokenizer we're so when we train the tokenizer we're going to see that there's a training set going to see that there's a training set going to see that there's a training set as well and there's a lot more English as well and there's a lot more English as well and there's a lot more English than non-english and what ends up than non-english and what ends up than non-english and what ends up happening is that we're going to have a happening is that we're going to have a happening is that we're going to have a lot more longer tokens for lot more longer tokens for lot more longer tokens for English",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 82,
      "text": "so how do I put this if you have English",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 83,
      "text": "so how do I put this if you have English",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 84,
      "text": "so how do I put this if you have a single sentence in English and you a single sentence in English and you a single sentence in English and you tokenize it you might see that it's 10 tokenize it you might see that it's 10 tokenize it you might see that it's 10 tokens or something like that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 85,
      "text": "but if you tokens or something like that but if you tokens or something like that but if you translate that sentence into say Korean translate that sentence into say Korean translate that sentence into say Korean or Japanese or something else you'll or Japanese or something else you'll or Japanese or something else you'll typically see that the number of tokens typically see that the number of tokens typically see that the number of tokens used is much larger and that's because used is much larger and that's because used is much larger and that's because the chunks here are a lot more broken up the chunks here are a lot more broken up the chunks here are a lot more broken up so we're using a lot more tokens for the so",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 86,
      "text": "we're using a lot more tokens for the so we're using a lot more tokens for the exact same thing and what this does is exact same thing and what this does is exact same thing and what this does is it bloats up the sequence length of all it bloats up the sequence length of all it bloats up the sequence length of all the documents so you're using up more the documents so you're using up more the documents so you're using up more tokens and then in the attention of the tokens and then in the attention of the tokens and then in the attention of the Transformer when these tokens try to Transformer when these tokens try to Transformer when these tokens try to attend each other you are running out of attend each other you are running out of attend each other you are running out of context um in the maximum context length context um in the maximum context length context um in the maximum context length of that Transformer and so basically all of that Transformer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 87,
      "text": "and so basically all of that Transformer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 88,
      "text": "and so basically all the non-english text is stretched out the non-english text is stretched out the non-english text is stretched out from the perspective of the Transformer from the perspective of the Transformer from the perspective of the Transformer and this just has to do with the um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 89,
      "text": "and this just has to do with the um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 90,
      "text": "and this just has to do with the um trainings that used for the tokenizer trainings that used for the tokenizer trainings that used for the tokenizer and the tokenization itself so it will and the tokenization itself so it will and the tokenization itself so it will create a lot bigger tokens and a lot create a lot bigger tokens and a lot create a lot bigger tokens and a lot larger groups in English and it will larger groups in English and it will larger groups in English and it will have a lot of little boundaries for all have a lot of little boundaries for all have a lot of little boundaries for all the other non-english text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 91,
      "text": "um so if we the other non-english text um so if we the other non-english text um so if we translated this into English it would be translated this into English it would be translated this into English it would be significantly fewer significantly fewer significantly fewer tokens the final example I have here is tokens the final example I have here is tokens the final example I have here is a little snippet of python for doing FS a little snippet of python for doing FS a little snippet of python for doing FS buuz and what I'd like you to notice is buuz and what I'd like you to notice is buuz and what I'd like you to notice is look all these individual spaces are all look all these individual spaces are all look all these individual spaces are all separate tokens they are token separate tokens they are token separate tokens they are token 220",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 92,
      "text": "so uh 220 220 220 220 and then space 220",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 93,
      "text": "so uh 220 220 220 220 and then space 220",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 94,
      "text": "so uh 220 220 220 220 and then space if is a single",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 95,
      "text": "token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 96,
      "text": "and so what's going if is",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 97,
      "text": "a single",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 98,
      "text": "token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 99,
      "text": "and so what's going if is",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 100,
      "text": "a single",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 101,
      "text": "token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 102,
      "text": "and so what's going on here is that when the Transformer is on here is that when the Transformer is on here is that when the Transformer is going to consume or try to uh create going to consume or try to uh create going to consume or try to uh create this text it needs to um handle all this text it needs to um handle all this text it needs to um handle all these spaces individually they all feed these spaces individually they all feed these spaces individually they all feed in one by one into the entire in one by one into the entire in one by one into the entire Transformer in the sequence and so this Transformer in the sequence and so this Transformer in the sequence and so this is being extremely wasteful tokenizing is being extremely wasteful tokenizing is being extremely wasteful tokenizing it in this way and so as a result of it in this way and so as a result of it in this way and so as a result of that gpt2 is not very good with python that gpt2 is not very good with python that gpt2 is not very good with python and it's not anything to do with coding and it's not anything to do with coding and it's not anything to do with coding or the language model itself it's just or the language model itself it's just or the language model itself it's just that if he use a lot of indentation that if he use a lot of indentation that if he use a lot of indentation using space in Python like we usually do using space in Python like we usually do using space in Python like we usually do uh you just end up bloating out all the uh you just end up bloating out all the uh you just end up bloating out all the text and it's separated across way too text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 103,
      "text": "and it's separated across way too text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 104,
      "text": "and it's separated across way too much of the sequence and we are running much of the sequence and we are running much of the sequence and we are running out of the context length in the out of the context length in the out of the context length in the sequence uh that's roughly speaking sequence uh that's roughly speaking sequence uh that's roughly speaking what's what's happening we're being way",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 105,
      "text": "what's what's happening we're being way what's what's happening we're being way too wasteful we're taking up way too too wasteful we're taking up way too too wasteful we're taking up way too much token space now we can also scroll much token space now we can also scroll much token space now we can also scroll up here and we can change the tokenizer up here and we can change the tokenizer up here and we can change the tokenizer so note here that gpt2 tokenizer creates so note here that gpt2 tokenizer creates so note here that gpt2 tokenizer creates a token count of 300 for this string a token count of 300 for this string a token count of 300 for this string here we can change it to CL 100K base here we can change it to CL 100K base here we can change it to CL 100K base which is the GPT for tokenizer and we which is the GPT for tokenizer and we which is the GPT for tokenizer and we see that the token count drops to 185 so see that the token count drops to 185 so see that the token count drops to 185 so for the exact same string we are now for the exact same string we are now for the exact same string we are now roughly having the number of tokens and roughly having the number of tokens and roughly having the number of tokens and roughly speaking this is because uh the roughly speaking this is because uh the roughly speaking this is because uh the number of tokens in the GPT 4 tokenizer number of tokens in the GPT 4 tokenizer number of tokens in the GPT 4 tokenizer is roughly double that of the number of is roughly double that of the number of is roughly double that of the number of tokens in the gpt2 tokenizer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 106,
      "text": "so we went tokens in the gpt2 tokenizer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 107,
      "text": "so we went tokens in the gpt2 tokenizer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 108,
      "text": "so we went went from roughly 50k to roughly 100K went from roughly 50k to roughly 100K went from roughly 50k to roughly 100K now you can imagine that this is a good now you can imagine that this is a good now you can imagine that this is a good thing because the same text is now thing because the same text is now thing because the same text is now squished into half as many tokens so uh squished into half as many tokens so uh squished into half as many tokens so uh this is a lot denser input to the this is a lot denser input to the this is a lot denser input to the Transformer and in the Transformer every Transformer and in the Transformer every Transformer and in the Transformer every single token has a finite number of single token has a finite number of single token has a finite number of tokens before it that it's going to pay tokens before it that it's going to pay tokens before it that it's going to pay attention to and so what this is doing attention to and so what this is doing attention to and so what this is doing is we're roughly able to see twice as is we're roughly able to see twice as is we're roughly able to see twice as much text as a context for what token to much text as a context for what token to much text as a context for what token to predict next uh because of this change predict next uh because of this change predict next uh because of this change but of course just increasing the number but of course just increasing the number but of course just increasing the number of tokens is uh not strictly better of tokens is uh not strictly better of tokens is uh not strictly better infinitely uh because as you increase infinitely uh because as you increase infinitely uh because as you increase the number of tokens now your embedding the number of tokens now your embedding the number of tokens now your embedding table is um sort of getting a lot larger table is um sort of getting a lot larger table is um sort of getting a lot larger and also at the output we are trying to and also at the output we are trying to and also at the output we are trying to predict the next token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 109,
      "text": "and there's the predict the next token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 110,
      "text": "and there's the predict the next token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 111,
      "text": "and there's the soft Max there and that grows as well soft Max there and that grows as well soft Max there and that grows as well we're going to go into more detail later we're going to go into more detail later we're going to go into more detail later on this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 112,
      "text": "but there's some kind of a Sweet on this but there's some kind of a Sweet on this but there's some kind of a Sweet Spot somewhere where you have a just Spot somewhere where you have a just Spot somewhere where you have a just right number of tokens in your right number of tokens in your right number of tokens in your vocabulary where everything is vocabulary where everything is vocabulary where everything is appropriately dense and still fairly appropriately dense and still fairly appropriately dense and still fairly efficient now one thing I would like you efficient now one thing I would like you efficient now one thing I would like you to note specifically for the gp4 to note specifically for the gp4 to note specifically for the gp4 tokenizer is that the handling of the tokenizer is that the handling of the tokenizer is that the handling of the white space for python has improved a white space for python has improved a white space for python has improved a lot you see that here these four spaces lot you see that here these four spaces lot you see that here these four spaces are represented as one single token for are represented as one single token for are represented as one single token for the three spaces here and then the token the three spaces here and then the token the three spaces here and then the token SPF and here seven spaces were all SPF and here seven spaces were all SPF and here seven spaces were all grouped into a single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 113,
      "text": "so we're grouped into a single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 114,
      "text": "so we're grouped into a single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 115,
      "text": "so we're being a lot more efficient in how we being a lot more efficient in how we being a lot more efficient in how we represent Python and this was a represent Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 116,
      "text": "and this was a represent Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 117,
      "text": "and this was a deliberate Choice made by open aai when deliberate Choice made by open aai when deliberate Choice made by open aai when they designed the gp4 tokenizer and they they designed the gp4 tokenizer and they they designed the gp4 tokenizer and they group a lot more space into a single group a lot more space into a single group a lot more space into a single character what this does is this character what this does is this character what this does is this densifies",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 118,
      "text": "Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 119,
      "text": "and therefore we can densifies Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 120,
      "text": "and therefore we can densifies Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 121,
      "text": "and therefore we can attend to more code before it when we're attend to more code before it when we're attend to more code before it when we're trying to predict the next token in the trying to predict the next token in the trying to predict the next token in the sequence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 122,
      "text": "and so the Improvement in the sequence and so the Improvement in the sequence and so the Improvement in the python coding ability from gbt2 to gp4 python coding ability from gbt2 to gp4 python coding ability from gbt2 to gp4 is not just a matter of the language is not just a matter of the language is not just a matter of the language model and the architecture and the model and the architecture and the model and the architecture and the details of the optimization but a lot of details of the optimization but a lot of details of the optimization but a lot of the Improvement here is also coming from the Improvement here is also coming from the Improvement here is also coming from the design of the tokenizer and how it the design of the tokenizer and how it the design of the tokenizer and how it groups characters into tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 123,
      "text": "okay so groups characters into tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 124,
      "text": "okay so groups characters into tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 125,
      "text": "okay so let's now start writing some code let's now start writing some code let's now start writing some code",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 126,
      "text": "so remember what we want to do we want so remember what we want to do we want so remember what we want to do we want to take strings and feed them into to take strings and feed them into to take strings and feed them into language models for that we need to language models for that we need to language models for that we need to somehow tokenize strings into some somehow tokenize strings into some somehow tokenize strings into some integers in some fixed vocabulary and integers in some fixed vocabulary and integers in some fixed vocabulary",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 127,
      "text": "and then we will use those integers to make then we will use those integers to make then we will use those integers to make a look up into a lookup table of vectors a look up into a lookup table of vectors a look up into a lookup table of vectors and feed those vectors into the and feed those vectors into the and feed those vectors into the Transformer as an input now the reason Transformer as an input now the reason Transformer as an input now the reason this gets a little bit tricky of course this gets a little bit tricky of course this gets a little bit tricky of course is that we don't just want to support is that we don't just want to support is that we don't just want to support the simple English alphabet we want to the simple English alphabet we want to the simple English alphabet we want to support different kinds of languages so support different kinds of languages so support different kinds of languages so this is anango in Korean which is hello",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 128,
      "text": "this is anango in Korean which is hello this is anango in Korean which is hello",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 129,
      "text": "and we also want to support many kinds and we also want to support many kinds and we also want to support many kinds of special characters that we might find of special characters that we might find of special characters that we might find on the internet for example on the internet for example on the internet for example",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 130,
      "text": "Emoji",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 131,
      "text": "so how do we feed this text into Emoji so how do we feed this text into Emoji",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 132,
      "text": "so how do we feed this text into uh uh uh Transformers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 133,
      "text": "well how's the what is this Transformers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 134,
      "text": "well how's the what is this Transformers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 135,
      "text": "well how's the what is this text anyway in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 136,
      "text": "so if you go to text anyway in Python so if you go to text anyway in Python so if you go to the documentation of a string in Python the documentation of a string in Python the documentation of a string in Python you can see that strings are immutable you can see that strings are immutable you can see that strings are immutable sequences of Unicode code sequences of Unicode code sequences of Unicode code points",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 137,
      "text": "okay what are Unicode code points points",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 138,
      "text": "okay what are Unicode code points points",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 139,
      "text": "okay what are Unicode code points we can go to PDF so",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 140,
      "text": "Unicode code points we can go to PDF so Unicode code points we can go to PDF so Unicode code points are defined by the Unicode Consortium as are defined by the Unicode Consortium as are defined by the Unicode Consortium as part of the Unicode standard and what part of the Unicode standard and what part of the Unicode standard and what this is really is that it's just a this is really is that it's just a this is really is that it's just a definition of roughly 150,000 characters definition of roughly 150,000 characters definition of roughly 150,000 characters right now and roughly speaking what they right now and roughly speaking what they right now and roughly speaking what they look like and what integers um represent look like and what integers um represent look like and what integers um represent those characters so it says 150,000 those characters",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 141,
      "text": "so it says 150,000 those characters",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 142,
      "text": "so it says 150,000 characters across 161 scripts as of characters across 161 scripts as of characters across 161 scripts as of right now",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 143,
      "text": "so if you scroll down here you right now",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 144,
      "text": "so if you scroll down here you right now so if you scroll down here you can see that the standard is very much can see that the standard is very much can see that the standard is very much alive the latest standard 15.1 in alive the latest standard 15.1 in alive the latest standard 15.1 in September September September 2023 and basically this is just a way to 2023 and basically this is just a way to 2023 and basically this is just a way to define lots of types of define lots of types of define lots of types of characters like for example all these characters like for example all these characters like for example all these characters across different scripts",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 145,
      "text": "so characters across different scripts so characters across different scripts so the way we can access the unic code code the way we can access the unic code code the way we can access the unic code code Point given Single Character is by using Point given Single Character is by using Point given Single Character is by using the or function in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 146,
      "text": "so for example the or function in Python so for example the or function in Python so for example I can pass in Ord of H",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 147,
      "text": "and I can see I can pass in Ord of H",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 148,
      "text": "and I can see I can pass in Ord of H",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 149,
      "text": "and I can see that for the Single Character H the unic that for the Single Character H the unic that for the Single Character H the unic code code point is code code point is code code point is 104",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 150,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 151,
      "text": "um but this can be arbitr 104",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 152,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 153,
      "text": "um but this can be arbitr 104",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 154,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 155,
      "text": "um but this can be arbitr complicated so we can take for example complicated so we can take for example complicated so we can take for example our Emoji here and we can see that the our Emoji here and we can see that the our Emoji here and we can see that the code point for this one is code point for this one is code point for this one is 128,000 or we can take 128,000 or we can take 128,000 or we can take un and this is 50,000 now keep in mind un and this is 50,000 now keep in mind un and this is 50,000 now keep in mind you can't plug in strings here because you can't plug in strings here because you can't plug in strings here because you uh this doesn't have a single code you uh this doesn't have a single code",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 156,
      "text": "you uh this doesn't have a single code point it only takes a single uni code point it only takes a single uni code point it only takes a single uni code code Point character and tells you its code Point character and tells you its code Point character and tells you its integer so in this way we can look integer so in this way we can look integer so in this way we can look up all the um characters of this up all the um characters of this up all the um characters of this specific string and their code points so specific string and their code points so specific string and their code points so or of X forx in this string and we get or of X forx in this string and we get or of X forx in this string and we get this encoding here now see here we've this encoding here now see here we've this encoding here now see here we've already turned the raw code points already turned the raw code points already turned the raw code points already have integers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 157,
      "text": "so why can't we already have integers so why can't we already have integers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 158,
      "text": "so why can't we simply just use these integers and not simply just use these integers and not simply just use these integers and not have any tokenization at all why can't have any tokenization at all why can't have any tokenization at all why can't we just use this natively as is and just we just use this natively as is and just we just use this natively as is and just use the code Point well one reason for use the code Point well one reason for use the code Point well one reason for that of course is that the vocabulary in that of course is that the vocabulary in that of course is that the vocabulary in that case would be quite long so in this that case would be quite long so in this that case would be quite long so in this case for Unicode the this is a case for Unicode",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 159,
      "text": "the this is a case for Unicode the this is a vocabulary of vocabulary of vocabulary of 150,000 different code points but more 150,000 different code points but more 150,000 different code points but more worryingly than that I think the Unicode worryingly than that I think the Unicode worryingly than that I think the Unicode standard is very much alive and it keeps standard is very much alive and it keeps standard is very much alive and it keeps changing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 160,
      "text": "and so it's not kind of a changing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 161,
      "text": "and so it's not kind of a changing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 162,
      "text": "and so it's not kind of a stable representation necessarily that stable representation necessarily that stable representation necessarily that we may want to use directly so for those we may want to use directly so for those we may want to use directly so for those reasons we need something a bit better reasons we need something a bit better reasons we need something a bit better so to find something better we turn to so to find something better we turn to so to find something better we turn to encodings",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 163,
      "text": "so if we go to the Wikipedia encodings so if we go to the Wikipedia encodings so if we go to the Wikipedia page here we see that the Unicode page here we see that the Unicode page here we see that the Unicode consortion defines three types of consortion defines three types of consortion defines three types of encodings utf8 UTF 16 and UTF 32 these encodings utf8 UTF 16 and UTF 32 these encodings utf8 UTF 16 and UTF 32 these encoding are the way by which we can encoding are the way by which we can encoding are the way by which we can take Unicode text and translate it into take Unicode text and translate it into take Unicode text and translate it into binary data or by streams utf8 is by far binary data or by streams utf8 is by far binary data or by streams utf8 is by far the most common uh so this is the utf8 the most common",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 164,
      "text": "uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 165,
      "text": "so this is the utf8 the most common",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 166,
      "text": "uh so this is the utf8 page now this Wikipedia page is actually page now this Wikipedia page is actually page now this Wikipedia page is actually quite long",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 167,
      "text": "but what's important for our quite long but what's important for our quite long but what's important for our purposes is that utf8 takes every single purposes is that utf8 takes every single purposes is that utf8 takes every single Cod point and it translates it to a by Cod point and it translates it to a by Cod point and it translates it to a by stream and this by stream is between one stream and this by stream is between one stream and this by stream is between one to four bytes so it's a variable length to four bytes so it's a variable length to four bytes so it's a variable length encoding so depending on the Unicode encoding so depending on the Unicode encoding so depending on the Unicode Point according to the schema you're Point according to the schema you're Point according to the schema you're going to end up with between 1 to four going to end up with between 1 to four going to end up with between 1 to four bytes for each code point on top of that bytes for each code point on top of that bytes for each code point on top of that there's utf8 uh there's utf8 uh there's utf8 uh utf16 and UTF 32 UTF 32 is nice because utf16 and UTF 32 UTF 32 is nice because utf16 and UTF 32 UTF 32 is nice because it is fixed length instead of variable it is fixed length instead of variable it is fixed length instead of variable length",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 168,
      "text": "but it has many other downsides length but it has many other downsides length",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 169,
      "text": "but it has many other downsides as well so the full kind of spectrum of as well so the full kind of spectrum of as well so the full kind of spectrum of pros and cons of all these different pros and cons of all these different pros and cons of all these different three encodings are beyond the scope of three encodings are beyond the scope of three encodings are beyond the scope of this video I just like to point out that this video I just like to point out that this video I just like to point out that I enjoyed this block post and this block I enjoyed this block post and this block I enjoyed this block post and this block post at the end of it also has a number post at the end of it also has a number post at the end of it also has a number of references that can be quite useful of references that can be quite useful of references that can be quite useful uh one of them is uh utf8 everywhere uh one of them is uh utf8 everywhere uh one of them is uh utf8 everywhere Manifesto um and this Manifesto Manifesto um and this Manifesto Manifesto",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 170,
      "text": "um and this Manifesto describes the reason why utf8 is describes the reason why utf8 is describes the reason why utf8 is significantly preferred and a lot nicer significantly preferred and a lot nicer significantly preferred and a lot nicer than the other encodings and why it is than the other encodings and why it is than the other encodings and why it is used a lot more prominently um on the used a lot more prominently um on the used a lot more prominently um on the internet one of the major advantages internet one of the major advantages internet one of the major advantages just just to give you a sense is that just just to give you a sense is that just just to give you a sense is that utf8 is the only one of these that is utf8 is the only one of these that is utf8 is the only one of these that is backwards compatible to the much simpler backwards compatible to the much simpler backwards compatible to the much simpler asky encoding of text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 171,
      "text": "um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 172,
      "text": "but I'm not asky encoding of text um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 173,
      "text": "but I'm not asky encoding of text um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 174,
      "text": "but I'm not going to go into the full detail in this going to go into the full detail in this going to go into the full detail in this video so suffice to say that we like the video so suffice to say that we like the video so suffice to say that we like the utf8 encoding and uh let's try to take utf8 encoding and uh let's try to take utf8 encoding and uh let's try to take the string and see what we get if we the string and see what we get if we the string and see what we get if we encoded into encoded into encoded into utf8 the string class in Python actually utf8 the string class in Python actually utf8 the string class in Python actually has do encode",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 175,
      "text": "and you can give it the has do encode",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 176,
      "text": "and you can give it the has do encode",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 177,
      "text": "and you can give it the encoding which is say utf8 now we get encoding which is say utf8 now we get encoding which is say utf8 now we get out of this is not very nice because out of this is not very nice because out of this is not very nice because this is the bytes is a bytes object and this is the bytes is a bytes object and this is the bytes is a bytes object and it's not very nice in the way that it's it's not very nice in the way that it's it's not very nice in the way that it's printed",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 178,
      "text": "so I personally like to take it printed",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 179,
      "text": "so I personally like to take it printed",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 180,
      "text": "so I personally like to take it through list because then we actually through list because then we actually through list because then we actually get the raw B get the raw B get the raw B of this uh encoding",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 181,
      "text": "so this is the raw of this uh encoding so this is the raw of this uh encoding",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 182,
      "text": "so this is the raw byes that represent this string byes that represent this string byes that represent this string according to the utf8 en coding we can according to the utf8 en coding we can according to the utf8 en coding we can also look at utf16 we get a slightly also look at utf16 we get a slightly also look at utf16 we get a slightly different by stream",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 183,
      "text": "and we here we start different by stream and we here we start different by stream",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 184,
      "text": "and we here we start to see one of the disadvantages of utf16 to see one of the disadvantages of utf16 to see one of the disadvantages of utf16 you see how we have zero Z something",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 185,
      "text": "Z you see how we have zero Z something",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 186,
      "text": "Z you see how we have zero Z something Z something Z something we're starting to something Z something we're starting to something Z something we're starting to get a sense that this is a bit of a get a sense that this is a bit of a get a sense that this is a bit of a wasteful encoding and indeed for simple wasteful encoding and indeed for simple wasteful encoding and indeed for simple asky characters or English characters asky characters or English characters asky characters or English characters here uh we just have the structure of 0 here uh we just have the structure of 0 here uh we just have the structure of 0",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 187,
      "text": "something Z something",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 188,
      "text": "and it's not something Z something",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 189,
      "text": "and it's not something Z something",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 190,
      "text": "and it's not exactly nice same for UTF 32 when we exactly nice same for UTF 32 when we exactly nice same for UTF 32 when we expand this we can start to get a sense expand this we can start to get a sense expand this we can start to get a sense of the wastefulness of this encoding for of the wastefulness of this encoding for of the wastefulness of this encoding for our purposes you see a lot of zeros our purposes you see a lot of zeros our purposes you see a lot of zeros followed by followed by followed by something",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 191,
      "text": "and so uh this is not something",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 192,
      "text": "and so uh this is not something",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 193,
      "text": "and so uh this is not desirable so suffice it to say that we desirable so suffice it to say that we desirable so suffice it to say that we would like to stick with utf8 for our would like to stick with utf8 for our would like to stick with utf8 for our purposes however if we just use utf8 purposes however if we just use utf8 purposes however if we just use utf8 naively these are by streams so that naively these are by streams so that naively these are by streams so that would imply a vocabulary length of only would imply a vocabulary length of only would imply a vocabulary length of only 256 possible tokens uh but this this 256 possible tokens uh but this this 256 possible tokens uh but this this vocabulary size is very very small what vocabulary size is very very small what vocabulary size is very very small what this is going to do if we just were to this is going to do if we just were to this is going to do if we just were to use it naively is that all of our text use it naively is that all of our text use it naively is that all of our text would be stretched out over very very would be stretched out over very very would be stretched out over very very long sequences of bytes and so long sequences of bytes and so long sequences of bytes and so um what what this does is that certainly um what what this does is that certainly um what what this does is that certainly the embeding table is going to be tiny the embeding table is going to be tiny the embeding table is going to be tiny and the prediction at the top at the and the prediction at the top at the and the prediction at the top at the final layer is going to be very tiny but final layer is going to be very tiny but final layer is going to be very tiny but our sequences are very long and remember our sequences are very long and remember our sequences are very long and remember that we have pretty finite um context that we have pretty finite um context that we have pretty finite um context length and the attention that we can length and the attention that we can length and the attention that we can support in a transformer for support in a transformer for support in a transformer for computational reasons",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 194,
      "text": "and so we only computational reasons",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 195,
      "text": "and so we only computational reasons",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 196,
      "text": "and so we only have as much context length",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 197,
      "text": "but now we have as much context length",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 198,
      "text": "but now we have as much context length",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 199,
      "text": "but now we have very very long sequences and this have very very long sequences and this have very very long sequences and this is just inefficient and it's not going is just inefficient and it's not going is just inefficient and it's not going to allow us to attend to sufficiently to allow us to attend to sufficiently to allow us to attend to sufficiently long text uh before us for the purposes long text uh before us for the purposes long text uh before us for the purposes of the next token prediction task so we of the next token prediction task",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 200,
      "text": "so we of the next token prediction task so we don't want to use the raw bytes of the don't want to use the raw bytes of the don't want to use the raw bytes of the utf8 encoding we want to be able to utf8 encoding we want to be able to utf8 encoding we want to be able to support larger vocabulary size that we support larger vocabulary size that we support larger vocabulary size that we can tune as a hyper can tune as a hyper can tune as a hyper but we want to stick with the utf8 but we want to stick with the utf8",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 201,
      "text": "but we want to stick with the utf8 encoding of these strings so what do we encoding of these strings so what do we encoding of these strings so what do we do well the answer of course is we turn do well the answer of course is we turn do well the answer of course is we turn to the bite pair encoding algorithm to the bite pair encoding algorithm to the bite pair encoding algorithm which will allow us to compress these which will allow us to compress these which will allow us to compress these bite sequences um to a variable amount bite sequences um to a variable amount bite sequences um to a variable amount so we'll get to that in a bit",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 202,
      "text": "but I just so we'll get to that in a bit",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 203,
      "text": "but I just so we'll get to that in a bit",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 204,
      "text": "but I just want to briefly speak to the fact that I want to briefly speak to the fact that I want to briefly speak to the fact that I would love nothing more than to be able would love nothing more than to be able would love nothing more than to be able to feed raw bite sequences into uh to feed raw bite sequences into uh to feed raw bite sequences into uh language models in fact there's a paper language models in fact there's a paper language models in fact there's a paper about how this could potentially be done about how this could potentially be done about how this could potentially be done uh from Summer last last year now the uh from Summer last last year now the uh from Summer last last year now the problem is you actually have to go in problem is you actually have to go in problem is you actually have to go in and you have to modify the Transformer and you have to modify the Transformer and you have to modify the Transformer architecture because as I mentioned architecture because as I mentioned architecture because as I mentioned you're going to have a problem where the you're going to have a problem where the you're going to have a problem where the attention will start to become extremely attention will start to become extremely attention will start to become extremely expensive because the sequences are so expensive because the sequences are so expensive because the sequences are so long and so in this paper they propose long and so in this paper they propose long and so in this paper they propose kind of a hierarchical structuring of kind of a hierarchical structuring of kind of a hierarchical structuring of the Transformer that could allow you to the Transformer that could allow you to the Transformer that could allow you to just feed in raw bites and so at the end just feed in raw bites and so at the end just feed in raw bites and so at the end they say together these results they say together these results they say together these results establish the viability of tokenization establish the viability of tokenization establish the viability of tokenization free autor regressive sequence modeling free autor regressive sequence modeling free autor regressive sequence modeling at scale so tokenization free",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 205,
      "text": "would at scale so tokenization free would at scale so tokenization free would indeed be amazing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 206,
      "text": "we would just feed B indeed be amazing we would just feed B indeed be amazing we would just feed B streams directly into our models but streams directly into our models but streams directly into our models but unfortunately I don't know that this has unfortunately I don't know that this has unfortunately I don't know that this has really been proven out yet by really been proven out yet by really been proven out yet by sufficiently many groups and a sufficiently many groups and a sufficiently many groups and a sufficient scale",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 207,
      "text": "uh but something like sufficient scale uh but something like sufficient scale uh but something like this at one point would be amazing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 208,
      "text": "and I this at one point would be amazing and I this at one point would be amazing and I hope someone comes up with it but for hope someone comes up with it but for hope someone comes up with it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 209,
      "text": "but for now we have to come back",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 210,
      "text": "and we can't now we have to come back",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 211,
      "text": "and we can't now we have to come back",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 212,
      "text": "and we can't feed this directly into language models feed this directly into language models feed this directly into language models and we have to compress it using the B and we have to compress it using the B and we have to compress it using the B paare encoding algorithm",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 213,
      "text": "so let's see paare encoding algorithm",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 214,
      "text": "so let's see paare encoding algorithm",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 215,
      "text": "so let's see how that works so as I mentioned the B how that works so as I mentioned the B how that works so as I mentioned the B paare encoding algorithm is not all that paare encoding algorithm is not all that paare encoding algorithm is not all that complicated and the Wikipedia page is complicated and the Wikipedia page is complicated and the Wikipedia page is actually quite instructive as far as the actually quite instructive as far as the actually quite instructive as far as the basic idea goes go what we're doing is basic idea goes go what we're doing is basic idea goes go what we're doing is we have some kind of a input sequence uh we have some kind of a input sequence uh we have some kind of a input sequence uh like for example here we have only four like for example here we have only four like for example here we have only four elements in our vocabulary a b c and d elements in our vocabulary a b c and d elements in our vocabulary a b c and d",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 216,
      "text": "and we have a sequence of them",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 217,
      "text": "so and we have a sequence of them",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 218,
      "text": "so and we have a sequence of them so instead of bytes let's say we just have instead of bytes let's say we just have instead of bytes let's say we just have four a vocab size of four a vocab size of four a vocab size of four the sequence is too long and we'd four the sequence is too long and we'd four the sequence is too long and we'd like to compress it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 219,
      "text": "so what we do is like to compress it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 220,
      "text": "so what we do is like to compress it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 221,
      "text": "so what we do is that we iteratively find the pair of uh that we iteratively find the pair of uh that we iteratively find the pair of uh tokens that occur the most tokens that occur the most tokens that occur the most frequently and then once we've frequently and then once we've frequently and then once we've identified that pair we repl replace identified that pair we repl replace identified that pair we repl replace that pair with just a single new token that pair with just a single new token that pair with just a single new token that we append to our vocabulary so for that we append to our vocabulary so for that we append to our vocabulary so for example here the bite pair AA occurs example here the bite pair AA occurs example here the bite pair AA occurs most often so we mint a new token let's most often",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 222,
      "text": "so we mint a new token let's most often so we mint a new token let's call it capital Z and we replace every call it capital Z",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 223,
      "text": "and we replace every call it capital Z",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 224,
      "text": "and we replace every single occurrence of AA by Z",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 225,
      "text": "so now we single occurrence of AA by Z",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 226,
      "text": "so now we single occurrence of AA by Z",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 227,
      "text": "so now we have two Z's here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 228,
      "text": "so here we took a have two Z's here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 229,
      "text": "so here we took a have two Z's here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 230,
      "text": "so here we took a sequence of 11 characters with sequence of 11 characters with sequence of 11 characters with vocabulary size four",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 231,
      "text": "and we've converted vocabulary size four",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 232,
      "text": "and we've converted vocabulary size four",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 233,
      "text": "and we've converted it to a um sequence of only nine tokens it to a um sequence of only nine tokens it to a um sequence of only nine tokens but now with a vocabulary of five but now with a vocabulary of five but now with a vocabulary of five because we have a fifth vocabulary because we have a fifth vocabulary because we have a fifth vocabulary element that we just created and it's Z element that we just created and it's Z element that we just created",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 234,
      "text": "and it's Z standing for concatination of AA and we standing for concatination of AA and we standing for concatination of AA",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 235,
      "text": "and we can again repeat this process so we can again repeat this process so we can again repeat this process so we again look at the sequence and identify again look at the sequence and identify again look at the sequence and identify the pair of tokens that are most the pair of tokens that are most the pair of tokens that are most frequent let's say that that is now AB frequent let's say that that is now AB frequent let's say that that is now AB",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 236,
      "text": "well we are going to replace AB with a well we are going to replace AB with a well we are going to replace AB with a new token that we meant call Y",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 237,
      "text": "so y new token that we meant call",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 238,
      "text": "Y",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 239,
      "text": "so y new token that we meant call",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 240,
      "text": "Y",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 241,
      "text": "so y becomes ab",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 242,
      "text": "and then every single becomes ab",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 243,
      "text": "and then every single becomes ab",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 244,
      "text": "and then every single occurrence of ab is now replaced with y occurrence of ab is now replaced with y occurrence of ab is now replaced with y",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 245,
      "text": "so we end up with this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 246,
      "text": "so now we only so we end up with this so now we only so we end up with this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 247,
      "text": "so now we only have 1 2 3 4 5 6 seven characters in our have 1 2 3 4 5 6 seven characters in our have 1 2 3 4 5 6 seven characters in our sequence but we have not just um four sequence but we have not just um four sequence but we have not just um four vocabulary elements or five but now we vocabulary elements or five but now we vocabulary elements or five but now we have six and for the final round we have six and for the final round we have six and for the final round we again look through the sequence find again look through the sequence find again look through the sequence find that the phrase zy or the pair zy is that the phrase zy or the pair zy is that the phrase zy or the pair zy is most common and replace it one more time most common and replace it one more time most common and replace it one more time with another um character",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 248,
      "text": "let's say x so with another um character let's say x so with another um character let's say x so X is z y",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 249,
      "text": "and we replace all curses of zy X is z y",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 250,
      "text": "and we replace all curses of zy X is z y",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 251,
      "text": "and we replace all curses of zy",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 252,
      "text": "and we get this following sequence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 253,
      "text": "so and we get this following sequence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 254,
      "text": "so and we get this following sequence so basically after we have gone through basically after we have gone through basically after we have gone through this process instead of having a um this process instead of having a um this process instead of having a um sequence of sequence of sequence of 11 uh tokens with a vocabulary length of 11 uh tokens with a vocabulary length of 11 uh tokens with a vocabulary length of four we now have a sequence of 1 2 3 four we now have a sequence of 1 2 3 four we now have a sequence of 1 2 3 four five tokens but our vocabulary four five tokens but our vocabulary four five tokens but our vocabulary length now is seven and so in this way length now is seven and so in this way length now is seven and so in this way we can iteratively compress our sequence we can iteratively compress our sequence we can iteratively compress our sequence I we Mint new tokens so in the in the I we Mint new tokens so in the in the",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 255,
      "text": "I",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 256,
      "text": "we Mint new tokens so in the in the exact same way we start we start out exact same way we start we start out exact same way we start we start out with bite sequences",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 257,
      "text": "so we have 256 with bite sequences",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 258,
      "text": "so we have 256 with bite sequences",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 259,
      "text": "so we have 256 vocabulary size but we're now going to vocabulary size but we're now going to vocabulary size",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 260,
      "text": "but we're now going to go through these and find the bite pairs go through these and find the bite pairs go through these and find the bite pairs that occur the most",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 261,
      "text": "and we're going to that occur the most",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 262,
      "text": "and we're going to that occur the most",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 263,
      "text": "and we're going to iteratively start minting new tokens iteratively start minting new tokens iteratively start minting new tokens appending them to our vocabulary and appending them to our vocabulary and appending them to our vocabulary and replacing things and in this way we're replacing things and in this way we're replacing things and in this way we're going to end up with a compressed going to end up with a compressed going to end up with a compressed training data set and also an algorithm training data set and also an algorithm training data set and also an algorithm for taking any arbitrary sequence and for taking any arbitrary sequence and for taking any arbitrary sequence and encoding it using this uh vocabul encoding it using this uh vocabul encoding it using this uh vocabul and also decoding it back to Strings so and also decoding it back to Strings so and also decoding it back to Strings so let's now Implement all that so here's let's now Implement all that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 264,
      "text": "so here's let's now Implement all that so here's what I did I went to this block post what I did I went to this block post what I did I went to this block post that I enjoyed",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 265,
      "text": "and I took the first that I enjoyed",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 266,
      "text": "and I took the first that I enjoyed",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 267,
      "text": "and I took the first paragraph",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 268,
      "text": "and I copy pasted it here into paragraph",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 269,
      "text": "and I copy pasted it here into paragraph",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 270,
      "text": "and I copy pasted it here into text so this is one very long line text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 271,
      "text": "so this is one very long line text so this is one very long line here now to get the tokens as I here now to get the tokens as I here now to get the tokens as I mentioned we just take our text and we mentioned we just take our text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 272,
      "text": "and we mentioned we just take our text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 273,
      "text": "and we encode it into utf8 the tokens here at encode it into utf8 the tokens here at encode it into utf8 the tokens here at this point will be a raw bites single this point will be a raw bites single this point will be a raw bites single stream of bytes and just so that it's stream of bytes and just so that it's stream of bytes and just so that it's easier to work with instead of just a easier to work with instead of just a easier to work with instead of just a bytes object I'm going to convert all bytes object I'm going to convert all bytes object I'm going to convert all those bytes to integers and then create those bytes to integers and then create those bytes to integers and then create a list of it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 274,
      "text": "just so it's easier for us a list of it just so it's easier for us a list of it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 275,
      "text": "just so it's easier for us to manipulate and work with in Python to manipulate and work with in Python to manipulate and work with in Python and visualize and here I'm printing all and visualize",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 276,
      "text": "and here I'm printing all and visualize",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 277,
      "text": "and here I'm printing all of that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 278,
      "text": "so this is the original um this of that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 279,
      "text": "so this is the original um this of that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 280,
      "text": "so this is the original um this is the original paragraph and its length is the original paragraph and its length is the original paragraph and its length is is is 533 uh code points",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 281,
      "text": "and then here are the 533 uh code points",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 282,
      "text": "and then here are the 533 uh code points",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 283,
      "text": "and then here are the bytes encoded in ut utf8 and we see that bytes encoded in ut utf8",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 284,
      "text": "and we see that bytes encoded in ut utf8 and we see that this has a length of 616 bytes at this this has a length of 616 bytes at this this has a length of 616 bytes at this point or 616 tokens and the reason this point or 616 tokens and the reason this point or 616 tokens and the reason this is more is because a lot of these simple is more is because a lot of these simple is more is because a lot of these simple asky characters or simple characters asky characters or simple characters asky characters or simple characters they just become a single bite",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 285,
      "text": "but a lot they just become a single bite",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 286,
      "text": "but a lot they just become a single bite",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 287,
      "text": "but a lot of these Unicode more complex characters of these Unicode more complex characters of these Unicode more complex characters become multiple bytes up to four and so become multiple bytes up to four and so become multiple bytes up to four",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 288,
      "text": "and so we are expanding that we are expanding that we are expanding that size so now what we'd like to do as a size so now what we'd like to do as a size so now what we'd like to do as a first step of the algorithm is we'd like first step of the algorithm is we'd like first step of the algorithm is we'd like to iterate over here and find the pair to iterate over here and find the pair to iterate over here and find the pair of bites that occur most frequently of bites that occur most frequently of bites that occur most frequently because we're then going to merge it so because we're then going to merge it so because we're then going to merge it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 289,
      "text": "so if you are working long on a notebook on if you are working long on a notebook on if you are working long on a notebook on a side then I encourage you to basically a side then I encourage you to basically a side then I encourage you to basically click on the link find this notebook and click on the link find this notebook and click on the link find this notebook and try to write that function yourself try to write that function yourself try to write that function yourself",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 290,
      "text": "otherwise I'm going to come here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 291,
      "text": "and otherwise I'm going to come here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 292,
      "text": "and otherwise I'm going to come here and Implement first the function that finds Implement first the function that finds Implement first the function that finds the most common pair",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 293,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 294,
      "text": "so here's what the most common pair",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 295,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 296,
      "text": "so here's what the most common pair",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 297,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 298,
      "text": "so here's what I came up with there are many different I came up with there are many different",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 299,
      "text": "I came up with there are many different ways to implement this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 300,
      "text": "but I'm calling ways to implement this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 301,
      "text": "but I'm calling ways to implement this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 302,
      "text": "but I'm calling the function get stats it expects a list the function get stats it expects a list the function get stats it expects a list of integers I'm using a dictionary to of integers I'm using a dictionary to of integers I'm using a dictionary to keep track of basically the counts and keep track of basically the counts and keep track of basically the counts",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 303,
      "text": "and then this is a pythonic way to iterate then this is a pythonic way to iterate then this is a pythonic way to iterate consecutive elements of this list uh consecutive elements of this list uh consecutive elements of this list uh which we covered in the previous video which we covered in the previous video which we covered in the previous video and then here I'm just keeping track of and then here I'm just keeping track of and then here I'm just keeping track of just incrementing by one um for all the just incrementing by one um for all the just incrementing by one um for all the pairs so if I call this on all the pairs so if I call this on all the pairs so if I call this on all the tokens here then the stats comes out tokens here then the stats comes out tokens here then the stats comes out here so this is the dictionary the keys here so this is the dictionary the keys here so this is the dictionary the keys are these topples of consecutive are these topples of consecutive are these topples of consecutive elements and this is the count so just elements and this is the count so just elements and this is the count",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 304,
      "text": "so just to uh print it in a slightly better way to uh print it in a slightly better way to uh print it in a slightly better way this is one way that I like to do that this is one way that I like to do that this is one way that I like to do that where you it's a little bit compound where you it's a little bit compound where you it's a little bit compound here so you can pause if you like but we here so you can pause if you like but we here so you can pause if you like but we iterate all all the items the items iterate all all the items the items iterate all all the items the items called on dictionary returns pairs of called on dictionary returns pairs of called on dictionary returns pairs of key value",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 305,
      "text": "and instead I create a list key value and instead I create a list key value and instead I create a list here of value key because if it's a here of value key because if it's a here of value key because if it's a value key list then I can call sort on value key list then I can call sort on value key list then I can call sort on it and by default python will uh use the it and by default python will uh use the it and by default python will uh use the first element which in this case will be first element which in this case will be first element which in this case will be value to sort by if it's given tles and value to sort by if it's given tles and value to sort by if it's given tles and then reverse so it's descending and then reverse so it's descending and then reverse so it's descending and print that so basically it looks like print that so basically it looks like print that so basically it looks like 101 comma 32 was the most commonly 101 comma 32 was the most commonly 101 comma 32 was the most commonly occurring consecutive pair and it occurring consecutive pair and it occurring consecutive pair and it occurred 20 times we can double check occurred 20 times we can double check occurred 20 times we can double check that that makes reasonable sense",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 306,
      "text": "so if I that that makes reasonable sense so if I that that makes reasonable sense so if I just search just search just search 10132 then you see that these are the 20 10132 then you see that these are the 20 10132 then you see that these are the 20 occurrences of that um pair and if we'd occurrences of that um pair and if we'd occurrences of that um pair and if we'd like to take a look at what exactly that like to take a look at what exactly that like to take a look at what exactly that pair is we can use Char which is the pair is we can use Char which is the pair is we can use Char which is the opposite of or in Python so we give it a opposite of or in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 307,
      "text": "so we give it a opposite of or in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 308,
      "text": "so we give it a um unic code Cod point so 101 and of 32 um unic code Cod point so 101 and of 32 um unic code Cod point so 101 and of 32 and we see that this is e and space",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 309,
      "text": "so and we see that this is e and space",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 310,
      "text": "so and we see that this is e and space so basically there's a lot of E space here basically there's a lot of E space here basically there's a lot of E space here meaning that a lot of these words seem meaning that a lot of these words seem meaning that a lot of these words seem to end with e",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 311,
      "text": "so here's eace as an to end with e",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 312,
      "text": "so here's eace as an to end with e",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 313,
      "text": "so here's eace as an example so there's a lot of that going example",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 314,
      "text": "so there's a lot of that going example so there's a lot of that going on here and this is the most common pair on here and this is the most common pair on here and this is the most common pair so now that we've identified the most so now that we've identified the most so now that we've identified the most common pair we would like to iterate common pair we would like to iterate common pair we would like to iterate over this sequence we're going to Mint a over this sequence we're going to Mint a over this sequence we're going to Mint a new token with the ID of new token with the ID of new token with the ID of 256 right because these tokens currently 256 right because these tokens currently 256 right because these tokens currently go from Z to 255 so when we create a new go from Z to 255 so when we create a new go from Z to 255 so when we create a new token it will have an ID of token it will have an ID of token it will have an ID of 256 and we're going to iterate over this 256",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 315,
      "text": "and we're going to iterate over this 256",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 316,
      "text": "and we're going to iterate over this entire um list and every every time we entire um list and every every time we entire um list and every every time we see 101 comma 32 we're going to swap see 101 comma 32 we're going to swap see 101 comma 32 we're going to swap that out for that out for that out for 256 so let's Implement that now and feel 256 so let's Implement that now and feel 256 so let's Implement that now and feel free to uh do that yourself as well so free to uh do that yourself as well so free to uh do that yourself as well",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 317,
      "text": "so first I commented uh this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 318,
      "text": "just so we first",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 319,
      "text": "I commented uh this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 320,
      "text": "just so we first",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 321,
      "text": "I commented uh this just so we don't pollute uh the notebook too much don't pollute uh the notebook too much don't pollute uh the notebook too much this is a nice way of in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 322,
      "text": "this is a nice way of in Python this is a nice way of in Python obtaining the highest ranking pair so obtaining the highest ranking pair so obtaining the highest ranking pair so we're basically calling the Max on this we're basically calling the Max on this we're basically calling the Max on this dictionary stats and this will return dictionary stats and this will return dictionary stats and this will return the maximum the maximum the maximum key and then the question is how does it key and then the question is how does it key",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 323,
      "text": "and then the question is how does it rank keys so you can provide it with a rank keys so you can provide it with a rank keys so you can provide it with a function that ranks keys and that function that ranks keys and that function that ranks keys and that function is just stats.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 324,
      "text": "getet uh stats.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 325,
      "text": "function is just stats.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 326,
      "text": "getet uh stats.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 327,
      "text": "function is just stats.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 328,
      "text": "getet uh stats.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 329,
      "text": "getet would basically return the value getet would basically return the value getet would basically return the value",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 330,
      "text": "and so we're ranking by the value",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 331,
      "text": "and and so we're ranking by the value",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 332,
      "text": "and and so we're ranking by the value and getting the maximum key",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 333,
      "text": "so it's 101 getting the maximum key",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 334,
      "text": "so it's 101 getting the maximum key",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 335,
      "text": "so it's 101 comma 32 as we saw now to actually merge comma 32 as we saw now to actually merge comma 32 as we saw now to actually merge 10132 um this is the function that I 10132 um this is the function that I 10132 um this is the function that I wrote but again there are many different wrote",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 336,
      "text": "but again there are many different wrote",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 337,
      "text": "but again there are many different versions of it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 338,
      "text": "so we're going to take a versions of it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 339,
      "text": "so we're going to take a versions of it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 340,
      "text": "so we're going to take a list of IDs and the the pair that we list of IDs and the the pair that we list of IDs and the the pair that we want to replace and that pair will be want to replace and that pair will be want to replace and that pair will be replaced with the new index replaced with the new index replaced with the new index idx so iterating through IDs if we find idx so iterating through IDs if we find idx so iterating through IDs if we find the pair swap it out for idx so we the pair swap it out for idx so we the pair swap it out for idx so we create this new list",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 341,
      "text": "and then we start create this new list",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 342,
      "text": "and then we start create this new list",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 343,
      "text": "and then we start at zero",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 344,
      "text": "and then we go through this at zero",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 345,
      "text": "and then we go through this at zero",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 346,
      "text": "and then we go through this entire list sequentially from left to entire list sequentially from left to entire list sequentially from left to right",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 347,
      "text": "and here we are checking for right",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 348,
      "text": "and here we are checking for right",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 349,
      "text": "and here we are checking for equality at the current position with equality at the current position with equality at the current position with the the the pair um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 350,
      "text": "so here we are checking that the pair",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 351,
      "text": "um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 352,
      "text": "so here we are checking that the pair",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 353,
      "text": "um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 354,
      "text": "so here we are checking that the pair matches now here is a bit of a pair matches now here is a bit of a pair matches now here is a bit of a tricky condition that you have to append tricky condition that you have to append tricky condition that you have to append if you're trying to be careful and that if you're trying to be careful and that if you're trying to be careful and that is that um you don't want this here to is that um you don't want this here to is that um you don't want this here to be out of Bounds at the very last be out of Bounds at the very last be out of Bounds at the very last position when you're on the rightmost position when you're on the rightmost position when you're on the rightmost element of this list otherwise this element of this list otherwise this element of this list otherwise this would uh give you an autof bounds error would uh give you an autof bounds error would uh give you an autof bounds error so we have to make sure that we're not so we have to make sure that we're not so we have to make sure that we're not at the very very last element",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 355,
      "text": "so uh this at the very very last element so uh this at the very very last element so uh this would be false for that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 356,
      "text": "so if we find a would be false for that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 357,
      "text": "so if we find a would be false for that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 358,
      "text": "so if we find a match we append to this new list that match we append to this new list that match we append to this new list that replacement index and we increment the replacement index and we increment the replacement index and we increment the position by two so we skip over that position by two so we skip over that position by two so we skip over that entire pair",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 359,
      "text": "but otherwise if we we entire pair",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 360,
      "text": "but otherwise if we we entire pair",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 361,
      "text": "but otherwise if we we haven't found a matching pair we just haven't found a matching pair we just haven't found a matching pair",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 362,
      "text": "we just sort of copy over the um element at that sort of copy over the um element at that sort of copy over the um element at that position and increment by one then position and increment by one then position and increment by one then return this so here's a very small toy return this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 363,
      "text": "so here's a very small toy return this so here's a very small toy example if we have a list 566 791",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 364,
      "text": "and we example if we have a list 566 791",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 365,
      "text": "and we example if we have a list 566 791",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 366,
      "text": "and we want to replace the occurrences of 67 want to replace the occurrences of 67 want to replace the occurrences of 67 with 99 then calling this on that will with 99 then calling this on that will with 99 then calling this on that will give us what we're asking for so here give us what we're asking for so here give us what we're asking for so here the 67 is replaced with the 67 is replaced with the 67 is replaced with 99",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 367,
      "text": "so now I'm going to uncomment this 99",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 368,
      "text": "so now I'm going to uncomment this 99",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 369,
      "text": "so now I'm going to uncomment this for our actual use case where we want to for our actual use case where we want to for our actual use case where we want to take our tokens we want to take the top take our tokens we want to take the top take our tokens we want to take the top pair here and replace it with 256 to get pair here and replace it with 256 to get pair here and replace it with 256 to get tokens to if we run this we get the tokens to if we run this we get the tokens to if we run this we get the following so recall that previously we following so recall that previously we following so recall that previously we had a length 616 in this list and now we had a length 616 in this list and now we had a length 616 in this list and now we have a length 596 right",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 370,
      "text": "so this have a length 596 right",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 371,
      "text": "so this have a length 596 right",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 372,
      "text": "so this decreased by 20 which makes sense decreased by 20 which makes sense decreased by 20 which makes sense because there are 20 occurrences because there are 20 occurrences because there are 20 occurrences moreover we can try to find 256 here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 373,
      "text": "and moreover we can try to find 256 here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 374,
      "text": "and moreover we can try to find 256 here and we see plenty of occurrences on off it we see plenty of occurrences on off it we see plenty of occurrences on off it and moreover just double check there and moreover just double check there and moreover just double check there should be no occurrence of 10132",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 375,
      "text": "so this should be no occurrence of 10132 so this should be no occurrence of 10132",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 376,
      "text": "so this is the original array plenty of them and is the original array plenty of them and is the original array plenty of them and in the second array there are no in the second array there are no in the second array there are no occurrences of 1032",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 377,
      "text": "so we've occurrences of 1032 so we've occurrences of 1032 so we've successfully merged this single pair and successfully merged this single pair and successfully merged this single pair and now we just uh iterate this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 378,
      "text": "so we are now we just uh iterate this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 379,
      "text": "so we are now we just uh iterate this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 380,
      "text": "so we are going to go over the sequence again find going to go over the sequence again find going to go over the sequence again find the most common pair and replace it so the most common pair and replace it so the most common pair and replace it so let me now write a y Loop that uses let me now write a y Loop that uses let me now write a y Loop that uses these functions to do this um sort of these functions to do this um sort of these functions to do this um sort of iteratively and how many times do we do iteratively and how many times do we do iteratively and how many times do we do it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 381,
      "text": "four",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 382,
      "text": "well that's totally up to us as it four",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 383,
      "text": "well that's totally up to us as it four",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 384,
      "text": "well that's totally up to us as a hyper parameter a hyper parameter a hyper parameter the more um steps we take the larger the more um steps we take the larger the more um steps we take the larger will be our vocabulary and the shorter will be our vocabulary and the shorter will be our vocabulary and the shorter will be our sequence and there is some will be our sequence and there is some will be our sequence and there is some sweet spot that we usually find works sweet spot that we usually find works sweet spot that we usually find works the best in practice",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 385,
      "text": "and so this is kind the best in practice",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 386,
      "text": "and so this is kind the best in practice",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 387,
      "text": "and so this is kind of a hyperparameter and we tune it and of a hyperparameter",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 388,
      "text": "and we tune it and of a hyperparameter",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 389,
      "text": "and we tune it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 390,
      "text": "and we find good vocabulary sizes as an we find good vocabulary sizes as an we find good vocabulary sizes as an example gp4 currently uses roughly example gp4 currently uses roughly example gp4 currently uses roughly 100,000 tokens and um bpark that those 100,000 tokens and um bpark that those 100,000 tokens and um bpark that those are reasonable numbers currently instead are reasonable numbers currently instead are reasonable numbers currently instead the are large language models so let me the are large language models",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 391,
      "text": "so let me the are large language models so let me now write uh putting putting it all now write uh putting putting it all now write uh putting putting it all together and uh iterating these steps together and uh iterating these steps together and uh iterating these steps okay now before we dive into the Y loop okay now before we dive into the Y loop okay now before we dive into the Y loop I wanted to add one more cell here where I wanted to add one more cell here where I wanted to add one more cell here where I went to the block post and instead of I went to the block post and instead of I went to the block post and instead of grabbing just the first paragraph or two grabbing just the first paragraph or two grabbing just the first paragraph or two I took the entire block post",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 392,
      "text": "and I I took the entire block post",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 393,
      "text": "and I I took the entire block post and I stretched it out in a single line and stretched it out in a single line and stretched it out in a single line and basically just using longer text will basically just using longer text will basically just using longer text will allow us to have more representative allow us to have more representative allow us to have more representative statistics for the bite Pairs",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 394,
      "text": "and we'll statistics for the bite Pairs and we'll statistics for the bite Pairs",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 395,
      "text": "and we'll just get a more sensible results out of just get a more sensible results out of just get a more sensible results out of it because it's longer text um so here it because it's longer text um so here it because it's longer text um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 396,
      "text": "so here we have the raw text we encode it into we have the raw text we encode it into we have the raw text we encode it into bytes using the utf8 encoding bytes using the utf8 encoding bytes using the utf8 encoding and then here as before we are just and then here as before we are just and then here as before we are just changing it into a list of integers in changing it into a list of integers in changing it into a list of integers in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 397,
      "text": "just so it's easier to work with Python just so it's easier to work with Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 398,
      "text": "just so it's easier to work with instead of the raw byes objects and then instead of the raw byes objects and then instead of the raw byes objects",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 399,
      "text": "and then this is the code that I came up with uh this is the code that I came up with uh this is the code that I came up with uh to actually do the merging in Loop these to actually do the merging in Loop these to actually do the merging in Loop these two functions here are identical to what two functions here are identical to what two functions here are identical to what we had above",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 400,
      "text": "I only included them here we had above I only included them here we had above I only included them here just so that you have the point of just so that you have the point of just so that you have the point of reference here so uh these two are reference here so uh these two are reference here so uh these two are identical",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 401,
      "text": "and then this is the new code identical",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 402,
      "text": "and then this is the new code identical",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 403,
      "text": "and then this is the new code that I added so the first first thing we that I added so the first first thing we that I added so the first first thing we want to do is we want to decide on the want to do is we want to decide on the want to do is we want to decide on the final vocabulary size that we want our final vocabulary size that we want our final vocabulary size that we want our tokenizer to have",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 404,
      "text": "and as I mentioned tokenizer to have and as I mentioned tokenizer to have and as I mentioned this is a hyper parameter and you set it this is a hyper parameter and you set it this is a hyper parameter and you set it in some way depending on your best in some way depending on your best in some way depending on your best performance so let's say for us we're performance",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 405,
      "text": "so let's say for us we're performance",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 406,
      "text": "so let's say for us we're going to use 276 because that way we're going to use 276 because that way we're going to use 276 because that way we're going to be doing exactly 20 going to be doing exactly 20 going to be doing exactly 20 merges and uh 20 merges because we merges and uh 20 merges because we merges and uh 20 merges because we already have already have already have 256 tokens for the raw bytes and to 256 tokens for the raw bytes and to 256 tokens for the raw bytes and to reach 276 we have to do 20 merges uh to reach 276 we have to do 20 merges uh to reach 276 we have to do 20 merges uh to add 20 new add 20 new add 20 new tokens here uh this is uh one way in tokens here uh this is uh one way in tokens here uh this is uh one way in Python to just create a copy of a list Python to just create a copy of a list Python to just create a copy of a list",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 407,
      "text": "so I'm taking the tokens list and by so I'm taking the tokens list and by so I'm taking the tokens list and by wrapping it in a list python will wrapping it in a list python will wrapping it in a list python will construct a new list of all the construct a new list of all the construct a new list of all the individual elements so this is just a individual elements so this is just a individual elements so this is just a copy copy copy operation then here I'm creating a operation then here I'm creating a operation then here I'm creating a merges uh dictionary so this merges merges uh dictionary",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 408,
      "text": "so this merges merges uh dictionary so this merges dictionary is going to maintain dictionary is going to maintain dictionary is going to maintain basically the child one child two basically the child one child two basically the child one child two mapping to a new uh token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 409,
      "text": "and so what mapping to a new uh token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 410,
      "text": "and so what mapping to a new uh token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 411,
      "text": "and so what we're going to be building up here is a we're going to be building up here is a we're going to be building up here is a binary tree of merges",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 412,
      "text": "but actually it's binary tree of merges",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 413,
      "text": "but actually it's binary tree of merges",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 414,
      "text": "but actually it's not exactly a tree because a tree would not exactly a tree because a tree would not exactly a tree because a tree would have a single root node with a bunch of have a single root node with a bunch of have a single root node with a bunch of leaves for us we're starting with the leaves for us we're starting with the leaves for us we're starting with the leaves on the bottom which are the leaves on the bottom which are the leaves on the bottom which are the individual bites those are the starting individual bites those are the starting individual bites those are the starting 256 tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 415,
      "text": "and then we're starting to 256 tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 416,
      "text": "and then we're starting to 256 tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 417,
      "text": "and then we're starting to like merge two of them at a time",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 418,
      "text": "and so like merge two of them at a time",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 419,
      "text": "and so like merge two of them at a time",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 420,
      "text": "and so it's not a tree it's more like a forest it's not a tree it's more like a forest it's not a tree it's more like a forest um uh as we merge these elements um uh as we merge these elements um uh as we merge these elements so for 20 merges we're going to find the so for 20 merges we're going to find the so for 20 merges we're going to find the most commonly occurring pair we're going most commonly occurring pair we're going most commonly occurring pair we're going to Mint a new token integer for it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 421,
      "text": "so I to Mint a new token integer for it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 422,
      "text": "so I to Mint a new token integer for it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 423,
      "text": "so I here will start at zero so we'll going here will start at zero so we'll going here will start at zero so we'll going to start at 256 we're going to print to start at 256 we're going to print to start at 256 we're going to print that we're merging it and we're going to that we're merging it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 424,
      "text": "and we're going to that we're merging it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 425,
      "text": "and we're going to replace all of the occurrences of that replace all of the occurrences of that replace all of the occurrences of that pair with the new new lied token and pair with the new new lied token and pair with the new new lied token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 426,
      "text": "and we're going to record that this pair of we're going to record that this pair of we're going to record that this pair of integers merged into this new integers merged into this new integers merged into this new integer so running this gives us the integer so running this gives us the integer so running this gives us the following following following output",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 427,
      "text": "so we did 20 merges and for output",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 428,
      "text": "so we did 20 merges and for output",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 429,
      "text": "so we did 20 merges and for example the first merge was exactly as example the first merge was exactly as example the first merge was exactly as before the before the before the 10132 um tokens merging into a new token 10132 um tokens merging into a new token 10132 um tokens merging into a new token 2556 now keep in mind that the 2556 now keep in mind that the 2556 now keep in mind that the individual uh tokens 101 and 32 can individual uh tokens 101 and 32 can individual uh tokens 101 and 32 can still occur in the sequence after still occur in the sequence after still occur in the sequence after merging it's only when they occur merging it's only when they occur merging it's only when they occur exactly consecutively that that becomes exactly consecutively that that becomes exactly consecutively that that becomes 256 256 256 now um and in particular the other thing now um and in particular the other thing now um and in particular the other thing to notice here is that the token 256 to notice here is that the token 256 to notice here is that the token 256 which is the newly minted token is also which is the newly minted token is also which is the newly minted token is also eligible for merging so here on the eligible for merging so here on the eligible for merging so here on the bottom the 20th merge was a merge of 25 bottom the 20th merge was a merge of 25 bottom the 20th merge was a merge of 25 and 259 becoming and 259 becoming and 259 becoming 275 so every time we replace these 275 so every time we replace these 275 so every time we replace these tokens they become eligible for merging tokens they become eligible for merging tokens they become eligible for merging in the next round of data ration so in the next round of data ration so in the next round of data ration so that's why we're building up a small that's why we're building up a small that's why we're building up a small sort of binary Forest instead of a sort of binary Forest instead of a sort of binary Forest instead of a single individual single individual single individual tree one thing we can take a look at as tree one thing we can take a look at as tree one thing we can take a look at as well is we can take a look at the well is we can take a look at the well is we can take a look at the compression ratio that we've achieved so compression ratio that we've achieved so compression ratio that we've achieved so in particular we started off with this in particular we started off with this in particular we started off with this tokens list",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 430,
      "text": "um so we started off with tokens list um so we started off with tokens list um so we started off with 24,000 bytes and after merging 20 times 24,000 bytes and after merging 20 times 24,000 bytes and after merging 20 times uh we now have only uh we now have only uh we now have only 19,000 um tokens and so therefore the 19,000 um tokens and so therefore the 19,000 um tokens and so therefore the compression ratio simply just dividing compression ratio simply just dividing compression ratio simply just dividing the two is roughly 1.27 so that's the the two is roughly 1.27 so that's the the two is roughly 1.27 so that's the amount of compression we were able to amount of compression we were able to amount of compression we were able to achieve of this text with only 20 achieve of this text with only 20 achieve of this text with only 20 merges um and of course the more merges um and of course the more merges um and of course the more vocabulary elements you add uh the vocabulary elements you add uh the vocabulary elements you add uh the greater the compression ratio here would greater the compression ratio here would greater the compression ratio here would be finally so that's kind of like um the be finally so that's kind of like um the be finally so that's kind of like um the training of the tokenizer if you will training of the tokenizer if you will training of the tokenizer if you will now 1 Point I wanted to make is that and now 1 Point I wanted to make is that and now 1 Point I wanted to make is that and maybe this is a diagram that can help um maybe this is a diagram that can help um maybe this is a diagram that can help um kind of illustrate is that tokenizer is kind of illustrate is that tokenizer is kind of illustrate is that tokenizer is a completely separate object from the a completely separate object from the a completely separate object from the large language model itself so large language model itself so large language model itself so everything in this lecture we're not everything in this lecture we're not everything in this lecture we're not really touching the llm itself uh we're really touching the llm itself uh we're really touching the llm itself uh we're just training the tokenizer this is a just training the tokenizer this is a just training the tokenizer this is a completely separate pre-processing stage completely separate pre-processing stage completely separate pre-processing stage usually so the tokenizer will have its usually so the tokenizer will have its usually so the tokenizer will have its own training set just like a large own training set just like a large own training set just like a large language model has a potentially language model has a potentially language model has a potentially different training set so the tokenizer different training set so the tokenizer different training set so the tokenizer has a training set of documents on which has a training set of documents on which has a training set of documents on which you're going to train the you're going to train the you're going to train the tokenizer and then and um we're tokenizer and then and um we're tokenizer and then and um we're performing The Bite pair encoding performing The Bite pair encoding performing The Bite pair encoding algorithm as we saw above to train the algorithm as we saw above to train the algorithm as we saw above to train the vocabulary of this vocabulary of this vocabulary of this tokenizer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 431,
      "text": "so it has its own training set tokenizer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 432,
      "text": "so it has its own training set tokenizer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 433,
      "text": "so it has its own training set it is a pre-processing stage that you it is a pre-processing stage that you it is a pre-processing stage that you would run a single time in the beginning would run a single time in the beginning would run a single time in the beginning um and the tokenizer is trained using um and the tokenizer is trained using um and the tokenizer is trained using bipar coding algorithm once you have the bipar coding algorithm once you have the bipar coding algorithm once you have the tokenizer once it's trained and you have tokenizer once it's trained and you have tokenizer once it's trained and you have the vocabulary and you have the merges the vocabulary",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 434,
      "text": "and you have the merges the vocabulary",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 435,
      "text": "and you have the merges uh we can do both encoding and decoding uh we can do both encoding and decoding uh we can do both encoding and decoding so these two arrows here so the so these two arrows here so the so these two arrows here so the tokenizer is a translation layer between tokenizer is a translation layer between tokenizer is a translation layer between raw text which is as we saw the sequence raw text which is as we saw the sequence raw text which is as we saw the sequence of Unicode code points it can take raw of Unicode code points it can take raw of Unicode code points it can take raw text and turn it into a token sequence text and turn it into a token sequence text and turn it into a token sequence and vice versa it can take a token and vice versa it can take a token and vice versa it can take a token sequence and translate it back into raw sequence and translate it back into raw sequence and translate it back into raw text so now that we have trained uh text so now that we have trained uh text so now that we have trained uh tokenizer and we have these merges we tokenizer and we have these merges we tokenizer and we have these merges we are going to turn to how we can do the are going to turn to how we can do the are going to turn to how we can do the encoding and the decoding step if you encoding and the decoding step if you encoding and the decoding step if you give me text here are the tokens and give me text here are the tokens and give me text here are the tokens and vice versa if you give me tokens here's vice versa if you give me tokens here's vice versa if you give me tokens here's the text once we have that we can the text once we have that we can the text once we have that we can translate between these two Realms and translate between these two Realms and translate between these two Realms and then the language model is going to be then the language model is going to be then the language model is going to be trained as a step two afterwards and trained as a step two afterwards and trained as a step two afterwards and typically in a in a sort of a typically in a in a sort of a typically in a in a sort of a state-of-the-art application you might state-of-the-art application you might state-of-the-art application you might take all of your training data for the take all of your training data for the take all of your training data for the language model and you might run it language model and you might run it language model and you might run it through the tokenizer and sort of through the tokenizer and sort of through the tokenizer and sort of translate everything into a massive translate everything into a massive translate everything into a massive token sequence and then you can throw token sequence and then you can throw token sequence and then you can throw away the raw text you're just left with away the raw text you're just left with away the raw text you're just left with the tokens themselves and those are the tokens themselves and those are the tokens themselves and those are stored on disk and that is what the stored on disk and that is what the stored on disk and that is what the large language model is actually reading large language model is actually reading large language model is actually reading when it's training on them",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 436,
      "text": "so this one when it's training on them",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 437,
      "text": "so this one when it's training on them so this one approach that you can take as a single approach that you can take as a single approach that you can take as a single massive pre-processing step a massive pre-processing step a massive pre-processing step a stage",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 438,
      "text": "um so",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 439,
      "text": "yeah basically I think the stage um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 440,
      "text": "so",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 441,
      "text": "yeah basically I think the stage um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 442,
      "text": "so",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 443,
      "text": "yeah basically I think the most important thing I want to get most important thing I want to get most important thing I want to get across is that this is completely across is that this is completely across is that this is completely separate stage it usually has its own separate stage it usually has its own separate stage it usually has its own entire uh training set you may want to entire uh training set you may want to entire uh training set you may want to have those training sets be different have those training sets be different have those training sets be different between the tokenizer and the logge between the tokenizer and the logge between the tokenizer and the logge language model so for example",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 444,
      "text": "when language model so for example when language model so for example when you're training the tokenizer as I you're training the tokenizer as I you're training the tokenizer as I mentioned we don't just care about the mentioned we don't just care about the mentioned we don't just care about the performance of English text we care performance of English text we care performance of English text we care about uh multi many different languages about uh multi many different languages about uh multi many different languages",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 445,
      "text": "and we also care about code or not code and we also care about code or not code and we also care about code or not code so you may want to look into different so you may want to look into different so you may want to look into different kinds of mixtures of different kinds of kinds of mixtures of different kinds of kinds of mixtures of different kinds of languages and different amounts of code languages and different amounts of code languages and different amounts of code and things like that because the amount and things like that because the amount and things like that because the amount of different language that you have in of different language that you have in of different language that you have in your tokenizer training set will your tokenizer training set will your tokenizer training set will determine how many merges of it there determine how many merges of it there determine how many merges of it there will be and therefore that determines will be and therefore that determines will be and therefore that determines the density with which uh this type of the density with which uh this type of the density with which uh this type of data is um sort of has in the token data is um sort of has in the token data is um sort of has in the token space and so roughly speaking space and so roughly speaking space and so roughly speaking intuitively if you add some amount of intuitively if you add some amount of intuitively if you add some amount of data like say you have a ton of Japanese data like say you have a ton of Japanese data like say you have a ton of Japanese data in your uh tokenizer training set data in your uh tokenizer training set data in your uh tokenizer training set then that means that more Japanese then that means that more Japanese then that means that more Japanese tokens will get merged tokens will get merged tokens will get merged and therefore Japanese will have shorter",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 446,
      "text": "and therefore Japanese will have shorter and therefore Japanese will have shorter sequences uh and that's going to be sequences uh and that's going to be sequences uh and that's going to be beneficial for the large language model beneficial for the large language model beneficial for the large language model which has a finite context length on which has a finite context length on which has a finite context length on which it can work on in in the token which it can work on in in the token which it can work on in in the token space uh so hopefully that makes sense space uh so hopefully that makes sense space uh so hopefully that makes sense",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 447,
      "text": "so we're now going to turn to encoding so we're now going to turn to encoding",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 448,
      "text": "so we're now going to turn to encoding and decoding now that we have trained a and decoding now that we have trained a and decoding now that we have trained a tokenizer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 449,
      "text": "so we have our merges and now tokenizer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 450,
      "text": "so we have our merges and now tokenizer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 451,
      "text": "so we have our merges and now how do we do encoding and decoding",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 452,
      "text": "okay how do we do encoding and decoding",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 453,
      "text": "okay how do we do encoding and decoding",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 454,
      "text": "okay so let's begin with decoding which is so let's begin with decoding which is so let's begin with decoding which is this Arrow over here so given a token this Arrow over here so given a token this Arrow over here so given a token sequence let's go through the tokenizer sequence let's go through the tokenizer sequence let's go through the tokenizer to get back a python string object so to get back a python string object so to get back a python string object so the raw text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 455,
      "text": "so this is the function the raw text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 456,
      "text": "so this is the function the raw text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 457,
      "text": "so this is the function that we' like to implement um we're that we' like to implement um we're that we' like to implement um we're given the list of integers and we want given the list of integers and we want given the list of integers and we want to return a python string if you'd like to return a python string if you'd like to return a python string if you'd like uh try to implement this function",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 458,
      "text": "uh try to implement this function uh try to implement this function yourself it's a fun exercise otherwise yourself",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 459,
      "text": "it's a fun exercise otherwise yourself",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 460,
      "text": "it's a fun exercise",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 461,
      "text": "otherwise I'm going to start uh pasting in my own I'm going to start uh pasting in my own I'm going to start uh pasting in my own solution so there are many different solution so there are many different solution so there are many different ways to do it um here's one way I will ways to do it um here's one way I will ways to do it um here's one way I will create an uh kind of pre-processing create an uh kind of pre-processing create an uh kind of pre-processing variable that I will call variable that I will call variable that I will call vocab and vocab is a mapping or a vocab and vocab is a mapping or a vocab and vocab is a mapping or a dictionary in Python for from the token dictionary in Python for from the token dictionary in Python for from the token uh ID to the bytes object for that token uh ID to the bytes object for that token uh ID to the bytes object for that token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 462,
      "text": "so we begin with the raw bytes for so we begin with the raw bytes for so we begin with the raw bytes for tokens from 0 to 255",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 463,
      "text": "and then we go in tokens from 0 to 255",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 464,
      "text": "and then we go in tokens from 0 to 255",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 465,
      "text": "and then we go in order of all the merges and we sort of order of all the merges and we sort of order of all the merges and we sort of uh populate this vocab list by doing an uh populate this vocab list by doing an uh populate this vocab list by doing an addition here so this is the basically addition here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 466,
      "text": "so this is the basically addition here so this is the basically the bytes representation of the first the bytes representation of the first the bytes representation of the first child followed by the second one and child followed by the second one and child followed by the second one and remember these are bytes objects so this remember these are bytes objects",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 467,
      "text": "so this remember these are bytes objects so this addition here is an addition of two addition here is an addition of two addition here is an addition of two bytes objects just concatenation bytes objects just concatenation bytes objects just concatenation so that's what we get",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 468,
      "text": "so that's what we get",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 469,
      "text": "so that's what we get here one tricky thing to be careful with here one tricky thing to be careful with here one tricky thing to be careful with by the way is that I'm iterating a by the way is that I'm iterating a by the way is that I'm iterating a dictionary in Python using a DOT items dictionary in Python using a DOT items dictionary in Python using a DOT items and uh it really matters that this runs and uh it really matters that this runs and uh it really matters that this runs in the order in which we inserted items in the order in which we inserted items in the order in which we inserted items into the merous dictionary luckily into the merous dictionary luckily into the merous dictionary luckily starting with python 3.7 this is starting with python 3.7 this is starting with python 3.7 this is guaranteed to be the case but before guaranteed to be the case but before guaranteed to be the case but before python 3.7 this iteration may have been python 3.7 this iteration may have been python 3.7 this iteration may have been out of order with respect to how we out of order with respect to how we out of order with respect to how we inserted elements into merges and this inserted elements into merges and this inserted elements into merges and this may not have worked but we are using an may not have worked",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 470,
      "text": "but we are using an may not have worked",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 471,
      "text": "but we are using an um modern python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 472,
      "text": "so we're okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 473,
      "text": "and then um modern python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 474,
      "text": "so we're okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 475,
      "text": "and then um modern python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 476,
      "text": "so we're okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 477,
      "text": "and then here uh given the IDS the first thing here uh given the IDS the first thing here uh given the IDS the first thing we're going to do is get the we're going to do is get the we're going to do is get the tokens so the way I implemented this tokens so the way I implemented this tokens so the way I implemented this here is I'm taking I'm iterating over here is I'm taking I'm iterating over here is I'm taking I'm iterating over all the IDS I'm using vocap to look up all the IDS",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 478,
      "text": "I'm using vocap to look up all the IDS",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 479,
      "text": "I'm using vocap to look up their bytes and then here this is one their bytes and then here this is one their bytes and then here this is one way in Python to concatenate all these way in Python to concatenate all these way in Python to concatenate all these bytes together to create our tokens and bytes together to create our tokens and bytes together to create our tokens and then these tokens here at this point are then these tokens here at this point are then these tokens here at this point are raw bytes",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 480,
      "text": "so I have to decode using UTF raw bytes",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 481,
      "text": "so I have to decode using UTF raw bytes",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 482,
      "text": "so I have to decode using UTF F now back into python strings",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 483,
      "text": "so F now back into python strings so F now back into python strings so previously we called that encode on a previously we called that encode on a previously we called that encode on a string object to get the bytes and now string object to get the bytes and now string object to get the bytes and now we're doing it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 484,
      "text": "Opposite we're taking the",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 485,
      "text": "we're doing it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 486,
      "text": "Opposite we're taking the",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 487,
      "text": "we're doing it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 488,
      "text": "Opposite we're taking the bytes and calling a decode on the bytes bytes and calling a decode on the bytes bytes and calling a decode on the bytes object to get a string in Python and object to get a string in Python and object to get a string in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 489,
      "text": "and then we can return then we can return then we can return text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 490,
      "text": "so um this is how we can do it now text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 491,
      "text": "so um this is how we can do it now text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 492,
      "text": "so um this is how we can do it now this actually has a um issue um in the this actually has a um issue um in the this actually has a um issue um in the way I implemented it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 493,
      "text": "and this could way I implemented it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 494,
      "text": "and this could way I implemented it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 495,
      "text": "and this could actually throw an error so try to think actually throw an error so try to think actually throw an error so try to think figure out why this code could actually figure out why this code could actually figure out why this code could actually result in an error if we plug in um uh result in an error if we plug in um uh result in an error if we plug in um uh some sequence of IDs that is some sequence of IDs that is some sequence of IDs that is unlucky",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 496,
      "text": "so let me demonstrate the issue unlucky",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 497,
      "text": "so let me demonstrate the issue unlucky",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 498,
      "text": "so let me demonstrate the issue when I try to decode just something like when I try to decode just something like when I try to decode just something like 97 I am going to get letter A here back 97",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 499,
      "text": "I am going to get letter A here back 97",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 500,
      "text": "I am going to get letter A here back so nothing too crazy happening but when so nothing too crazy happening but when so nothing too crazy happening but when I try to decode 128 as a single element I try to decode 128 as a single element I try to decode 128 as a single element the token 128 is what in string or in the token 128 is what in string or in the token 128 is what in string or in Python object uni Cod decoder utfa can't Python object uni Cod decoder utfa can't Python object uni Cod decoder utfa can't Decode by um 0x8 which is this in HEX in Decode by um 0x8 which is this in HEX in Decode by um 0x8 which is this in HEX in position zero invalid start bite what position zero invalid start bite what position zero invalid start bite what does that mean well to understand what does that mean well to understand what does that mean well to understand what this means we have to go back to our this means we have to go back to our this means we have to go back to our utf8 page uh that I briefly showed utf8 page uh that I briefly showed utf8 page uh that I briefly showed earlier and this is Wikipedia utf8 and earlier and this is Wikipedia utf8 and earlier and this is Wikipedia utf8 and basically there's a specific schema that basically there's a specific schema that basically there's a specific schema that utfa bytes take so in particular if you utfa bytes take so in particular if you utfa bytes take so in particular if you have a multi-te object for some of the have a multi-te object for some of the have a multi-te object for some of the Unicode characters they have to have Unicode characters they have to have Unicode characters they have to have this special sort of envelope in how the this special sort of envelope in how the this special sort of envelope in how the encoding works and so what's happening encoding works and so what's happening encoding works and so what's happening here is that invalid start pite",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 501,
      "text": "that's here is that invalid start pite that's here is that invalid start pite",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 502,
      "text": "that's because because because 128 the binary representation of it is 128 the binary representation of it is 128 the binary representation of it is one followed by all zeros",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 503,
      "text": "so we have one one followed by all zeros",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 504,
      "text": "so we have one one followed by all zeros",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 505,
      "text": "so we have one",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 506,
      "text": "and then all zero",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 507,
      "text": "and we see here that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 508,
      "text": "and then all zero",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 509,
      "text": "and we see here that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 510,
      "text": "and then all zero",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 511,
      "text": "and we see here that that doesn't conform to the format that doesn't conform to the format that doesn't conform to the format because one followed by all zero just because one followed by all zero just because one followed by all zero just doesn't fit any of these rules so to doesn't fit any of these rules so to doesn't fit any of these rules so to speak so it's an invalid start bite speak",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 512,
      "text": "so it's an invalid start bite speak",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 513,
      "text": "so it's an invalid start bite which is byte one this one must have a which is byte one this one must have a which is byte one this one must have a one following it and then a zero one following it and then a zero one following it and then a zero following it and then the content of following it and then the content of following it and then the content of your uni codee in x here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 514,
      "text": "so basically we your uni codee in x here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 515,
      "text": "so basically we your uni codee in x here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 516,
      "text": "so basically we don't um exactly follow the utf8 don't um exactly follow the utf8 don't um exactly follow the utf8 standard and this cannot be decoded and standard and this cannot be decoded and standard and this cannot be decoded and so the way to fix this um is to so the way to fix this um is to so the way to fix this um is to use this errors equals in bytes.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 517,
      "text": "decode use this errors equals in bytes.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 518,
      "text": "decode use this errors equals in bytes.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 519,
      "text": "decode function of python and by default errors function of python and by default errors function of python and by default errors is strict so we will throw an error if is strict so we will throw an error if is strict so we will throw an error if um it's not valid utf8 bytes encoding um it's not valid utf8 bytes encoding um it's not valid utf8 bytes encoding but there are many different things that but there are many different things that but there are many different things that you could put here on error handling you could put here on error handling you could put here on error handling this is the full list of all the errors this is the full list of all the errors this is the full list of all the errors that you can use and in particular that you can use and in particular that you can use and in particular instead of strict let's change it to instead of strict let's change it to instead of strict let's change it to replace and that will replace uh with replace and that will replace uh with replace and that will replace uh with this special marker this replacement this special marker this replacement this special marker this replacement character so errors equals replace and character so errors equals replace and character so errors equals replace and now we just get that character now we just get that character",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 520,
      "text": "now we just get that character back so basically not every single by back so basically not every single by back so basically not every single by sequence is valid sequence is valid sequence is valid utf8 and if it happens that your large utf8 and if it happens that your large utf8 and if it happens that your large language model for example predicts your language model for example predicts your language model for example predicts your tokens in a bad manner then they might tokens in a bad manner then they might tokens in a bad manner then they might not fall into valid utf8 and then we not fall into valid utf8 and then we not fall into valid utf8 and then we won't be able to decode them so the won't be able to decode them so the won't be able to decode them so the standard practice is to basically uh use standard practice is to basically uh use standard practice is to basically uh use errors equals replace and this is what errors equals replace and this is what errors equals replace and this is what you will also find in the openai um code you will also find in the openai um code you will also find in the openai um code that they released as well but basically that they released as well but basically that they released as well but basically whenever you see um this kind of a whenever you see um this kind of a whenever you see um this kind of a character in your output in that case uh character in your output in that case uh character in your output in that case uh something went wrong and the LM output something went wrong and the LM output something went wrong and the LM output not was not valid uh sort of sequence of not was not valid uh sort of sequence of not was not valid uh sort of sequence of tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 521,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 522,
      "text": "and now we're going to go tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 523,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 524,
      "text": "and now we're going to go tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 525,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 526,
      "text": "and now we're going to go the other way so we are going to the other way so we are going to the other way so we are going to implement implement implement this Arrow right here where we are going this Arrow right here where we are going this Arrow right here where we are going to be given a string",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 527,
      "text": "and we want to to be given a string and we want to to be given a string and we want to encode it into encode it into encode it into tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 528,
      "text": "so this is the signature of the tokens so this is the signature of the tokens so this is the signature of the function that we're interested in and um function that we're interested in and um function that we're interested in and um this should basically print a list of this should basically print a list of this should basically print a list of integers of the tokens so again uh try integers of the tokens so again uh try integers of the tokens so again uh try to maybe implement this yourself if to maybe implement this yourself if to maybe implement this yourself if you'd like a fun exercise uh and pause you'd like a fun exercise uh and pause you'd like a fun exercise uh and pause here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 529,
      "text": "otherwise I'm going to start here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 530,
      "text": "otherwise I'm going to start here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 531,
      "text": "otherwise I'm going to start putting in my putting in my putting in my solution",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 532,
      "text": "so again there are many ways to solution so again there are many ways to solution so again there are many ways to do this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 533,
      "text": "so um this is one of the ways",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 534,
      "text": "do this so um this is one of the ways do this so um this is one of the ways that sort of I came came up with so the that sort of I came came up with so the that sort of I came came up with so the first thing we're going to do is we are first thing we're going to do is we are first thing we're going to do is we are going going going to uh take our text encode it into utf8 to uh take our text encode it into utf8 to uh take our text encode it into utf8 to get the raw bytes and then as before to get the raw bytes and then as before to get the raw bytes and then as before we're going to call list on the bytes we're going to call list on the bytes we're going to call list on the bytes object to get a list of integers of object to get a list of integers of object to get a list of integers of those bytes so those are the starting those bytes so those are the starting those bytes so those are the starting tokens those are the raw bytes of our tokens those are the raw bytes of our tokens those are the raw bytes of our sequence but now of course according to sequence but now of course according to sequence but now of course according to the merges dictionary above and recall the merges dictionary above and recall the merges dictionary above and recall this was the this was the this was the merges some of the bytes may be merged merges some of the bytes may be merged merges some of the bytes may be merged according to this lookup in addition to according to this lookup in addition to according to this lookup in addition to that remember that the merges was built that remember that the merges was built that remember that the merges was built from top to bottom and this is sort of from top to bottom and this is sort of from top to bottom and this is sort of the order in which we inserted stuff the order in which we inserted stuff the order in which we inserted stuff into merges",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 535,
      "text": "and so we prefer to do all into merges",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 536,
      "text": "and so we prefer to do all into merges",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 537,
      "text": "and so we prefer to do all these merges in the beginning before we these merges in the beginning before we these merges in the beginning before we do these merges later because um for do these merges later because um for do these merges later because um for example this merge over here relies on example this merge over here relies on example this merge over here relies on the 256 which got merged here so we have the 256 which got merged here so we have the 256 which got merged here so we have to go in the order from top to bottom to go in the order from top to bottom to go in the order from top to bottom sort of if we are going to be merging sort of if we are going to be merging sort of if we are going to be merging anything now we expect to be doing a few anything now we expect to be doing a few anything now we expect to be doing a few merges so we're going to be doing W merges so we're going to be doing W merges so we're going to be doing W true",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 538,
      "text": "um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 539,
      "text": "and now we want to find a pair true um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 540,
      "text": "and now we want to find a pair true um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 541,
      "text": "and now we want to find a pair of byes that is consecutive that we are of byes that is consecutive that we are of byes that is consecutive that we are allowed to merge according to this in allowed to merge according to this in allowed to merge according to this in order to reuse some of the functionality order to reuse some of the functionality order to reuse some of the functionality that we've already written I'm going to that we've already written I'm going to that we've already written I'm going to reuse the function uh get reuse the function uh get reuse the function uh get stats so recall that get stats uh will stats so recall that get stats uh will stats so recall that get stats uh will give us the we'll basically count up how give us the we'll basically count up how give us the we'll basically count up how many times every single pair occurs in many times every single pair occurs in many times every single pair occurs in our sequence of tokens and return that our sequence of tokens and return that our sequence of tokens and return that as a dictionary and the dictionary was a as a dictionary and the dictionary was a as a dictionary and the dictionary was a mapping from all the different uh by mapping from all the different uh by mapping from all the different uh by pairs to the number of times that they pairs to the number of times that they pairs to the number of times that they occur right um at this point we don't occur right um at this point we don't occur right um at this point we don't actually care how many times they occur actually care how many times they occur actually care how many times they occur in the sequence we only care what the in the sequence we only care what the in the sequence we only care what the raw pairs are in that sequence and so raw pairs are in that sequence and so raw pairs are in that sequence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 542,
      "text": "and so I'm only going to be using basically the I'm only going to be using basically the I'm only going to be using basically the keys of the dictionary I only care about keys of the dictionary I only care about keys of the dictionary I only care about the set of possible merge candidates if the set of possible merge candidates if the set of possible merge candidates if that makes that makes that makes sense now we want to identify the pair sense now we want to identify the pair sense now we want to identify the pair that we're going to be merging at this that we're going to be merging at this that we're going to be merging at this stage of the loop so what do we want we stage of the loop",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 543,
      "text": "so what do we want we stage of the loop so what do we want we want to find the pair or like the a key want to find the pair or like the a key want to find the pair or like the a key inside stats that has the lowest index inside stats that has the lowest index inside stats that has the lowest index in the merges uh dictionary because we in the merges uh dictionary because we in the merges uh dictionary because we want to do all the early merges before want to do all the early merges before want to do all the early merges before we work our way to the late we work our way to the late we work our way to the late merges so again there are many different merges so again there are many different merges so again there are many different ways to implement this but I'm going to ways to implement this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 544,
      "text": "but I'm going to ways to implement this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 545,
      "text": "but I'm going to do something a little bit fancy do something a little bit fancy do something a little bit fancy here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 546,
      "text": "so I'm going to be using the Min here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 547,
      "text": "so I'm going to be using the Min here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 548,
      "text": "so I'm going to be using the Min over an iterator in Python when you call over an iterator in Python when you call over an iterator in Python when you call Min on an iterator and stats here as a Min on an iterator and stats here as a Min on an iterator and stats here as a dictionary we're going to be iterating dictionary we're going to be iterating dictionary we're going to be iterating the keys of this dictionary in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 549,
      "text": "so the keys of this dictionary in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 550,
      "text": "so the keys of this dictionary in Python so we're looking at all the pairs inside we're looking at all the pairs inside we're looking at all the pairs inside stats um which are all the consecutive stats um which are all the consecutive stats um which are all the consecutive Pairs and we're going to be taking the Pairs and we're going to be taking the Pairs and we're going to be taking the consecutive pair inside tokens that has consecutive pair inside tokens that has consecutive pair inside tokens that has the minimum what the Min takes a key the minimum what the Min takes a key the minimum what the Min takes a key which gives us the function that is which gives us the function that is which gives us the function that is going to return a value over which we're going to return a value over which we're going to return a value over which we're going to do the Min and the one we care going to do the Min and the one we care going to do the Min and the one we care about is we're we care about taking about is we're we care about taking about is we're we care about taking merges and basically getting um that merges and basically getting um that merges and basically getting um that pairs pairs pairs index so basically for any pair inside index so basically for any pair inside index",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 551,
      "text": "so basically for any pair inside stats we are going to be looking into stats we are going to be looking into stats we are going to be looking into merges at what index it has",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 552,
      "text": "and we want merges at what index it has and we want merges at what index it has",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 553,
      "text": "and we want to get the pair with the Min number so to get the pair with the Min number so to get the pair with the Min number",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 554,
      "text": "so as an example if there's a pair 101 and as an example if there's a pair 101 and as an example if there's a pair 101 and 32 we definitely want to get that pair 32 we definitely want to get that pair 32 we definitely want to get that pair uh we want to identify it here and uh we want to identify it here and uh we want to identify it here and return it and pair would become 10132 if return it and pair would become 10132 if return it and pair would become 10132 if it it it occurs and the reason that I'm putting a occurs and the reason that I'm putting a occurs and the reason that I'm putting a float INF here as a fall back is that in float INF here as a fall back is that in float INF here as a fall back is that in the get function when we call uh when we the get function when we call uh when we the get function when we call uh when we basically consider a pair that doesn't basically consider a pair that doesn't basically consider a pair that doesn't occur in the merges then that pair is occur in the merges then that pair is occur in the merges then that pair is not eligible to be merged right",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 555,
      "text": "so if in not eligible to be merged right so if in not eligible to be merged right so if in the token sequence there's some pair the token sequence there's some pair the token sequence there's some pair that is not a merging pair it cannot be that is not a merging pair it cannot be that is not a merging pair it cannot be merged then uh it doesn't actually occur merged then uh it doesn't actually occur merged then uh it doesn't actually occur here and it doesn't have an index and",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 556,
      "text": "uh here and it doesn't have an index and uh here and it doesn't have an index and uh it cannot be merged which we will denote it cannot be merged which we will denote it cannot be merged which we will denote as float INF and the reason Infinity is as float INF and the reason Infinity is as float INF and the reason Infinity is nice here is because for sure we're nice here is because for sure we're nice here is because for sure we're guaranteed that it's not going to guaranteed that it's not going to guaranteed that it's not going to participate in the list of candidates participate in the list of candidates participate in the list of candidates when we do the men",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 557,
      "text": "so uh so this is one when we do the men so uh so this is one when we do the men",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 558,
      "text": "so uh so this is one way to do it so B basically long story way to do it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 559,
      "text": "so B basically long story way to do it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 560,
      "text": "so B basically long story short this Returns the most eligible short this Returns the most eligible short this Returns the most eligible merging candidate pair uh that occurs in merging candidate pair uh that occurs in merging candidate pair uh that occurs in the tokens now one thing to be careful the tokens now one thing to be careful the tokens now one thing to be careful with here is this uh function here might with here is this uh function here might with here is this uh function here might fail in the following way if there's fail in the following way if there's fail in the following way if there's nothing to merge then uh uh then there's nothing to merge then uh uh then there's nothing to merge then uh uh then there's nothing in merges um that satisfi that nothing in merges um that satisfi that nothing in merges um that satisfi that is satisfied anymore there's nothing to is satisfied anymore there's nothing to is satisfied anymore there's nothing to merge everything just returns float",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 561,
      "text": "imps merge everything just returns float imps merge everything just returns float imps and then the pair I think will just and then the pair I think will just and then the pair I think will just become the very first element of stats become the very first element of stats become the very first element of stats um but this pair is not actually a",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 562,
      "text": "um but this pair is not actually a",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 563,
      "text": "um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 564,
      "text": "but this pair is not actually a mergeable pair it just becomes the first mergeable pair it just becomes the first mergeable pair it just becomes the first pair inside stats arbitrarily because pair inside stats arbitrarily because pair inside stats arbitrarily because all of these pairs evaluate to float in all of these pairs evaluate to float in all of these pairs evaluate to float in for the merging Criterion so basically for the merging Criterion so basically for the merging Criterion so basically it could be that this this doesn't look it could be that this this doesn't look it could be that this this doesn't look succeed because there's no more merging succeed because there's no more merging succeed because there's no more merging pairs so if this pair is not in merges pairs so if this pair is not in merges pairs so if this pair is not in merges that was returned then this is a signal that was returned then this is a signal that was returned then this is a signal for us that actually there was nothing for us that actually there was nothing for us that actually there was nothing to merge no single pair can be merged to merge no single pair can be merged to merge no single pair can be merged anymore in that case we will break anymore in that case we will break anymore in that case we will break out um nothing else can be merged you may come up with a different merged you may come up with a different implementation by the way this is kind implementation by the way this is kind implementation by the way this is kind of like really trying hard in of like really trying hard in of like really trying hard in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 565,
      "text": "um but really we're just trying Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 566,
      "text": "um but really we're just trying Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 567,
      "text": "um but really we're just trying to find a pair that can be merged with to find a pair that can be merged with to find a pair that can be merged with the lowest index the lowest index the lowest index here now if we did find a pair that is here now if we did find a pair that is here now if we did find a pair that is inside merges with the lowest index then inside merges with the lowest index then inside merges with the lowest index then we can merge it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 568,
      "text": "so we're going to look into the merger so we're going to look into the merger dictionary for that pair to look up the dictionary for that pair to look up the dictionary for that pair to look up the index",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 569,
      "text": "and we're going to now merge that index and we're going to now merge that index",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 570,
      "text": "and we're going to now merge that into that index so we're going to do into that index so we're going to do into that index so we're going to do tokens equals",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 571,
      "text": "and we're going to tokens equals and we're going to tokens equals and we're going to replace the original tokens we're going replace the original tokens we're going replace the original tokens we're going to be replacing the pair pair",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 572,
      "text": "and we're to be replacing the pair pair and we're to be replacing the pair pair",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 573,
      "text": "and we're going to be replacing it with index idx going to be replacing it with index idx going to be replacing it with index idx and this returns a new list of tokens and this returns a new list of tokens and this returns a new list of tokens where every occurrence of pair is where every occurrence of pair is where every occurrence of pair is replaced with idx so we're doing a merge replaced with idx so we're doing a merge replaced with idx so we're doing a merge",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 574,
      "text": "and we're going to be continuing this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 575,
      "text": "and we're going to be continuing this and we're going to be continuing this until eventually nothing can be merged until eventually nothing can be merged until eventually nothing can be merged we'll come out here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 576,
      "text": "and we'll break out we'll come out here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 577,
      "text": "and we'll break out we'll come out here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 578,
      "text": "and we'll break out",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 579,
      "text": "and here we just return and here we just return and here we just return tokens and so that that's the tokens and so that that's the tokens and so that that's the implementation I think so hopefully this implementation I think so hopefully this implementation I think so hopefully this runs",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 580,
      "text": "okay cool",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 581,
      "text": "um yeah",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 582,
      "text": "and this looks uh runs",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 583,
      "text": "okay cool",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 584,
      "text": "um yeah",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 585,
      "text": "and this looks uh runs",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 586,
      "text": "okay cool",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 587,
      "text": "um yeah",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 588,
      "text": "and this looks uh reasonable so for example 32 is a space reasonable so for example 32 is a space reasonable so for example 32 is a space in asky",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 589,
      "text": "so that's here um so this looks in asky",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 590,
      "text": "so that's here um so this looks in asky",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 591,
      "text": "so that's here um so this looks like it worked",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 592,
      "text": "great",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 593,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 594,
      "text": "so let's wrap like it worked great",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 595,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 596,
      "text": "so let's wrap like it worked great",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 597,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 598,
      "text": "so let's wrap up this section of the video at least I up this section of the video at least I up this section of the video at least I wanted to point out that this is not wanted to point out that this is not wanted to point out that this is not quite the right implementation just yet quite the right implementation just yet quite the right implementation just yet because we are leaving out a special because we are leaving out a special because we are leaving out a special case so in particular if uh we try to do case so in particular if uh we try to do case so in particular if uh we try to do this this would give us an error and the this this would give us an error and the this this would give us an error and the issue is that um if we only have a issue is that um if we only have a issue is that um if we only have a single character or an empty string then single character or an empty string then single character or an empty string then stats is empty and that causes an issue stats is empty and that causes an issue stats is empty and that causes an issue inside Min so one way to fight this is inside Min so one way to fight this is inside Min so one way to fight this is if L of tokens is at least two because if L of tokens is at least two because if L of tokens is at least two because if it's less than two it's just a single if it's less than two it's just a single if it's less than two it's just a single token or no tokens then let's just uh token or no tokens then let's just uh token or no tokens then let's just uh there's nothing to merge so we just there's nothing to merge so we just there's nothing to merge so we just return so that would fix uh that return so that would fix uh that return so that would fix uh that case",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 599,
      "text": "Okay and then second I have a few case",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 600,
      "text": "Okay and then second I have a few case",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 601,
      "text": "Okay and then second I have a few test cases here for us as well so first test cases here for us as well so first test cases here for us as well so first let's make sure uh about or let's note let's make sure uh about or let's note let's make sure uh about or let's note the following if we take a string and we the following if we take a string and we the following if we take a string and we try to encode it and then decode it back try to encode it and then decode it back try to encode it and then decode it back you'd expect to get the same string back you'd expect to get the same string back you'd expect to get the same string back right is that true for all strings so I think uh so here it is the strings",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 602,
      "text": "so I think uh so here it is the case",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 603,
      "text": "and I think in general this is case",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 604,
      "text": "and I think in general this is case",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 605,
      "text": "and I think in general this is probably the case um but notice that probably the case um but notice that probably the case um but notice that going backwards is not is not you're not going backwards is not is not you're not going backwards is not is not you're not going to have an identity going going to have an identity going going to have an identity going backwards because as I mentioned us not backwards because as I mentioned us not backwards because as I mentioned us not all token sequences are valid utf8 uh all token sequences are valid utf8 uh all token sequences are valid utf8 uh sort of by streams",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 606,
      "text": "and so so therefore sort of by streams and so so therefore sort of by streams and so so therefore you're some of them can't even be you're some of them can't even be you're some of them can't even be decodable um so this only goes in One decodable um so this only goes in One decodable um so this only goes in One Direction but for that one direction we Direction but for that one direction we Direction but for that one direction we can check uh here if we take the can check uh here if we take the can check uh here if we take the training text which is the text that we training text which is the text that we training text which is the text that we train to tokenizer around we can make train to tokenizer around we can make train to tokenizer around we can make sure that when we encode and decode we sure that when we encode and decode we sure that when we encode and decode we get the same thing back which is true get the same thing back which is true get the same thing back which is true and here I took some validation data",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 607,
      "text": "so",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 608,
      "text": "and here I took some validation data",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 609,
      "text": "so",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 610,
      "text": "and here I took some validation data",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 611,
      "text": "so I went to I think this web page",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 612,
      "text": "and I I went to I think this web page",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 613,
      "text": "and I I went to I think this web page",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 614,
      "text": "and I grabbed some text so this is text that grabbed some text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 615,
      "text": "so this is text that grabbed some text so this is text that the tokenizer has not seen and we can the tokenizer has not seen and we can the tokenizer has not seen and we can make sure that this also works um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 616,
      "text": "okay make sure that this also works um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 617,
      "text": "okay make sure that this also works um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 618,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 619,
      "text": "so that gives us some confidence that so that gives us some confidence that so that gives us some confidence that this was correctly implemented this was correctly implemented this was correctly implemented so those are the basics of the bite pair so those are the basics of the bite pair so those are the basics of the bite pair encoding algorithm we saw how we can uh encoding algorithm we saw how we can uh encoding algorithm we saw how we can uh take some training set train a tokenizer take some training set train a tokenizer take some training set train a tokenizer the parameters of this tokenizer really the parameters of this tokenizer really the parameters of this tokenizer really are just this dictionary of merges and are just this dictionary of merges and are just this dictionary of merges and that basically creates the little binary that basically creates the little binary that basically creates the little binary Forest on top of raw Forest on top of raw Forest on top of raw bites once we have this the merges table bites once we have this the merges table bites once we have this the merges table we can both encode and decode between we can both encode and decode between we can both encode and decode between raw text and token sequences so that's raw text and token sequences so that's raw text and token sequences so that's the the simplest setting of The the the simplest setting of The the the simplest setting of The tokenizer what we're going to do now tokenizer what we're going to do now tokenizer what we're going to do now though is we're going to look at some of though is we're going to look at some of though is we're going to look at some of the St the art lar language models and the St the art lar language models and the St the art lar language models and the kinds of tokenizers that they use the kinds of tokenizers that they use the kinds of tokenizers that they use and we're going to see that this picture and we're going to see that this picture and we're going to see that this picture complexifies very quickly so we're going complexifies very quickly so we're going complexifies very quickly so we're going to go through the details of this comp to go through the details of this comp to go through the details of this comp complexification one at a time so let's complexification one at a time so let's complexification one at a time",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 620,
      "text": "so let's kick things off by looking at the GPD kick things off by looking at the GPD kick things off by looking at the GPD Series so in particular I have the gpt2 Series so in particular I have the gpt2 Series so in particular I have the gpt2 paper here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 621,
      "text": "um and this paper is from paper here um and this paper is from paper here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 622,
      "text": "um and this paper is from 2019 or so so 5 years ago and let's 2019 or so so 5 years ago and let's 2019 or so so 5 years ago and let's scroll down to input representation this scroll down to input representation this scroll down to input representation this is where they talk about the tokenizer is where they talk about the tokenizer is where they talk about the tokenizer that they're using for gpd2",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 623,
      "text": "now this is that they're using for gpd2 now this is that they're using for gpd2 now this is all fairly readable",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 624,
      "text": "so I encourage you all fairly readable",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 625,
      "text": "so I encourage you all fairly readable",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 626,
      "text": "so I encourage you to pause and um read this yourself but to pause and um read this yourself but to pause and um read this yourself",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 627,
      "text": "but this is where they motivate the use of this is where they motivate the use of this is where they motivate the use of the bite pair encoding algorithm on the the bite pair encoding algorithm on the the bite pair encoding algorithm on the bite level representation of utf8 bite level representation of utf8 bite level representation of utf8 encoding",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 628,
      "text": "so this is where they motivate encoding so this is where they motivate encoding",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 629,
      "text": "so this is where they motivate it and they talk about the vocabulary",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 630,
      "text": "it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 631,
      "text": "and they talk about the vocabulary it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 632,
      "text": "and they talk about the vocabulary sizes and everything now everything here sizes and everything now everything here sizes and everything now everything here is exactly as we've covered it so far is exactly as we've covered it so far is exactly as we've covered it so far but things start to depart around here but things start to depart around here but things start to depart around here so what they mention is that they don't",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 633,
      "text": "so what they mention is that they don't so what they mention is that they don't just apply the naive algorithm as we just apply the naive algorithm as we just apply the naive algorithm as we have done it and in particular here's a have done it and in particular here's a have done it and in particular here's a example suppose that you have common example suppose that you have common example suppose that you have common words like dog what will happen is that words like dog what will happen is that words like dog what will happen is that dog of course occurs very frequently in dog of course occurs very frequently in dog of course occurs very frequently in the text and it occurs right next to all the text and it occurs right next to all the text and it occurs right next to all kinds of punctuation as an example",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 634,
      "text": "so kinds of punctuation as an example so kinds of punctuation as an example so doc dot dog exclamation mark dog doc dot dog exclamation mark dog doc dot dog exclamation mark dog question mark Etc and naively you might question mark Etc and naively you might question mark Etc and naively you might imagine that the BP algorithm could imagine that the BP algorithm could imagine that the BP algorithm could merge these to be single tokens and then merge these to be single tokens and then merge these to be single tokens and then you end up with lots of tokens that are you end up with lots of tokens that are you end up with lots of tokens that are just like dog with a slightly different just like dog with a slightly different just like dog with a slightly different punctuation and so it feels like you're punctuation and so it feels like you're punctuation and so it feels like you're clustering things that shouldn't be clustering things that shouldn't be clustering things that shouldn't be clustered you're combining kind of clustered you're combining kind of clustered you're combining kind of semantics with semantics with semantics with uation and this uh feels suboptimal and uation and this uh feels suboptimal and uation and this uh feels suboptimal and indeed they also say that this is indeed they also say that this is indeed they also say that this is suboptimal according to some of the suboptimal according to some of the suboptimal according to some of the experiments so what they want to do is experiments",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 635,
      "text": "so what they want to do is experiments so what they want to do is they want to top down in a manual way they want to top down in a manual way they want to top down in a manual way enforce that some types of um characters enforce that some types of um characters enforce that some types of um characters should never be merged together um so should never be merged together um so should never be merged together",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 636,
      "text": "um so they want to enforce these merging rules they want to enforce these merging rules they want to enforce these merging rules on top of the bite",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 637,
      "text": "PA encoding algorithm on top of the bite PA encoding algorithm on top of the bite PA encoding algorithm so let's take a look um at their code so let's take a look um at their code",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 638,
      "text": "so let's take a look um at their code and see how they actually enforce this and see how they actually enforce this and see how they actually enforce this and what kinds of mergy they actually do and what kinds of mergy they actually do and what kinds of mergy they actually do perform",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 639,
      "text": "so I have to to tab open here perform so I have to to tab open here perform so I have to to tab open here for gpt2 under open AI on GitHub and for gpt2 under open AI on GitHub and for gpt2 under open AI on GitHub and when we go to when we go to when we go to Source there is an encoder thatp now I Source there is an encoder thatp now I Source there is an encoder thatp now I don't personally love that they call it don't personally love that they call it don't personally love that they call it encoder dopy because this is the encoder dopy because this is the encoder dopy because this is the tokenizer and the tokenizer can do both tokenizer and the tokenizer can do both tokenizer and the tokenizer can do both encode and decode",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 640,
      "text": "uh so it feels kind of encode and decode",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 641,
      "text": "uh so it feels kind of encode and decode",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 642,
      "text": "uh so it feels kind of awkward to me that it's called encoder awkward to me that it's called encoder awkward to me that it's called encoder",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 643,
      "text": "but that is the tokenizer and there's a",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 644,
      "text": "but that is the tokenizer and there's a",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 645,
      "text": "but that is the tokenizer and there's a lot going on here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 646,
      "text": "and we're going to lot going on here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 647,
      "text": "and we're going to lot going on here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 648,
      "text": "and we're going to step through it in detail at one point step through it in detail at one point step through it in detail at one point for now I just want to focus on this for now I just want to focus on this for now I just want to focus on this part here the create a rigix pattern part here the create a rigix pattern part here the create a rigix pattern here that looks very complicated and here that looks very complicated and here that looks very complicated and we're going to go through it in a bit uh we're going to go through it in a bit uh we're going to go through it in a bit",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 649,
      "text": "uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 650,
      "text": "but this is the core part that allows but this is the core part that allows but this is the core part that allows them to enforce rules uh for what parts them to enforce rules uh for what parts them to enforce rules uh for what parts of the text Will Never Be merged for of the text Will Never Be merged for of the text Will Never Be merged for sure now notice that re. compile here is sure now notice that re. compile here is sure now notice that re. compile here is a little bit misleading because we're a little bit misleading because we're a little bit misleading because we're not just doing import re which is the not just doing import re which is the not just doing import re which is the python re module we're doing import reex python re module we're doing import reex python re module we're doing import reex as re and reex is a python package that as re and reex is a python package that as re and reex is a python package that you can install P install r x",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 651,
      "text": "and it's you can install P install r x",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 652,
      "text": "and it's you can install P install r x",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 653,
      "text": "and it's basically an extension of re",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 654,
      "text": "so it's a basically an extension of re",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 655,
      "text": "so it's a basically an extension of re",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 656,
      "text": "so it's a bit more powerful bit more powerful bit more powerful re um re um re um so let's take a look at this pattern and so let's take a look at this pattern and so let's take a look at this pattern and what it's doing and why this is actually what it's doing and why this is actually what it's doing and why this is actually doing the separation that they are doing the separation that they are doing the separation that they are looking for",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 657,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 658,
      "text": "so I've copy pasted the looking for",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 659,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 660,
      "text": "so I've copy pasted the looking for",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 661,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 662,
      "text": "so I've copy pasted the pattern here to our jupit notebook where pattern here to our jupit notebook where pattern here to our jupit notebook where we left off and let's take this pattern we left off and let's take this pattern we left off and let's take this pattern for a spin so in the exact same way that for a spin so in the exact same way that for a spin so in the exact same way that their code does we're going to call an their code does we're going to call an their code does we're going to call an re. findall for this pattern on any re. findall for this pattern on any re. findall for this pattern on any arbitrary string that we are interested arbitrary string that we are interested arbitrary string that we are interested so this is the string that we want to so this is the string that we want to so this is the string that we want to encode into tokens um to feed into n llm encode into tokens um to feed into n llm encode into tokens um to feed into n",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 663,
      "text": "llm like gpt2",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 664,
      "text": "so what exactly is this doing like gpt2 so what exactly is this doing like gpt2",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 665,
      "text": "so what exactly is this doing well re. findall will take this pattern well re. findall will take this pattern well re. findall will take this pattern and try to match it against a and try to match it against a and try to match it against a string um the way this works is that you string um the way this works is that you string um the way this works is that you are going from left to right in the are going from left to right in the are going from left to right in the string and you're trying to match the string and you're trying to match the string and you're trying to match the pattern and R.F find all will get all pattern and R.F find all will get all pattern and R.F find all will get all the occurrences and organize them into a the occurrences and organize them into a the occurrences and organize them into a list now when you look at the um when list now when you look at the um when list now when you look at the um when you look at this pattern first of all you look at this pattern first of all you look at this pattern first of all notice that this is a raw string um and notice that this is a raw string um and notice that this is a raw string",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 666,
      "text": "um and then these are three double quotes just then these are three double quotes just then these are three double quotes just to start the string so really the string to start the string so really the string to start the string so really the string itself this is the pattern itself itself this is the pattern itself itself this is the pattern itself right and notice that it's made up of a right and notice that it's made up of a right and notice that it's made up of a lot of ores so see these vertical bars lot of ores so see these vertical bars lot of ores so see these vertical bars those are ores in reg X",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 667,
      "text": "and so you go those are ores in reg X",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 668,
      "text": "and so you go those are ores in reg X",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 669,
      "text": "and so you go from left to right in this pattern and from left to right in this pattern and from left to right in this pattern and try to match it against the string try to match it against the string try to match it against the string wherever you are so we have",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 670,
      "text": "hello",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 671,
      "text": "and wherever you are so we have hello and wherever you are so we have hello",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 672,
      "text": "and we're going to try to match it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 673,
      "text": "well it's we're going to try to match it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 674,
      "text": "well it's we're going to try to match it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 675,
      "text": "well it's not apostrophe s",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 676,
      "text": "it's not apostrophe t not apostrophe s",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 677,
      "text": "it's not apostrophe t not apostrophe s",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 678,
      "text": "it's not apostrophe t or any of these",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 679,
      "text": "but it is an optional or any of these",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 680,
      "text": "but it is an optional or any of these",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 681,
      "text": "but it is an optional space followed by- P of uh sorry SL P of space followed by- P of uh sorry SL P of space followed by- P of uh sorry SL P of L one or more times what is/ P of L it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 682,
      "text": "L one or more times what is/ P of L it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 683,
      "text": "L one or more times what is/ P of L it is coming to some documentation that I is coming to some documentation that I is coming to some documentation that I found um there might be other sources as found um there might be other sources as found um there might be other sources as well uh SLP is a letter any kind of well uh SLP is a letter any kind of well uh SLP is a letter any kind of letter from any language and hello is letter from any language and hello is letter from any language and hello is made up of letters",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 684,
      "text": "h e l Etc so optional made up of letters h e l Etc so optional made up of letters",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 685,
      "text": "h e l Etc so optional space followed by a bunch of letters one space followed by a bunch of letters one space followed by a bunch of letters one or more letters is going to match hello or more letters is going to match hello or more letters is going to match",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 686,
      "text": "hello",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 687,
      "text": "but then the match ends because a white but then the match ends because a white but then the match ends because a white space is not a letter so from there on space is not a letter so from there on space is not a letter so from there on begins a new sort of attempt to match begins a new sort of attempt to match begins a new sort of attempt to match against the string again and starting in against the string again and starting in against the string again and starting in here we're going to skip over all of here we're going to skip over all of here we're going to skip over all of these again until we get to the exact these again until we get to the exact these again until we get to the exact same Point again and we see that there's same Point again and we see that there's same Point again and we see that there's an optional space this is the optional an optional space this is the optional an optional space this is the optional space followed by a bunch of letters one space followed by a bunch of letters one or more of them",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 688,
      "text": "and so that matches so or more of them and so that matches so or more of them and so that matches",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 689,
      "text": "so when we run this we get a list of two when we run this we get a list of two when we run this we get a list of two elements hello",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 690,
      "text": "and then space world elements hello and then space world elements hello and then space world",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 691,
      "text": "so how are you if we add more letters we so how are you if we add more letters we",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 692,
      "text": "so how are you if we add more letters we would just get them like this now what would just get them like this now what would just get them like this now what is this doing and why is this important is this doing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 693,
      "text": "and why is this important is this doing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 694,
      "text": "and why is this important we are taking our string and instead of we are taking our string and instead of we are taking our string and instead of directly encoding it um for directly encoding it um for directly encoding it um for tokenization we are first splitting it tokenization we are first splitting it tokenization we are first splitting it up and when you actually step through up and when you actually step through up and when you actually step through the code and we'll do that in a bit more the code and we'll do that in a bit more the code and we'll do that in a bit more detail what really is doing on a high detail what really is doing on a high detail what really is doing on a high level is that it first splits your text level is that it first splits your text level is that it first splits your text into a list of texts just like this one into a list of texts just like this one into a list of texts just like this one and all these elements of this list are and all these elements of this list are and all these elements of this list are processed independently by the tokenizer processed independently by the tokenizer processed independently by the tokenizer and all of the results of that and all of the results of that and all of the results of that processing are simply processing are simply processing are simply concatenated so",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 695,
      "text": "hello world",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 696,
      "text": "oh I",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 697,
      "text": "I concatenated so hello world",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 698,
      "text": "oh I",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 699,
      "text": "I concatenated so hello world",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 700,
      "text": "oh I",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 701,
      "text": "I missed how hello world how are you we missed how hello world how are you we missed how hello world how are you we have five elements of list all of these have five elements of list all of these have five elements of list all of these will independent will independent will independent independently go from text to a token independently go from text to a token independently go from text to a token sequence and then that token sequence is sequence and then that token sequence is sequence and then that token sequence is going to be concatenated it's all going going to be concatenated it's all going going to be concatenated it's all going to be joined up and roughly speaking to be joined up and roughly speaking to be joined up and roughly speaking what that does is you're only ever what that does is you're only ever what that does is you're only ever finding merges between the elements of finding merges between the elements of finding merges between the elements of this list so you can only ever consider this list so you can only ever consider this list",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 702,
      "text": "so you can only ever consider merges within every one of these merges within every one of these merges within every one of these elements in elements in elements in individually and um after you've done individually and um after you've done individually and um after you've done all the possible merging for all of all the possible merging for all of all the possible merging for all of these elements individually the results these elements individually the results these elements individually the results of all that will be joined um by of all that will be joined um by of all that will be joined um by concatenation",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 703,
      "text": "and so you are basically concatenation",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 704,
      "text": "and so you are basically concatenation",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 705,
      "text": "and so you are basically what what you're doing effectively is what what you're doing effectively is what what you're doing effectively is you are never going to be merging this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 706,
      "text": "e you are never going to be merging this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 707,
      "text": "e you are never going to be merging this e with this space because they are now with this space because they are now with this space because they are now parts of the separate elements of this parts of the separate elements of this parts of the separate elements of this list",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 708,
      "text": "and so you are saying we are never list",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 709,
      "text": "and so you are saying we are never list",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 710,
      "text": "and so you are saying we are never going to merge going to merge going to merge eace um because we're breaking it up in eace um because we're breaking it up in eace um because we're breaking it up in this way so basically using this regx this way so basically using this regx this way so basically using this regx pattern to Chunk Up the text is just one pattern to Chunk Up the text is just one pattern to Chunk Up the text is just one way of enforcing that some merges are way of enforcing that some merges are way of enforcing that some merges are not to happen and we're going to go into not to happen",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 711,
      "text": "and we're going to go into not to happen and we're going to go into more of this text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 712,
      "text": "and we'll see that more of this text and we'll see that more of this text and we'll see that what this is trying to do on a high what this is trying to do on a high what this is trying to do on a high level is we're trying to not merge level is we're trying to not merge level is we're trying to not merge across letters across numbers across across letters across numbers across across letters across numbers across punctuation and so on so let's see in punctuation and so on so let's see in punctuation and so on so let's see in more detail how that works so let's more detail how that works so let's more detail how that works so let's continue now we have/",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 713,
      "text": "P ofn if you go to continue now we have/",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 714,
      "text": "P ofn if you go to continue now we have/",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 715,
      "text": "P ofn if you go to the documentation SLP of n is any kind the documentation SLP of n is any kind the documentation SLP of n is any kind of numeric character in any script so of numeric character in any script so of numeric character in any script so it's numbers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 716,
      "text": "so we have an optional it's numbers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 717,
      "text": "so we have an optional it's numbers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 718,
      "text": "so we have an optional space followed by numbers and those space followed by numbers and those space followed by numbers and those would be separated out so letters and would be separated out so letters and would be separated out so letters and numbers are being separated so if I do numbers are being separated so if I do numbers are being separated so if I do Hello World 123 how are you then world Hello World 123 how are you then world Hello World 123 how are you then world will stop matching here because one is will stop matching here because one is will stop matching here because one is not a letter anymore but one is a number not a letter anymore but one is a number not a letter anymore but one is a number so this group will match for that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 719,
      "text": "and so this group will match for that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 720,
      "text": "and so this group will match for that and we'll get it as a separate entity uh let's see how these apostrophes work uh let's see how these apostrophes work so here if we have so here if we have so here if we have um uh Slash V or I mean apostrophe V as um uh Slash V or I mean apostrophe V as um uh Slash V or I mean apostrophe V as an example then apostrophe here is not a an example then apostrophe here is not a an example then apostrophe here is not a letter or a letter or a letter or a number",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 721,
      "text": "so hello will stop matching and number",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 722,
      "text": "so hello will stop matching and number",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 723,
      "text": "so hello will stop matching",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 724,
      "text": "and then we will exactly match this with then we will exactly match this with then we will exactly match this with that so that will come out as a separate that so that will come out as a separate that so that will come out as a separate thing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 725,
      "text": "so why are they doing the thing so why are they doing the thing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 726,
      "text": "so why are they doing the apostrophes here honestly I think that apostrophes here honestly I think that apostrophes here honestly I think that these are just like very common these are just like very common these are just like very common apostrophes p uh that are used um apostrophes p uh that are used um apostrophes p uh that are used um typically I don't love that they've done typically I don't love that they've done typically I don't love that they've done this this this because uh let me show you what happens because uh let me show you what happens because uh let me show you what happens when you have uh some Unicode when you have uh some Unicode when you have uh some Unicode apostrophes like for example you can apostrophes like for example you can apostrophes like for example you can have if you have house then this will be have if you have house then this will be have if you have house then this will be separated out because of this matching separated out because of this matching separated out because of this matching",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 727,
      "text": "but if you use the Unicode apostrophe but if you use the Unicode apostrophe but if you use the Unicode apostrophe like like like this then suddenly this does not work this then suddenly this does not work this then suddenly this does not work and so this apostrophe will actually",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 728,
      "text": "and so this apostrophe will actually",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 729,
      "text": "and so this apostrophe will actually become its own thing now",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 730,
      "text": "and so so um become its own thing now",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 731,
      "text": "and so so um become its own thing now",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 732,
      "text": "and so",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 733,
      "text": "so um it's basically hardcoded for this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 734,
      "text": "it's basically hardcoded for this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 735,
      "text": "it's basically hardcoded for this specific kind of apostrophe and uh specific kind of apostrophe and uh specific kind of apostrophe and uh otherwise they become completely otherwise they become completely otherwise they become completely separate tokens in addition to this you separate tokens in addition to this you separate tokens in addition to this you can go to the gpt2 docs and here when can go to the gpt2 docs and here when can go to the gpt2 docs",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 736,
      "text": "and here when they Define the pattern they say should they Define the pattern they say should they Define the pattern they say should have added re. ignore case so BP merges have added re.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 737,
      "text": "ignore case so BP merges have added re.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 738,
      "text": "ignore case so BP merges can happen for capitalized versions of can happen for capitalized versions of can happen for capitalized versions of contractions",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 739,
      "text": "so what they're pointing contractions so what they're pointing contractions so what they're pointing out is that you see how this is out is that you see how this is out is that you see how this is apostrophe and then lowercase letters apostrophe and then lowercase letters apostrophe and then lowercase letters well because they didn't do re. ignore well because they didn't do re. ignore well because they didn't do re.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 740,
      "text": "ignore case then then um these rules will not case then then um these rules will not case then then um these rules will not separate out the apostrophes if it's separate out the apostrophes if it's separate out the apostrophes if it's uppercase so uppercase so uppercase so house would be like this but if I did house would be like this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 741,
      "text": "but if I did house would be like this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 742,
      "text": "but if I did house if I'm uppercase then notice house if I'm uppercase then notice house if I'm uppercase then notice suddenly the apostrophe comes by suddenly the apostrophe comes by suddenly the apostrophe comes by itself so the tokenization will work itself so the tokenization will work itself so the tokenization will work differently in uppercase and lower case differently in uppercase and lower case differently in uppercase and lower case inconsistently separating out these inconsistently separating out these inconsistently separating out these apostrophes so it feels extremely gnarly apostrophes so it feels extremely gnarly apostrophes",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 743,
      "text": "so it feels extremely gnarly and slightly gross um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 744,
      "text": "but that's that's and slightly gross um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 745,
      "text": "but that's that's and slightly gross um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 746,
      "text": "but that's that's how that works",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 747,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 748,
      "text": "so let's come back how that works",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 749,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 750,
      "text": "so let's come back how that works",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 751,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 752,
      "text": "so let's come back after trying to match a bunch of after trying to match a bunch of after trying to match a bunch of apostrophe Expressions by the way the apostrophe Expressions by the way the apostrophe Expressions by the way the other issue here is that these are quite other issue here is that these are quite other issue here is that these are quite language specific probably",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 753,
      "text": "so I don't language specific probably",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 754,
      "text": "so I don't language specific probably",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 755,
      "text": "so I don't know that all the languages for example know that all the languages for example know that all the languages for example use or don't use apostrophes but that use or don't use apostrophes but that use or don't use apostrophes but that would be inconsistently tokenized as a would be inconsistently tokenized as a would be inconsistently tokenized as a result then we try to match letters then result then we try to match letters then result then we try to match letters then we try to match numbers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 756,
      "text": "and then if that we try to match numbers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 757,
      "text": "and then if that we try to match numbers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 758,
      "text": "and then if that doesn't work we fall back to here and doesn't work we fall back to here and doesn't work we fall back to here and what this is saying is again optional what this is saying is again optional what this is saying is again optional space followed by something that is not space followed by something that is not space followed by something that is not a letter number or a space in one or a letter number or a space in one or a letter number or a space in one or more of that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 759,
      "text": "so what this is doing more of that so what this is doing more of that so what this is doing effectively is this is trying to match effectively is this is trying to match effectively is this is trying to match punctuation roughly speaking not letters punctuation roughly speaking not letters punctuation roughly speaking not letters and not numbers so this group will try and not numbers so this group will try and not numbers so this group will try to trigger for that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 760,
      "text": "so if I do something to trigger for that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 761,
      "text": "so if I do something to trigger for that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 762,
      "text": "so if I do something like this then these parts here are not like this then these parts here are not like this then these parts here are not letters or numbers but they will letters or numbers but they will letters or numbers but they will actually they are uh they will actually actually they are uh they will actually actually they are uh they will actually get caught here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 763,
      "text": "and so they become its get caught here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 764,
      "text": "and so they become its get caught here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 765,
      "text": "and so they become its own group so we've separated out the own group so we've separated out the own group so we've separated out the punctuation and finally this um this is punctuation and finally this um this is punctuation and finally this um this is also a little bit confusing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 766,
      "text": "so this is also a little bit confusing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 767,
      "text": "so this is also a little bit confusing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 768,
      "text": "so this is matching white space but this is using a matching white space but this is using a matching white space but this is using a negative look ahead assertion in regex negative look ahead assertion in regex negative look ahead assertion in regex",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 769,
      "text": "so what this is doing is it's matching",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 770,
      "text": "so what this is doing is it's matching",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 771,
      "text": "so what this is doing is it's matching wh space up to but not including the wh space up to but not including the wh space up to but not including the last Whit space last Whit space last Whit space character why is this important um this character",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 772,
      "text": "why is this important um this character why is this important um this is pretty subtle",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 773,
      "text": "I think so you see how is pretty subtle",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 774,
      "text": "I think so you see how is pretty subtle",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 775,
      "text": "I think so you see how the white space is always included at the white space is always included at the white space is always included at the beginning of the word so um space r the beginning of the word so um space r the beginning of the word so um space r space u",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 776,
      "text": "Etc suppose we have a lot of space u",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 777,
      "text": "Etc suppose we have a lot of space u",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 778,
      "text": "Etc suppose we have a lot of spaces spaces spaces here what's going to happen here is that here what's going to happen here is that here what's going to happen here is that these spaces up to not including the these spaces up to not including the these spaces up to not including the last character will get caught by this last character will get caught by this last character will get caught by this and what that will do is it will and what that will do is it will and what that will do is it will separate out the spaces up to but not separate out the spaces up to but not separate out the spaces up to but not including the last character so that the including the last character so that the including the last character so that the last character can come here and join last character can come here and join last character can come here and join with the um space you and the reason with the um space you and the reason with the um space you and the reason that's nice is because space you is the that's nice is because space you is the that's nice is because space you is the common token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 779,
      "text": "so if I didn't have these common token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 780,
      "text": "so if I didn't have these common token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 781,
      "text": "so if I didn't have these Extra Spaces here you would just have Extra Spaces here you would just have Extra Spaces here you would just have space you and if I add tokens if I add space you and if I add tokens if I add space you and if I add tokens if I add spaces we still have a space view but spaces we still have a space view but spaces we still have a space view but now we have all this extra white space now we have all this extra white space now we have all this extra white space so basically the GB to tokenizer really so basically the GB to tokenizer really so basically the GB to tokenizer really likes to have a space letters or numbers likes to have a space letters or numbers likes to have a space letters or numbers um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 782,
      "text": "and it it preens these spaces and um and it it preens these spaces and",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 783,
      "text": "um and it it preens these spaces and this is just something that it is this is just something that it is this is just something that it is consistent about so that's what that is consistent about so that's what that is consistent about",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 784,
      "text": "so that's what that is for",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 785,
      "text": "and then finally we have all the the for and then finally we have all the the for",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 786,
      "text": "and then finally we have all the the last fallback is um whites space last fallback is um whites space last fallback is um whites space characters",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 787,
      "text": "uh so um that would be characters uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 788,
      "text": "so um that would be characters uh so um that would be just um if that doesn't get caught then just um if that doesn't get caught then just um if that doesn't get caught then this thing will catch any trailing this thing will catch any trailing this thing will catch any trailing spaces and so on I wanted to show one spaces and so on I wanted to show one spaces and so on I wanted to show one more real world example here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 789,
      "text": "so if we more real world example here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 790,
      "text": "so if we more real world example here so if we have this string which is a piece of have this string which is a piece of have this string which is a piece of python code",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 791,
      "text": "and then we try to split it python code",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 792,
      "text": "and then we try to split it python code",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 793,
      "text": "and then we try to split it up then this is the kind of output we up then this is the kind of output we up then this is the kind of output we get so you'll notice that the list has get so you'll notice that the list has get so you'll notice that the list has many elements here and that's because we many elements here and that's because we many elements here and that's because we are splitting up fairly often uh every are splitting up fairly often uh every are splitting up fairly often uh every time sort of a category time sort of a category time sort of a category changes um so there will never be any changes um so there will never be any changes um so there will never be any merges Within These merges Within These merges Within These elements and um that's what you are elements and um that's what you are elements and um that's what you are seeing here now you might think that in seeing here now you might think that in seeing here now you might think that in order to train the order to train the order to train the tokenizer uh open AI has used this to tokenizer uh open AI has used this to tokenizer uh open AI has used this to split up text into chunks and then run split up text into chunks and then run split up text into chunks and then run just a BP algorithm within all the just a BP algorithm within all the just a BP algorithm within all the chunks but that is not exactly what chunks",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 794,
      "text": "but that is not exactly what chunks",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 795,
      "text": "but that is not exactly what happened and the reason is the following happened and the reason is the following happened and the reason is the following notice that we have the spaces here uh notice that we have the spaces here uh notice that we have the spaces here uh those Spaces end up being entire those Spaces end up being entire those Spaces end up being entire elements but these spaces never actually elements",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 796,
      "text": "but these spaces never actually elements but these spaces never actually end up being merged by by open Ai and end up being merged by by open Ai and end up being merged by by open Ai and the way you can tell is that if you copy the way you can tell is that if you copy the way you can tell is that if you copy paste the exact same chunk here into Tik paste the exact same chunk here into Tik paste the exact same chunk here into Tik token U Tik tokenizer you see that all token U Tik tokenizer you see that all token U Tik tokenizer you see that all the spaces are kept independent and the spaces are kept independent and the spaces are kept independent and they're all token they're all token they're all token 220",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 797,
      "text": "so I think opena at some point Point 220",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 798,
      "text": "so I think opena at some point Point 220",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 799,
      "text": "so I think opena at some point Point en Force some rule that these spaces en Force some rule that these spaces en Force some rule that these spaces would never be merged",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 800,
      "text": "and so um there's would never be merged",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 801,
      "text": "and so um there's would never be merged",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 802,
      "text": "and so um there's some additional rules on top of just some additional rules on top of just some additional rules on top of just chunking and bpe that open ey is not uh chunking and bpe that open ey is not uh chunking and bpe that open ey is not uh clear about now the training code for clear about now the training code for clear about now the training code for the gpt2 tokenizer was never released so the gpt2 tokenizer was never released so the gpt2 tokenizer was never released so all we have is uh the code that I've all we have is uh the code that I've all we have is uh the code that I've already shown you but this code here already shown you",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 803,
      "text": "but this code here already shown you but this code here that they've released is only the that they've released is only the that they've released is only the inference code for the tokens so this is inference code for the tokens so this is inference code for the tokens so this is not the training code you can't give it not the training code you can't give it not the training code you can't give it a piece of text and training tokenizer a piece of text and training tokenizer a piece of text and training tokenizer this is just the inference code which this is just the inference code which this is just the inference code which Tak takes the merges that we have up Tak takes the merges that we have up Tak takes the merges that we have up above and applies them to a new piece of above and applies them to a new piece of above and applies them to a new piece of text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 804,
      "text": "and so we don't know exactly how text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 805,
      "text": "and so we don't know exactly how text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 806,
      "text": "and so we don't know exactly how opening ey trained um train the opening ey trained um train the opening ey trained um train the tokenizer but it wasn't as simple as tokenizer but it wasn't as simple as tokenizer but it wasn't as simple as chunk it up and BP it uh whatever it was chunk it up and BP it uh whatever it was chunk it up and BP it uh whatever it was next I wanted to introduce you to the next I wanted to introduce you to the next I wanted to introduce you to the Tik token library from openai which is Tik token library from openai which is Tik token library from openai which is the official library for tokenization the official library for tokenization the official library for tokenization from openai",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 807,
      "text": "so this is Tik token bip from openai",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 808,
      "text": "so this is Tik token bip from openai",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 809,
      "text": "so this is Tik token bip install P to Tik token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 810,
      "text": "and then um you install P to Tik token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 811,
      "text": "and then um you install P to Tik token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 812,
      "text": "and then um you can do the tokenization in inference can do the tokenization in inference can do the tokenization in inference this is again not training code this is this is again not training code this is this is again not training code this is only inference code for only inference code for only inference code for tokenization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 813,
      "text": "um I wanted to show you how tokenization um I wanted to show you how tokenization um I wanted to show you how you would use it quite simple and you would use it quite simple and you would use it quite simple and running this just gives us the gpt2 running this just gives us the gpt2 running this just gives us the gpt2 tokens or the GPT 4 tokens so this is tokens or the GPT 4 tokens so this is tokens or the GPT 4 tokens so this is the tokenizer use for GPT 4 and so in the tokenizer use for GPT 4 and so in the tokenizer use for GPT 4 and so in particular we see that the Whit space in particular we see that the Whit space in particular we see that the Whit space in gpt2 remains unmerged but in GPT 4 uh gpt2 remains unmerged but in GPT 4 uh gpt2 remains unmerged but in GPT 4 uh these Whit spaces merge as we also saw these Whit spaces merge as we also saw these Whit spaces merge as we also saw in this one where here they're all in this one where here they're all in this one where here they're all unmerged",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 814,
      "text": "but if we go down to GPT 4 uh unmerged",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 815,
      "text": "but if we go down to GPT 4 uh unmerged",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 816,
      "text": "but if we go down to GPT 4 uh they become merged they become merged they become merged um now in the um now in the um now in the gp4 uh tokenizer they changed the gp4 uh tokenizer they changed the gp4 uh tokenizer they changed the regular expression that they use to regular expression that they use to regular expression that they use to Chunk Up text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 817,
      "text": "so the way to see this is Chunk Up text so the way to see this is Chunk Up text so the way to see this is that if you come to your the Tik token that if you come to your the Tik token that if you come to your the Tik token uh library",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 818,
      "text": "and then you go to this file uh library",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 819,
      "text": "and then you go to this file uh library",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 820,
      "text": "and then you go to this file Tik token X openi public this is where Tik token X openi public this is where Tik token X openi public this is where sort of like the definition of all these sort of like the definition of all these sort of like the definition of all these different tokenizers that openi different tokenizers that openi different tokenizers that openi maintains is and so uh necessarily to do maintains is",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 821,
      "text": "and so uh necessarily to do maintains is and so uh necessarily to do the inference they had to publish some the inference they had to publish some the inference they had to publish some of the details about the strings of the details about the strings of the details about the strings so this is the string that we already so this is the string that we already so this is the string that we already saw for gpt2 it is slightly different saw for gpt2 it is slightly different saw for gpt2 it is slightly different but it is actually equivalent uh to what but it is actually equivalent uh to what but it is actually equivalent uh to what we discussed here so this pattern that we discussed here so this pattern that we discussed here so this pattern that we discussed is equivalent to this we discussed is equivalent to this we discussed is equivalent to this pattern this one just executes a little pattern this one just executes a little pattern this one just executes a little bit faster",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 822,
      "text": "so here you see a little bit bit faster so here you see a little bit bit faster so here you see a little bit of a slightly different definition but of a slightly different definition but of a slightly different definition",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 823,
      "text": "but otherwise it's the same we're going to otherwise it's the same we're going to otherwise it's the same we're going to go into special tokens in a bit and then go into special tokens in a bit and then go into special tokens in a bit",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 824,
      "text": "and then if you scroll down to CL 100k",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 825,
      "text": "this is if you scroll down to CL 100k",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 826,
      "text": "this is if you scroll down to CL 100k this is the GPT 4 tokenizer you see that the the GPT 4 tokenizer you see that the the GPT 4 tokenizer you see that the pattern has changed um and this is kind pattern has changed um and this is kind pattern has changed um and this is kind of like the main the major change in of like the main the major change in of like the main the major change in addition to a bunch of other special addition to a bunch of other special addition to a bunch of other special tokens which I'll go into in a bit again tokens which I'll go into in a bit again tokens which I'll go into in a bit again now some I'm not going to actually go now some",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 827,
      "text": "I'm not going to actually go now some I'm not going to actually go into the full detail of the pattern into the full detail of the pattern into the full detail of the pattern change because honestly this is my change because honestly this is my change because honestly this is my numbing uh I would just advise that you numbing uh I would just advise that you numbing uh I would just advise that you pull out chat GPT and the regex pull out chat GPT and the regex pull out chat GPT and the regex documentation and just step through it documentation and just step through it documentation and just step through it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 828,
      "text": "but really the major changes are number",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 829,
      "text": "but really the major changes are number",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 830,
      "text": "but really the major changes are number one you see this eye here that means one you see this eye here that means one you see this eye here that means that the um case sensitivity this is that the um case sensitivity this is that the um case sensitivity this is case insensitive match",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 831,
      "text": "and so the case insensitive match",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 832,
      "text": "and so the case insensitive match",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 833,
      "text": "and so the comment that we saw earlier on",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 834,
      "text": "oh we comment that we saw earlier on",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 835,
      "text": "oh we comment that we saw earlier on",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 836,
      "text": "oh we should have used re. uppercase uh should have used re. uppercase uh should have used re. uppercase uh basically we're now going to be matching basically we're now going to be matching basically we're now going to be matching these apostrophe s apostrophe D",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 837,
      "text": "these apostrophe s apostrophe D these apostrophe s apostrophe D apostrophe M Etc uh we're going to be apostrophe",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 838,
      "text": "M Etc uh we're going to be apostrophe",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 839,
      "text": "M Etc uh we're going to be matching them both in lowercase and in matching them both in lowercase and in matching them both in lowercase and in uppercase so that's fixed there's a uppercase",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 840,
      "text": "so that's fixed there's a uppercase",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 841,
      "text": "so that's fixed there's a bunch of different like handling of the bunch of different like handling of the bunch of different like handling of the whites space that I'm not going to go whites space that I'm not going to go whites space that I'm not going to go into the full details of and then one into the full details of and then one into the full details of and then one more thing here is you will notice that more thing here is you will notice that more thing here is you will notice that when they match the numbers they only when they match the numbers they only when they match the numbers they only match one to three numbers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 842,
      "text": "so so they match one to three numbers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 843,
      "text": "so so they match one to three numbers so so they will never merge will never merge will never merge numbers that are in low in more than numbers that are in low in more than numbers that are in low in more than three digits only up to three digits of three digits only up to three digits of three digits only up to three digits of numbers will ever be merged and uh numbers will ever be merged and uh numbers will ever be merged and uh that's one change that they made as well that's one change that they made as well that's one change that they made as well to prevent uh tokens that are very very to prevent uh tokens that are very very to prevent uh tokens that are very very long number long number long number sequences uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 844,
      "text": "but again we don't really sequences uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 845,
      "text": "but again we don't really sequences uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 846,
      "text": "but again we don't really know why they do any of this stuff uh know why they do any of this stuff",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 847,
      "text": "uh know why they do any of this stuff uh because none of this is documented and because none of this is documented and because none of this is documented and uh it's just we just get the pattern",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 848,
      "text": "so uh it's just we just get the pattern",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 849,
      "text": "so uh it's just we just get the pattern",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 850,
      "text": "so",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 851,
      "text": "um yeah it is what it is but those are um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 852,
      "text": "yeah it is what it is",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 853,
      "text": "but those are um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 854,
      "text": "yeah it is what it is",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 855,
      "text": "but those are some of the changes that gp4 has made some of the changes that gp4 has made some of the changes that gp4 has made and of course the vocabulary size went and of course the vocabulary size went and of course the vocabulary size went from roughly 50k to roughly from roughly 50k to roughly from roughly 50k to roughly 100K the next thing I would like to do 100K the next thing I would like to do 100K the next thing I would like to do very briefly is to take you through the very briefly is to take you through the very briefly is to take you through the gpt2 encoder dopy that openi has gpt2 encoder dopy that openi has gpt2 encoder dopy that openi has released uh this is the file that I released uh this is the file that I released uh this is the file that I already mentioned to you briefly now already mentioned to you briefly now already mentioned to you briefly now this file is uh fairly short and should this file is uh fairly short and should this file is uh fairly short and should be relatively understandable to you at be relatively understandable to you at be relatively understandable to you at this point um starting at the bottom this point um starting at the bottom this point um starting at the bottom here they are loading two files encoder here they are loading two files encoder here they are loading two files encoder Json and vocab bpe",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 856,
      "text": "and they do some Json and vocab bpe and they do some Json and vocab bpe and they do some light processing on it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 857,
      "text": "and then they light processing on it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 858,
      "text": "and then they light processing on it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 859,
      "text": "and then they call this encoder object which is the call this encoder object which is the call this encoder object which is the tokenizer now if you'd like to inspect tokenizer now if you'd like to inspect tokenizer now if you'd like to inspect these two files which together these two files which together these two files which together constitute their saved tokenizer then constitute their saved tokenizer then constitute their saved tokenizer then you can do that with a piece of code you can do that with a piece of code you can do that with a piece of code like like this um this is where you can download this um this is where you can download this um this is where you can download these two files and you can inspect them these two files and you can inspect them these two files and you can inspect them if you'd like and what you will find is if you'd like and what you will find is if you'd like and what you will find is that this encoder as they call it in that this encoder as they call it in that this encoder as they call it in their code is exactly equivalent to our their code is exactly equivalent to our their code is exactly equivalent to our vocab so remember here where we have vocab so remember here where we have vocab so remember here where we have this vocab object which allowed us us to this vocab object which allowed us us to this vocab object which allowed us us to decode very efficiently and basically it decode very efficiently and basically it decode very efficiently and basically it took us from the integer to the byes uh took us from the integer to the byes uh took us from the integer to the byes uh for that integer so our vocab is exactly for that integer so our vocab is exactly for that integer so our vocab is exactly their encoder",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 860,
      "text": "and then their vocab bpe their encoder and then their vocab bpe their encoder and then their vocab bpe confusingly is actually are merges so confusingly is actually are merges so confusingly is actually are merges so their BP merges which is based on the their BP merges which is based on the their BP merges which is based on the data inside vocab bpe ends up being data inside vocab bpe ends up being data inside vocab bpe ends up being equivalent to our merges",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 861,
      "text": "so uh basically equivalent to our merges so uh basically equivalent to our merges so uh basically they are saving and loading the two uh they are saving and loading the two uh they are saving and loading the two uh variables that for us are also critical variables that for us are also critical variables that for us are also critical the merges variable and the vocab the merges variable and the vocab the merges variable and the vocab variable using just these two variables variable using just these two variables variable using just these two variables you can represent a tokenizer and you you can represent a tokenizer and you you can represent a tokenizer and you can both do encoding and decoding once can both do encoding and decoding once can both do encoding and decoding once you've trained this you've trained this you've trained this tokenizer now the only thing that um is tokenizer now the only thing that um is tokenizer now the only thing that um is actually slightly confusing inside what actually slightly confusing inside what actually slightly confusing inside what opening ey does here is that in addition opening ey does here is that in addition opening ey does here is that in addition to this encoder and a decoder they also to this encoder and a decoder they also to this encoder and a decoder they also have something called a bite encoder and have something called a bite encoder and have something called a bite encoder and a bite decoder and this is actually a bite decoder and this is actually a bite decoder and this is actually unfortunately just unfortunately just unfortunately just kind of a spirous implementation detail kind of a spirous implementation detail kind of a spirous implementation detail and isn't actually deep or interesting and isn't actually deep or interesting and isn't actually deep or interesting in any way",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 862,
      "text": "so I'm going to skip the in any way",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 863,
      "text": "so I'm going to skip the in any way",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 864,
      "text": "so I'm going to skip the discussion of it but what opening ey discussion of it but what opening ey discussion of it but what opening ey does here for reasons that I don't fully does here for reasons that I don't fully does here for reasons that I don't fully understand is that not only have they understand is that not only have they understand is that not only have they this tokenizer which can encode and this tokenizer which can encode and this tokenizer which can encode and decode but they have a whole separate decode but they have a whole separate decode but they have a whole separate layer here in addition that is used layer here in addition that is used layer here in addition that is used serially with the tokenizer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 865,
      "text": "and so you serially with the tokenizer and so you serially with the tokenizer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 866,
      "text": "and so you first do um bite encode and then encode first do um bite encode and then encode first do um bite encode and then encode",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 867,
      "text": "and then you do decode and then bite",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 868,
      "text": "and then you do decode and then bite",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 869,
      "text": "and then you do decode and then bite decode so that's the loop and they are decode so that's the loop and they are decode",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 870,
      "text": "so that's the loop and they are just stacked serial on top of each other just stacked serial on top of each other just stacked serial on top of each other and and it's not that interesting",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 871,
      "text": "so I and and it's not that interesting",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 872,
      "text": "so I and and it's not that interesting",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 873,
      "text": "so I won't cover it and you can step through won't cover it and you can step through won't cover it and you can step through it if you'd like otherwise this file if it if you'd like otherwise this file if it if you'd like otherwise this file if you ignore the bite encoder and the bite you ignore the bite encoder and the bite you ignore the bite encoder and the bite decoder will be algorithmically very decoder will be algorithmically very decoder will be algorithmically very familiar with you and the meat of it familiar with you and the meat of it familiar with you and the meat of it here is the what they call bpe function here is the what they call bpe function here is the what they call bpe function and you should recognize this Loop here and you should recognize this Loop here and you should recognize this Loop here which is very similar to our own y Loop which is very similar to our own y Loop which is very similar to our own y Loop where they're trying to identify the where they're trying to identify the where they're trying to identify the Byram uh a pair that they should be Byram uh a pair that they should be Byram uh a pair that they should be merging next and then here just like we merging next and then here just like we merging next and then here just like we had they have a for Loop trying to merge had they have a for Loop trying to merge had they have a for Loop trying to merge this pair uh so they will go over all of this pair uh so they will go over all of this pair uh so they will go over all of the sequence and they will merge the the sequence and they will merge the the sequence and they will merge the pair whenever they find it and they keep pair whenever they find it and they keep pair whenever they find it and they keep repeating that until they run out of repeating that until they run out of repeating that until they run out of possible merges in the in the text so possible merges in the in the text so possible merges in the in the text so that's the meat of this file and uh that's the meat of this file and uh that's the meat of this file and uh there's an encode and a decode function there's an encode and a decode function there's an encode and a decode function just like we have implemented it so long just like we have implemented it so long just like we have implemented it so long story short what I want you to take away story short what I want you to take away story short what I want you to take away at this point is that unfortunately it's at this point is that unfortunately it's at this point is that unfortunately it's a little bit of a messy code that they a little bit of a messy code that they a little bit of a messy code that they have but algorithmically it is identical have",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 874,
      "text": "but algorithmically it is identical have",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 875,
      "text": "but algorithmically it is identical to what we've built up above and what to what we've built up above and what to what we've built up above and what we've built up above if you understand we've built up above if you understand we've built up above if you understand it is algorithmically what is necessary it is algorithmically what is necessary it is algorithmically what is necessary to actually build a BP to organizer to actually build a BP to organizer to actually build a BP to organizer train it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 876,
      "text": "and then both encode and decode train it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 877,
      "text": "and then both encode and decode train it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 878,
      "text": "and then both encode and decode the next topic I would like to turn to the next topic I would like to turn to the next topic I would like to turn to is that of special tokens so in addition is that of special tokens so in addition is that of special tokens so in addition to tokens that are coming from you know to tokens that are coming from you know to tokens that are coming from you know raw bytes and the BP merges we can raw bytes and the BP merges we can raw bytes and the BP merges we can insert all kinds of tokens that we are insert all kinds of tokens that we are insert all kinds of tokens that we are going to use to delimit different parts going to use to delimit different parts going to use to delimit different parts of the data or introduced to create a of the data or introduced to create a of the data or introduced to create a special structure of the token streams special structure of the token streams special structure of the token streams so in uh if you look at this encoder so in uh if you look at this encoder so in uh if you look at this encoder object from open AIS gpd2 right here we object from open AIS gpd2 right here we object from open AIS gpd2 right here we mentioned this is very similar to our mentioned this is very similar to our mentioned this is very similar to our vocab you'll notice that the length of vocab you'll notice that the length of vocab you'll notice that the length of this is 50257 and as I mentioned it's mapping uh 50257 and as I mentioned it's mapping uh and it's inverted from the mapping of and it's inverted from the mapping of and it's inverted from the mapping of our vocab our vocab goes from integer to our vocab our vocab goes from integer to our vocab our vocab goes from integer to string and they go the other way around string and they go the other way around string and they go the other way around for no amazing reason",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 879,
      "text": "um but the thing for no amazing reason um but the thing for no amazing reason um but the thing to note here is that this the mapping to note here is that this the mapping to note here is that this the mapping table here is table here is table here is 50257 where does that number come from 50257 where does that number come from 50257 where does that number come from where what are the tokens as I mentioned where what are the tokens as I mentioned where what are the tokens as I mentioned there are 256 raw bite token there are 256 raw bite token there are 256 raw bite token tokens and then opena actually did tokens and then opena actually did tokens and then opena actually did 50,000 50,000 50,000 merges so those become the other tokens merges so those become the other tokens merges so those become the other tokens but this would have been but this would have been but this would have been 50256 so what is the 57th token and 50256 so what is the 57th token and 50256 so what is the 57th token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 880,
      "text": "and there is basically one special there is basically one special there is basically one special token and that one special token you can token and that one special token you can token and that one special token you can see is called end of text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 881,
      "text": "so this is a see is called end of text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 882,
      "text": "so this is a see is called end of text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 883,
      "text": "so this is a special token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 884,
      "text": "and it's the very last special token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 885,
      "text": "and it's the very last special token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 886,
      "text": "and it's the very last token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 887,
      "text": "and this token is used to delimit token and this token is used to delimit token and this token is used to delimit documents ments in the training set so documents ments in the training set so documents ments in the training set",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 888,
      "text": "so when we're creating the training data we when we're creating the training data we when we're creating the training data we have all these documents and we tokenize have all these documents and we tokenize have all these documents and we tokenize them and we get a stream of tokens those them",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 889,
      "text": "and we get a stream of tokens those them",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 890,
      "text": "and we get a stream of tokens those tokens only range from Z to tokens only range from Z to tokens only range from Z to 50256 and then in between those 50256 and then in between those 50256 and then in between those documents we put special end of text documents we put special end of text documents we put special end of text token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 891,
      "text": "and we insert that token in token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 892,
      "text": "and we insert that token in token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 893,
      "text": "and we insert that token in between documents and we are using this between documents and we are using this between documents and we are using this as a signal to the language model that as a signal to the language model that as a signal to the language model that the document has ended and what follows the document has ended and what follows the document has ended and what follows is going to be unrelated to the document is going to be unrelated to the document is going to be unrelated to the document previously that said the language model previously that said the language model previously that said the language model has to learn this from data it it needs has to learn this from data it it needs has to learn this from data it it needs to learn that this token usually means to learn that this token usually means to learn that this token usually means that it should wipe its sort of memory that it should wipe its sort of memory that it should wipe its sort of memory of what came before and what came before of what came before and what came before of what came before and what came before this token is not actually informative this token is not actually informative this token is not actually informative to what comes next but we are expecting to what comes next but we are expecting to what comes next but we are expecting the language model to just like learn the language model to just like learn the language model to just like learn this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 894,
      "text": "but we're giving it the Special this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 895,
      "text": "but we're giving it the Special this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 896,
      "text": "but we're giving it the Special sort of the limiter of these documents sort of the limiter of these documents sort of the limiter of these documents we can go here to Tech tokenizer and um we can go here to Tech tokenizer and um we can go here to Tech tokenizer and um this the gpt2 tokenizer uh our code that this the gpt2 tokenizer uh our code that this the gpt2 tokenizer uh our code that we've been playing with before",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 897,
      "text": "so we can we've been playing with before so we can we've been playing with before so we can add here right hello world world",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 898,
      "text": "how are add here right hello world world how are add here right hello world world how are you and we're getting different tokens you and we're getting different tokens you and we're getting different tokens but now you can see what if what happens but now you can see what if what happens but now you can see what if what happens if I put end of text you see how until I if I put end of text you see how until I if I put end of text you see how until I finished it these are all different finished it these are all different finished it these are all different tokens end of tokens end of tokens end of text still set different tokens and now text still set different tokens and now text still set different tokens and now when I finish it suddenly we get token when I finish it suddenly we get token when I finish it suddenly we get token 50256 and the reason this works is 50256 and the reason this works is 50256 and the reason this works is because this didn't actually go through because this didn't actually go through because this didn't actually go through the bpe merges instead the code that the bpe merges instead the code that the bpe merges instead the code that actually outposted tokens has special actually outposted tokens has special actually outposted tokens has special case instructions for handling special case instructions for handling special case instructions for handling special tokens um we did not see these special tokens um we did not see these special tokens um we did not see these special instructions for handling special tokens instructions for handling special tokens instructions for handling special tokens in the encoder dopy",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 899,
      "text": "it's absent there in the encoder dopy it's absent there in the encoder dopy it's absent there",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 900,
      "text": "but if you go to Tech token Library but if you go to Tech token Library",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 901,
      "text": "but if you go to Tech token Library which is uh implemented in Rust you will which is uh implemented in Rust you will which is uh implemented in Rust you will find all kinds of special case handling find all kinds of special case handling find all kinds of special case handling for these special tokens that you can for these special tokens that you can for these special tokens that you can register uh create adds to the register uh create adds to the register uh create adds to the vocabulary and then it looks for them vocabulary",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 902,
      "text": "and then it looks for them vocabulary",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 903,
      "text": "and then it looks for them and it uh whenever it sees these special and it uh whenever it sees these special and it uh whenever it sees these special tokens like this it will actually come tokens like this it will actually come tokens like this it will actually come in and swap in that special token so in and swap in that special token so in and swap in that special token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 904,
      "text": "so these things are outside of the typical these things are outside of the typical these things are outside of the typical algorithm of uh B PA en algorithm of uh B PA en algorithm of uh B PA en coding so these special tokens are used coding so these special tokens are used coding so these special tokens are used pervasively uh not just in uh basically pervasively uh not just in uh basically pervasively uh not just in uh basically base language modeling of predicting the base language modeling of predicting the base language modeling of predicting the next token in the sequence but next token in the sequence but next token in the sequence but especially when it gets to later to the especially when it gets to later to the especially when it gets to later to the fine tuning stage and all of the chat uh fine tuning stage and all of the chat uh fine tuning stage and all of the chat uh gbt sort of aspects of it uh because we gbt sort of aspects of it uh because we gbt sort of aspects of it uh because we don't just want to Del limit documents don't just want to Del limit documents don't just want to Del limit documents we want to delimit entire conversations we want to delimit entire conversations we want to delimit entire conversations between an assistant and a user",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 905,
      "text": "so if I between an assistant and a user",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 906,
      "text": "so if I between an assistant and a user so if I refresh this sck tokenizer page the refresh this sck tokenizer page the refresh this sck tokenizer page the default example that they have here is default example that they have here is default example that they have here is using not sort of base model encoders using not sort of base model encoders using not sort of base model encoders but ftuned model uh sort of tokenizers but ftuned model uh sort of tokenizers but ftuned model uh sort of tokenizers um so for example using the GPT 3.5",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 907,
      "text": "um so for example using the GPT 3.5 um so for example using the GPT 3.5 turbo scheme these here are all special turbo scheme these here are all special turbo scheme these here are all special tokens I am start",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 908,
      "text": "I end Etc uh this is tokens I am start",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 909,
      "text": "I end Etc uh this is tokens I am start",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 910,
      "text": "I end Etc uh this is short for Imaginary mcore start by the short for Imaginary mcore start by the short for Imaginary mcore start by the way but you can see here that there's a way",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 911,
      "text": "but you can see here that there's a way",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 912,
      "text": "but you can see here that there's a sort of start and end of every single sort of start and end of every single sort of start and end of every single message and there can be many other message and there can be many other message and there can be many other other tokens lots of tokens um in use to other tokens lots of tokens um in use to other tokens lots of tokens um in use to delimit these conversations and kind of delimit these conversations and kind of delimit these conversations and kind of keep track of the flow of the messages keep track of the flow of the messages keep track of the flow of the messages here now we can go back to the Tik token here now we can go back to the Tik token here now we can go back to the Tik token library and here when you scroll to the library",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 913,
      "text": "and here when you scroll to the library and here when you scroll to the bottom they talk about how you can bottom they talk about how you can bottom they talk about how you can extend tick token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 914,
      "text": "and I can you can extend tick token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 915,
      "text": "and I can you can extend tick token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 916,
      "text": "and I can you can create basically you can Fork uh the um create basically you can Fork uh the um create basically you can Fork uh the um CL 100K base tokenizers in gp4 and for CL 100K base tokenizers in gp4 and for CL 100K base tokenizers in gp4 and for example you can extend it by adding more example you can extend it by adding more example you can extend it by adding more special tokens and these are totally up special tokens and these are totally up special tokens and these are totally up to you you can come up with any to you you can come up with any to you you can come up with any arbitrary tokens and add them with the arbitrary tokens and add them with the arbitrary tokens and add them with the new ID afterwards and the tikken library new ID afterwards and the tikken library new ID afterwards and the tikken library will uh correctly swap them out uh when will uh correctly swap them out uh when will uh correctly swap them out uh when it sees this in the it sees this in the it sees this in the strings now we can also go back to this strings now we can also go back to this strings now we can also go back to this file which we've looked at previously file which we've looked at previously file which we've looked at previously",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 917,
      "text": "and I mentioned that the gpt2 in Tik and I mentioned that the gpt2 in Tik and I mentioned that the gpt2 in Tik toen open toen open toen open I.P we have the vocabulary we have the I.P we have the vocabulary we have the I.P we have the vocabulary we have the pattern for splitting",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 918,
      "text": "and then here we pattern for splitting",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 919,
      "text": "and then here we pattern for splitting",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 920,
      "text": "and then here we are registering the single special token are registering the single special token are registering the single special token in gpd2 which was the end of text token in gpd2 which was the end of text token in gpd2 which was the end of text token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 921,
      "text": "and we saw that it has this ID and we saw that it has this ID and we saw that it has this ID in GPT 4 when they defy this here you in GPT 4 when they defy this here you in GPT 4 when they defy this here you see that the pattern has changed as see that the pattern has changed as see that the pattern has changed as we've discussed but also the special we've discussed but also the special we've discussed but also the special tokens have changed in this tokenizer so tokens have changed in this tokenizer so tokens have changed in this tokenizer so we of course have the end of text just we of course have the end of text just we of course have the end of text just like in gpd2",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 922,
      "text": "but we also see three sorry like in gpd2",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 923,
      "text": "but we also see three sorry like in gpd2",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 924,
      "text": "but we also see three sorry four additional tokens here Thim prefix four additional tokens here Thim prefix four additional tokens here Thim prefix middle and suffix what is fim fim is middle and suffix what is fim fim is middle and suffix what is fim fim is short for fill in the middle and if short for fill in the middle and if short for fill in the middle",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 925,
      "text": "and if you'd like to learn more about this idea you'd like to learn more about this idea you'd like to learn more about this idea it comes from this paper",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 926,
      "text": "um and I'm not it comes from this paper",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 927,
      "text": "um and I'm not it comes from this paper",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 928,
      "text": "um and I'm not going to go into detail in this video going to go into detail in this video going to go into detail in this video it's beyond this video",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 929,
      "text": "and then there's it's beyond this video",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 930,
      "text": "and then there's it's beyond this video",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 931,
      "text": "and then there's one additional uh serve token here so one additional uh serve token here so one additional uh serve token here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 932,
      "text": "so that's that encoding as well",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 933,
      "text": "so it's that's that encoding",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 934,
      "text": "as well",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 935,
      "text": "so it's that's that encoding",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 936,
      "text": "as well",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 937,
      "text": "so it's very common basically to train a very common basically to train a very common basically to train a language model",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 938,
      "text": "and then if you'd like uh language model and then if you'd like uh language model and then if you'd like uh you can add special tokens now when you you can add special tokens now when you you can add special tokens now when you add special tokens you of course have to add special tokens you of course have to add special tokens you of course have to um do some model surgery to the um do some model surgery to the um do some model surgery to the Transformer and all the parameters Transformer and all the parameters Transformer and all the parameters involved in that Transformer because you involved in that Transformer because you involved in that Transformer because you are basically adding an integer and you are basically adding an integer and you are basically adding an integer and you want to make sure that for example your want to make sure that for example your want to make sure that for example your embedding Matrix for the vocabulary embedding Matrix for the vocabulary embedding Matrix for the vocabulary tokens has to be extended by adding a tokens has to be extended by adding a tokens has to be extended by adding a row and typically this row would be row and typically this row would be row and typically this row would be initialized uh with small random numbers initialized uh with small random numbers initialized uh with small random numbers or something like that because we need or something like that because we need or something like that because we need to have a vector that now stands for to have a vector that now stands for to have a vector that now stands for that token in addition to that you have that token in addition to that you have that token in addition to that you have to go to the final layer of the to go to the final layer of the to go to the final layer of the Transformer and you have to make sure Transformer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 939,
      "text": "and you have to make sure Transformer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 940,
      "text": "and you have to make sure that that projection at the very end that that projection at the very end that that projection at the very end into the classifier uh is extended by into the classifier uh is extended by into the classifier uh is extended by one as well",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 941,
      "text": "so basically there's some one as well",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 942,
      "text": "so basically there's some one as well",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 943,
      "text": "so basically there's some model surgery involved that you have to model surgery involved that you have to model surgery involved that you have to couple with the tokenization changes if couple with the tokenization changes if couple with the tokenization changes if you are going to add special tokens but you are going to add special tokens but you are going to add special tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 944,
      "text": "but this is a very common operation that this is a very common operation that this is a very common operation that people do especially if they'd like to people do especially if they'd like to people do especially if they'd like to fine tune the model for example taking fine tune the model for example taking fine tune the model for example taking it from a base model to a chat model it from a base model to a chat model it from a base model to a chat model like chat",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 945,
      "text": "like chat like chat GPT",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 946,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 947,
      "text": "so at this point you should GPT",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 948,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 949,
      "text": "so at this point you should GPT",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 950,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 951,
      "text": "so at this point you should have everything you need in order to have everything you need in order to have everything you need in order to build your own gp4 tokenizer now in the build your own gp4 tokenizer now in the build your own gp4 tokenizer now in the process of developing this lecture I've process of developing this lecture I've process of developing this lecture I've done that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 952,
      "text": "and I published the code under done that and I published the code under done that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 953,
      "text": "and I published the code under this repository this repository this repository MBP so MBP looks like this right now as MBP so MBP looks like this right now as MBP so MBP looks like this right now as I'm recording but uh the MBP repository I'm recording but uh the MBP repository I'm recording",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 954,
      "text": "but uh the MBP repository will probably change quite a bit because will probably change quite a bit because will probably change quite a bit because I intend to continue working on it um in I intend to continue working on it um in I intend to continue working on it um in addition to the MBP repository I've addition to the MBP repository I've addition to the MBP repository I've published the this uh exercise published the this uh exercise published the this uh exercise progression that you can follow",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 955,
      "text": "so if progression that you can follow so if progression that you can follow so if you go to exercise.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 956,
      "text": "MD here uh this is you go to exercise.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 957,
      "text": "MD here uh this is you go to exercise.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 958,
      "text": "MD here uh this is sort of me breaking up the task ahead of sort of me breaking up the task ahead of sort of me breaking up the task ahead of you into four steps that sort of uh you into four steps that sort of uh you into four steps that sort of uh build up to what can be a gp4 tokenizer build up to what can be a gp4 tokenizer build up to what can be a gp4 tokenizer and so feel free to follow these steps and so feel free to follow these steps and so feel free to follow these steps exactly and follow a little bit of the exactly and follow a little bit of the exactly and follow a little bit of the guidance that I've laid out here and guidance that I've laid out here and guidance that I've laid out here and anytime you feel stuck just reference anytime you feel stuck just reference anytime you feel stuck just reference the MBP repository here so either the the MBP repository here so either the the MBP repository here so either the tests could be useful or the MBP tests could be useful or the MBP tests could be useful or the MBP repository itself I try to keep the code repository itself I try to keep the code repository itself I try to keep the code fairly clean and understandable and so fairly clean and understandable and so fairly clean and understandable and so um feel free to reference it whenever um um feel free to reference it whenever um um feel free to reference it whenever um you get you get you get stuck uh in addition to that basically stuck uh in addition to that basically stuck uh in addition to that basically once you write it you should be able to once you write it you should be able to once you write it you should be able to reproduce this behavior from Tech token reproduce this behavior from Tech token reproduce this behavior from Tech token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 959,
      "text": "so getting the gb4 tokenizer you can so getting the gb4 tokenizer you can so getting the gb4 tokenizer you can take uh you can encode the string and take uh you can encode the string and take uh you can encode the string and you should get these tokens and then you you should get these tokens and then you you should get these tokens and then you can encode and decode the exact same can encode and decode the exact same can encode and decode the exact same string to recover it and in addition to string to recover it and in addition to string to recover it and in addition to all that you should be able to implement all that you should be able to implement all that you should be able to implement your own train function uh which Tik your own train function uh which Tik your own train function uh which Tik token Library does not provide it's it's token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 960,
      "text": "Library does not provide it's it's token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 961,
      "text": "Library does not provide it's it's again only inference code but you could again only inference code but you could again only inference code but you could write your own train MBP does it as well write your own train MBP does it as well write your own train MBP does it as well and that will allow you to train your and that will allow you to train your and that will allow you to train your own token own token own token vocabularies so here are some of the vocabularies so here are some of the vocabularies so here are some of the code inside M be mean bpe uh shows the code inside M be mean bpe uh shows the code inside M be mean bpe uh shows the token vocabularies that you might obtain token vocabularies that you might obtain token vocabularies that you might obtain so on the left uh here we have the GPT 4 so on the left uh here we have the GPT 4 so on the left uh here we have the GPT 4 merges uh so the first 256 are raw merges uh so the first 256 are raw merges uh so the first 256 are raw individual bytes and then here I am individual bytes and then here I am individual bytes and then here I am visualizing the merges that gp4 visualizing the merges that gp4 visualizing the merges that gp4 performed during its training so the performed during its training so the performed during its training so the very first merge that gp4 did was merge very first merge that gp4 did was merge very first merge that gp4 did was merge two spaces into a single token for you two spaces into a single token for you two spaces into a single token for you know two spaces and that is a token 256 know two spaces and that is a token 256 know two spaces and that is a token 256 and so this is the order in which things and so this is the order in which things and so this is the order in which things merged during gb4 training and this is merged during gb4 training and this is merged during gb4 training and this is the merge order that um we obtain in MBP the merge order that um we obtain in MBP the merge order that um we obtain in MBP by training a tokenizer and in this case by training a tokenizer and in this case by training a tokenizer and in this case I trained it on a Wikipedia page of I trained it on a Wikipedia page of I trained it on a Wikipedia page of Taylor Swift uh not because I'm a Swifty Taylor Swift uh not because I'm a Swifty Taylor Swift uh not because I'm a Swifty but because that is one of the longest but because that is one of the longest but because that is one of the longest um Wikipedia Pages apparently that's um Wikipedia Pages apparently that's um Wikipedia Pages apparently that's available",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 962,
      "text": "but she is pretty cool and available",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 963,
      "text": "but she is pretty cool and available",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 964,
      "text": "but she is pretty cool",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 965,
      "text": "and um what was I going to say",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 966,
      "text": "yeah",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 967,
      "text": "so you um what was I going to say",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 968,
      "text": "yeah",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 969,
      "text": "so you um what was I going to say",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 970,
      "text": "yeah",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 971,
      "text": "so you can compare these two uh vocabularies can compare these two uh vocabularies can compare these two uh vocabularies and so as an example um here GPT for and so as an example um here GPT for and so as an example um here GPT for merged I in to become in and we've done merged I in to become in and we've done merged I in to become in and we've done the exact same thing on this token 259 the exact same thing on this token 259 the exact same thing on this token 259 here space t becomes space t and that here space t becomes space t and that here space t becomes space t and that happened for us a little bit later as happened for us a little bit later as happened for us a little bit later as well so the difference here is again to",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 972,
      "text": "well so the difference here is again to",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 973,
      "text": "well so the difference here is again to my understanding only a difference of my understanding only a difference of my understanding only a difference of the training set so as an example the training set so as an example the training set so as an example because I see a lot of white space I because I see a lot of white space I because I see a lot of white space I supect that gp4 probably had a lot of supect that gp4 probably had a lot of supect that gp4 probably had a lot of python code in its training set I'm not python code in its training set I'm not python code in its training set I'm not sure uh for the sure uh for the sure uh for the tokenizer and uh here we see much less tokenizer and uh here we see much less tokenizer and uh here we see much less of that of course in the Wikipedia page of that of course in the Wikipedia page of that of course in the Wikipedia page so roughly speaking they look the same so roughly speaking they look the same so roughly speaking they look the same",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 974,
      "text": "and they look the same because they're and they look the same because they're and they look the same because they're running the same algorithm and when you running the same algorithm and when you running the same algorithm and when you train your own you're probably going to train your own you're probably going to train your own you're probably going to get something similar depending on what get something similar depending on what get something similar depending on what you train it on",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 975,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 976,
      "text": "so we are now going you train it on",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 977,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 978,
      "text": "so we are now going you train it on",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 979,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 980,
      "text": "so we are now going to move on from tick token and the way to move on from tick token and the way to move on from tick token and the way that open AI tokenizes its strings and that open AI tokenizes its strings and that open AI tokenizes its strings and we're going to discuss one more very we're going to discuss one more very we're going to discuss one more very commonly used library for working with commonly used library for working with commonly used library for working with tokenization inlm tokenization inlm tokenization inlm and that is sentence piece so sentence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 981,
      "text": "and that is sentence piece so sentence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 982,
      "text": "and that is sentence piece so sentence piece is very commonly used in language piece is very commonly used in language piece is very commonly used in language models because unlike Tik token it can models because unlike Tik token it can models because unlike Tik token it can do both training and inference and is do both training and inference and is do both training and inference and is quite efficient at both it supports a quite efficient at both it supports a quite efficient at both it supports a number of algorithms for training uh number of algorithms for training uh number of algorithms for training uh vocabularies but one of them is the B vocabularies but one of them is the B vocabularies but one of them is the B pair en coding algorithm that we've been pair en coding algorithm that we've been pair en coding algorithm that we've been looking at so it supports it now looking at so it supports it now looking at so it supports it now sentence piece is used both by llama and sentence piece is used both by llama and sentence piece is used both by llama and mistal series and many other models as mistal series and many other models as mistal series and many other models as well it is on GitHub under Google",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 983,
      "text": "well it is on GitHub under Google",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 984,
      "text": "well it is on GitHub under Google sentence piece sentence piece sentence piece and the big difference with sentence and the big difference with sentence and the big difference with sentence piece and we're going to look at example piece and we're going to look at example piece and we're going to look at example because this is kind of hard and subtle because this is kind of hard and subtle because this is kind of hard and subtle to explain is that they think different to explain is that they think different to explain is that they think different about the order of operations here so in about the order of operations here so in about the order of operations here so in the case of Tik token we first take our the case of Tik token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 985,
      "text": "we first take our the case of Tik token we first take our code points in the string we encode them code points in the string we encode them code points in the string we encode them using mutf to bytes and then we're using mutf to bytes and then we're using mutf to bytes and then we're merging bytes",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 986,
      "text": "it's fairly merging bytes it's fairly merging bytes it's fairly straightforward for sentence piece um it straightforward for sentence piece um it straightforward for sentence piece um it works directly on the level of the code works directly on the level of the code works directly on the level of the code points themselves",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 987,
      "text": "so so it looks at points themselves",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 988,
      "text": "so so it looks at points themselves",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 989,
      "text": "so so it looks at whatever code points are available in whatever code points are available in whatever code points are available in your training set",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 990,
      "text": "and then it starts your training set",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 991,
      "text": "and then it starts your training set",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 992,
      "text": "and then it starts merging those code points and um the bpe merging those code points and um the bpe merging those code points and um the bpe is running on the level of code is running on the level of code is running on the level of code points and if you happen to run out of points and if you happen to run out of points and if you happen to run out of code points",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 993,
      "text": "so there are maybe some rare code points so there are maybe some rare code points",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 994,
      "text": "so there are maybe some rare uh code points that just don't come up uh code points that just don't come up uh code points that just don't come up too often and the Rarity is determined too often and the Rarity is determined too often and the Rarity is determined by this character coverage hyper by this character coverage hyper by this character coverage hyper parameter then these uh code points will parameter then these uh code points will parameter then these uh code points will either get mapped to a special unknown either get mapped to a special unknown either get mapped to a special unknown",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 995,
      "text": "token like ank or if you have the bite token like ank",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 996,
      "text": "or if you have the bite token like ank or if you have the bite foldback option turned on then that will foldback option turned on then that will foldback option turned on then that will take those rare Cod points it will take those rare Cod points it will take those rare Cod points it will encode them using utf8 and then the encode them using utf8 and then the encode them using utf8 and then the individual bytes of that encoding will individual bytes of that encoding will individual bytes of that encoding will be translated into tokens and there are be translated into tokens and there are be translated into tokens and there are these special bite tokens that basically these special bite tokens that basically these special bite tokens that basically get added to the vocabulary so it uses get added to the vocabulary so it uses get added to the vocabulary so it uses BP on on the code points and then it BP on on the code points",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 997,
      "text": "and then it BP on on the code points",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 998,
      "text": "and then it falls back to bytes for rare Cod points falls back to bytes for rare Cod points falls back to bytes for rare Cod points um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 999,
      "text": "and so that's kind of like difference um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1000,
      "text": "and so that's kind of like difference um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1001,
      "text": "and so that's kind of like difference personally I find the Tik token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1002,
      "text": "we personally I find the Tik token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1003,
      "text": "we personally I find the Tik token we significantly cleaner uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1004,
      "text": "but it's kind significantly cleaner uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1005,
      "text": "but it's kind significantly cleaner uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1006,
      "text": "but it's kind of like a subtle but pretty major of like a subtle but pretty major of like a subtle but pretty major difference between the way they approach difference between the way they approach difference between the way they approach tokenization let's work with with a tokenization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1007,
      "text": "let's work with with a tokenization let's work with with a concrete example because otherwise this concrete example because otherwise this concrete example because otherwise this is kind of hard to um to get your head is kind of hard to um to get your head is kind of hard to um to get your head around so let's work with a concrete around so let's work with a concrete around so let's work with a concrete example this is how we can import example this is how we can import example this is how we can import sentence piece and then here we're going sentence piece and then here we're going sentence piece and then here we're going to take I think I took like the to take I think I took like the to take I think I took like the description of sentence piece",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1008,
      "text": "and I just description of sentence piece and I just description of sentence piece",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1009,
      "text": "and I just created like a little toy data set it created like a little toy data set it created like a little toy data set it really likes to have a file",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1010,
      "text": "so I created really likes to have a file",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1011,
      "text": "so I created really likes to have a file",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1012,
      "text": "so I created a toy.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1013,
      "text": "txt file with this a toy.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1014,
      "text": "txt file with this a toy.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1015,
      "text": "txt file with this content now what's kind of a little bit content now what's kind of a little bit content now what's kind of a little bit crazy about sentence piece is that crazy about sentence piece is that crazy about sentence piece is that there's a ton of options and there's a ton of options and there's a ton of options and configurations and the reason this is so configurations and the reason this is so configurations and the reason this is so is because sentence piece has been is because sentence piece has been is because sentence piece has been around I think for a while and",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1016,
      "text": "it really around I think for a while and",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1017,
      "text": "it really around I think for a while and it really tries to handle a large diversity of tries to handle a large diversity of tries to handle a large diversity of things and um because it's been around I things",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1018,
      "text": "and um because it's been around I things",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1019,
      "text": "and um because it's been around I think it has quite a bit of accumulated think it has quite a bit of accumulated think it has quite a bit of accumulated historical baggage uh as well and so in historical baggage uh as well and so in historical baggage uh as well and so in particular there's like a ton of particular there's like a ton of particular there's like a ton of configuration arguments this is not even configuration arguments this is not even configuration arguments this is not even all of it you can go to here to see all all of it you can go to here to see all all of it you can go to here to see all the training the training the training options um and uh there's also quite options um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1020,
      "text": "and uh there's also quite options um and uh there's also quite useful documentation when you look at useful documentation when you look at useful documentation when you look at the raw Proto buff uh that is used to the raw Proto buff uh that is used to the raw Proto buff uh that is used to represent the trainer spec and so on um represent the trainer spec and so on um represent the trainer spec and so on um many of these options are irrelevant to many of these options are irrelevant to many of these options are irrelevant to us so maybe to point out one example Das us so maybe to point out one example Das us so maybe to point out one example Das Das shrinking Factor uh this shrinking Das shrinking Factor uh this shrinking Das shrinking Factor uh this shrinking factor is not used in the B pair en factor is not used in the B pair en factor is not used in the B pair en coding algorithm",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1021,
      "text": "so this is just an coding algorithm",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1022,
      "text": "so this is just an coding algorithm",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1023,
      "text": "so this is just an argument that is irrelevant to us um it argument that is irrelevant to us um it argument that is irrelevant to us um it applies to a different training algorithm now what I tried to do here is algorithm now what I tried to do here is I tried to set up sentence piece in a I tried to set up sentence piece in a I tried to set up sentence piece in a way that is very very similar as far as way that is very very similar as far as way that is very very similar as far as I can tell to maybe identical hopefully I can tell to maybe identical",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1024,
      "text": "hopefully I can tell to maybe identical hopefully to the way that llama 2 was strained so to the way that llama 2 was strained so to the way that llama 2 was strained so the way they trained their own um their the way they trained their own um their the way they trained their own um their own tokenizer and the way I did this was own tokenizer and the way I did this was own tokenizer and the way I did this was basically you can take the tokenizer basically you can take the tokenizer basically you can take the tokenizer model file that meta released and you model file that meta released and you model file that meta released and you can um open it using the Proto protuff can um open it using the Proto protuff can um open it using the Proto protuff uh sort of file that you can generate uh sort of file that you can generate uh sort of file that you can generate and then you can inspect all the options and then you can inspect all the options and then you can inspect all the options and I tried to copy over all the options and I tried to copy over all the options and I tried to copy over all the options that looked relevant so here we set up that looked relevant so here we set up that looked relevant so here we set up the input it's raw text in this file the input it's raw text in this file the input it's raw text in this file here's going to be the output",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1025,
      "text": "so it's here's going to be the output",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1026,
      "text": "so it's here's going to be the output",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1027,
      "text": "so it's going to be for talk 400.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1028,
      "text": "model and going to be for talk 400.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1029,
      "text": "model and going to be for talk 400.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1030,
      "text": "model and vocab vocab vocab we're saying that we're going to use the we're saying that we're going to use the we're saying that we're going to use the BP algorithm",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1031,
      "text": "and we want to Bap size of BP algorithm",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1032,
      "text": "and we want to Bap size of BP algorithm",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1033,
      "text": "and we want to",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1034,
      "text": "Bap size of 400 then there's a ton of configurations 400 then there's a ton of configurations 400 then there's a ton of configurations here for um for basically pre-processing and for um for basically pre-processing and normalization rules as they're called normalization rules as they're called normalization rules as they're called normalization used to be very prevalent normalization used to be very prevalent normalization used to be very prevalent I would say before llms in natural I would say before llms in natural I would say before llms in natural language processing so in machine language processing so in machine language processing so in machine translation and uh text classification translation and uh text classification translation and uh text classification and so on you want to normalize and and so on you want to normalize and and so on you want to normalize and simplify the text and you want to turn simplify the text and you want to turn simplify the text and you want to turn it all lowercase",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1035,
      "text": "and you want to remove it all lowercase",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1036,
      "text": "and you want to remove it all lowercase",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1037,
      "text": "and you want to remove all double whites space Etc all double whites space",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1038,
      "text": "Etc all double whites space Etc and in language models we prefer not to and in language models we prefer not to and in language models we prefer not to do any of it or at least that is my do any of it or at least that is my do any of it or at least that is my preference as a deep learning person you preference as a deep learning person you preference as a deep learning person you want to not touch your data you want to want to not touch your data you want to want to not touch your data you want to keep the raw data as much as possible um keep the raw data as much as possible um keep the raw data as much as possible um in a raw in a raw in a raw form so you're basically trying to turn form so you're basically trying to turn form",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1039,
      "text": "so you're basically trying to turn off a lot of this if you can the other off a lot of this if you can the other off a lot of this if you can the other thing that sentence piece does is that thing that sentence piece does is that thing that sentence piece does is that it has this concept of sentences so it has this concept of sentences so it has this concept of sentences so sentence piece it's back",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1040,
      "text": "it's kind of sentence piece it's back",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1041,
      "text": "it's kind of sentence piece it's back",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1042,
      "text": "it's kind of like was developed I think early in the like was developed I think early in the like was developed I think early in the days where there was um an idea that days where there was um an idea that days where there was um an idea that they you're training a tokenizer on a they you're training a tokenizer on a they you're training a tokenizer on a bunch of independent sentences so it has bunch of independent sentences so it has bunch of independent sentences so it has a lot of like how many sentences you're a lot of like how many sentences you're a lot of like how many sentences you're going to train on what is the maximum going to train on what is the maximum going to train on what is the maximum sentence length sentence length sentence length um shuffling sentences",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1043,
      "text": "and so for it um shuffling sentences and so for it um shuffling sentences and so for it sentences are kind of like the sentences are kind of like the sentences are kind of like the individual training examples but again individual training examples but again individual training examples but again in the context of llms I find that this in the context of llms I find that this in the context of llms I find that this is like a very spous and weird is like a very spous and weird is like a very spous and weird distinction like sentences are just like distinction like sentences are just like distinction like sentences are just like don't touch the raw data sentences don't touch the raw data sentences don't touch the raw data sentences happen to exist but in raw data sets happen to exist but in raw data sets happen to exist but in raw data sets there are a lot of like inet like what there are a lot of like inet like what there are a lot of like inet like what exactly is a sentence what isn't a exactly is a sentence what isn't a exactly is a sentence what isn't a sentence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1044,
      "text": "um and so I think like it's sentence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1045,
      "text": "um and so I think like it's sentence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1046,
      "text": "um and so I think like it's really hard to Define what an actual really hard to Define what an actual really hard to Define what an actual sentence is if you really like dig into sentence is if you really like dig into sentence is if you really like dig into it and there could be different concepts it and there could be different concepts it and there could be different concepts of it in different languages or of it in different languages or of it in different languages or something like that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1047,
      "text": "so why even something like that so why even something like that so why even introduce the concept it it doesn't introduce the concept it it doesn't introduce the concept it it doesn't honestly make sense to me I would just honestly make sense to me I would just honestly make sense to me I would just prefer to treat a file as a giant uh prefer to treat a file as a giant uh prefer to treat a file as a giant uh stream of stream of stream of bytes it has a lot of treatment around bytes it has a lot of treatment around bytes it has a lot of treatment around rare word characters and when I say word rare word characters and when I say word rare word characters and when I say word I mean code points we're going to come I mean code points we're going to come I mean code points we're going to come back to this in a second and it has a back to this in a second and it has a back to this in a second and it has a lot of other rules for um basically lot of other rules for um basically lot of other rules for um basically splitting digits splitting white space splitting digits splitting white space splitting digits splitting white space and numbers and how you deal with that and numbers and how you deal with that and numbers and how you deal with that so these are some kind of like merge so these are some kind of like merge",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1048,
      "text": "so these are some kind of like merge rules",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1049,
      "text": "so I think this is a little bit rules",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1050,
      "text": "so I think this is a little bit rules",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1051,
      "text": "so I think this is a little bit equivalent to tick token using the equivalent to tick token using the equivalent to tick token using the regular expression to split up regular expression to split up regular expression to split up categories there's like kind of categories there's like kind of categories there's like kind of equivalence of it if you squint T it in equivalence of it if you squint T it in equivalence of it if you squint T it in sentence piece where you can also for sentence piece where you can also for sentence piece where you can also for example split up split up the digits uh example split up split up the digits uh example split up split up the digits uh and",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1052,
      "text": "uh so",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1053,
      "text": "and uh so and uh so on there's a few more things here that on there's a few more things here that on there's a few more things here that I'll come back to in a bit",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1054,
      "text": "and then I'll come back to in a bit",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1055,
      "text": "and then I'll come back to in a bit",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1056,
      "text": "and then there are some special tokens that you there are some special tokens that you there are some special tokens that you can indicate and it hardcodes the UN can indicate and it hardcodes the UN can indicate and it hardcodes the UN token the beginning of sentence end of token the beginning of sentence end of token the beginning of sentence end of sentence and a pad",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1057,
      "text": "token um and the UN sentence and a pad token um and the UN sentence and a pad token um and the UN token must exist for my understanding token must exist for my understanding token must exist for my understanding and then some some things so we can and then some some things so we can and then some some things so we can train and when when I press train it's train and when when I press train",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1058,
      "text": "it's train and when when I press train",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1059,
      "text": "it's going to create this file talk 400.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1060,
      "text": "going to create this file talk 400.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1061,
      "text": "going to create this file talk 400.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1062,
      "text": "model and talk 400.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1063,
      "text": "wab I can then load model and talk 400.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1064,
      "text": "wab I can then load model and talk 400.",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1065,
      "text": "wab I can then load the model file and I can inspect the the model file and I can inspect the the model file and I can inspect the vocabulary off it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1066,
      "text": "and so we trained vocabulary off it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1067,
      "text": "and so we trained vocabulary off it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1068,
      "text": "and so we trained vocab size 400 on this text here and vocab size 400 on this text here and vocab size 400 on this text here and these are the individual pieces the these are the individual pieces the these are the individual pieces the individual tokens that sentence piece individual tokens that sentence piece individual tokens that sentence piece will create so in the beginning we see will create so in the beginning we see will create so in the beginning we see that we have the an token uh with the ID that we have the an token uh with the ID that we have the an token uh with the ID zero",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1069,
      "text": "then we have the beginning of zero then we have the beginning of zero then we have the beginning of sequence end of sequence one and two and sequence end of sequence one and two and sequence end of sequence one and two",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1070,
      "text": "and then we said that the pad ID is negative then we said that the pad ID is negative then we said that the pad ID is negative 1",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1071,
      "text": "so we chose not to use it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1072,
      "text": "so there's 1",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1073,
      "text": "so we chose not to use it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1074,
      "text": "so there's 1",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1075,
      "text": "so we chose not to use it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1076,
      "text": "so there's no pad ID no pad ID no pad ID here then these are individual bite here then these are individual bite here then these are individual bite tokens so here we saw that bite fallback tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1077,
      "text": "so here we saw that bite fallback tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1078,
      "text": "so here we saw that bite fallback in llama was turned on so it's true",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1079,
      "text": "so in llama was turned on so it's true",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1080,
      "text": "so in llama was turned on so it's true",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1081,
      "text": "so what follows are going to be the 256 what follows are going to be the 256 what follows are going to be the 256 bite bite bite tokens and these are their IDs and then at the bottom after the IDs and then at the bottom after the bite tokens come the bite tokens come the bite tokens come the merges and these are the parent nodes in merges and these are the parent nodes in merges and these are the parent nodes in the merges",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1082,
      "text": "so we're not seeing the the merges so we're not seeing the the merges so we're not seeing the children we're just seeing the parents children we're just seeing the parents children we're just seeing the parents and their and their and their ID and then after the ID and then after the ID and then after the merges comes eventually the individual merges comes eventually the individual merges comes eventually the individual tokens and their IDs and so these are tokens and their IDs and so these are tokens and their IDs and so these are the individual tokens so these are the the individual tokens so these are the the individual tokens so these are the individual code Point tokens if you will individual code Point tokens if you will individual code Point tokens if you will and they come at the end so that is the and they come at the end so that is the and they come at the end so that is the ordering with which sentence piece sort ordering with which sentence piece sort ordering with which sentence piece sort of like represents its vocabularies it of like represents its vocabularies it of like represents its vocabularies it starts with special tokens then the bike starts with special tokens then the bike starts with special tokens then the bike tokens then the merge tokens and then tokens then the merge tokens and then tokens then the merge tokens and then the individual codo tokens and all these the individual codo tokens and all these the individual codo tokens and all these raw codepoint to tokens are the ones raw codepoint to tokens are the ones raw codepoint to tokens are the ones that it encountered in the training that it encountered in the training that it encountered in the training set so those individual code points are set so those individual code points are set so those individual code points are all the the entire set of code points all the the entire set of code points all the the entire set of code points that occurred that occurred that occurred here so those all get put in there and here so those all get put in there and here so those all get put in there and then those that are extremely rare as then those that are extremely rare as then those that are extremely rare as determined by character coverage so if a determined by character coverage so if a determined by character coverage so if a code Point occurred only a single time code Point occurred only a single time code Point occurred only a single time out of like a million um sentences or out of like a million um sentences or out of like a million um sentences or something like that then it would be something like that then it would be something like that then it would be ignored and it would not be added to our ignored and it would not be added to our ignored and it would not be added to our uh uh vocabulary once we have a vocabulary we vocabulary once we have a vocabulary we vocabulary once we have a vocabulary we can encode into IDs and we can um sort can encode into IDs and we can um sort can encode into IDs and we can um sort of get a of get a of get a list and then here I am also decoding list and then here I am also decoding list and then here I am also decoding the indiv idual tokens back into little the indiv idual tokens back into little the indiv idual tokens back into little pieces as they call it so let's take a pieces as they call it so let's take a pieces as they call it so let's take a look at what happened here hello space look at what happened here hello space look at what happened here hello space on so these are the token IDs we got on so these are the token IDs we got on so these are the token IDs we got back and when we look here uh a few back and when we look here uh a few back and when we look here uh a few things sort of uh jump to mind number things sort of uh jump to mind number things sort of uh jump to mind number one take a look at these characters the one take a look at these characters the one take a look at these characters the Korean characters of course were not Korean characters of course were not Korean characters of course were not part of the training set so sentence part of the training set so sentence part of the training set so sentence piece is encountering code points that piece is encountering code points that piece is encountering code points that it has not seen during training time and it has not seen during training time and it has not seen during training time and those code points do not have a token those code points do not have a token those code points do not have a token associated with them so suddenly",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1083,
      "text": "these associated with them so suddenly these associated with them so suddenly these are un tokens unknown tokens but because are un tokens unknown tokens but because are un tokens unknown tokens but because bite fall back as true instead sentence bite fall back as true instead sentence bite fall back as true instead sentence piece falls back to bytes and so it piece falls back to bytes and so it piece falls back to bytes and so it takes this it encodes it with utf8 and takes this it encodes it with utf8 and takes this it encodes it with utf8 and then it uses these tokens to represent then it uses these tokens to represent then it uses these tokens to represent uh those bytes and that's what we are uh those bytes and that's what we are uh those bytes and that's what we are getting sort of here this is the utf8 uh getting sort of here this is the utf8 uh getting sort of here this is the utf8 uh encoding and in this shifted by three uh encoding and in this shifted by three uh encoding and in this shifted by three uh because of these um special tokens here because of these um special tokens here because of these um special tokens here that have IDs earlier on so that's what that have IDs earlier on so that's what that have IDs earlier on so that's what happened here now one more thing that um happened here now one more thing that um happened here now one more thing that um well first before I go on with respect well first before I go on with respect well first before I go on with respect to the bitef back let me remove bite to the bitef back let me remove bite to the bitef back let me remove bite foldback if this is false what's going foldback if this is false what's going foldback if this is false what's going to happen let's to happen let's to happen let's retrain so the first thing that happened retrain so the first thing that happened retrain so the first thing that happened is all the bite tokens disappeared right is all the bite tokens disappeared right is all the bite tokens disappeared right and now we just have the merges and we and now we just have the merges and we and now we just have the merges and we have a lot more merges now because we have a lot more merges now because we have a lot more merges now because we have a lot more space because we're not have a lot more space because we're not have a lot more space because we're not taking up space in the wab size uh with taking up space in the wab size uh with taking up space in the wab size uh with all the all the all the bytes and now if we encode bytes and now if we encode bytes and now if we encode this we get a zero so this entire string this we get a zero so this entire string this we get a zero",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1084,
      "text": "so this entire string here suddenly there's no bitef back so here suddenly there's no bitef back so here suddenly there's no bitef back so this is unknown and unknown is an and so this is unknown and unknown is an and so this is unknown and unknown is an and so this is zero because the an token is this is zero because the an token is this is zero because the an token is token zero",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1085,
      "text": "and you have to keep in mind token zero and you have to keep in mind token zero and you have to keep in mind that this would feed into your uh that this would feed into your uh that this would feed into your uh language model",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1086,
      "text": "so what is a language language model",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1087,
      "text": "so what is a language language model",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1088,
      "text": "so what is a language model supposed to do when all kinds of model supposed to do when all kinds of model supposed to do when all kinds of different things that are unrecognized different things that are unrecognized different things that are unrecognized because they're rare just end up mapping because they're rare just end up mapping because they're rare just end up mapping into",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1089,
      "text": "Unk",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1090,
      "text": "it's not exactly the property into",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1091,
      "text": "Unk",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1092,
      "text": "it's not exactly the property into",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1093,
      "text": "Unk",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1094,
      "text": "it's not exactly the property that you want so that's why I think that you want so that's why I think that you want so that's why I think llama correctly uh used by fallback true llama correctly uh used by fallback true llama correctly uh used by fallback true uh because we definitely want to feed uh because we definitely want to feed uh because we definitely want to feed these um unknown or rare code points these um unknown or rare code points these um unknown or rare code points into the model and some uh some manner into the model and some uh some manner into the model and some uh some manner the next thing I want to show you is the the next thing I want to show you is the the next thing I want to show you is the following notice here when we are following notice here when we are following notice here when we are decoding all the individual tokens you decoding all the individual tokens you decoding all the individual tokens you see how spaces uh space here ends up see how spaces uh space here ends up see how spaces uh space here ends up being this um bold underline I'm not being this um bold underline I'm not being this um bold underline I'm not 100% sure by the way why sentence piece 100% sure by the way why sentence piece 100% sure by the way why sentence piece switches whites space into these bold switches whites space into these bold switches whites space into these bold underscore characters maybe it's for underscore characters maybe it's for underscore characters maybe it's for visualization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1095,
      "text": "I'm not 100% sure why that visualization I'm not 100% sure why that visualization I'm not 100% sure why that happens uh but notice this why do we happens uh but notice this why do we happens uh but notice this why do we have an extra space in the front of have an extra space in the front of have an extra space in the front of hello um what where is this coming from hello um what where is this coming from hello um what where is this coming from",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1096,
      "text": "well it's coming from this option",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1097,
      "text": "well it's coming from this option well it's coming from this option here here here um add dummy prefix is true and when you um add dummy prefix is true and when you um add dummy prefix is true and when you go to the go to the go to the documentation add D whites space at the documentation add D whites space at the documentation add D whites space at the beginning of text in order to treat beginning of text in order to treat beginning of text in order to treat World in world and hello world in the World in world and hello world in the World in world and hello world in the exact same way so what this is trying to exact same way so what this is trying to exact same way so what this is trying to do is the do is the do is the following if we go back to our tick following if we go back to our tick following if we go back to our tick tokenizer world as uh token by itself tokenizer world as uh token by itself tokenizer world as uh token by itself has a different ID than space world so has a different ID than space world so has a different ID than space world",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1098,
      "text": "so we have this is 1917",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1099,
      "text": "but this is 14",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1100,
      "text": "Etc we have this is 1917",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1101,
      "text": "but this is 14",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1102,
      "text": "Etc we have this is 1917",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1103,
      "text": "but this is 14 Etc so these are two different tokens for so these are two different tokens for so these are two different tokens for the language model and the language the language model and the language the language model and the language model has to learn from data that they model has to learn from data that they model has to learn from data that they are actually kind of like a very similar are actually kind of like a very similar",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1104,
      "text": "are actually kind of like a very similar concept so to the language model in the concept so to the language model in the concept so to the language model in the Tik token World um basically words in Tik token World um basically words in Tik token World um basically words in the beginning of sentences and words in the beginning of sentences and words in the beginning of sentences and words in the middle of sentences actually look the middle of sentences actually look the middle of sentences actually look completely different um and it has to completely different um and it has to completely different um and it has to learned that they are roughly the same learned that they are roughly the same learned that they are roughly the same so this add dami prefix is trying to so this add dami prefix is trying to so this add dami prefix is trying to fight that a little bit and the way that fight that a little bit and the way that fight that a little bit and the way that works is that it basically works is that it basically works is that it basically uh adds a dummy prefix so for as a as a uh adds a dummy prefix so for as a as a uh adds a dummy prefix so for as a as a part of pre-processing it will take the part of pre-processing it will take the part of pre-processing it will take the string and it will add a space it will string and it will add a space it will string and it will add a space it will do this and that's done in an effort to do this and that's done in an effort to do this and that's done in an effort to make this world and that world the same make this world and that world the same make this world and that world the same they will both be space world so that's they will both be space world so that's they will both be space world so that's one other kind of pre-processing option one other kind of pre-processing option one other kind of pre-processing option that is turned on and llama 2 also uh that is turned on and llama 2 also uh that is turned on and llama 2 also uh uses this option and that's I think uses this option and that's I think uses this option and that's I think everything that I want to say for my everything that I want to say for my everything that I want to say for my preview of sentence piece and how it is preview of sentence piece and how it is preview of sentence piece and how it is different",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1105,
      "text": "um maybe here what I've done different um maybe here what I've done different",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1106,
      "text": "um maybe here what I've done is I just uh put in the Raw protocol is I just uh put in the Raw protocol is I just uh put in the Raw protocol buffer representation basically of the buffer representation basically of the buffer representation basically of the tokenizer the too trained so feel free tokenizer the too trained so feel free tokenizer the too trained so feel free to sort of Step through this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1107,
      "text": "and if you to sort of Step through this and if you to sort of Step through this and if you would like uh your tokenization to look would like uh your tokenization to look would like uh your tokenization to look identical to that of the meta uh llama 2 identical to that of the meta uh llama 2 identical to that of the meta uh llama 2 then you would be copy pasting these then you would be copy pasting these then you would be copy pasting these settings as I tried to do up above and settings as I tried to do up above and settings as I tried to do up above and",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1108,
      "text": "uh yeah",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1109,
      "text": "that's I think that's it for uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1110,
      "text": "yeah",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1111,
      "text": "that's I think that's it for uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1112,
      "text": "yeah",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1113,
      "text": "that's I think that's it for this section I think my summary for this section I think my summary for this section I think my summary for sentence piece from all of this is sentence piece from all of this is sentence piece from all of this is number one",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1114,
      "text": "I think that there's a lot of number one I think that there's a lot of number one I think that there's a lot of historical baggage in sentence piece a historical baggage in sentence piece a historical baggage in sentence piece a lot of Concepts that I think are lot of Concepts that I think are lot of Concepts that I think are slightly confusing and I think slightly confusing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1115,
      "text": "and I think slightly confusing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1116,
      "text": "and I think potentially um contain foot guns like potentially um contain foot guns like potentially um contain foot guns like this concept of a sentence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1117,
      "text": "and it's this concept of a sentence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1118,
      "text": "and it's this concept of a sentence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1119,
      "text": "and it's maximum length and stuff like that um maximum length and stuff like that um maximum length and stuff like that um otherwise it is fairly commonly used in otherwise it is fairly commonly used in otherwise it is fairly commonly used in the industry um because it is efficient the industry um because it is efficient the industry um because it is efficient and can do both training and inference and can do both training and inference and can do both training and inference uh it has a few quirks like for example uh it has a few quirks like for example uh it has a few quirks like for example un token must exist and the way the bite un token must exist and the way the bite un token must exist and the way the bite fallbacks are done and so on I don't fallbacks are done and so on I don't fallbacks are done and so on I don't find particularly elegant and find particularly elegant and find particularly elegant and unfortunately I have to say it's not unfortunately I have to say it's not unfortunately I have to say it's not very well documented so it took me a lot very well documented so it took me a lot very well documented so it took me a lot of time working with this myself",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1120,
      "text": "um and of time working with this myself um and of time working with this myself um and just visualizing things and trying to just visualizing things and trying to just visualizing things and trying to really understand what is happening here really understand what is happening here really understand what is happening here because uh the documentation because uh the documentation because uh the documentation unfortunately is in my opion not not unfortunately is in my opion not not unfortunately is in my opion not not super amazing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1121,
      "text": "but it is a very nice repo super amazing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1122,
      "text": "but it is a very nice repo super amazing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1123,
      "text": "but it is a very nice repo that is available to you if you'd like that is available to you if you'd like that is available to you if you'd like to train your own tokenizer right now to train your own tokenizer right now to train your own tokenizer right now okay let me now switch gears again as okay let me now switch gears again as okay let me now switch gears again as we're starting to slowly wrap up here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1124,
      "text": "I we're starting to slowly wrap up here I we're starting to slowly wrap up here I want to revisit this issue in a bit more want to revisit this issue in a bit more want to revisit this issue in a bit more detail of how we should set the vocap detail of how we should set the vocap detail of how we should set the vocap size and what are some of the size and what are some of the size and what are some of the considerations around it so for this I'd considerations around it so for this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1125,
      "text": "I'd considerations around it so for this I'd like to go back to the model like to go back to the model like to go back to the model architecture that we developed in the architecture that we developed in the architecture that we developed in the last video when we built the GPT from last video when we built the GPT from last video when we built the GPT from scratch",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1126,
      "text": "so this here was uh the file scratch",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1127,
      "text": "so this here was uh the file scratch",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1128,
      "text": "so this here was uh the file that we built in the previous video and that we built in the previous video and that we built in the previous video and we defined the Transformer model and and we defined the Transformer model and and we defined the Transformer model and and let's specifically look at Bap size and let's specifically look at Bap size and let's specifically look at Bap size and where it appears in this file so here we where it appears in this file so here we where it appears in this file so here we Define the voap size uh at this time it Define the voap size uh at this time it Define the voap size uh at this time it was 65 or something like that extremely was 65 or something like that extremely was 65 or something like that extremely small number so this will grow much small number so this will grow much small number so this will grow much larger you'll see that Bap size doesn't larger you'll see that Bap size doesn't larger you'll see that Bap size doesn't come up too much in most of these layers come up too much in most of these layers come up too much in most of these layers the only place that it comes up to is in the only place that it comes up to is in the only place that it comes up to is in exactly these two places here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1129,
      "text": "so when we exactly these two places here so when we exactly these two places here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1130,
      "text": "so when we Define the language model there's the Define the language model there's the Define the language model there's the token embedding table which is this token embedding table which is this token embedding table which is this two-dimensional array where the vocap two-dimensional array where the vocap two-dimensional array where the vocap size is basically the number of rows and size is basically the number of rows and size is basically the number of rows and uh each vocabulary element each token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1131,
      "text": "uh each vocabulary element each token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1132,
      "text": "uh each vocabulary element each token has a vector that we're going to train has a vector that we're going to train has a vector that we're going to train using back propagation that Vector is of using back propagation that Vector is of using back propagation that Vector is of size and embed which is number of size and embed which is number of size and embed which is number of channels in the Transformer and channels in the Transformer and channels in the Transformer and basically as voap size increases this basically as voap size increases this basically as voap size increases this embedding table as I mentioned earlier embedding table as I mentioned earlier embedding table as I mentioned earlier is going to also grow we're going to be is going to also grow we're going to be is going to also grow we're going to be adding rows in addition to that at the adding rows in addition to that at the adding rows in addition to that at the end of the Transformer there's this LM end of the Transformer there's this LM end of the Transformer there's this LM head layer which is a linear layer and head layer which is a linear layer and head layer which is a linear layer and you'll notice that that layer is used at you'll notice that that layer is used at you'll notice that that layer is used at the very end to produce the logits uh the very end to produce the logits uh the very end to produce the logits uh which become the probabilities for the which become the probabilities for the which become the probabilities for the next token in sequence and so next token in sequence and so next token in sequence and so intuitively we're trying to produce a intuitively we're trying to produce a intuitively we're trying to produce a probability for every single token that probability for every single token that probability for every single token that might come next at every point in time might come next at every point in time might come next at every point in time of that Transformer and if we have more of that Transformer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1133,
      "text": "and if we have more of that Transformer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1134,
      "text": "and if we have more and more tokens we need to produce more and more tokens we need to produce more and more tokens we need to produce more and more probabilities so every single and more probabilities so every single and more probabilities so every single token is going to introduce an token is going to introduce an token is going to introduce an additional dot product that we have to additional dot product that we have to additional dot product that we have to do here in this linear layer for this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1135,
      "text": "do here in this linear layer for this do here in this linear layer for this final layer in a final layer in a final layer in a Transformer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1136,
      "text": "so why can't vocap size be Transformer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1137,
      "text": "so why can't vocap size be Transformer",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1138,
      "text": "so why can't vocap size be infinite why can't we grow to Infinity infinite why can't we grow to Infinity infinite why can't we grow to Infinity well number one your token embedding well number one your token embedding well number one your token embedding table is going to grow uh your linear table is going to grow uh your linear table is going to grow uh your linear layer is going to grow so we're going to layer is going to grow so we're going to layer is going to grow so we're going to be doing a lot more computation here be doing a lot more computation here be doing a lot more computation here because this LM head layer will become because this LM head layer will become because this LM head layer will become more computational expensive number two more computational expensive number two more computational expensive number two because we have more parameters we could because we have more parameters we could because we have more parameters we could be worried that we are going to be under be worried that we are going to be under be worried that we are going to be under trining some of these trining some of these trining some of these parameters so intuitively if you have a parameters so intuitively if you have a parameters so intuitively if you have a very large vocabulary size say we have a very large vocabulary size say we have a very large vocabulary size say we have a million uh tokens then every one of million uh tokens then every one of million uh tokens then every one of these tokens is going to come up more these tokens is going to come up more these tokens is going to come up more and more rarely in the training data and more rarely in the training data and more rarely in the training data because there's a lot more other tokens because there's a lot more other tokens because there's a lot more other tokens all over the place",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1139,
      "text": "and so we're going to all over the place",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1140,
      "text": "and so we're going to all over the place",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1141,
      "text": "and so we're going to be seeing fewer and fewer examples uh be seeing fewer and fewer examples uh be seeing fewer and fewer examples uh for each individual token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1142,
      "text": "and you might for each individual token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1143,
      "text": "and you might for each individual token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1144,
      "text": "and you might be worried that basically the vectors be worried that basically the vectors be worried that basically the vectors associated with every token will be associated with every token will be associated with every token will be undertrained as a result because they undertrained as a result because they undertrained as a result because they just don't come up too often and they just don't come up too often",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1145,
      "text": "and they just don't come up too often and they don't participate in the forward don't participate in the forward don't participate in the forward backward pass in addition to that as backward pass in addition to that as backward pass in addition to that as your vocab size grows you're going to your vocab size grows you're going to your vocab size grows you're going to start shrinking your sequences a lot start shrinking your sequences a lot start shrinking your sequences a lot",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1146,
      "text": "right",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1147,
      "text": "and that's really nice",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1148,
      "text": "because right",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1149,
      "text": "and that's really nice",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1150,
      "text": "because right",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1151,
      "text": "and that's really nice because that means that we're going to be that means that we're going to be that means that we're going to be attending to more and more text so attending to more and more text so attending to more and more text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1152,
      "text": "so that's nice",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1153,
      "text": "but also you might be that's nice",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1154,
      "text": "but also you might be that's nice",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1155,
      "text": "but also you might be worrying that two large of chunks are worrying that two large of chunks are worrying that two large of chunks are being squished into single tokens and so being squished into single tokens and so being squished into single tokens and so the model just doesn't have as much of the model just doesn't have as much of the model just doesn't have as much of time to think per sort of um some number time to think per sort of um some number time to think per sort of um some number of characters in the text or you can of characters in the text or you can of characters in the text or you can think about it that way",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1156,
      "text": "right so think about it that way",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1157,
      "text": "right",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1158,
      "text": "so think about it that way",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1159,
      "text": "right",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1160,
      "text": "so basically we're squishing too much basically we're squishing too much basically we're squishing too much information into a single token and then information into a single token and then information into a single token and then the forward pass of the Transformer is the forward pass of the Transformer is the forward pass of the Transformer is not enough to actually process that not enough to actually process that not enough to actually process that information appropriately and so these information appropriately and so these information appropriately and so these are some of the considerations you're are some of the considerations you're are some of the considerations you're thinking about when you're designing the thinking about when you're designing the thinking about when you're designing the vocab size as I mentioned this is mostly vocab size as I mentioned this is mostly vocab size as I mentioned this is mostly an empirical hyperparameter",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1161,
      "text": "and it seems an empirical hyperparameter",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1162,
      "text": "and it seems an empirical hyperparameter",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1163,
      "text": "and it seems like in state-of-the-art architectures like in state-of-the-art architectures like in state-of-the-art architectures today this is usually in the high 10,000 today this is usually in the high 10,000 today this is usually in the high 10,000 or somewhere around 100,000 today and or somewhere around 100,000 today and or somewhere around 100,000 today and the next consideration I want to briefly the next consideration I want to briefly the next consideration I want to briefly talk about is what if we want to take a talk about is what if we want to take a talk about is what if we want to take a pre-trained model and we want to extend pre-trained model and we want to extend pre-trained model and we want to extend the vocap size and this is done fairly the vocap size and this is done fairly the vocap size and this is done fairly commonly actually so for example when commonly actually so for example when commonly actually so for example when you're doing fine-tuning for cha GPT um you're doing fine-tuning for cha GPT um you're doing fine-tuning for cha GPT um a lot more new special tokens get a lot more new special tokens get a lot more new special tokens get introduced on top of the base model to introduced on top of the base model to introduced on top of the base model to maintain the metadata and all the maintain the metadata and all the maintain the metadata and all the structure of conversation objects structure of conversation objects structure of conversation objects between a user and an assistant so that between a user and an assistant so that between a user and an assistant so that takes a lot of special tokens you might takes a lot of special tokens you might takes a lot of special tokens you might also try to throw in more special tokens also try to throw in more special tokens also try to throw in more special tokens for example for using the browser or any for example for using the browser or any for example for using the browser or any other tool",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1164,
      "text": "and so it's very tempting to other tool",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1165,
      "text": "and so it's very tempting to other tool",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1166,
      "text": "and so it's very tempting to add a lot of tokens for all kinds of add a lot of tokens for all kinds of add a lot of tokens for all kinds of special functionality",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1167,
      "text": "so if you want to special functionality so if you want to special functionality",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1168,
      "text": "so if you want to be adding a token that's totally be adding a token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1169,
      "text": "that's totally be adding a token that's totally possible",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1170,
      "text": "Right all we have to do is we possible",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1171,
      "text": "Right all we have to do is we possible",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1172,
      "text": "Right all we have to do is we have to resize this embedding",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1173,
      "text": "so we have have to resize this embedding",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1174,
      "text": "so we have have to resize this embedding",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1175,
      "text": "so we have to add rows we would initialize these uh to add rows we would initialize these uh to add rows we would initialize these uh parameters from scratch to be small parameters from scratch to be small parameters from scratch to be small random numbers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1176,
      "text": "and then we have to random numbers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1177,
      "text": "and then we have to random numbers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1178,
      "text": "and then we have to extend the weight inside this linear uh extend the weight inside this linear uh extend the weight inside this linear",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1179,
      "text": "uh so we have to start making dot products",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1180,
      "text": "so we have to start making dot products",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1181,
      "text": "so we have to start making dot products um with the associated parameters as um with the associated parameters as um with the associated parameters as well to basically calculate the well to basically calculate the well to basically calculate the probabilities for these new tokens so probabilities for these new tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1182,
      "text": "so probabilities for these new tokens so both of these are just a resizing both of these are just a resizing both of these are just a resizing operation it's a very mild operation it's a very mild operation it's a very mild model surgery and can be done fairly model surgery and can be done fairly model surgery and can be done fairly easily and it's quite common that easily and it's quite common that easily and it's quite common that basically you would freeze the base basically you would freeze the base basically you would freeze the base model you introduce these new parameters model you introduce these new parameters model you introduce these new parameters and then you only train these new",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1183,
      "text": "and then you only train these new",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1184,
      "text": "and then you only train these new parameters to introduce new tokens into parameters to introduce new tokens into parameters to introduce new tokens into the architecture um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1185,
      "text": "and so you can the architecture um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1186,
      "text": "and so you can the architecture um",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1187,
      "text": "and so you can freeze arbitrary parts of it or you can freeze arbitrary parts of it or you can freeze arbitrary parts of it or you can train arbitrary parts of it and that's train arbitrary parts of it and that's train arbitrary parts of it and that's totally up to you but basically minor totally up to you but basically minor totally up to you but basically minor surgery required if you'd like to surgery required if you'd like to surgery required if you'd like to introduce new tokens and finally I'd introduce new tokens and finally I'd introduce new tokens and finally I'd like to mention that actually there's an like to mention that actually there's an like to mention that actually there's an entire design space of applications in entire design space of applications in entire design space of applications in terms of introducing new tokens into a terms of introducing new tokens into a terms of introducing new tokens into a vocabulary that go Way Beyond just vocabulary that go Way Beyond just vocabulary that go Way Beyond just adding special tokens and special new adding special tokens and special new adding special tokens and special new functionality so just to give you a functionality so just to give you a functionality so just to give you a sense of the design space but this could sense of the design space but this could sense of the design space but this could be an entire video just by itself uh be an entire video just by itself uh be an entire video just by itself uh this is a paper on learning to compress this is a paper on learning to compress this is a paper on learning to compress prompts with what they called uh gist prompts with what they called uh gist prompts with what they called uh gist tokens and the rough idea is suppose tokens and the rough idea is suppose tokens and the rough idea is suppose that you're using language models in a that you're using language models in a that you're using language models in a setting that requires very long prompts setting that requires very long prompts setting that requires very long prompts while these long prompts just slow while these long prompts just slow while these long prompts just slow everything down because you have to everything down because you have to everything down because you have to encode them and then you have to use encode them",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1188,
      "text": "and then you have to use encode them",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1189,
      "text": "and then you have to use them",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1190,
      "text": "and then you're tending over them them",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1191,
      "text": "and then you're tending over them them",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1192,
      "text": "and then you're tending over them",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1193,
      "text": "and it's just um you know heavy to have",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1194,
      "text": "and it's just um you know heavy to have",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1195,
      "text": "and it's just um you know heavy to have very large prompts so instead what they very large prompts",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1196,
      "text": "so instead what they very large prompts so instead what they do here in this paper is they introduce do here in this paper is they introduce do here in this paper is they introduce new tokens and um imagine basically new tokens and um imagine basically new tokens and um imagine basically having a few new tokens you put them in having a few new tokens you put them in having a few new tokens you put them in a sequence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1197,
      "text": "and then you train the model a sequence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1198,
      "text": "and then you train the model a sequence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1199,
      "text": "and then you train the model by distillation so you are keeping the by distillation so you are keeping the by distillation so you are keeping the entire model Frozen",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1200,
      "text": "and you're only entire model Frozen",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1201,
      "text": "and you're only entire model Frozen",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1202,
      "text": "and you're only training the representations of the new training the representations of the new training the representations of the new tokens their embeddings and you're tokens their embeddings and you're tokens their embeddings and you're optimizing over the new tokens such that optimizing over the new tokens such that optimizing over the new tokens such that the behavior of the language model is the behavior of the language model is the behavior of the language model is identical uh to the model that has a identical uh to the model that has a identical uh to the model that has a very long prompt that works for you and very long prompt that works for you and very long prompt that works for you",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1203,
      "text": "and so it's a compression technique of so it's a compression technique of so it's a compression technique of compressing that very long prompt into compressing that very long prompt into compressing that very long prompt into those few new gist tokens and so you can those few new gist tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1204,
      "text": "and so you can those few new gist tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1205,
      "text": "and so you can train this and then at test time you can train this and then at test time you can train this and then at test time you can discard your old prompt and just swap in discard your old prompt and just swap in discard your old prompt and just swap in those tokens and they sort of like uh those tokens and they sort of like uh those tokens and they sort of like uh stand in for that very long prompt and stand in for that very long prompt and stand in for that very long prompt and have an almost identical performance and have an almost identical performance and have an almost identical performance",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1206,
      "text": "and so this is one um technique and a class",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1207,
      "text": "so this is one um technique and a class",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1208,
      "text": "so this is one um technique and a class of parameter efficient fine-tuning of parameter efficient fine-tuning of parameter efficient fine-tuning techniques where most of the model is techniques where most of the model is techniques where most of the model is basically fixed and there's no training basically fixed and there's no training basically fixed and there's no training of the model weights there's no training of the model weights there's no training of the model weights there's no training of Laura or anything like that of new of Laura or anything like that of new of Laura or anything like that of new parameters the the parameters that parameters the the parameters that parameters the the parameters that you're training are now just the uh you're training are now just the uh you're training are now just the uh token embeddings",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1209,
      "text": "so that's just one token embeddings so that's just one token embeddings so that's just one example",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1210,
      "text": "but this could again be like an example",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1211,
      "text": "but this could again be like an example",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1212,
      "text": "but this could again be like an entire video but just to give you a entire video but just to give you a entire video but just to give you a sense that there's a whole design space sense that there's a whole design space sense that there's a whole design space here that is potentially worth exploring here that is potentially worth exploring here that is potentially worth exploring in the future the next thing I want to in the future the next thing I want to in the future the next thing I want to briefly address is that I think recently briefly address is that I think recently briefly address is that I think recently there's a lot of momentum in how you there's a lot of momentum in how you there's a lot of momentum in how you actually could construct Transformers actually could construct Transformers actually could construct Transformers that can simultaneously process not just that can simultaneously process not just that can simultaneously process not just text as the input modality but a lot of text as the input modality but a lot of text as the input modality but a lot of other modalities",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1213,
      "text": "so be it images videos other modalities so be it images videos other modalities so be it images videos audio Etc and how do you feed in all audio Etc and how do you feed in all audio Etc and how do you feed in all these modalities and potentially predict these modalities and potentially predict these modalities and potentially predict these modalities from a Transformer uh these modalities from a Transformer uh these modalities from a Transformer uh do you have to change the architecture do you have to change the architecture do you have to change the architecture in some fundamental way",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1214,
      "text": "and I think what in some fundamental way and I think what in some fundamental way and I think what a lot of people are starting to converge a lot of people are starting to converge a lot of people are starting to converge towards is that you're not changing the towards is that you're not changing the towards is that you're not changing the architecture you stick with the architecture you stick with the architecture you stick with the Transformer you just kind of tokenize Transformer you just kind of tokenize Transformer you just kind of tokenize your input domains and then call the day your input domains and then call the day your input domains and then call the day and pretend it's just text tokens and and pretend it's just text tokens and and pretend it's just text tokens and just do everything else identical in an just do everything else identical in an just do everything else identical in an identical manner so here for example identical manner",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1215,
      "text": "so here for example identical manner so here for example there was a early paper that has nice there was a early paper that has nice there was a early paper that has nice graphic for how you can take an image graphic for how you can take an image graphic for how you can take an image and you can chunc at it into and you can chunc at it into and you can chunc at it into integers um and these sometimes uh so integers um and these sometimes uh so integers um and these sometimes uh so these will basically become the tokens these will basically become the tokens these will basically become the tokens of images as an example and uh these of images as an example and uh these of images as an example and uh these tokens can be uh hard tokens where you tokens can be uh hard tokens where you tokens can be uh hard tokens where you force them to be integers they can also force them to be integers they can also force them to be integers they can also be soft tokens where you uh sort of be soft tokens where you uh sort of be soft tokens where you uh sort of don't require uh these to be discrete don't require uh these to be discrete don't require uh these to be discrete",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1216,
      "text": "but you do Force these representations but you do Force these representations but you do Force these representations to go through bottlenecks like in Auto to go through bottlenecks like in Auto to go through bottlenecks like in Auto encoders uh also in this paper that came encoders uh also in this paper that came encoders uh also in this paper that came out from open a SORA which I think out from open a SORA which I think out from open a SORA which I think really um uh blew the mind of many really um uh blew the mind of many really um uh blew the mind of many people and inspired a lot of people in people and inspired a lot of people in people and inspired a lot of people in terms of what's possible they have a terms of what's possible they have a terms of what's possible they have a Graphic here and they talk briefly about Graphic here and they talk briefly about Graphic here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1217,
      "text": "and they talk briefly about how llms have text tokens Sora has how llms have text tokens Sora has how llms have text tokens Sora has visual patches",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1218,
      "text": "so again they came up visual patches",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1219,
      "text": "so again they came up visual patches",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1220,
      "text": "so again they came up with a way to chunc a videos into with a way to chunc a videos into with a way to chunc a videos into basically tokens when they own basically tokens when they own basically tokens when they own vocabularies and then you can either vocabularies",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1221,
      "text": "and then you can either vocabularies",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1222,
      "text": "and then you can either process discrete tokens say with autog process discrete tokens say with autog process discrete tokens say with autog regressive models or even soft tokens regressive models or even soft tokens regressive models or even soft tokens with diffusion models and uh all of that with diffusion models and uh all of that with diffusion models and uh all of that is sort of uh being actively worked on is sort of uh being actively worked on is sort of uh being actively worked on designed on and is beyond the scope of designed on and is beyond the scope of designed on and is beyond the scope of this video but just something I wanted this video but just something I wanted this video but just something I wanted to mention briefly okay now that we have to mention briefly okay now that we have to mention briefly okay now that we have come quite deep into the tokenization come quite deep into the tokenization come quite deep into the tokenization algorithm and we understand a lot more algorithm",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1223,
      "text": "and we understand a lot more algorithm",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1224,
      "text": "and we understand a lot more about how it works let's loop back about how it works let's loop back about how it works let's loop back around to the beginning of this video around to the beginning of this video around to the beginning of this video and go through some of these bullet and go through some of these bullet and go through some of these bullet points and really see why they happen so points and really see why they happen so points and really see why they happen so first of all why can't my llm spell first of all why can't my llm spell first of all why can't my llm spell words very well or do other spell words very well or do other spell words very well or do other spell related related related tasks so fundamentally this is because tasks so fundamentally this is because tasks so fundamentally this is because as we saw these characters are chunked as we saw these characters are chunked as we saw these characters are chunked up into tokens and some of these tokens up into tokens and some of these tokens up into tokens and some of these tokens are actually fairly long so as an are actually fairly long so as an are actually fairly long so as an example I went to the gp4 vocabulary and",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1225,
      "text": "example",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1226,
      "text": "I went to the gp4 vocabulary and example",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1227,
      "text": "I went to the gp4 vocabulary and I looked at uh one of the longer tokens I looked at uh one of the longer tokens I looked at uh one of the longer tokens so that default style turns out to be a so that default style turns out to be a so that default style turns out to be a single individual token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1228,
      "text": "so that's a lot single individual token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1229,
      "text": "so that's a lot single individual token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1230,
      "text": "so that's a lot of characters for a single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1231,
      "text": "so my of characters for a single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1232,
      "text": "so my of characters for a single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1233,
      "text": "so my suspicion is that there's just too much suspicion is that there's just too much suspicion is that there's just too much crammed into this single token and my crammed into this single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1234,
      "text": "and my crammed into this single token and my suspicion was that the model should not suspicion was that the model should not suspicion was that the model should not be very good at tasks related to be very good at tasks related to be very good at tasks related to spelling of this uh single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1235,
      "text": "so I spelling of this uh single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1236,
      "text": "so I spelling of this uh single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1237,
      "text": "so I asked how many letters L are there in asked how many letters L are there in asked how many letters L are there in the word default style and of course my the word default style and of course my the word default style and of course my prompt is intentionally done that way prompt is intentionally done that way prompt is intentionally done that way and you see how default style will be a",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1238,
      "text": "and you see how default style will be a",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1239,
      "text": "and you see how default style will be a single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1240,
      "text": "so this is what the model single",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1241,
      "text": "token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1242,
      "text": "so this is what the model single",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1243,
      "text": "token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1244,
      "text": "so this is what the model sees so my suspicion is that it wouldn't sees so my suspicion is that it wouldn't sees so my suspicion is that it wouldn't be very good at this and indeed it is be very good at this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1245,
      "text": "and indeed it is be very good at this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1246,
      "text": "and indeed it is not it doesn't actually know how many not it doesn't actually know how many not it doesn't actually know how many L's are in there it thinks there are L's are in there it thinks there are L's are in there it thinks there are three and actually there are four if I'm three and actually there are four if I'm three and actually there are four if I'm not getting this wrong myself so that not getting this wrong myself so that not getting this wrong myself so that didn't go extremely well let's look look didn't go extremely well let's look look didn't go extremely well let's look look at another kind of uh character level at another kind of uh character level at another kind of uh character level task so for example here I asked uh gp4 task so for example here I asked uh gp4 task so for example here I asked uh gp4 to reverse the string default style and to reverse the string default style and to reverse the string default style and they tried to use a code interpreter and they tried to use a code interpreter and they tried to use a code interpreter and I stopped it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1247,
      "text": "and I said just do it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1248,
      "text": "just I stopped it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1249,
      "text": "and I said just do it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1250,
      "text": "just I stopped it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1251,
      "text": "and I said just do it just try it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1252,
      "text": "and uh it gave me jumble",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1253,
      "text": "so it try it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1254,
      "text": "and uh it gave me jumble",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1255,
      "text": "so it try it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1256,
      "text": "and uh it gave me jumble",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1257,
      "text": "so it doesn't actually really know how to doesn't actually really know how to doesn't actually really know how to reverse this string going from right to reverse this string going from right to reverse this string going from right to left",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1258,
      "text": "uh so it gave a wrong result so left uh so it gave a wrong result so left uh so it gave a wrong result so again like working with this working again like working with this working again like working with this working hypothesis that maybe this is due to the hypothesis that maybe this is due to the hypothesis that maybe this is due to the tokenization I tried a different tokenization I tried a different tokenization I tried a different approach I said okay let's reverse the approach I said okay let's reverse the approach I said okay let's reverse the exact same string but take the following exact same string but take the following exact same string but take the following approach step one just print out every approach step one just print out every approach step one just print out every single character separated by spaces and single character separated by spaces and single character separated by spaces and then as a step two reverse that list and then as a step two reverse that list and then as a step two reverse that list",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1259,
      "text": "and it again Tred to use a tool but when I it again Tred to use a tool but when I it again Tred to use a tool but when I stopped it it uh first uh produced all stopped it it uh first uh produced all stopped it it uh first uh produced all the characters and that was actually the characters and that was actually the characters and that was actually correct and then It reversed them and correct and then It reversed them and correct and then It reversed them and that was correct once it had this so that was correct once it had this so that was correct once it had this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1260,
      "text": "so somehow it can't reverse it directly",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1261,
      "text": "but somehow it can't reverse it directly",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1262,
      "text": "but somehow it can't reverse it directly",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1263,
      "text": "but when you go just first uh you know when you go just first uh you know when you go just first uh you know listing it out in order it can do that listing it out in order it can do that listing it out in order it can do that somehow",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1264,
      "text": "and then it can once it's uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1265,
      "text": "somehow",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1266,
      "text": "and then it can once it's uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1267,
      "text": "somehow",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1268,
      "text": "and then it can once it's uh broken up this way this becomes all broken up this way this becomes all broken up this way this becomes all these individual characters",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1269,
      "text": "and so now these individual characters and so now these individual characters",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1270,
      "text": "and so now this is much easier for it to see these this is much easier for it to see these this is much easier for it to see these individual tokens and reverse them and individual tokens and reverse them and individual tokens and reverse them and print them out so that is kind of print them out so that is kind of print them out so that is kind of interesting so let's continue now why interesting so let's continue now why interesting so let's continue now why are llms worse at uh non-english langu are llms worse at uh non-english langu are llms worse at uh non-english langu and I briefly covered this already",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1271,
      "text": "but and I briefly covered this already",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1272,
      "text": "but and I briefly covered this already but basically um it's not only that the basically um it's not only that the basically um it's not only that the language model sees less non-english language model sees less non-english language model sees less non-english data during training of the model data during training of the model data during training of the model parameters but also the tokenizer is not parameters but also the tokenizer is not parameters but also the tokenizer is not um is not sufficiently trained on um is not sufficiently trained on um is not sufficiently trained on non-english data",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1273,
      "text": "and so here for example non-english data",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1274,
      "text": "and so here for example non-english data",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1275,
      "text": "and so here for example hello how are you is five tokens and its hello",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1276,
      "text": "how are you is five tokens and its hello how are you is five tokens and its translation is 15 tokens so this is a translation is 15 tokens so this is a translation is 15 tokens so this is a three times blow up and so for example three times blow up and so for example three times blow up and so for example anang is uh just hello basically in anang is uh just hello basically in anang is uh just hello basically in Korean and that end up being three Korean and that end up being three Korean and that end up being three tokens I'm actually kind of surprised by tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1277,
      "text": "I'm actually kind of surprised by tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1278,
      "text": "I'm actually kind of surprised by that because that is a very common that because that is a very common that because that is a very common phrase there just the typical greeting phrase there just the typical greeting phrase there just the typical greeting of like",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1279,
      "text": "hello and that ends up being of like",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1280,
      "text": "hello and that ends up being of like",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1281,
      "text": "hello and that ends up being three tokens whereas our hello is a three tokens whereas our hello is a three tokens whereas our hello is a single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1282,
      "text": "and so basically everything single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1283,
      "text": "and so basically everything single token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1284,
      "text": "and so basically everything is a lot more bloated and diffuse and is a lot more bloated and diffuse and is a lot more bloated and diffuse and this is I think partly the reason that this is I think partly the reason that this is I think partly the reason that the model Works worse on other the model Works worse on other the model Works worse on other languages uh coming back why is LM bad languages uh coming back why is LM bad languages uh coming back why is LM bad at simple arithmetic um that has to do at simple arithmetic um that has to do at simple arithmetic um that has to do with the tokenization of numbers and so with the tokenization of numbers and so with the tokenization of numbers",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1285,
      "text": "and so um you'll notice that for example um you'll notice that for example um you'll notice that for example addition is very sort of addition is very sort of addition is very sort of like uh there's an algorithm that is like uh there's an algorithm that is like uh there's an algorithm that is like character level for doing addition like character level for doing addition like character level for doing addition so for example here we would first add so for example here we would first add so for example here we would first add the ones and then the tens and then the the ones and then the tens and then the the ones and then the tens and then the hundreds you have to refer to specific hundreds you have to refer to specific hundreds you have to refer to specific parts of these digits but uh these parts of these digits but uh these parts of these digits",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1286,
      "text": "but uh these numbers are represented completely numbers are represented completely numbers are represented completely arbitrarily based on whatever happened arbitrarily based on whatever happened arbitrarily based on whatever happened to merge or not merge during the to merge or not merge during the to merge or not merge during the tokenization process there's an entire tokenization process there's an entire tokenization process there's an entire blog post about this that I think is blog post about this that I think is blog post about this that I think is quite good integer tokenization is quite good integer tokenization is quite good integer tokenization is insane and this person basically insane and this person basically insane and this person basically systematically explores the tokenization systematically explores the tokenization systematically explores the tokenization of numbers in I believe this is gpt2 and of numbers in I believe this is gpt2 and of numbers in I believe this is gpt2",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1287,
      "text": "and so they notice that for example for the so they notice that for example for the so they notice that for example for the for um four-digit numbers you can take a for um four-digit numbers you can take a for um four-digit numbers you can take a look at whether it is uh a single token look at whether it is uh a single token look at whether it is uh a single token or whether it is two tokens that is a 1 or whether it is two tokens that is a 1 or whether it is two tokens that is a 1 three or a 2 two or a 31 combination and three or a 2 two or a 31 combination and three or a 2 two or a 31 combination and so all the different numbers are all the so all the different numbers are all the so all the different numbers are all the different combinations and you can different combinations and you can different combinations and you can imagine this is all completely imagine this is all completely imagine this is all completely arbitrarily so and the model arbitrarily so and the model",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1288,
      "text": "arbitrarily so and the model unfortunately sometimes sees uh four um unfortunately sometimes sees uh four um unfortunately sometimes sees uh four um a token for for all four digits a token for for all four digits a token for for all four digits sometimes for three sometimes for two sometimes for three sometimes for two sometimes for three sometimes for two sometimes for one",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1289,
      "text": "and it's in an sometimes for one",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1290,
      "text": "and it's in an sometimes for one",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1291,
      "text": "and it's in an arbitrary uh Manner and so this is arbitrary uh Manner",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1292,
      "text": "and so this is arbitrary uh Manner",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1293,
      "text": "and so this is definitely a headwind if you will for definitely a headwind if you will for definitely a headwind if you will for the language model",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1294,
      "text": "and it's kind of the language model",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1295,
      "text": "and it's kind of the language model",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1296,
      "text": "and it's kind of incredible that it can kind of do it and incredible that it can kind of do it and incredible that it can kind of do it and deal with it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1297,
      "text": "but it's also kind of not deal with it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1298,
      "text": "but it's also kind of not deal with it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1299,
      "text": "but it's also kind of not ideal",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1300,
      "text": "and so that's why for example we ideal",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1301,
      "text": "and so that's why for example we ideal",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1302,
      "text": "and so that's why for example we saw that meta when they train the Llama saw that meta when they train the Llama saw that meta when they train the Llama 2 algorithm",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1303,
      "text": "and they use sentence piece 2 algorithm and they use sentence piece 2 algorithm and they use sentence piece they make sure to split up all the um they make sure to split up all the um they make sure to split up all the um all the digits as an example for uh all the digits as an example for uh all the digits as an example for uh llama 2",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1304,
      "text": "and this is partly to improve a llama 2 and this is partly to improve a llama 2 and this is partly to improve a simple arithmetic kind of simple arithmetic kind of simple arithmetic kind of performance and finally why is gpt2 not performance and finally why is gpt2 not performance and finally why is gpt2 not as good in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1305,
      "text": "again this is partly a as good in Python again this is partly a as good in Python again this is partly a modeling issue on in the architecture modeling issue on in the architecture modeling issue on in the architecture and the data set and the strength of the and the data set and the strength of the and the data set and the strength of the model but it's also partially model",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1306,
      "text": "but it's also partially model",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1307,
      "text": "but it's also partially tokenization because as we saw here with tokenization because as we saw here with tokenization because as we saw here with the simple python example the encoding the simple python example the encoding the simple python example the encoding efficiency of the tokenizer for handling efficiency of the tokenizer for handling efficiency of the tokenizer for handling spaces in Python is terrible and every spaces in Python is terrible and every spaces in Python is terrible and every single space is an individual token and single space is an individual token and single space is an individual token and this dramatically reduces the context this dramatically reduces the context this dramatically reduces the context length that the model can attend to length that the model can attend to length that the model can attend to cross so that's almost like a cross",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1308,
      "text": "so that's almost like a cross so that's almost like a tokenization bug for gpd2 and that was tokenization bug for gpd2 and that was tokenization bug for gpd2 and that was later fixed with gp4",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1309,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1310,
      "text": "so here's later fixed with gp4",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1311,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1312,
      "text": "so here's later fixed with gp4",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1313,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1314,
      "text": "so here's another fun one my llm abruptly halts another fun one my llm abruptly halts another fun one my llm abruptly halts when it sees the string end of text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1315,
      "text": "so when it sees the string end of text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1316,
      "text": "so when it sees the string end of text so here's um here's a very strange Behavior here's um here's a very strange Behavior here's um here's a very strange Behavior print a string end of text is what I print a string end of text is what I print a string end of text is what I told jt4 and it says could you please told jt4 and it says could you please told jt4 and it says could you please specify the string and I'm I'm telling specify the string and I'm I'm telling specify the string",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1317,
      "text": "and I'm I'm telling it give me end of text and it seems like it give me end of text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1318,
      "text": "and it seems like it give me end of text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1319,
      "text": "and it seems like there's an issue it's not seeing end of there's an issue it's not seeing end of there's an issue it's not seeing end of text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1320,
      "text": "and then I give it end of text is text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1321,
      "text": "and then I give it end of text is text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1322,
      "text": "and then I give it end of text is the string",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1323,
      "text": "and then here's a string and the string and then here's a string and the string and then here's a string",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1324,
      "text": "and then it just doesn't print it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1325,
      "text": "so then it just doesn't print it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1326,
      "text": "so then it just doesn't print it so obviously something is breaking here obviously something is breaking here obviously something is breaking here with respect to the handling of the with respect to the handling of the with respect to the handling of the special token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1327,
      "text": "and I don't actually know special token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1328,
      "text": "and I don't actually know special token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1329,
      "text": "and I don't actually know what open ey is doing under the hood what open ey is doing under the hood what open ey is doing under the hood here and whether they are potentially here and whether they are potentially here and whether they are potentially parsing this as an um as an actual token parsing this as an um as an actual token parsing this as an um as an actual token instead of this just being uh end of instead of this just being uh end of instead of this just being uh end of text um as like individual sort of text um as like individual sort of text um as like individual sort of pieces of it without the special token pieces of it without the special token pieces of it without the special token handling logic",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1330,
      "text": "and so it might be that handling logic",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1331,
      "text": "and so it might be that handling logic",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1332,
      "text": "and so it might be that someone when they're calling do encode someone when they're calling do encode someone when they're calling do encode uh they are passing in the allowed uh they are passing in the allowed",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1333,
      "text": "uh they are passing in the allowed special",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1334,
      "text": "and they are allowing end of special and they are allowing end of special and they are allowing end of text as a special character in the user text as a special character in the user text as a special character in the user prompt but the user prompt of course is prompt but the user prompt of course is prompt but the user prompt of course is is a sort of um attacker controlled text is a sort of um attacker controlled text is a sort of um attacker controlled text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1335,
      "text": "so you would hope that they don't really",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1336,
      "text": "so you would hope that they don't really",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1337,
      "text": "so you would hope that they don't really parse or use special tokens or you know parse or use special tokens or you know parse or use special tokens or you know from that kind of input",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1338,
      "text": "but it appears from that kind of input",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1339,
      "text": "but it appears from that kind of input",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1340,
      "text": "but it appears that there's something definitely going that there's something definitely going that there's something definitely going wrong here and um so your knowledge of wrong here and um so your knowledge of wrong here and um so your knowledge of these special tokens ends up being in a these special tokens ends up being in a these special tokens ends up being in a tax surface potentially",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1341,
      "text": "and so if you'd tax surface potentially and so if you'd tax surface potentially and so if you'd like to confuse llms then just um try to like to confuse llms then just um try to like to confuse llms then just um try to give them some special tokens and see if give them some special tokens and see if give them some special tokens and see if you're breaking something by chance",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1342,
      "text": "okay you're breaking something by chance",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1343,
      "text": "okay you're breaking something by chance",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1344,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1345,
      "text": "so this next one is a really fun one",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1346,
      "text": "uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1347,
      "text": "so this next one is a really fun one",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1348,
      "text": "uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1349,
      "text": "so this next one is a really fun one uh the trailing whites space issue so if the trailing whites space issue so if the trailing whites space issue so if you come to playground and uh we come you come to playground and uh we come you come to playground and uh we come here to GPT",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1350,
      "text": "3.5 turbo instruct so this here to GPT 3.5 turbo instruct so this here to GPT 3.5 turbo instruct so this is not a chat model this is a completion is not a chat model this is a completion is not a chat model this is a completion model so think of it more like it's a model so think of it more like it's a model so think of it more like it's a lot more closer to a base model it does lot more closer to a base model it does lot more closer to a base model it does completion it will continue the token completion it will continue the token completion it will continue the token sequence so here's a tagline for ice sequence so here's a tagline for ice sequence so here's a tagline for ice cream shop and we want to continue the cream shop and we want to continue the cream shop and we want to continue the sequence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1351,
      "text": "and so we can submit and get a sequence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1352,
      "text": "and so we can submit and get a sequence",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1353,
      "text": "and so we can submit and get a bunch of tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1354,
      "text": "okay no problem",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1355,
      "text": "but now bunch of tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1356,
      "text": "okay no problem",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1357,
      "text": "but now bunch of tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1358,
      "text": "okay no problem but now suppose I do this but instead of suppose I do this but instead of suppose I do this but instead of pressing submit here I do here's a pressing submit here I do here's a pressing submit here I do here's a tagline for ice cream shop space",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1359,
      "text": "so I tagline for ice cream shop space",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1360,
      "text": "so I tagline for ice cream shop space",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1361,
      "text": "so I have a space here before I click have a space here before I click have a space here before I click submit we get a warning your text ends submit we get a warning your text ends submit we get a warning your text ends in a trail Ling space which causes worse in a trail Ling space which causes worse in a trail Ling space which causes worse performance due to how API splits text performance due to how API splits text performance due to how API splits text into tokens so what's happening here it into tokens so what's happening here it into tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1362,
      "text": "so what's happening here it still gave us a uh sort of completion still gave us a uh sort of completion still gave us a uh sort of completion here but let's take a look at what's here but let's take a look at what's here but let's take a look at what's happening so here's a tagline for an ice happening so here's a tagline for an ice happening so here's a tagline for an ice cream shop and then what does this look cream shop and then what does this look cream shop and then what does this look like in the actual actual training data like in the actual actual training data like in the actual actual training data suppose you found the completion in the suppose you found the completion in the suppose you found the completion in the training document somewhere on the training document somewhere on the training document somewhere on the internet and the llm trained on this internet and the llm trained on this internet and the llm trained on this data",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1363,
      "text": "so maybe it's something like",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1364,
      "text": "oh data",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1365,
      "text": "so maybe it's something like",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1366,
      "text": "oh data",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1367,
      "text": "so maybe it's something like",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1368,
      "text": "oh yeah",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1369,
      "text": "maybe that's the tagline that's a",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1370,
      "text": "yeah",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1371,
      "text": "maybe that's the tagline that's a",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1372,
      "text": "yeah",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1373,
      "text": "maybe that's the tagline that's a terrible tagline but notice here that terrible tagline but notice here that terrible tagline but notice here that when I create o you see that because when I create o you see that because when I create o you see that because there's the the space character is there's the the space character is there's the the space character is always a prefix to these tokens in GPT always a prefix to these tokens in GPT always a prefix to these tokens in GPT",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1374,
      "text": "so it's not an O",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1375,
      "text": "token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1376,
      "text": "it's a space o",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1377,
      "text": "so it's not an O",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1378,
      "text": "token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1379,
      "text": "it's a space o",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1380,
      "text": "so it's not an O",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1381,
      "text": "token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1382,
      "text": "it's a space o token the space is part of the O and token the space is part of the O and token the space is part of the O",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1383,
      "text": "and together they are token 8840",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1384,
      "text": "that's together they are token 8840",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1385,
      "text": "that's together they are token 8840",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1386,
      "text": "that's that's space o",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1387,
      "text": "so what's What's that's space o",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1388,
      "text": "so what's What's that's space o",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1389,
      "text": "so what's What's Happening Here is that when I just have Happening Here is that when I just have Happening Here is that when I just have it like this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1390,
      "text": "and I let it complete the it like this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1391,
      "text": "and I let it complete the it like this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1392,
      "text": "and I let it complete the next token it can sample the space o next token it can sample the space o next token it can sample the space o token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1393,
      "text": "but instead if I have this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1394,
      "text": "and I token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1395,
      "text": "but instead if I have this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1396,
      "text": "and I token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1397,
      "text": "but instead if I have this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1398,
      "text": "and I add my space then what I'm doing here add my space then what I'm doing here add my space then what I'm doing here when I incode this string is I have when I incode this string is I have when I incode this string is I have basically here's a t line for an ice basically here's a t line for an ice basically here's a t line for an ice cream uh shop and this space at the very cream uh shop and this space at the very cream uh shop and this space at the very end becomes a token end becomes a token end becomes a token 220 and so we've added token 220 and 220 and so we've added token 220 and 220 and so we've added token 220 and this token otherwise would be part of this token otherwise would be part of this token otherwise would be part of the tagline because if there actually is the tagline because if there actually is the tagline because if there actually is a tagline here so space o is the token a tagline here so space o is the token a tagline here so space o is the token and so this is suddenly a of",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1399,
      "text": "and so this is suddenly a of and so this is suddenly a of distribution for the model because this distribution for the model because this distribution for the model because this space is part of the next token but space is part of the next token but space is part of the next token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1400,
      "text": "but we're putting it here like this and the we're putting it here like this and the we're putting it here like this and the model has seen very very little data of model has seen very very little data of model has seen very very little data of actual Space by itself and we're asking actual Space by itself and we're asking actual Space by itself and we're asking it to complete the sequence like add in it to complete the sequence like add in it to complete the sequence like add in more tokens but the problem is that more tokens but the problem is that more tokens but the problem is that we've sort of begun the first token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1401,
      "text": "and we've sort of begun the first token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1402,
      "text": "and we've sort of begun the first token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1403,
      "text": "and now it's been split up and now we're out now it's been split up and now we're out now it's been split up and now we're out of this distribution and now arbitrary of this distribution and now arbitrary of this distribution and now arbitrary bad things happen and it's just a very bad things happen",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1404,
      "text": "and it's just a very bad things happen",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1405,
      "text": "and it's just a very rare example for it to see something rare example for it to see something rare example for it to see something like that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1406,
      "text": "and uh that's why we get the like that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1407,
      "text": "and uh that's why we get the like that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1408,
      "text": "and uh that's why we get the warning so the fundamental issue here is warning so the fundamental issue here is warning so the fundamental issue here is of course that um the llm is on top of of course that um the llm is on top of of course that um the llm is on top of these tokens and these tokens are text these tokens and these tokens are text these tokens and these tokens are text chunks they're not characters in a way chunks they're not characters in a way chunks they're not characters in a way",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1409,
      "text": "you and I would think of them they are you and I would think of them they are you",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1410,
      "text": "and I would think of them they are these are the atoms of what the LM is these are the atoms of what the LM is these are the atoms of what the LM is seeing",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1411,
      "text": "and there's a bunch of weird seeing and there's a bunch of weird seeing and there's a bunch of weird stuff that comes out of it let's go back stuff that comes out of it let's go back stuff that comes out of it let's go back to our default cell style I bet you that to our default cell style I bet you that to our default cell style I bet you that the model has never in its training set the model has never in its training set the model has never in its training set seen default cell sta without Le in seen default cell sta without Le in seen default cell sta without Le in there it's always seen this as a single there",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1412,
      "text": "it's always seen this as a single there",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1413,
      "text": "it's always seen this as a single group because uh this is some kind of a group because uh this is some kind of a group because uh this is some kind of a function in um I'm guess I don't function in um I'm guess I don't function in um I'm guess I don't actually know what this is part of this actually know what this is part of this actually know what this is part of this is some kind of API",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1414,
      "text": "but I bet you that is some kind of API",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1415,
      "text": "but I bet you that is some kind of API",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1416,
      "text": "but I bet you that it's never seen this combination of it's never seen this combination of it's never seen this combination of tokens uh in its training data because tokens uh in its training data because tokens uh in its training data because or I think it would be extremely rare",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1417,
      "text": "so",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1418,
      "text": "or I think it would be extremely rare",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1419,
      "text": "so",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1420,
      "text": "or I think it would be extremely rare",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1421,
      "text": "so I took this and I copy pasted it here I took this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1422,
      "text": "and I copy pasted it here I took this and I copy pasted it here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1423,
      "text": "and I had I tried to complete from it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1424,
      "text": "and I had I tried to complete from it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1425,
      "text": "and I had I tried to complete from it and the it immediately gave me a big and the it immediately gave me a big and the it immediately gave me a big error and it said the model predicted to error and it said the model predicted to error and it said the model predicted to completion that begins with a stop completion that begins with a stop completion that begins with a stop sequence resulting in no output consider sequence resulting in no output consider sequence resulting in no output consider adjusting your prompt or stop sequences adjusting your prompt or stop sequences adjusting your prompt or stop sequences",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1426,
      "text": "so what happened here when I clicked so what happened here when I clicked so what happened here when I clicked submit is that immediately the model submit is that immediately the model submit is that immediately the model emitted and sort of like end of text emitted and sort of like end of text emitted and sort of like end of text",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1427,
      "text": "token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1428,
      "text": "I think or something like that it token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1429,
      "text": "I think or something like that it token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1430,
      "text": "I think or something like that it basically predicted the stop sequence basically predicted the stop sequence basically predicted the stop sequence immediately so it had no completion and immediately so it had no completion and immediately so it had no completion and so this is why I'm getting a warning so this is why I'm getting a warning",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1431,
      "text": "so this is why I'm getting a warning again because we're off the data again because we're off the data again because we're off the data distribution and the model is just uh distribution and the model is just uh distribution and the model is just uh predicting just totally arbitrary things predicting just totally arbitrary things predicting just totally arbitrary things it's just really confused basically this it's just really confused basically this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1432,
      "text": "it's just really confused basically this is uh this is giving it brain damage is uh this is giving it brain damage is uh this is giving it brain damage it's never seen this before it's shocked it's never seen this before it's shocked it's never seen this before it's shocked and it's predicting end of text or and it's predicting end of text or and it's predicting end of text or something I tried it again here and it something I tried it again here and it something I tried it again here and it in this case it completed it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1433,
      "text": "but then in this case it completed it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1434,
      "text": "but then in this case it completed it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1435,
      "text": "but then for some reason this request May violate for some reason this request May violate for some reason this request May violate our usage policies this was our usage policies this was our usage policies this was flagged um basically something just like flagged um basically something just like flagged um basically something just like goes wrong and there's something like goes wrong and there's something like goes wrong and there's something like Jank you can just feel the Jank because Jank you can just feel the Jank because Jank you can just feel the Jank because the model is like extremely unhappy with the model is like extremely unhappy with the model is like extremely unhappy with just this and it doesn't know how to just this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1436,
      "text": "and it doesn't know how to just this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1437,
      "text": "and it doesn't know how to complete it because it's never occurred complete it because it's never occurred complete it because it's never occurred in training set in a training set it in training set in a training set it in training set in a training set it always appears like this and becomes a always appears like this and becomes a always appears like this and becomes a single token single token single",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1438,
      "text": "token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1439,
      "text": "so these kinds of issues where tokens so these kinds of issues where tokens so these kinds of issues where tokens are either you sort of like complete the are either you sort of like complete the are either you sort of like complete the first character of the next token or you first character of the next token or you first character of the next token or you are sort of you have long tokens that are sort of you have long tokens that are sort of you have long tokens that you then have just some of the you then have just some of the you then have just some of the characters off all of these are kind of characters off all of these are kind of characters off all of these are kind of like issues with partial tokens is how I like issues with partial tokens is how I like issues with partial tokens is how I would describe it and if you actually would describe it and if you actually would describe it and if you actually dig into the T token dig into the T token dig into the T token repository go to the rust code and repository go to the rust code and repository go to the rust code and search for search for search for unstable and you'll see um en code unstable",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1440,
      "text": "and you'll see um en code unstable",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1441,
      "text": "and you'll see um en code unstable native unstable token tokens unstable native unstable token tokens unstable native unstable token tokens and a lot of like special case handling and a lot of like special case handling and a lot of like special case handling none of this stuff about unstable tokens none of this stuff about unstable tokens none of this stuff about unstable tokens is documented anywhere",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1442,
      "text": "but there's a ton is documented anywhere",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1443,
      "text": "but there's a ton is documented anywhere",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1444,
      "text": "but there's a ton of code dealing with unstable tokens and of code dealing with unstable tokens and of code dealing with unstable tokens and unstable tokens is exactly kind of like unstable tokens is exactly kind of like unstable tokens is exactly kind of like what I'm describing here what you would what I'm describing here what you would what I'm describing here what you would like out of a completion API is like out of a completion API is like out of a completion API is something a lot more fancy like if we're something a lot more fancy like if we're something a lot more fancy like if we're putting in default cell sta if we're putting in default cell sta if we're putting in default cell sta if we're asking for the next token sequence we're asking for the next token sequence we're asking for the next token sequence we're not actually trying to append the next not actually trying to append the next not actually trying to append the next token exactly after this list we're token exactly after this list we're token exactly after this list we're actually trying to append we're trying actually trying to append we're trying actually trying to append we're trying to consider lots of tokens um to consider lots of tokens um to consider lots of tokens um that if we were or I guess like we're that if we were or I guess like we're that if we were or I guess like we're trying to search over characters that if trying to search over characters that if trying to search over characters that if we retened would be of high probability we retened would be of high probability we retened would be of high probability if that makes sense um so that we can if that makes sense um so that we can if that makes sense um so that we can actually add a single individual actually add a single individual actually add a single individual character uh instead of just like adding character uh instead of just like adding character uh instead of just like adding the next full token that comes after the next full token that comes after the next full token that comes after this partial token list",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1445,
      "text": "so I this is this partial token list",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1446,
      "text": "so I this is this partial token list",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1447,
      "text": "so I this is very tricky to describe and I invite you very tricky to describe",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1448,
      "text": "and I invite you very tricky to describe",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1449,
      "text": "and I invite you to maybe like look through this it ends to maybe like look through this it ends to maybe like look through this it ends up being extremely gnarly and hairy kind up being extremely gnarly and hairy kind up being extremely gnarly and hairy kind of topic it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1450,
      "text": "and it comes from of topic it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1451,
      "text": "and it comes from of topic it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1452,
      "text": "and it comes from tokenization fundamentally",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1453,
      "text": "so um maybe I tokenization fundamentally",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1454,
      "text": "so um maybe I tokenization fundamentally",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1455,
      "text": "so um maybe I can even spend an entire video talking can even spend an entire video talking can even spend an entire video talking about unstable tokens sometime in the about unstable tokens sometime in the about unstable tokens sometime in the future",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1456,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1457,
      "text": "and I'm really saving the future",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1458,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1459,
      "text": "and I'm really saving the future",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1460,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1461,
      "text": "and I'm really saving the best for last my favorite one by far is best for last my favorite one by far is best for last my favorite one by far is the solid gold the solid gold the solid gold Magikarp",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1462,
      "text": "and it just okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1463,
      "text": "so this comes Magikarp",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1464,
      "text": "and it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1465,
      "text": "just okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1466,
      "text": "so this comes Magikarp",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1467,
      "text": "and it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1468,
      "text": "just okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1469,
      "text": "so this comes from this blog post uh solid gold from this blog post uh solid gold from this blog post uh solid gold Magikarp and uh this is um internet Magikarp and uh this is um internet Magikarp and uh this is um internet famous now for those of us in llms and famous now for those of us in llms and famous now for those of us in llms and basically I I would advise you to uh",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1470,
      "text": "basically I I would advise you to uh basically I I would advise you to uh read this block Post in full but read this block Post in full but read this block Post in full but basically what this person was doing is basically what this person was doing is basically what this person was doing is this person went to the um this person went to the um this person went to the um token embedding stable and clustered the token embedding stable and clustered the token embedding stable and clustered the tokens based on their embedding tokens based on their embedding tokens based on their embedding representation and this person noticed representation and this person noticed representation and this person noticed that there's a cluster of tokens that that there's a cluster of tokens that that there's a cluster of tokens that look really strange so there's a cluster look really strange so there's a cluster look really strange",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1471,
      "text": "so there's a cluster here at rot e stream Fame solid gold here at rot e stream Fame solid gold here at rot e stream Fame solid gold Magikarp Signet message like really Magikarp Signet message like really Magikarp Signet message like really weird tokens in uh basically in this weird tokens in uh basically in this weird tokens in uh basically in this embedding cluster",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1472,
      "text": "and so what are these embedding cluster",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1473,
      "text": "and so what are these embedding cluster",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1474,
      "text": "and so what are these tokens and where do they even come from tokens and where do they even come from tokens and where do they even come from like what is solid gold magikarpet makes like what is solid gold magikarpet makes like what is solid gold magikarpet makes no sense",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1475,
      "text": "and then they found bunch of no sense",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1476,
      "text": "and then they found bunch of no sense",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1477,
      "text": "and then they found bunch of these these these tokens and then they notice that tokens and then they notice that tokens and then they notice that actually the plot thickens here because actually the plot thickens here because actually the plot thickens here because if you ask the model about these tokens if you ask the model about these tokens if you ask the model about these tokens like you ask it uh some very benign like you ask it uh some very benign",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1478,
      "text": "like you ask it uh some very benign question like please can you repeat back question like please can you repeat back question like please can you repeat back to me the string sold gold Magikarp uh to me the string sold gold Magikarp uh to me the string sold gold Magikarp uh then you get a variety of basically then you get a variety of basically then you get a variety of basically totally broken llm Behavior so either totally broken llm Behavior so either totally broken llm Behavior so either you get evasion",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1479,
      "text": "so I'm sorry I can't you get evasion",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1480,
      "text": "so I'm sorry I can't you get evasion",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1481,
      "text": "so I'm sorry I can't hear you or you get a bunch of hear you or you get a bunch of hear you or you get a bunch of hallucinations as a response um you can hallucinations as a response um you can hallucinations as a response um you can even get back like insults",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1482,
      "text": "so you ask it even get back like insults",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1483,
      "text": "so you ask it even get back like insults",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1484,
      "text": "so you ask it uh about streamer bot it uh tells the uh about streamer bot it uh tells the uh about streamer bot it uh tells the and the model actually just calls you and the model actually just calls you and the model actually just calls you names uh or it kind of comes up with names uh or it kind of comes up with names uh or it kind of comes up with like weird humor like you're actually like weird humor like you're actually like weird humor like you're actually breaking the model by asking about these breaking the model by asking about these breaking the model by asking about these very simple strings like at Roth and very simple strings like at Roth and very simple strings like at Roth and sold gold Magikarp so like what the hell sold gold Magikarp",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1485,
      "text": "so like what the hell sold gold Magikarp",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1486,
      "text": "so like what the hell is happening and there's a variety of is happening",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1487,
      "text": "and there's a variety of is happening and there's a variety of here documented behaviors uh there's a here documented behaviors uh there's a here documented behaviors uh there's a bunch of tokens not just so good bunch of tokens not just so good bunch of tokens not just so good Magikarp that have that kind of a Magikarp that have that kind of a Magikarp that have that kind of a behavior and so basically there's a behavior and so basically there's a behavior and so basically there's a bunch of like trigger words and if you bunch of like trigger words and if you bunch of like trigger words and if you ask the model about these trigger words ask the model about these trigger words ask the model about these trigger words or you just include them in your prompt or you just include them in your prompt or you just include them in your prompt the model goes haywire and has all kinds the model goes haywire and has all kinds the model goes haywire and has all kinds of uh really Strange Behaviors including of uh really Strange Behaviors including of uh really Strange Behaviors including sort of ones that violate typical safety sort of ones that violate typical safety sort of ones that violate typical safety guidelines uh and the alignment of the guidelines uh and the alignment of the guidelines uh and the alignment of the model like it's swearing back at you so model like it's swearing back at you so model like it's swearing back at you so what is happening here and how can this what is happening here and how can this what is happening here and how can this possibly be true",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1488,
      "text": "well this again comes possibly be true",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1489,
      "text": "well this again comes possibly be true",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1490,
      "text": "well this again comes down to tokenization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1491,
      "text": "so what's happening down to tokenization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1492,
      "text": "so what's happening down to tokenization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1493,
      "text": "so what's happening here is that sold gold Magikarp if you here is that sold gold Magikarp if you here is that sold gold Magikarp if you actually dig into it is a Reddit user so actually dig into it is a Reddit user so actually dig into it is a Reddit user so there's a u Sol gold there's a u Sol gold there's a u Sol gold Magikarp and probably what happened here Magikarp and probably what happened here Magikarp and probably what happened here even though I I don't know that this has even though I I don't know that this has even though I I don't know that this has been like really definitively explored been like really definitively explored been like really definitively explored but what is thought to have happened is but what is thought to have happened is but what is thought to have happened is that the tokenization data set was very that the tokenization data set was very that the tokenization data set was very different from the training data set for different from the training data set for different from the training data set for the actual language model so in the the actual language model so in the the actual language model so in the tokenization data set there was a ton of tokenization data set there was a ton of tokenization data set there was a ton of redded data potentially where the user redded data potentially where the user redded data potentially where the user solid gold Magikarp was mentioned in the solid gold Magikarp was mentioned in the solid gold Magikarp was mentioned in the text because solid gold Magikarp was a text because solid gold Magikarp was a text because solid gold Magikarp was a very common um sort of uh person who very common um sort of uh person who very common um sort of uh person who would post a lot uh this would be a would post a lot uh this would be a would post a lot uh this would be a string that occurs many times in a string that occurs many times in a string that occurs many times in a tokenization data set because it occurs tokenization data set because it occurs tokenization data set because it occurs many times in a tokenization data set many times in a tokenization data set many times in a tokenization data set these tokens would end up getting merged these tokens would end up getting merged these tokens would end up getting merged to the single individual token for that to the single individual token for that to the single individual token for that single Reddit user sold gold Magikarp so single Reddit user sold gold Magikarp so single Reddit user sold gold Magikarp so they would have a dedicated token in a they would have a dedicated token in a they would have a dedicated token in a vocabulary of was it 50,000 tokens in vocabulary of was it 50,000 tokens in vocabulary of was it 50,000 tokens in gpd2 that is devoted to that Reddit user gpd2 that is devoted to that Reddit user gpd2 that is devoted to that Reddit user and then what happens is the",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1494,
      "text": "and then what happens is the",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1495,
      "text": "and then what happens is the tokenization data set has those strings tokenization data set has those strings tokenization data set has those strings but then later when you train the model",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1496,
      "text": "but then later when you train the model but then later when you train the model the language model itself um this data the language model itself um this data the language model itself um this data from Reddit was not present and so from Reddit was not present and so from Reddit was not present",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1497,
      "text": "and so therefore in the entire training set for therefore in the entire training set for therefore in the entire training set for the language model sold gold Magikarp the language model sold gold Magikarp the language model sold gold Magikarp never occurs that token never appears in never occurs that token never appears in never occurs that token never appears in the training set for the actual language the training set for the actual language the training set for the actual language model later so this token never gets model later",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1498,
      "text": "so this token never gets model later so this token never gets activated it's initialized at random in activated it's initialized at random in activated it's initialized at random in the beginning of optimization then you the beginning of optimization then you the beginning of optimization then you have forward backward passes and updates have forward backward passes and updates have forward backward passes and updates to the model and this token is just to the model and this token is just to the model and this token is just never updated in the embedding table never updated in the embedding table never updated in the embedding table that row Vector never gets sampled it that row Vector never gets sampled it that row Vector never gets sampled it never gets used so it never gets trained never gets used",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1499,
      "text": "so it never gets trained never gets used so it never gets trained and it's completely untrained it's kind",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1500,
      "text": "and it's completely untrained it's kind",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1501,
      "text": "and it's completely untrained it's kind of like unallocated memory in a typical of like unallocated memory in a typical of like unallocated memory in a typical binary program written in C or something binary program written in C or something binary program written in C or something like that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1502,
      "text": "that so it's unallocated like that that so it's unallocated like that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1503,
      "text": "that so it's unallocated memory and then at test time if you memory and then at test time if you memory and then at test time if you evoke this",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1504,
      "text": "token then you're basically evoke this token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1505,
      "text": "then you're basically evoke this token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1506,
      "text": "then you're basically plucking out a row of the embedding plucking out a row of the embedding plucking out a row of the embedding table that is completely untrained and table that is completely untrained and table that is completely untrained and that feeds into a Transformer and that feeds into a Transformer and that feeds into a Transformer and creates undefined behavior and that's creates undefined behavior and that's creates undefined behavior and that's what we're seeing here this completely what we're seeing here this completely what we're seeing here this completely undefined never before seen in a undefined never before seen in a undefined never before seen in a training behavior and so any of these training behavior and so any of these training behavior and so any of these kind of like weird tokens would evoke kind of like weird tokens would evoke kind of like weird tokens would evoke this Behavior because fundamentally the this Behavior because fundamentally the this Behavior because fundamentally the model is um is uh uh out of sample out model is um is uh uh out of sample out model is um is uh uh out of sample out of distribution",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1507,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1508,
      "text": "and the very last of distribution",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1509,
      "text": "okay and the very last of distribution",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1510,
      "text": "okay and the very last thing I wanted to just briefly mention thing I wanted to just briefly mention thing I wanted to just briefly mention point out although I think a lot of point out although I think a lot of point out although I think a lot of people are quite aware of this is that people are quite aware of this is that people are quite aware of this is that different kinds of formats and different different kinds of formats and different different kinds of formats and different representations and different languages representations and different languages representations and different languages and so on might be more or less and so on might be more or less and so on might be more or less efficient with GPD tokenizers uh or any efficient with GPD tokenizers uh or any efficient with GPD tokenizers uh or any tokenizers for any other L for that tokenizers for any other L for that tokenizers for any other L for that matter so for example Json is actually matter",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1511,
      "text": "so for example Json is actually matter so for example Json is actually really dense in tokens and yaml is a lot really dense in tokens and yaml is a lot really dense in tokens and yaml is a lot more efficient in tokens",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1512,
      "text": "um so for more efficient in tokens um so for more efficient in tokens um so for example this are these are the same in example this are these are the same in example this are these are the same in Json and in yaml the Json is Json and in yaml the Json is Json and in yaml the Json is 116 and the yaml is 99 so quite a bit of 116 and the yaml is 99 so quite a bit of 116 and the yaml is 99 so quite a bit of an Improvement and so in the token an Improvement and so in the token an Improvement and so in the token economy where we are paying uh per token economy where we are paying uh per token economy where we are paying uh per token in many ways and you are paying in the in many ways and you are paying in the in many ways and you are paying in the context length and you're paying in um context length and you're paying in um context length and you're paying in um dollar amount for uh the cost of dollar amount for uh the cost of dollar amount for uh the cost of processing all this kind of structured processing all this kind of structured processing all this kind of structured data when you have to um so prefer to data when you have to um so prefer to data when you have to um so prefer to use theal over Json and in general kind use theal over Json and in general kind use theal over Json and in general kind of like the tokenization density is of like the tokenization density is of like the tokenization density is something that you have to um sort of something that you have to um sort of something that you have to um sort of care about and worry about at all times care about and worry about at all times care about and worry about at all times and try to find efficient encoding and try to find efficient encoding and try to find efficient encoding schemes and spend a lot of time in tick schemes and spend a lot of time in tick schemes and spend a lot of time in tick tokenizer and measure the different tokenizer and measure the different tokenizer and measure the different token efficiencies of different formats token efficiencies of different formats token efficiencies of different formats and settings and",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1513,
      "text": "so on",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1514,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1515,
      "text": "so that and settings and so on",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1516,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1517,
      "text": "so that and settings and so on",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1518,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1519,
      "text": "so that concludes my fairly long video on concludes my fairly long video on concludes my fairly long video on tokenization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1520,
      "text": "I know it's a try I know tokenization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1521,
      "text": "I know it's a try I know tokenization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1522,
      "text": "I know it's a try I know it's annoying",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1523,
      "text": "I know it's irritating I",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1524,
      "text": "it's annoying",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1525,
      "text": "I know it's irritating I",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1526,
      "text": "it's annoying",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1527,
      "text": "I know it's irritating I personally really dislike the stage what personally really dislike the stage what personally really dislike the stage what I do have to say at this point is don't I do have to say at this point is don't I do have to say at this point is don't brush it off there's a lot of foot guns brush it off there's a lot of foot guns brush it off there's a lot of foot guns sharp edges here security issues uh AI sharp edges here security issues uh AI sharp edges here security issues uh AI safety issues as we saw plugging in safety issues as we saw plugging in safety issues as we saw plugging in unallocated memory into uh language unallocated memory into uh language unallocated memory into uh language models",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1528,
      "text": "so um it's worth understanding models",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1529,
      "text": "so um it's worth understanding models",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1530,
      "text": "so um it's worth understanding this stage um that said I will say that this stage um that said I will say that this stage um that said I will say that eternal glory goes to anyone who can get eternal glory goes to anyone who can get eternal glory goes to anyone who can get rid of it uh I showed you one possible rid of it",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1531,
      "text": "uh I showed you one possible rid of it uh I showed you one possible paper that tried to uh do that and I paper that tried to uh do that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1532,
      "text": "and I paper that tried to uh do that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1533,
      "text": "and I think I hope a lot more can follow over think I hope a lot more can follow over think I hope a lot more can follow over time and my final recommendations for time and my final recommendations for time and my final recommendations for the application right now are if you can the application right now are if you can the application right now are if you can reuse the GPT 4 tokens and the reuse the GPT 4 tokens and the reuse the GPT 4 tokens and the vocabulary uh in your application then vocabulary uh in your application then vocabulary uh in your application then that's something you should consider and that's something you should consider and that's something you should consider and just use Tech token because it is very just use Tech token because it is very just use Tech token because it is very efficient and nice library for inference efficient and nice library for inference efficient and nice library for inference for bpe",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1534,
      "text": "I also really like the bite for bpe I also really like the bite for bpe",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1535,
      "text": "I also really like the bite level BP that uh Tik toen and openi uses level BP that uh Tik toen and openi uses level BP that uh Tik toen and openi uses uh if you for some reason want to train uh if you for some reason want to train uh if you for some reason want to train your own vocabulary from scratch",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1536,
      "text": "um then your own vocabulary from scratch um then your own vocabulary from scratch",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1537,
      "text": "um then I would use uh the bpe with sentence I would use uh the bpe with sentence I would use uh the bpe with sentence piece um oops as I mentioned I'm not a piece um oops as I mentioned I'm not a piece um oops as I mentioned I'm not a huge fan of sentence piece I don't like huge fan of sentence piece I don't like huge fan of sentence piece I don't like its uh bite fallback",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1538,
      "text": "and I don't like its uh bite fallback",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1539,
      "text": "and I don't like its uh bite fallback",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1540,
      "text": "and I don't like that",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1541,
      "text": "it's doing BP on unic code code that it's doing BP on unic code code that it's doing BP on unic code code points I think it's uh it also has like points I think it's uh it also has like points I think it's uh it also has like a million settings",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1542,
      "text": "and I think there's a a million settings",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1543,
      "text": "and I think there's a a million settings",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1544,
      "text": "and I think there's a lot of foot gonss here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1545,
      "text": "and I think it's lot of foot gonss here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1546,
      "text": "and I think it's lot of foot gonss here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1547,
      "text": "and I think it's really easy to Mis calibrate them and really easy to Mis calibrate them and really easy to Mis calibrate them and you end up cropping your sentences or you end up cropping your sentences or you end up cropping your sentences or something like that uh because of some something like that uh because of some something like that uh because of some type of parameter that you don't fully type of parameter that you don't fully type of parameter that you don't fully understand so so be very careful with understand",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1548,
      "text": "so so be very careful with understand",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1549,
      "text": "so so be very careful with the settings try to copy paste exactly the settings try to copy paste exactly the settings try to copy paste exactly maybe where what meta did or basically maybe where what meta did or basically maybe where what meta did or basically spend a lot of time looking at all the spend a lot of time looking at all the spend a lot of time looking at all the hyper parameters and go through the code hyper parameters and go through the code hyper parameters and go through the code of sentence piece and make sure that you of sentence piece and make sure that you of sentence piece and make sure that you have this correct um but even if you have this correct um but even if you have this correct um but even if you have all the settings correct I still have all the settings correct I still have all the settings correct I still think that the algorithm is kind of think that the algorithm is kind of think that the algorithm is kind of inferior to what's happening here and inferior to what's happening here and inferior to what's happening here and maybe the best if you really need to maybe the best if you really need to maybe the best if you really need to train your vocabulary maybe the best train your vocabulary maybe the best train your vocabulary maybe the best thing is to just wait for M bpe to thing is to just wait for M bpe to thing is to just wait for M bpe to becomes as efficient as possible and uh becomes as efficient as possible and uh becomes as efficient as possible",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1550,
      "text": "and uh that's something that maybe I hope to that's something that maybe I hope to that's something that maybe I hope to work on and at some point maybe we can work on and at some point maybe we can work on and at some point maybe we can be training basically really what we be training basically really what we be training basically really what we want is we want tick token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1551,
      "text": "but training want is we want tick token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1552,
      "text": "but training want is we want tick token",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1553,
      "text": "but training code and that is the ideal thing that code and that is the ideal thing that code and that is the ideal thing that currently does not exist and",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1554,
      "text": "MBP is um currently does not exist and MBP is um currently does not exist and MBP is um is in implementation of it but currently is in implementation of it but currently is in implementation of it but currently it's in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1555,
      "text": "so that's currently what it's in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1556,
      "text": "so that's currently what it's in Python",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1557,
      "text": "so that's currently what I have to say for uh tokenization",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1558,
      "text": "there I have to say for uh tokenization there I have to say for uh tokenization there might be an advanced video that has even might be an advanced video that has even might be an advanced video that has even drier and even more detailed in the drier and even more detailed in the drier and even more detailed in the future",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1559,
      "text": "but for now I think we're going future",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1560,
      "text": "but for now I think we're going future",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1561,
      "text": "but for now I think we're going to leave things off here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1562,
      "text": "and uh I hope to leave things off here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1563,
      "text": "and uh I hope to leave things off here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1564,
      "text": "and uh I hope that was helpful",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1565,
      "text": "bye and uh they increase this contact size and uh they increase this contact size from gpt1 of 512 uh to 1024 and GPT 4 from gpt1 of 512 uh to 1024 and GPT 4 from gpt1 of 512 uh to 1024 and GPT 4 two the two the two the next",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1566,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1567,
      "text": "next I would like us to next",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1568,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1569,
      "text": "next I would like us to next",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1570,
      "text": "okay",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1571,
      "text": "next I would like us to briefly walk through the code from open briefly walk through the code from open briefly walk through the code from open AI on the gpt2 encoded ATP",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1572,
      "text": "I'm sorry I'm gonna sneeze ATP I'm sorry I'm gonna sneeze and then what's Happening Here and then what's Happening Here and then what's Happening Here is this is a spous layer that I will is this is a spous layer that I will is this is a spous layer that I will explain in a explain in a explain in a bit",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    },
    {
      "id": 1573,
      "text": "What's Happening Here",
      "start_time": "00:00:02.030",
      "end_time": "02:13:33.149"
    }
  ]
}