{
  "video_id": "kCc8FmEb1nY",
  "sentences": [
    {
      "id": 1,
      "text": "hi everyone so by now you have probably hi everyone so by now you have probably heard of chat GPT it has taken the world heard of chat GPT it has taken the world heard of chat GPT it has taken the world and AI Community by storm and it is a and AI Community by storm and it is a and AI Community by storm and it is a system that allows you to interact with system that allows you to interact with system that allows you to interact with an AI and give it text based tasks so an AI and give it text based tasks so an AI and give it text based tasks so for example we can ask chat GPT to write for example we can ask chat GPT to write for example we can ask chat GPT to write us a small Hau about how important it is us a small Hau about how important it is us a small Hau about how important it is that people understand Ai",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 2,
      "text": "and then they that people understand Ai",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 3,
      "text": "and then they that people understand Ai",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 4,
      "text": "and then they can use it to improve the world and make can use it to improve the world and make can use it to improve the world and make it more prosperous so when we run this it more prosperous",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 5,
      "text": "so when we run this it more prosperous so when we run this AI knowledge brings prosperity for all AI knowledge brings prosperity for all AI knowledge brings prosperity for all to see Embrace its to see Embrace its to see Embrace its power okay not bad",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 6,
      "text": "and so you could see power okay not bad",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 7,
      "text": "and so you could see power okay not bad",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 8,
      "text": "and so you could see that chpt went from left to right and that chpt went from left to right and that chpt went from left to right and generated all these words SE sort of generated all these words SE sort of generated all these words SE sort of sequentially now I asked it already the sequentially now I asked it already the sequentially",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 9,
      "text": "now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 10,
      "text": "I asked it already the exact same prompt a little bit earlier exact same prompt a little bit earlier exact same prompt a little bit earlier and it generated a slightly different and it generated a slightly different and it generated a slightly different outcome ai's power to grow ignorance outcome ai's power to grow ignorance outcome ai's power to grow ignorance holds us back learn Prosperity weights holds us back learn Prosperity weights holds us back learn Prosperity weights",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 11,
      "text": "so uh pretty good in both cases and so uh pretty good in both cases",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 12,
      "text": "and so uh pretty good in both cases and slightly different so you can see that slightly different",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 13,
      "text": "so you can see that slightly different so you can see",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 14,
      "text": "that chat GPT is a probabilistic system and chat GPT is a probabilistic system and chat GPT is a probabilistic system and for any one prompt it can give us for any one prompt it can give us for any one prompt it can give us multiple answers sort of uh replying to multiple answers sort of uh replying to multiple answers sort of uh replying to it now this is just one example of a it now this is just one example of a it now this is just one example of a problem people have come up with many problem people have come up with many problem people have come up with many many examples and there are entire many examples and there are entire many examples and there are entire websites that index interactions with websites that index interactions with websites that index interactions with chpt and so many of them are quite chpt and so many of them are quite chpt and so many of them are quite humorous explain HTML to me like I'm a humorous explain HTML to me like I'm a humorous explain HTML to me like I'm a dog uh write release notes for chess 2 dog uh write release notes for chess 2 dog uh write release notes for chess 2 write a note about Elon Musk buying a write a note about Elon Musk buying a write a note about Elon Musk buying a Twitter and so on so as an example uh Twitter and so on so as an example uh Twitter and so on so as an example uh please write a breaking news article please write a breaking news article please write a breaking news article about a leaf falling from a about a leaf falling from a about a leaf falling from a tree uh and a shocking turn of events a tree uh and a shocking turn of events a tree uh and a shocking turn of events a leaf has fallen from a tree in the local leaf has fallen from a tree in the local leaf has fallen from a tree in the local park Witnesses report that the leaf park Witnesses report that the leaf park Witnesses report that the leaf which was previously attached to a which was previously attached to a which was previously attached to a branch of a tree attached itself and branch of a tree attached itself and branch of a tree attached itself and fell to the ground very dramatic so you fell to the ground very dramatic",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 15,
      "text": "so you fell to the ground very dramatic so you can see that this is a pretty remarkable can see that this is a pretty remarkable can see that this is a pretty remarkable system and it is what we call a language system and it is what we call a language system and it is what we call a language model uh because it um it models the model uh because it um it models the model uh because it um it models the sequence of words or characters or sequence of words or characters or sequence of words or characters or tokens more generally",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 16,
      "text": "and it knows how tokens more generally and it knows how tokens more generally and it knows how sort of words follow each other in sort of words follow each other in sort of words follow each other in English language and so from its English language and so from its English language and so from its perspective what it is doing is it is perspective what it is doing is it is perspective",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 17,
      "text": "what it is doing is it is completing the sequence",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 18,
      "text": "so I give it the completing the sequence",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 19,
      "text": "so I give it the completing the sequence",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 20,
      "text": "so I give it the start of a sequence and it completes the start of a sequence and it completes the start of a sequence and it completes the sequence with the outcome",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 21,
      "text": "and so it's a sequence with the outcome",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 22,
      "text": "and so it's a sequence with the outcome",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 23,
      "text": "and so it's a language model in that sense now I would language model in that sense now I would language model in that sense now I would like to focus on the under the hood of like to focus on the under the hood of like to focus on the under the hood of um under the hood components of what um under the hood components of what um under the hood components of what makes CH GPT work so what is the neural makes CH GPT work so what is the neural makes CH GPT work",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 24,
      "text": "so what is the neural network under the hood that models the network under the hood that models the network under the hood that models the sequence of these words and that comes sequence of these words and that comes sequence of these words and that comes from this paper called attention is all from this paper called attention is all from this paper called attention is all you need in 2017 a landmark paper a you need in 2017 a landmark paper a you need in 2017 a landmark paper a landmark paper in AI that produced and landmark paper in AI that produced and landmark paper in AI that produced and proposed the Transformer proposed the Transformer proposed the Transformer architecture so GPT is uh short for architecture so GPT is uh short for architecture so GPT is uh short for generally generatively pre-trained generally generatively pre-trained generally generatively pre-trained Transformer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 25,
      "text": "so Transformer is the neuron Transformer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 26,
      "text": "so Transformer is the neuron Transformer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 27,
      "text": "so Transformer is the neuron nut that actually does all the heavy nut that actually does all the heavy nut that actually does all the heavy lifting under the hood it comes from lifting under the hood it comes from lifting under the hood it comes from this paper in 2017 now if you read this this paper in 2017 now if you read this this paper in 2017 now if you read this paper this uh reads like a pretty random paper this uh reads like a pretty random paper this uh reads like a pretty random machine translation paper and that's machine translation paper and that's machine translation paper and that's because I think the authors didn't fully because I think the authors didn't fully because I think the authors didn't fully anticipate the impact that the anticipate the impact that the anticipate the impact that the Transformer would have on the field and Transformer would have on the field and Transformer would have on the field and this architecture that they produced in this architecture that they produced in this architecture that they produced in the context of machine translation in the context of machine translation in the context of machine translation in their case actually ended up taking over their case actually ended up taking over their case actually ended up taking over uh the rest of AI in the next 5 years uh the rest of AI in the next 5 years uh the rest of AI in the next 5 years after and so this architecture with after",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 28,
      "text": "and so this architecture with after",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 29,
      "text": "and so this architecture with minor changes was copy pasted into a minor changes was copy pasted into a minor changes was copy pasted into a huge amount of applications in AI in huge amount of applications in AI in huge amount of applications in AI in more recent years and that includes at more recent years and that includes at more recent years and that includes at the core of chat GPT now we are not the core of chat GPT now we are not the core of chat GPT now we are not going to what I'd like to do now is I'd going to what I'd like to do now is I'd going to what I'd like to do now is I'd like to build out something like chat like to build out something like chat like to build out something like chat GPT",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 30,
      "text": "but uh we're not going to be able to GPT",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 31,
      "text": "but uh we're not going to be able to GPT",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 32,
      "text": "but uh we're not going to be able to of course reproduce chat GPT this is a of course reproduce chat GPT",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 33,
      "text": "this is a of course reproduce chat GPT this is a very serious production grade system it very serious production grade system",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 34,
      "text": "it very serious production grade system it is trained on uh a good chunk of is trained on uh a good chunk of is trained on uh a good chunk of internet and then there's a lot of uh internet and then there's a lot of uh internet and then there's a lot of uh pre-training and fine-tuning stages to pre-training and fine-tuning stages to pre-training and fine-tuning stages to it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 35,
      "text": "and so it's very complicated what I'd it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 36,
      "text": "and so it's very complicated what I'd it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 37,
      "text": "and so it's very complicated what I'd like to focus on is just to train a like to focus on is just to train a like to focus on is just to train a Transformer based language model and in Transformer based language model and in Transformer based language model and in our case it's going to be a character our case it's going to be a character our case it's going to be a character level language model I still think that level language model I still think that level language model I still think that is uh very educational with respect to is uh very educational with respect to is uh very educational with respect to how these systems work",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 38,
      "text": "so I don't want how these systems work so I don't want how these systems work",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 39,
      "text": "so I don't want to train on the chunk of Internet we to train on the chunk of Internet we to train on the chunk of Internet we need a smaller data set in this case I need a smaller data set in this case I need a smaller data set in this case I propose that we work with uh my favorite propose that we work with uh my favorite propose that we work with uh my favorite toy data set it's called tiny toy data set it's called tiny toy data set it's called tiny Shakespeare and um what it is is Shakespeare",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 40,
      "text": "and um what it is is Shakespeare",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 41,
      "text": "and um what it is is basically it's a concatenation of all of basically it's a concatenation of all of basically it's a concatenation of all of the works of sh Shakespeare in my the works of sh Shakespeare in my the works of sh Shakespeare in my understanding",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 42,
      "text": "and so this is all of understanding and so this is all of understanding and so this is all of Shakespeare in a single file uh this Shakespeare in a single file uh this Shakespeare in a single file uh this file is about 1 megab",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 43,
      "text": "and it's just all file is about 1 megab",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 44,
      "text": "and it's just all file is about 1 megab",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 45,
      "text": "and it's just all of of of Shakespeare and what we are going to do Shakespeare and what we are going to do Shakespeare and what we are going to do now is we're going to basically model now is we're going to basically model now is we're going to basically model how these characters uh follow each how these characters uh follow each how these characters uh follow each other so for example given a chunk of other so for example given a chunk of other so for example given a chunk of these characters like this uh given some these characters like this uh given some these characters like this uh given some context of characters in the past the context of characters in the past the context of characters in the past the Transformer neural network will look at Transformer neural network will look at Transformer neural network will look at the characters that I've highlighted and the characters that I've highlighted and the characters that I've highlighted and is going to predict that g is likely to is going to predict that g is likely to is going to predict that g is likely to come next in the sequence",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 46,
      "text": "and it's going come next in the sequence and it's going come next in the sequence and it's going to do that because we're going to train to do that because we're going to train to do that because we're going to train that Transformer on Shakespeare and it's that Transformer on Shakespeare and it's that Transformer on Shakespeare and it's just going to try to produce uh just going to try to produce uh just going to try to produce uh character sequences that look like this character sequences that look like this character sequences that look like this and in that process is going to model and in that process is going to model and in that process is going to model all the patterns inside this data so all the patterns inside this data so all the patterns inside this data so once we've trained the system",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 47,
      "text": "i' just once we've trained the system",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 48,
      "text": "i' just once we've trained the system i' just like to give you a preview we can like to give you a preview we can like to give you a preview we can generate infinite Shakespeare and of generate infinite Shakespeare and of generate infinite Shakespeare and of course it's a fake thing that looks kind course it's a fake thing that looks kind course it's a fake thing that looks kind of like of like of like Shakespeare Shakespeare Shakespeare um apologies for there's some Jank that um apologies for there's some Jank that um apologies for there's some Jank that I'm not able to resolve in in here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 49,
      "text": "but I'm not able to resolve in in here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 50,
      "text": "but I'm not able to resolve in in here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 51,
      "text": "but um you can see how this is going um you can see how this is going um you can see how this is going character by character",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 52,
      "text": "and it's kind of character by character",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 53,
      "text": "and it's kind of character by character",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 54,
      "text": "and it's kind of like predicting Shakespeare like like predicting Shakespeare like like predicting Shakespeare like language so verily my Lord the sites language so verily my Lord the sites language so verily my Lord the sites have left the again the king coming with have left the again the king coming with have left the again the king coming with my curses with precious pale and then my curses with precious pale and then my curses with precious pale and then tranos say something else Etc and this tranos say something else Etc and this tranos say something else Etc and this is just coming out of the Transformer in is just coming out of the Transformer in is just coming out of the Transformer in a very similar manner as it would come a very similar manner as it would come a very similar manner as it would come out in chat GPT in our case character by out in chat GPT in our case character by out in chat GPT in our case character by character in chat GPT uh it's coming out character in chat GPT uh it's coming out character in chat GPT uh it's coming out on the token by token level and tokens on the token by token level and tokens on the token by token level and tokens are these sort of like little subword are these sort of like little subword are these sort of like little subword pieces so they're not Word level they're pieces so they're not Word level they're pieces so they're not Word level they're kind of like word chunk kind of like word chunk kind of like word chunk level um and now I've already written level um",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 55,
      "text": "and now I've already written level um and now I've already written this entire code uh to train these this entire code uh to train these this entire code uh to train these Transformers um and it is in a GitHub Transformers um and it is in a GitHub Transformers um and it is in a GitHub repository that you can find",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 56,
      "text": "and it's repository that you can find and it's repository that you can find and it's called nanog called nanog called nanog GPT",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 57,
      "text": "so nanog GPT is a repository that GPT so nanog GPT is a repository that GPT so",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 58,
      "text": "nanog GPT is a repository that you can find in my GitHub and it's a you can find in my GitHub and it's a you can find in my GitHub and it's a repository for training Transformers um repository for training Transformers um repository for training Transformers um on any given text and what I think is on any given text and what I think is on any given text and what I think is interesting about it because there's interesting about it because there's interesting about it because there's many ways to train Transformers but this many ways to train Transformers but this many ways to train Transformers but this is a very simple implementation so it's is a very simple implementation so it's is a very simple implementation so it's just two files of 300 lines of code each just two files of 300 lines of code each just two files of 300 lines of code each one file defines the GPT model the one file defines the GPT model the one file defines the GPT model the Transformer and one file trains it on Transformer and one file trains it on Transformer and one file trains it on some given Text data set and here I'm some given Text data set",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 59,
      "text": "and here I'm some given Text data set",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 60,
      "text": "and here I'm showing that if you train it on a open showing that if you train it on a open showing that if you train it on a open web Text data set which is a fairly web Text data set which is a fairly web Text data set which is a fairly large data set of web pages then I large data set of web pages then I large data set of web pages then I reproduce the the performance of reproduce the the performance of reproduce the the performance of gpt2 so gpt2 is an early version of open gpt2 so gpt2 is an early version of open gpt2 so gpt2 is an early version of open AI GPT uh from 2017 if I recall AI GPT uh from 2017 if I recall AI GPT uh from 2017 if I recall correctly and I've only so far correctly and I've only so far correctly and I've only so far reproduced the the smallest 124 million reproduced the the smallest 124 million reproduced the the smallest 124 million parameter model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 61,
      "text": "uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 62,
      "text": "but basically this is parameter model uh but basically this is parameter model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 63,
      "text": "uh but basically this is just proving that the codebase is just proving that the codebase is just proving that the codebase is correctly arranged",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 64,
      "text": "and I'm able to load correctly arranged and I'm able to load correctly arranged",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 65,
      "text": "and I'm able to load the uh neural network weights that openi the uh neural network weights that openi the uh neural network weights that openi has released later so you can take a has released later so you can take a has released later so you can take a look at the finished code here in N GPT look at the finished code here in N GPT look at the finished code here in N GPT but what I would like to do in this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 66,
      "text": "but what I would like to do in this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 67,
      "text": "but what I would like to do in this lecture is I would like to basically uh lecture is I would like to basically uh lecture is I would like to basically uh write this repository from scratch so write this repository from scratch so write this repository from scratch",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 68,
      "text": "so we're going to begin with an empty file we're going to begin with an empty file we're going to begin with an empty file and we're we're going to define a",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 69,
      "text": "and we're we're going to define a",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 70,
      "text": "and we're we're going to define a Transformer piece by piece we're going Transformer piece by piece we're going Transformer piece by piece we're going to train it on the tiny Shakespeare data to train it on the tiny Shakespeare data to train it on the tiny Shakespeare data set and we'll see how we can then uh set and we'll see how we can then uh set and we'll see how we can then uh generate infinite Shakespeare and of generate infinite Shakespeare and of course this can copy paste to any course this can copy paste to any course this can copy paste to any arbitrary Text data set uh that you like arbitrary Text data set uh that you like arbitrary Text data set uh that you like",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 71,
      "text": "uh but my goal really here is to just uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 72,
      "text": "but my goal really here is to just uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 73,
      "text": "but my goal really here is to just make you understand and appreciate uh make you understand and appreciate uh make you understand and appreciate uh how under the hood chat GPT works and um how under the hood chat GPT works and um how under the hood chat GPT works and um really all that's required is a really all that's required is a really all that's required is a Proficiency in Python and uh some basic Proficiency in Python and uh some basic Proficiency in Python",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 74,
      "text": "and uh some basic understanding of um calculus and understanding of um calculus and understanding of um calculus and statistics statistics statistics and it would help if you also see my and it would help if you also see my and it would help if you also see my previous videos on the same YouTube previous videos on the same YouTube previous videos on the same YouTube channel in particular my make more channel in particular my make more channel in particular my make more series where I um Define smaller and series where I um Define smaller and series where I um Define smaller and simpler neural network language models simpler neural network language models simpler neural network language models uh so multi perceptrons and",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 75,
      "text": "so on it uh so multi perceptrons and so on it uh so multi perceptrons and so on it really introduces the language modeling really introduces the language modeling really introduces the language modeling framework and then uh here in this video framework and",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 76,
      "text": "then uh here in this video framework and then uh here in this video we're going to focus on the Transformer we're going to focus on the Transformer we're going to focus on the Transformer neural network itself",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 77,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 78,
      "text": "so I created neural network itself",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 79,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 80,
      "text": "so I created neural network itself",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 81,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 82,
      "text": "so I created a new Google collab uh jup notebook here a new Google collab uh jup notebook here a new Google collab uh jup notebook here and this will allow me to later easily and this will allow me to later easily and this will allow me to later easily share this code that we're going to share this code that we're going to share this code that we're going to develop together uh with you so you can develop together uh with you so you can develop together uh with you so you can follow along so this will be in a video follow along so this will be in a video follow along so this will be in a video description uh later now here I've just description uh later now here I've just description uh later now here I've just done some preliminaries I downloaded the done some preliminaries I downloaded the done some preliminaries I downloaded the data set the tiny Shakespeare data set data set the tiny Shakespeare data set data set the tiny Shakespeare data set at this URL and you can see that it's at this URL and you can see that it's at this URL and you can see that it's about a 1 Megabyte file then here I open about a 1 Megabyte file then here I open about a 1 Megabyte file then here I open the input.txt file and just read in all the input.txt file and just read in all the input.txt file and just read in all the text of the string and we see that the text of the string and we see that the text of the string and we see that we are working with 1 million characters we are working with 1 million characters we are working with 1 million characters roughly and the first 1,000 characters roughly and the first 1,000 characters roughly and the first 1,000 characters if we just print them out are basically if we just print them out are basically if we just print them out are basically what you would expect this is the first what you would expect this is the first what you would expect this is the first 1,000 characters of the tiny Shakespeare 1,000 characters of the tiny Shakespeare 1,000 characters of the tiny Shakespeare data set roughly up to here so so far so data set roughly up to here so so far so data set roughly up to here so so far so good next we're going to take this text good next we're going to take this text good next we're going to take this text and the text is a sequence of characters and the text is a sequence of characters and the text is a sequence of characters in Python",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 83,
      "text": "so when I call the set in Python so when I call the set in Python",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 84,
      "text": "so when I call the set Constructor on it I'm just going to get Constructor on it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 85,
      "text": "I'm just going to get Constructor on it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 86,
      "text": "I'm just going to get the set of all the characters that occur the set of all the characters that occur the set of all the characters that occur in this text",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 87,
      "text": "and then I call list on in this text",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 88,
      "text": "and then I call list on in this text",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 89,
      "text": "and then I call list on that to create a list of those that to create a list of those that to create a list of those characters instead of just a set so that characters instead of just a set so that characters instead of just a set so that I have an ordering an arbitrary ordering I have an ordering an arbitrary ordering I have an ordering an arbitrary ordering",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 90,
      "text": "and then I sort that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 91,
      "text": "so basically we get",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 92,
      "text": "and then I sort that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 93,
      "text": "so basically we get",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 94,
      "text": "and then I sort that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 95,
      "text": "so basically we get just all the characters that occur in just all the characters that occur in just all the characters that occur in the entire data set and they're sorted the entire data set and they're sorted the entire data set and they're sorted now the number of them is going to be now the number of them is going to be now the number of them is going to be our vocabulary size these are the our vocabulary size these are the our vocabulary size these are the possible elements of our sequences and possible elements of our sequences and possible elements of our sequences and we see that when I print here the we see that when I print here the we see that when I print here the characters there's 65 of them in total characters there's 65 of them in total characters there's 65 of them in total there's a space character and then all there's a space character and then all there's a space character and then all kinds of special characters and then U kinds of special characters and then U kinds of special characters and then U capitals and lowercase letters so that's capitals and lowercase letters so that's capitals and lowercase letters",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 96,
      "text": "so that's our vocabulary and that's the sort of our vocabulary and that's the sort of our vocabulary and that's the sort of like possible uh characters that the like possible uh characters that the like possible uh characters that the model can see or emit",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 97,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 98,
      "text": "so next we model can see or emit",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 99,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 100,
      "text": "so next we model can see or emit",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 101,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 102,
      "text": "so next we will would like to develop some strategy will would like to develop some strategy will would like to develop some strategy to tokenize the input text now when to tokenize the input text now when to tokenize the input text now when people say tokenize they mean convert people say tokenize they mean convert people say tokenize they mean convert the raw text as a string to some the raw text as a string to some the raw text as a string to some sequence of integers According to some sequence of integers According to some sequence of integers According to some uh notebook According to some vocabulary uh notebook",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 103,
      "text": "According to some vocabulary uh notebook According to some vocabulary of possible elements so as an example of possible elements so as an example of possible elements so as an example here we are going to be building a here we are going to be building a here we are going to be building a character level language model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 104,
      "text": "so we're character level language model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 105,
      "text": "so we're character level language model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 106,
      "text": "so we're simply going to be translating simply going to be translating simply going to be translating individual characters into integers so individual characters into integers so individual characters into integers so let me show you uh a chunk of code that let me show you uh a chunk of code that let me show you uh a chunk of code that sort of does that for us so we're sort of does that for us",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 107,
      "text": "so we're sort of does that for us so we're building both the encoder and the building both the encoder and the building both the encoder and the decoder decoder decoder and let me just talk through what's and let me just talk through what's and let me just talk through what's happening happening happening here when we encode an arbitrary text here when we encode an arbitrary text here when we encode an arbitrary text like hi there we're going to receive a like hi there we're going to receive a like hi there we're going to receive a list of integers that represents that list of integers that represents that list of integers that represents that string so for example 46 47 Etc and then string so for example 46 47 Etc and then string so for example 46 47 Etc",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 108,
      "text": "and then we also have the reverse mapping",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 109,
      "text": "so we we also have the reverse mapping",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 110,
      "text": "so we we also have the reverse mapping so we can take this list and decode it to get can take this list and decode it to get can take this list and decode it to get back the exact same string so it's back the exact same string",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 111,
      "text": "so it's back the exact same string so it's really just like a translation to really just like a translation to really just like a translation to integers and back for arbitrary string integers and back for arbitrary string integers and back for arbitrary string and for us it is done on a character and for us it is done on a character and for us it is done on a character level level level now the way this was achieved is we just now the way this was achieved is we just now the way this was achieved is we just iterate over all the characters here and iterate over all the characters here and iterate over all the characters here and create a lookup table from the character create a lookup table from the character create a lookup table from the character to the integer and vice versa and then to the integer and vice versa and then to the integer and vice versa and then to encode some string we simply to encode some string we simply to encode some string we simply translate all the characters translate all the characters translate all the characters individually and to decode it back we individually and to decode it back we individually and to decode it back we use the reverse mapping and concatenate use the reverse mapping and concatenate use the reverse mapping and concatenate all of it now this is only one of many all of it now this is only one of many all of it now this is only one of many possible encodings or many possible sort possible encodings or many possible sort possible encodings or many possible sort of tokenizers and it's a very simple one of tokenizers and it's a very simple one of tokenizers and it's a very simple one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 112,
      "text": "but there's many other schemas that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 113,
      "text": "but there's many other schemas that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 114,
      "text": "but there's many other schemas that people have come up with in practice so people have come up with in practice so people have come up with in practice so for example Google uses a sentence for example Google uses a sentence for example Google uses a sentence piece uh so sentence piece will also piece uh so sentence piece will also piece uh so sentence piece will also encode text into um integers but in a encode text into um integers but in a encode text into um integers but in a different schema and using a different different schema and using a different different schema and using a different vocabulary and sentence piece is a vocabulary and sentence piece is a vocabulary and sentence piece is a subword uh sort of tokenizer and what subword uh sort of tokenizer and what subword uh sort of tokenizer and what that means is that um you're not that means is that um you're not that means is that um you're not encoding entire words but you're not encoding entire words but you're not encoding entire words",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 115,
      "text": "but you're not also encoding individual characters it's also encoding individual characters it's also encoding individual characters it's it's a subword unit level",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 116,
      "text": "and that's it's a subword unit level",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 117,
      "text": "and that's it's a subword unit level",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 118,
      "text": "and that's usually what's adopted in practice for usually what's adopted in practice for usually what's adopted in practice for example also openai has this Library example also openai has this Library example also openai has this Library called tick token that uses a bite pair called tick token that uses a bite pair called tick token that uses a bite pair encode encode encode tokenizer um and that's what GPT uses tokenizer um and that's what GPT uses tokenizer um and that's what GPT uses and you can also just encode words into",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 119,
      "text": "and you can also just encode words into and you can also just encode words into like hell world into a list of integers like hell world into a list of integers like hell world into a list of integers so as an example I'm using the Tik token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 120,
      "text": "so as an example I'm using the Tik token so as an example I'm using the Tik token Library here I'm getting the encoding Library here I'm getting the encoding Library here I'm getting the encoding for gpt2 or that was used for gpt2 for gpt2 or that was used for gpt2 for gpt2 or that was used for gpt2 instead of just having 65 possible instead of just having 65 possible instead of just having 65 possible characters or tokens they have 50,000 characters or tokens they have 50,000 characters or tokens they have 50,000 tokens and so when they encode the exact tokens and so when they encode the exact tokens and so when they encode the exact same string High there we only get a same string High there we only get a same string High there we only get a list of three integers but those list of three integers but those list of three integers but those integers are not between 0 and 64 they integers are not between 0 and 64",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 121,
      "text": "they integers are not between 0 and 64 they are between Z and 5, are between Z and 5, are between Z and 5, 5,256 so basically you can trade off the 5,256 so basically you can trade off the 5,256 so basically you can trade off the code book size and the sequence lengths code book size and the sequence lengths code book size and the sequence lengths so you can have very long sequences of so you can have very long sequences of so you can have very long sequences of integers with very small vocabularies or integers with very small vocabularies or integers with very small vocabularies or we can have short um sequences of we can have short um sequences of we can have short um sequences of integers with very large vocabularies integers with very large vocabularies integers with very large vocabularies and so typically people use in practice and so typically people use in practice and so typically people use in practice these subword encodings",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 122,
      "text": "but I'd like to these subword encodings",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 123,
      "text": "but I'd like to these subword encodings",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 124,
      "text": "but I'd like to keep our token ier very simple",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 125,
      "text": "so we're keep our token ier very simple",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 126,
      "text": "so we're keep our token ier very simple",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 127,
      "text": "so we're using character level tokenizer and that using character level tokenizer and that using character level tokenizer and that means that we have very small code books means that we have very small code books means that we have very small code books we have very simple encode and decode we have very simple encode and decode we have very simple encode and decode functions",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 128,
      "text": "uh but we do get very long functions",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 129,
      "text": "uh but we do get very long functions",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 130,
      "text": "uh but we do get very long sequences as a result but that's the sequences as a result but that's the sequences as a result but that's the level at which we're going to stick with level at which we're going to stick with level at which we're going to stick with this lecture because it's the simplest this lecture because it's the simplest this lecture because it's the simplest thing",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 131,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 132,
      "text": "so now that we have an thing",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 133,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 134,
      "text": "so now that we have an thing",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 135,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 136,
      "text": "so now that we have an encoder and a decoder effectively a encoder and a decoder effectively a encoder and a decoder effectively a tokenizer we can tokenize the entire tokenizer we can tokenize the entire tokenizer we can tokenize the entire training set of Shakespeare so here's a training set of Shakespeare so here's a training set of Shakespeare so here's a chunk of code that does that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 137,
      "text": "and I'm chunk of code that does that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 138,
      "text": "and I'm chunk of code that does that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 139,
      "text": "and I'm going to start to use the pytorch going to start to use the pytorch going to start to use the pytorch library and specifically the torch.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 140,
      "text": "library and specifically the torch.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 141,
      "text": "library and specifically the torch.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 142,
      "text": "tensor from the pytorch library so we're tensor from the pytorch library so we're tensor from the pytorch library",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 143,
      "text": "so we're going to take all of the text in tiny going to take all of the text in tiny going to take all of the text in tiny Shakespeare encode it and then wrap it Shakespeare encode it and then wrap it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 144,
      "text": "Shakespeare encode it and then wrap it into a torch.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 145,
      "text": "tensor to get the data into a torch.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 146,
      "text": "tensor to get the data into a torch.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 147,
      "text": "tensor to get the data tensor",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 148,
      "text": "so here's what the data tensor tensor",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 149,
      "text": "so here's what the data tensor tensor",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 150,
      "text": "so here's what the data tensor looks like when I look at just the first looks like when I look at just the first looks like when I look at just the first 1,000 characters or the 1,000 elements 1,000 characters or the 1,000 elements 1,000 characters or the 1,000 elements of it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 151,
      "text": "so we see that we have a massive of it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 152,
      "text": "so we see that we have a massive of it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 153,
      "text": "so we see that we have a massive sequence of integers and this sequence sequence of integers and this sequence sequence of integers and this sequence of integers here is basically an of integers here is basically an of integers here is basically an identical translation of the first identical translation of the first identical translation of the first 10,000 characters 10,000 characters 10,000 characters here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 154,
      "text": "so I believe for example that zero here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 155,
      "text": "so I believe for example that zero here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 156,
      "text": "so I believe for example that zero is a new line character and maybe one is a new line character and maybe one is a new line character and maybe one one is a space not 100% sure",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 157,
      "text": "but from one is a space not 100% sure",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 158,
      "text": "but from one is a space not 100% sure",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 159,
      "text": "but from now on the entire data set of text is now on the entire data set of text is now on the entire data set of text is re-represented as just it's just re-represented as just it's just re-represented as just it's just stretched out as a single very large uh stretched out as a single very large uh stretched out as a single very large uh sequence of sequence of sequence of integers let me do one more thing before integers let me do one more thing before integers let me do one more thing before we move on here I'd like to separate out we move on here I'd like to separate out we move on here I'd like to separate out our data set into a train and a our data set into a train and a our data set into a train and a validation split so in particular we're validation split",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 160,
      "text": "so in particular we're validation split so in particular we're going to take the first 90% of the data going to take the first 90% of the data going to take the first 90% of the data set and consider that to be the training set and consider that to be the training set and consider that to be the training data for the Transformer and we're going data for the Transformer and we're going data for the Transformer and we're going to withhold the last 10% at the end of to withhold the last 10% at the end of to withhold the last 10% at the end of it to be the validation data and this it to be the validation data and this it to be the validation data and this will help us understand to what extent will help us understand to what extent will help us understand to what extent our model is overfitting so we're going our model is overfitting",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 161,
      "text": "so we're going our model is overfitting so we're going to basically hide and keep the to basically hide and keep the to basically hide and keep the validation data on the side because we validation data on the side because we validation data on the side because we don't want just a perfect memorization don't want just a perfect memorization don't want just a perfect memorization of this exact Shakespeare we want a of this exact Shakespeare we want a of this exact Shakespeare we want a neural network that sort of creates neural network that sort of creates neural network that sort of creates Shakespeare like uh text and so it Shakespeare like uh text",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 162,
      "text": "and so it Shakespeare like uh text",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 163,
      "text": "and so it should be fairly likely for it to should be fairly likely for it to should be fairly likely for it to produce the actual like stowed away uh produce the actual like stowed away uh produce the actual like stowed away uh true Shakespeare text",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 164,
      "text": "um and so we're true Shakespeare text",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 165,
      "text": "um and so we're true Shakespeare text",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 166,
      "text": "um and so we're going to use this to uh get a sense of going to use this to uh get a sense of going to use this to uh get a sense of the overfitting",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 167,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 168,
      "text": "so now we would the overfitting",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 169,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 170,
      "text": "so now we would the overfitting",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 171,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 172,
      "text": "so now we would like to start plugging these text like to start plugging these text like to start plugging these text sequences or integer sequences into the sequences or integer sequences into the sequences or integer sequences into the Transformer so that it can train and Transformer so that it can train and Transformer so that it can train and learn those patterns now the important learn those patterns now the important learn those patterns now the important thing to realize is we're never going to thing to realize is we're never going to thing to realize is we're never going to actually feed entire text into a actually feed entire text into a actually feed entire text into a Transformer all at once that would be Transformer all at once that would be Transformer all at once that would be computationally very expensive and computationally very expensive and computationally very expensive and prohibitive so when we actually train a prohibitive so when we actually train a prohibitive so when we actually train a Transformer on a lot of these data sets Transformer on a lot of these data sets Transformer on a lot of these data sets we only work with chunks of the data set we only work with chunks of the data set we only work with chunks of the data set and when we train the Transformer we and when we train the Transformer we and when we train the Transformer we basically sample random little chunks basically sample random little chunks basically sample random little chunks out of the training set and train on out of the training set and train on out of the training set and train on just chunks at a time and these chunks just chunks at a time and these chunks just chunks at a time and these chunks have basically some kind of a length and have basically some kind of a length and have basically some kind of a length and some maximum length now the maximum some maximum length now the maximum some maximum length now the maximum length typically at least in the code I length typically at least in the code I length typically at least in the code I usually write is called block size you usually write is called block size you usually write is called block size you can you can uh find it under different can you can uh find it under different can you can uh find it under different names like context length or something names like context length or something names like context length or something like that let's start with the block like that let's start with the block like that let's start with the block size of just eight and let me look at size of just eight and let me look at size of just eight and let me look at the first train data characters the the first train data characters the the first train data characters the first block size plus one characters first block size plus one characters first block size plus one characters I'll explain why plus one in a",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 173,
      "text": "I'll explain why plus one in a",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 174,
      "text": "I'll explain why plus one in a second",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 175,
      "text": "so this is the first nine second",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 176,
      "text": "so this is the first nine second",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 177,
      "text": "so this is the first nine characters in the sequence in the characters in the sequence in the characters in the sequence in the training set now what I'd like to point training set now what I'd like to point training set now what I'd like to point out is that when you sample a chunk of out is that when you sample a chunk of out is that when you sample a chunk of data like this so say the these nine data like this so say the these nine data like this so say the these nine characters out of the training set this characters out of the training set this characters out of the training set this actually has multiple examples packed actually has multiple examples packed actually has multiple examples packed into it and uh that's because all of into it and uh that's because all of into it and uh that's because all of these characters follow each other and these characters follow each other and these characters follow each other and so what this thing is going to say when so what this thing is going to say when so what this thing is going to say when we plug it into a Transformer is we're we plug it into a Transformer is we're we plug it into a Transformer is we're going to actually simultaneously train going to actually simultaneously train going to actually simultaneously train it to make prediction at every one of it to make prediction at every one of it to make prediction at every one of these these these positions now in the in a chunk of nine positions now in the in a chunk of nine positions now in the in a chunk of nine characters there's actually eight indiv characters there's actually eight indiv characters there's actually eight indiv ual examples packed in there so there's ual examples packed in there so there's ual examples packed in there so there's the example that when 18 when in the the example that when 18 when in the the example that when 18 when in the context of 18 47 likely comes next in a context of 18 47 likely comes next in a context of 18 47 likely comes next in a context of 18 and 47 56 comes next in a context of 18 and 47 56 comes next in a context of 18 and 47 56 comes next in a context of 18 47 56 57 can come next and context of 18 47 56 57 can come next and context of 18 47 56 57 can come next and so on so that's the eight individual so on so that's the eight individual so on so that's the eight individual examples let me actually spell it out examples let me actually spell it out examples let me actually spell it out with with with code so here's a chunk of code to code so here's a chunk of code to code so here's a chunk of code to illustrate X are the inputs to the illustrate X are the inputs to the illustrate X are the inputs to the Transformer it will just be the first Transformer it will just be the first Transformer it will just be the first block size characters y will be the uh block size characters y will be the uh block size characters y will be the uh next block size characters",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 178,
      "text": "so it's next block size characters",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 179,
      "text": "so it's next block size characters",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 180,
      "text": "so it's offset by one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 181,
      "text": "and that's because y are offset by one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 182,
      "text": "and that's because y are offset by one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 183,
      "text": "and that's because y are the targets for each position in the the targets for each position in the the targets for each position in the input",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 184,
      "text": "and then here I'm iterating over input",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 185,
      "text": "and then here I'm iterating over input",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 186,
      "text": "and then here I'm iterating over all the block size of eight and the all the block size of eight and the all the block size of eight and the context is always all the characters in context is always all the characters in context is always all the characters in x uh up to T and including T and the x uh up to T and including T and the x uh up to T and including T and the target is always the teth character but target is always the teth character but target is always the teth character but in the targets array",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 187,
      "text": "y so let me just in the targets array",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 188,
      "text": "y so let me just in the targets array",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 189,
      "text": "y so let me just run run run this and basically it spells out what I this and basically it spells out what I this and basically it spells out what I said in words uh these are the eight said in words uh these are the eight said in words uh these are the eight examples hidden in a chunk of nine examples hidden in a chunk of nine examples hidden in a chunk of nine characters that we uh sampled from the characters that we uh sampled from the characters that we uh sampled from the training set I want to mention one more training set I want to mention one more training set I want to mention one more thing we train on all the eight examples thing we train on all the eight examples thing we train on all the eight examples here with context between one all the here with context between one all the here with context between one all the way up to context of block size and we way up to context of block size and we way up to context of block size and we train on that not just for computational train on that not just for computational train on that not just for computational reasons because we happen to have the reasons because we happen to have the reasons because we happen to have the sequence already or something like that sequence already or something like that sequence already or something like that it's not just done for efficiency",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 190,
      "text": "it's it's not just done for efficiency",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 191,
      "text": "it's it's not just done for efficiency it's also done um to make the Transformer also done um to make the Transformer also done um to make the Transformer Network be used to seeing contexts all Network be used to seeing contexts all Network be used to seeing contexts all the way from as little as one all the the way from as little as one all the the way from as little as one all the way to block size and we'd like the way to block size",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 192,
      "text": "and we'd like the way to block size and we'd like the transform to be used to seeing transform to be used to seeing transform to be used to seeing everything in between and that's going everything in between and that's going everything in between and that's going to be useful later during inference to be useful later during inference to be useful later during inference because while we're sampling we can because while we're sampling we can because while we're sampling we can start the sampling generation with as start the sampling generation with as start the sampling generation with as little as one character of context and little as one character of context and little as one character of context and the Transformer knows how to predict the the Transformer knows how to predict the the Transformer knows how to predict the next character with all the way up to next character with all the way up to next character with all the way up to just context of one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 193,
      "text": "and so then it can just context of one and",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 194,
      "text": "so then it can just context of one and",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 195,
      "text": "so then it can predict everything up to block size and predict everything up to block size and predict everything up to block size and after block size we have to start after block size we have to start after block size we have to start truncating because the Transformer will truncating because the Transformer will truncating because the Transformer will will never um receive more than block will never um receive more than block will never um receive more than block size inputs when it's predicting the size inputs when it's predicting the size inputs when it's predicting the next next next character",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 196,
      "text": "Okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 197,
      "text": "so we've looked at the character",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 198,
      "text": "Okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 199,
      "text": "so we've looked at the character",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 200,
      "text": "Okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 201,
      "text": "so we've looked at the time dimension of the tensors that are time dimension of the tensors that are time dimension of the tensors that are going to be feeding into the Transformer going to be feeding into the Transformer going to be feeding into the Transformer there's one more Dimension to care about there's one more Dimension to care about there's one more Dimension to care about and that is the batch Dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 202,
      "text": "and so and that is the batch Dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 203,
      "text": "and so and that is the batch Dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 204,
      "text": "and so as we're sampling these chunks of text as we're sampling these chunks of text as we're sampling these chunks of text we're going to be actually every time we're going to be actually every time we're going to be actually every time we're going to feed them into a",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 205,
      "text": "we're going to feed them into a we're going to feed them into a Transformer we're going to have many Transformer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 206,
      "text": "we're going to have many Transformer we're going to have many batches of multiple chunks of text that batches of multiple chunks of text that batches of multiple chunks of text that are all like stacked up in a single are all like stacked up in a single are all like stacked up in a single tensor and that's just done for tensor and that's just done for tensor and that's just done for efficiency just so that we can keep the efficiency just so that we can keep the efficiency just so that we can keep the gpus busy uh because they are very good gpus busy uh because they are very good gpus busy uh because they are very good at parallel processing of um of data and at parallel processing of um of data and at parallel processing of um of data",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 207,
      "text": "and so we just want to process multiple",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 208,
      "text": "so we just want to process multiple",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 209,
      "text": "so we just want to process multiple chunks all at the same time",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 210,
      "text": "but those chunks all at the same time but those chunks all at the same time but those chunks are processed completely chunks are processed completely chunks are processed completely independently they don't talk to each independently they don't talk to each independently they don't talk to each other and so on so let me basically just other and so on so let me basically just other and so on so let me basically just generalize this and introduce a batch generalize this and introduce a batch generalize this and introduce a batch Dimension here's a chunk of Dimension here's a chunk of Dimension here's a chunk of code let me just run it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 211,
      "text": "and then I'm code let me just run it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 212,
      "text": "and then I'm code let me just run it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 213,
      "text": "and then I'm going to explain what it going to explain what it going to explain what it does so here because we're going to does so here because we're going to does so here because we're going to start sampling random locations in the start sampling random locations in the start sampling random locations in the data set to pull chunks from I am data set to pull chunks from I am data set to pull chunks from I am setting the seed so that um in the setting the seed so that um in the setting the seed so that um in the random number generator so that the random number generator so that the random number generator so that the numbers I see here are going to be the numbers I see here are going to be the numbers I see here are going to be the same numbers you see later if you try to same numbers you see later if you try to same numbers you see later if you try to reproduce this now the batch size here reproduce this now the batch size here reproduce this now the batch size here is how many independent sequences we are is how many independent sequences we are is how many independent sequences we are processing every forward backward pass processing every forward backward pass processing every forward backward pass of the of the of the Transformer the block size as I Transformer the block size as I Transformer the block size as I explained is the maximum context length explained is the maximum context length explained is the maximum context length to make those predictions so let's say B to make those predictions so let's say B to make those predictions so let's say B size four block size eight and then size four block size eight and then size four block size eight and then here's how we get batch for any here's how we get batch for any here's how we get batch for any arbitrary split if the split is a arbitrary split if the split is a arbitrary split if the split is a training split then we're going to look training split then we're going to look training split then we're going to look at train data otherwise at valid data at train data otherwise at valid data at train data otherwise at valid data that gives us the data array",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 214,
      "text": "and then that gives us the data array",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 215,
      "text": "and then that gives us the data array",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 216,
      "text": "and then when I Generate random positions to grab when I Generate random positions to grab when I Generate random positions to grab a chunk out of I actually grab I a chunk out of I actually grab I a chunk out of I actually grab I actually generate batch size number of actually generate batch size number of actually generate batch size number of Random offsets",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 217,
      "text": "so because this is four Random offsets so because this is four Random offsets so because this is four we are ex is going to be a uh four we are ex is going to be a uh four we are ex is going to be a uh four numbers that are randomly generated numbers that are randomly generated numbers that are randomly generated between zero and Len of data minus block between zero and Len of data minus block between zero and Len of data minus block size so it's just random offsets into size",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 218,
      "text": "so it's just random offsets into size",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 219,
      "text": "so it's just random offsets into the training the training the training set",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 220,
      "text": "and then X's as I explained are the set and then X's as I explained are the set",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 221,
      "text": "and then X's as I explained are the first first block size characters first first block size characters first first block size characters starting at I the Y's are the offset by starting at I the Y's are the offset by starting at I the Y's are the offset by one of that so just add plus one and one of that so just add plus one and one of that so just add plus one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 222,
      "text": "and then we're going to get those chunks for then we're going to get those chunks for then we're going to get those chunks for every one of integers I INX and use a every one of integers I INX and use a every one of integers I INX and use a torch.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 223,
      "text": "stack to take all those uh uh torch.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 224,
      "text": "stack to take all those uh uh torch.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 225,
      "text": "stack to take all those uh uh one-dimensional tensors as we saw here one-dimensional tensors as we saw here one-dimensional tensors as we saw here and we're going to um stack them up at and we're going to um stack them up at and we're going to um stack them up at rows and so they all become a row in a rows and so they all become a row in a rows and so they all become a row in a 4x8 tensor 4x8 tensor 4x8 tensor so here's where I'm printing then when I so here's where I'm printing then when I so here's where I'm printing then when I sample a batch XB and YB the inputs to sample a batch XB and YB the inputs to sample a batch XB and YB the inputs to the Transformer now are the input X is the Transformer now are the input X is the Transformer now are the input X is the 4x8 tensor four uh rows of eight the 4x8 tensor four uh rows of eight the 4x8 tensor four uh rows of eight columns and each one of these is a chunk columns and each one of these is a chunk columns and each one of these is a chunk of the training of the training of the training set and then the targets here are in the set and then the targets here are in the set and then the targets here are in the associated array Y and they will come in associated array Y",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 226,
      "text": "and they will come in associated array Y and they will come in to the Transformer all the way at the to the Transformer all the way at the to the Transformer all the way at the end uh to um create the loss function end uh to um create the loss function end uh to um create the loss function",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 227,
      "text": "uh so they will give us the correct uh so they will give us the correct uh so they will give us the correct answer for every single position inside answer for every single position inside answer for every single position inside X and then these are the four X and then these are the four X and then these are the four independent independent independent rows so spelled out as we did rows so spelled out as we did rows so spelled out as we did before uh this 4x8 array contains a before uh this 4x8 array contains a before uh this 4x8 array contains a total of 32 examples and they're total of 32 examples and they're total of 32 examples and they're completely independent as far as the completely independent as far as the completely independent as far as the Transformer is Transformer is Transformer is concerned",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 228,
      "text": "uh so when the input is 24 the concerned uh so when the input is 24 the concerned uh so when the input is 24 the target is 43 or rather 43 here in the Y target is 43 or rather 43 here in the Y target is 43 or rather 43 here in the Y array array array when the input is 2443 the target is when the input is 2443 the target is when the input is 2443 the target is 58 uh when the input is 24 43 58 the 58 uh when the input is 24 43 58 the 58 uh when the input is 24 43 58 the target is 5 Etc or like when it is a 52 target is 5 Etc or like when it is a 52 target is 5 Etc or like when it is a 52 581 the target is 581 the target is 581 the target is 58 right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 229,
      "text": "so you can sort of see this 58",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 230,
      "text": "right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 231,
      "text": "so you can sort of see this 58",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 232,
      "text": "right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 233,
      "text": "so you can sort of see this spelled out these are the 32 independent spelled out these are the 32 independent spelled out these are the 32 independent examples packed in to a single batch of examples packed in to a single batch of examples packed in to a single batch of the input X and then the desired targets the input X and then the desired targets the input X and then the desired targets are in y and so now this integer tensor are in y and so now this integer tensor are in y and so now this integer tensor of um X is going to feed into the of um X is going to feed into the of um X is going to feed into the Transformer and that Transformer is Transformer and that Transformer is Transformer and that Transformer is going to simultaneously process all going to simultaneously process all going to simultaneously process all these examples and then look up the these examples and then look up the these examples and then look up the correct um integers to predict in every correct um integers to predict in every correct um integers to predict in every one of these positions in the tensor y one of these positions in the tensor y one of these positions in the tensor y",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 234,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 235,
      "text": "so now that we have our batch of",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 236,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 237,
      "text": "so now that we have our batch of",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 238,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 239,
      "text": "so now that we have our batch of input that we'd like to feed into a input that we'd like to feed into a input that we'd like to feed into a Transformer let's start basically Transformer let's start basically Transformer let's start basically feeding this into neural networks now feeding this into neural networks now feeding this into neural networks now we're going to start off with the we're going to start off with the we're going to start off with the simplest possible neural network which simplest possible neural network which simplest possible neural network which in the case of language modeling in my in the case of language modeling in my in the case of language modeling in my opinion is the Byram language model and opinion is the Byram language model and opinion is the Byram language model and we've covered the Byram language model we've covered the Byram language model we've covered the Byram language model in my make more series in a lot of depth in my make more series in a lot of depth in my make more series in a lot of depth and so here I'm going to sort of go",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 240,
      "text": "and so here I'm going to sort of go",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 241,
      "text": "and so here I'm going to sort of go faster and let's just Implement pytorch faster and let's just Implement pytorch faster and let's just Implement pytorch module directly that implements the byr module directly that implements the byr module directly that implements the byr language language language model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 242,
      "text": "so I'm importing the pytorch um NN model so I'm importing the pytorch um NN model so I'm importing the pytorch um NN module uh for module uh for module uh for reproducibility and then here I'm reproducibility",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 243,
      "text": "and then here I'm reproducibility",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 244,
      "text": "and then here I'm constructing a Byram language model constructing a Byram language model constructing a Byram language model which is a subass of NN which is a subass of NN which is a subass of NN module and then I'm calling it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 245,
      "text": "and I'm module",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 246,
      "text": "and then I'm calling it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 247,
      "text": "and I'm module",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 248,
      "text": "and then I'm calling it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 249,
      "text": "and I'm passing it the inputs and the targets passing it the inputs and the targets passing it the inputs and the targets",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 250,
      "text": "and I'm just printing now when the and I'm just printing now when the and I'm just printing now when the inputs on targets come here you see that inputs on targets come here you see that inputs on targets come here you see that I'm just taking the index uh the inputs I'm just taking the index uh the inputs I'm just taking the index uh the inputs X here which I rename to idx",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 251,
      "text": "and I'm X here which I rename to idx",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 252,
      "text": "and I'm X here which I rename to idx",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 253,
      "text": "and I'm just passing them into this token just passing them into this token just passing them into this token embedding table so it's going on here is embedding table so it's going on here is embedding table so it's going on here is that here in the Constructor we are that here in the Constructor we are that here in the Constructor we are creating a token embedding table and it creating a token embedding table and it creating a token embedding table and it is of size vocap size by vocap is of size vocap size by vocap is of size vocap size by vocap size and we're using an. embedding which size and we're using an. embedding which size and we're using an. embedding which is a very thin wrapper around basically is a very thin wrapper around basically is a very thin wrapper around basically a tensor of shape voap size by vocab a tensor of shape voap size by vocab a tensor of shape voap size by vocab size and what's happening here is that size and what's happening here is that size and what's happening here is that when we pass idx here every single when we pass idx here every single when we pass idx here every single integer in our input is going to refer integer in our input is going to refer integer in our input is going to refer to this embedding table and it's going to this embedding table and it's going to this embedding table and it's going to pluck out a row of that embedding to pluck out a row of that embedding to pluck out a row of that embedding table corresponding to its index so 24 table corresponding to its index so 24 table corresponding to its index so 24 here will go into the embedding table here will go into the embedding table here will go into the embedding table and we'll pluck out the 24th row",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 254,
      "text": "and and we'll pluck out the 24th row",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 255,
      "text": "and and we'll pluck out the 24th row",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 256,
      "text": "and then 43 will go here and pluck out the then 43 will go here and pluck out the then 43 will go here and pluck out the 43d row",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 257,
      "text": "Etc",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 258,
      "text": "and then pytorch is going to 43d row Etc",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 259,
      "text": "and then pytorch is going to 43d row Etc",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 260,
      "text": "and then pytorch is going to arrange all of this into a batch by Time arrange all of this into a batch by Time arrange all of this into a batch by Time by channel uh tensor in this case batch by channel uh tensor in this case batch by channel uh tensor in this case batch is four time is eight and C which is the is four time is eight and C which is the is four time is eight and C which is the channels is vocab size or 65 and so channels is vocab size or 65 and so channels is vocab size or 65 and so we're just going to pluck out all those we're just going to pluck out all those we're just going to pluck out all those rows arrange them in a b by T by C and rows arrange them in a b by T by C and rows arrange them in a b by T by C and now we're going to interpret this as the now we're going to interpret this as the now we're going to interpret this as the logits which are basically the scores logits which are basically the scores logits which are basically the scores for the next character in the sequence for the next character in the sequence for the next character in the sequence",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 261,
      "text": "and so what's happening here is we are",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 262,
      "text": "and so what's happening here is we are",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 263,
      "text": "and so what's happening here is we are predicting what comes next based on just predicting what comes next based on just predicting what comes next based on just the individual identity of a single the individual identity of a single the individual identity of a single token and you can do that because um I token and you can do that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 264,
      "text": "because um I token and you can do that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 265,
      "text": "because um I mean currently the tokens are not mean currently the tokens are not mean currently the tokens are not talking to each other and they're not talking to each other and they're not talking to each other and they're not seeing any context except for they're seeing any context except for they're seeing any context except for they're just seeing themselves",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 266,
      "text": "so I'm a f I'm a just seeing themselves so I'm a f",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 267,
      "text": "I'm a just seeing themselves so I'm a f",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 268,
      "text": "I'm a token number five",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 269,
      "text": "and then I can token number five",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 270,
      "text": "and then I can token number five",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 271,
      "text": "and then I can actually make pretty decent predictions actually make pretty decent predictions actually make pretty decent predictions about what comes next just by knowing about what comes next just by knowing about what comes next just by knowing that I'm token five because some that I'm token five because some that I'm token five because some characters uh know um C follow other characters uh know um C follow other characters",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 272,
      "text": "uh know um C follow other characters in in typical scenarios so we characters in in typical scenarios so we characters in in typical scenarios so we saw a lot of this in a lot more depth in saw a lot of this in a lot more depth in saw a lot of this in a lot more depth in the make more series and here if I just the make more series and here if I just the make more series and here if I just run this then we currently get the run this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 273,
      "text": "then we currently get the run this then we currently get the predictions the scores the lits for predictions the scores the lits for predictions the scores the lits for every one of the 4x8 positions now that every one of the 4x8 positions now that every one of the 4x8 positions now that we've made predictions about what comes we've made predictions about what comes we've made predictions about what comes next we'd like to evaluate the loss next we'd like to evaluate the loss next we'd like to evaluate the loss function and so in make more series we function and so in make more series we function and so in make more series we saw that a good way to measure a loss or saw that a good way to measure a loss or saw that a good way to measure a loss or like a quality of the predictions is to like a quality of the predictions is to like a quality of the predictions is to use the negative log likelihood loss use the negative log likelihood loss use the negative log likelihood loss which is also implemented in pytorch which is also implemented in pytorch which is also implemented in pytorch under the name cross entropy",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 274,
      "text": "so what we' under the name cross entropy so what we' under the name cross entropy so what we' like to do here is loss is the cross like to do here is loss is the cross like to do here is loss is the cross entropy on the predictions and the entropy on the predictions and the entropy on the predictions and the targets and so this measures the quality targets and so this measures the quality targets and so this measures the quality of the logits with respect to the of the logits with respect to the of the logits with respect to the Targets in other words we have the Targets in other words we have the Targets in other words we have the identity of the next character so how identity of the next character so how identity of the next character so how well are we predicting the next well are we predicting the next well are we predicting the next character based on the lits and character based on the lits and character based on the lits and intuitively the correct um the correct intuitively the correct um the correct intuitively the correct um the correct dimension of low jits uh depending on dimension of low jits uh depending on dimension of low jits uh depending on whatever the target is should have a whatever the target is should have a whatever the target is should have a very high number and all the other very high number and all the other very high number and all the other dimensions should be very low number dimensions should be very low number dimensions should be very low number right now the issue is that this won't right now the issue is that this won't right now the issue is that this won't actually this is what we want we want to actually this is what we want we want to actually this is what we want we want to basically output the logits and the basically output the logits and the basically output the logits and the loss this is what we want but loss this is what we want but loss this is what we want but unfortunately uh this won't actually run unfortunately uh this won't actually run unfortunately uh this won't actually run we get an error message but intuitively we get an error message but intuitively we get an error message but intuitively we want to uh measure this now when we we want to uh measure this now when we we want to uh measure this now when we go to the pytorch um cross entropy go to the pytorch um cross entropy go to the pytorch um cross entropy documentation here um we're trying to documentation here um we're trying to documentation here um we're trying to call the cross entropy in its functional call the cross entropy in its functional call the cross entropy in its functional form uh so that means we don't have to form uh so that means we don't have to form uh so that means we don't have to create like a module for it but here create like a module for it but here create like a module for it but here when we go to the documentation you have when we go to the documentation you have when we go to the documentation you have to look into the details of how pitor to look into the details of how pitor to look into the details of how pitor expects these inputs and basically the expects these inputs and basically the expects these inputs and basically the issue here is ptor expects if you have issue here is ptor expects if you have issue here is ptor expects if you have multi-dimensional input which we do multi-dimensional input which we do multi-dimensional input which we do because we have a b BYT by C tensor then because we have a b BYT by C tensor then because we have a b BYT by C tensor",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 275,
      "text": "then it actually really wants the channels to it actually really wants the channels to it actually really wants the channels to be the second uh Dimension here so if be the second uh Dimension here so if be the second uh Dimension here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 276,
      "text": "so if you um so basically it wants a b by C you um so basically it wants a b by C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 277,
      "text": "you um so basically it wants a b by C BYT instead of a b by T by C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 278,
      "text": "and so it's BYT instead of a b by T by C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 279,
      "text": "and so it's BYT instead of a b by T by C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 280,
      "text": "and so it's just the details of how P torch treats just the details of how P torch treats just the details of how P torch treats um these kinds of inputs",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 281,
      "text": "and so we don't um these kinds of inputs and so we don't um these kinds of inputs and so we don't actually want to deal with that so what actually want to deal with that so what actually want to deal with that so what we're going to do instead is we need to we're going to do instead is we need to we're going to do instead is we need to basically reshape our logits so here's basically reshape our logits so here's basically reshape our logits so here's what I like to do I like to take what I like to do I like to take what I like to do I like to take basically give names to the dimensions basically give names to the dimensions basically give names to the dimensions so lit.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 282,
      "text": "shape is B BYT by C and unpack so lit.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 283,
      "text": "shape is B BYT by C and unpack so lit.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 284,
      "text": "shape is B BYT by C and unpack those numbers and then let's uh say that those numbers and then let's uh say that those numbers and then let's uh say that logits equals lit.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 285,
      "text": "View and we want it logits equals lit.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 286,
      "text": "View and we want it logits equals lit.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 287,
      "text": "View and we want it to be a b * c b * T by C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 288,
      "text": "so just a two- to be a b * c b * T by C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 289,
      "text": "so just a two- to be a b * c b * T by C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 290,
      "text": "so just a two- dimensional dimensional dimensional array",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 291,
      "text": "right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 292,
      "text": "so we're going to take all array",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 293,
      "text": "right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 294,
      "text": "so we're going to take all array",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 295,
      "text": "right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 296,
      "text": "so we're going to take all the we're going to take all of these um the we're going to take all of these um the we're going to take all of these um positions here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 297,
      "text": "and we're going to uh positions here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 298,
      "text": "and we're going to uh positions here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 299,
      "text": "and we're going to uh stretch them out in a onedimensional stretch them out in a onedimensional stretch them out in a onedimensional sequence and uh preserve the channel sequence and uh preserve the channel sequence and uh preserve the channel Dimension as the second Dimension as the second Dimension as the second dimension so we're just kind of like dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 300,
      "text": "so we're just kind of like dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 301,
      "text": "so we're just kind of like stretching out the array so it's two- stretching out the array so it's two- stretching out the array",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 302,
      "text": "so it's two- dimensional and in that case it's going dimensional and in that case it's going dimensional and in that case it's going to better conform to what pytorch uh to better conform to what pytorch uh to better conform to what pytorch uh sort of expects in its Dimensions now we sort of expects in its Dimensions now we sort of expects in its Dimensions now we have to do the same to targets because have to do the same to targets because have to do the same to targets because currently targets are um of shape B by T currently targets are um of shape B by T currently targets are um of shape B by T",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 303,
      "text": "and we want it to be just B * T",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 304,
      "text": "so and we want it to be just B * T",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 305,
      "text": "so and we want it to be just B * T",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 306,
      "text": "so onedimensional now alternatively you onedimensional now alternatively you onedimensional now alternatively you could always still just do minus one could always still just do minus one could always still just do minus one because pytor will guess what this because pytor will guess what this because pytor will guess what this should be if you want to lay it out uh should be if you want to lay it out uh should be if you want to lay it out uh but let me just be explicit and say p *",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 307,
      "text": "but let me just be explicit and say p *",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 308,
      "text": "but let me just be explicit and say p * t once we've reshaped this it will match t once we've reshaped this it will match t once we've reshaped this it will match the cross entropy case",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 309,
      "text": "and then we the cross entropy case and then we the cross entropy case and then we should be able to evaluate our should be able to evaluate our should be able to evaluate our loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 310,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 311,
      "text": "so that R now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 312,
      "text": "and we can do loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 313,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 314,
      "text": "so that R now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 315,
      "text": "and we can do loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 316,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 317,
      "text": "so that R now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 318,
      "text": "and we can do loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 319,
      "text": "and So currently we see that the loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 320,
      "text": "and So currently we see that the loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 321,
      "text": "and So currently we see that the loss is loss is loss is 4.87 now because our uh we have 65 4.87 now because our uh we have 65 4.87 now because our uh we have 65 possible vocabulary elements we can possible vocabulary elements we can possible vocabulary elements we can actually guess at what the loss should actually guess at what the loss should actually guess at what the loss should be and in be and in be and in particular we covered negative log particular we covered negative log particular we covered negative log likelihood in a lot of detail we are likelihood in a lot of detail we are likelihood in a lot of detail we are expecting log or lawn of um 1 over 65 expecting log or lawn of um 1 over 65 expecting log or lawn of um 1 over 65 and negative of that so we're expecting and negative of that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 322,
      "text": "so we're expecting and negative of that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 323,
      "text": "so we're expecting the loss to be about 4.1 17 but we're the loss to be about 4.1 17",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 324,
      "text": "but we're the loss to be about 4.1 17 but we're getting 4.87",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 325,
      "text": "and so that's telling us getting 4.87",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 326,
      "text": "and so that's telling us getting 4.87",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 327,
      "text": "and so that's telling us that the initial predictions are not uh that the initial predictions are not uh that the initial predictions are not uh super diffuse they've got a little bit super diffuse they've got a little bit super diffuse they've got a little bit of entropy",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 328,
      "text": "and so we're guessing wrong of entropy",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 329,
      "text": "and so we're guessing wrong of entropy",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 330,
      "text": "and so we're guessing wrong uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 331,
      "text": "so",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 332,
      "text": "uh yes",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 333,
      "text": "but actually we're I a",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 334,
      "text": "we",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 335,
      "text": "uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 336,
      "text": "so",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 337,
      "text": "uh yes",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 338,
      "text": "but actually we're I a",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 339,
      "text": "we",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 340,
      "text": "uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 341,
      "text": "so",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 342,
      "text": "uh yes",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 343,
      "text": "but actually we're I a we are able to evaluate the loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 344,
      "text": "okay so are able to evaluate the loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 345,
      "text": "okay so are able to evaluate the loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 346,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 347,
      "text": "so now that we can evaluate the quality of now that we can evaluate the quality of now that we can evaluate the quality of the model on some data we'd like to also the model on some data we'd like to also the model on some data we'd like to also be able to generate from the model so be able to generate from the model so be able to generate from the model so let's do the generation now I'm going to let's do the generation now I'm going to let's do the generation now I'm going to go again a little bit faster here go again a little bit faster here go again a little bit faster here because I covered all this already in because I covered all this already in because I covered all this already in previous previous previous videos videos videos so here's a generate function for the so here's a generate function for the so here's a generate function for the model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 348,
      "text": "so we take some uh we take the the model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 349,
      "text": "so we take some uh we take the the model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 350,
      "text": "so we take some uh we take the the same kind of input idx here and same kind of input idx here and same kind of input idx here and basically this is the current uh context basically this is the current uh context basically this is the current uh context of some characters in a batch in some of some characters in a batch in some of some characters in a batch in some batch so it's also B BYT and the job of batch",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 351,
      "text": "so it's also B BYT and the job of batch",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 352,
      "text": "so it's also B BYT and the job of generate is to basically take this B BYT generate is to basically take this B BYT generate is to basically take this B BYT and extend it to be B BYT",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 353,
      "text": "+ 1 plus 2",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 354,
      "text": "and extend it to be B BYT",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 355,
      "text": "+ 1 plus 2",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 356,
      "text": "and extend it to be B BYT",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 357,
      "text": "+ 1 plus 2 plus 3",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 358,
      "text": "and so it's just basically it plus 3",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 359,
      "text": "and so it's just basically it plus 3",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 360,
      "text": "and so it's just basically it continues the generation in all the continues the generation in all the continues the generation in all the batch dimensions in the time Dimension batch dimensions in the time Dimension batch dimensions in the time",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 361,
      "text": "Dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 362,
      "text": "So that's its job and it will do that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 363,
      "text": "So that's its job and it will do that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 364,
      "text": "So that's its job and it will do that for Max new tokens so you can see here for Max new tokens so you can see here for Max new tokens so you can see here on the bottom there's going to be some on the bottom there's going to be some on the bottom there's going to be some stuff here but on the bottom whatever is stuff here but on the bottom whatever is stuff here but on the bottom whatever is predicted is concatenated on top of the predicted is concatenated on top of the predicted is concatenated on top of the previous idx along the First Dimension previous idx along the First Dimension previous idx along the First Dimension which is the time Dimension to create a which is the time Dimension to create a which is the time Dimension to create a b BYT + one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 365,
      "text": "so that becomes a new idx so b BYT + one so that becomes a new idx so b BYT + one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 366,
      "text": "so that becomes a new idx so the job of generate is to take a b BYT the job of generate is to take a b BYT the job of generate is to take a b BYT and make it a b BYT plus 1 plus 2 plus and make it a b BYT plus 1 plus 2 plus and make it a b BYT plus 1 plus 2 plus three as many as we want Max new tokens three as many as we want Max new tokens three as many as we want Max new tokens so this is the generation from the model so this is the generation from the model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 367,
      "text": "so this is the generation from the model now inside the generation what what are now inside the generation what what are now inside the generation what what are we doing we're taking the current we doing we're taking the current we doing we're taking the current indices we're getting the predictions so indices we're getting the predictions so indices we're getting the predictions so we get uh those are in the low jits and we get uh those are in the low jits and we get uh those are in the low jits and then the loss here is going to be then the loss here is going to be then the loss here is going to be ignored because um we're not we're not ignored because um we're not we're not ignored because um we're not we're not using that and we have no targets that using that and we have no targets that using that and we have no targets that are sort of ground truth targets that are sort of ground truth targets that are sort of ground truth targets that we're going to be comparing with we're going to be comparing with we're going to be comparing with then once we get the logits we are only then once we get the logits we are only then once we get the logits we are only focusing on the last step so instead of focusing on the last step so instead of focusing on the last step so instead of a b by T by C we're going to pluck out a b by T by C we're going to pluck out a b by T by C we're going to pluck out the negative-1 the last element in the the negative-1 the last element in the the negative-1 the last element in the time Dimension because those are the time Dimension because those are the time Dimension because those are the predictions for what comes next so that predictions for what comes next so that predictions for what comes next so that gives us the logits which we then gives us the logits which we then gives us the logits which we then convert to probabilities via softmax and convert to probabilities via softmax and convert to probabilities via softmax and then we use tor. multinomial to sample then we use tor. multinomial to sample then we use tor. multinomial to sample from those probabilities and we ask from those probabilities and we ask from those probabilities and we ask pytorch to give us one sample and so idx pytorch to give us one sample and so idx pytorch to give us one sample and so idx next will become a b by one because in next will become a b by one because in next will become a b by one because in each uh one of the batch Dimensions each uh one of the batch Dimensions each uh one of the batch Dimensions we're going to have a single prediction we're going to have a single prediction we're going to have a single prediction for what comes next so this num samples for what comes next so this num samples for what comes next so this num samples equals one will make this be a equals one will make this be a equals one will make this be a one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 368,
      "text": "and then we're going to take those one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 369,
      "text": "and then we're going to take those one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 370,
      "text": "and then we're going to take those integers that come from the sampling integers that come from the sampling integers that come from the sampling process according to the probability process according to the probability process according to the probability distribution given here and those distribution given here and those distribution given here and those integers got just concatenated on top of integers got just concatenated on top of integers got just concatenated on top of the current sort of like running stream the current sort of like running stream the current sort of like running stream of integers and this gives us a b BYT + of integers and this gives us a b BYT + of integers and this gives us a b BYT + one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 371,
      "text": "and then we can return that now one one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 372,
      "text": "and then we can return that now one one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 373,
      "text": "and then we can return that now one thing here is you see how I'm calling thing here is you see how I'm calling thing here is you see how I'm calling self of idx which will end up going to self of idx which will end up going to self of idx which will end up going to the forward function I'm not providing the forward function I'm not providing the forward function I'm not providing any Targets",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 374,
      "text": "So currently this would give any Targets",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 375,
      "text": "So currently this would give any Targets",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 376,
      "text": "So currently this would give an error because targets is uh is uh an error because targets is uh is uh an error because targets is uh is uh sort of like not given so targets has to sort of like not given so targets has to sort of like not given so targets has to be optional so targets is none by be optional so targets is none by be optional so targets is none by default and then if targets is none then default",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 377,
      "text": "and then if targets is none then default",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 378,
      "text": "and then if targets is none then there's no loss to create so it's just there's no loss to create so it's just there's no loss to create so it's just loss is none but else all of this loss is none but else all of this loss is none but else all of this happens and we can create a loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 379,
      "text": "so this happens and we can create a loss so this happens and we can create a loss so this will make it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 380,
      "text": "so um if we have the will make it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 381,
      "text": "so um if we have the will make it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 382,
      "text": "so um if we have the targets we provide them and get a loss targets we provide them and get a loss targets we provide them and get a loss if we have no targets it will'll just if we have no targets it will'll just if we have no targets it will'll just get the get the get the loits so this here will generate from loits so this here will generate from loits so this here will generate from the model um and let's take that for a the model um and let's take that for a the model um and let's take that for a ride ride ride now oops",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 383,
      "text": "so I have another code chunk now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 384,
      "text": "oops",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 385,
      "text": "so I have another code chunk now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 386,
      "text": "oops",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 387,
      "text": "so I have another code chunk here which will generate for the model here which will generate for the model here which will generate for the model from the model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 388,
      "text": "and okay this is kind of from the model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 389,
      "text": "and okay this is kind of from the model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 390,
      "text": "and okay this is kind of crazy",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 391,
      "text": "so maybe let me let me break this crazy",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 392,
      "text": "so maybe let me let me break this crazy",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 393,
      "text": "so maybe let me let me break this down so these are the idx right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 394,
      "text": "I'm creating a batch will be just right I'm creating a batch will be just one time will be just one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 395,
      "text": "so I'm one time will be just one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 396,
      "text": "so I'm one time will be just one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 397,
      "text": "so I'm creating a little one by one tensor and creating a little one by one tensor and creating a little one by one tensor and it's holding a zero and the D type the it's holding a zero and the D type the it's holding a zero and the D type the data type is uh integer so zero is going data type is uh integer so zero is going data type is uh integer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 398,
      "text": "so zero is going to be how we kick off the generation and to be how we kick off the generation and to be how we kick off the generation and remember that zero is uh is the element remember that zero is uh is the element remember that zero is uh is the element standing for a new line character so standing for a new line character so standing for a new line character",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 399,
      "text": "so it's kind of like a reasonable thing to it's kind of like a reasonable thing to it's kind of like a reasonable thing to to feed in as the very first character to feed in as the very first character to feed in as the very first character in a sequence to be the new in a sequence to be the new in a sequence to be the new line",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 400,
      "text": "um so it's going to be idx which line um so it's going to be idx which line um so it's going to be idx which we're going to feed in here then we're we're going to feed in here then we're we're going to feed in here then we're going to ask for 100 tokens going to ask for 100 tokens going to ask for 100 tokens and then.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 401,
      "text": "generate will continue that and then.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 402,
      "text": "generate will continue that and then.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 403,
      "text": "generate will continue that now because uh generate works on the now because uh generate works on the now because uh generate works on the level of batches",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 404,
      "text": "we we then have to level of batches",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 405,
      "text": "we we then have to level of batches",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 406,
      "text": "we we then have to index into the zero throw to basically index into the zero throw to basically index into the zero throw to basically unplug the um the single batch Dimension unplug the um the single batch Dimension unplug the um the single batch Dimension that exists and then that gives us a um that exists and then that gives us a um that exists and then that gives us a um time steps just a onedimensional array time steps just a onedimensional array time steps just a onedimensional array of all the indices which we will convert of all the indices which we will convert of all the indices which we will convert to simple python list from pytorch to simple python list from pytorch to simple python list from pytorch tensor so that that can feed into our tensor so that that can feed into our tensor so that that can feed into our decode function and uh convert those decode function and uh convert those decode function and uh convert those integers into text so let me bring this integers into text so let me bring this integers into text so let me bring this back and we're generating 100 tokens back and we're generating 100 tokens back and we're generating 100 tokens let's let's let's run and uh here's the generation that we run and uh here's the generation that we run and uh here's the generation that we achieved so obviously it's garbage and achieved so obviously it's garbage and achieved so obviously it's garbage and the reason it's garbage is because this the reason it's garbage is because this the reason it's garbage is because this is a totally random model so next up is a totally random model so next up is a totally random model so next up we're going to want to train this model we're going to want to train this model we're going to want to train this model now one more thing I wanted to point out now one more thing I wanted to point out now one more thing I wanted to point out here is this function is written to be here is this function is written to be here is this function is written to be General",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 407,
      "text": "but it's kind of like ridiculous General",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 408,
      "text": "but it's kind of like ridiculous General",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 409,
      "text": "but it's kind of like ridiculous right now because right now because right now because we're feeding in all this we're building we're feeding in all this we're building we're feeding in all this we're building out this context",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 410,
      "text": "and we're concatenating out this context",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 411,
      "text": "and we're concatenating out this context",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 412,
      "text": "and we're concatenating it all",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 413,
      "text": "and we're always feeding it all it all",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 414,
      "text": "and we're always feeding it all it all",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 415,
      "text": "and we're always feeding it all into the model but that's kind of into the model but that's kind of into the model but that's kind of ridiculous because this is just a simple ridiculous because this is just a simple ridiculous because this is just a simple Byram model so to make for example this Byram model so to make for example this Byram model so to make for example this prediction about K we only needed this W prediction about K we only needed this W prediction about K we only needed this W",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 416,
      "text": "but actually what we fed into the model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 417,
      "text": "but actually what we fed into the model but actually what we fed into the model is we fed the entire sequence and then is we fed the entire sequence and then is we fed the entire sequence",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 418,
      "text": "and then we only looked at the very last piece we only looked at the very last piece we only looked at the very last piece and predicted K so the only reason I'm and predicted K so the only reason I'm and predicted K so the only reason I'm writing it in this way is because right writing it in this way is because right writing it in this way is because right now this is a byr model but I'd like to now this is a byr model but I'd like to now this is a byr model but I'd like to keep keep this function fixed and I'd keep keep this function fixed and I'd keep keep this function fixed",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 419,
      "text": "and I'd like it to work um later when our like it to work um later when our like it to work um later when our characters actually um basically look characters actually um basically look characters actually um basically look further in the history and so right now further in the history and so right now further in the history and so right now the history is not used so this looks the history is not used so this looks the history is not used so this looks silly uh but eventually the history will silly uh but eventually the history will silly uh but eventually the history will be used and so that's why we want to uh be used and so that's why we want to uh be used and so that's why we want to uh do it this way",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 420,
      "text": "so just a quick comment do it this way so just a quick comment do it this way so just a quick comment on that so now we see that this is um on that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 421,
      "text": "so now we see that this is um on that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 422,
      "text": "so now we see that this is um random",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 423,
      "text": "so let's train the model so it random",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 424,
      "text": "so let's train the model so it random",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 425,
      "text": "so let's train the model so it becomes a bit less random",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 426,
      "text": "okay let's Now becomes a bit less random",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 427,
      "text": "okay let's Now becomes a bit less random",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 428,
      "text": "okay let's Now train the model so first what I'm going train the model so first what I'm going train the model so first what I'm going to do is I'm going to create a pyour to do is I'm going to create a pyour to do is I'm going to create a pyour optimization object",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 429,
      "text": "so here we are using optimization object so here we are using optimization object so here we are using the optimizer ATM W um now in a make the optimizer ATM W um now in a make the optimizer ATM W um now in a make more series we've only ever use tastic more series we've only ever use tastic more series we've only ever use tastic gradi in descent the simplest possible gradi in descent the simplest possible gradi in descent the simplest possible Optimizer which you can get using the Optimizer which you can get using the Optimizer which you can get using the SGD instead",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 430,
      "text": "but I want to use Adam which SGD instead but I want to use Adam which SGD instead",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 431,
      "text": "but I want to use Adam which is a much more advanced and popular is a much more advanced and popular is a much more advanced and popular Optimizer and it works extremely well Optimizer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 432,
      "text": "and it works extremely well Optimizer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 433,
      "text": "and it works extremely well for uh typical good setting for the for uh typical good setting for the for uh typical good setting for the learning rate is roughly 3 E4",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 434,
      "text": "uh but for learning rate is roughly 3 E4 uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 435,
      "text": "but for learning rate is roughly 3 E4 uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 436,
      "text": "but for very very small networks like is the very very small networks like is the very very small networks like is the case here you can get away with much case here you can get away with much case here you can get away with much much higher learning rates R3 or even much higher learning rates R3 or even much higher learning rates R3 or even higher probably but let me create the higher probably but let me create the higher probably but let me create the optimizer object which will basically optimizer object which will basically optimizer object which will basically take the gradients and uh update the take the gradients and uh update the take the gradients and uh update the parameters using the parameters using the parameters using the gradients",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 437,
      "text": "and then here our batch size gradients",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 438,
      "text": "and then here our batch size gradients",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 439,
      "text": "and then here our batch size up above was only four so let me up above was only four so let me up above was only four so let me actually use something bigger let's say actually use something bigger let's say actually use something bigger let's say 32 and then for some number of steps um 32 and then for some number of steps um 32 and then for some number of steps um we are sampling a new batch of data we are sampling a new batch of data we are sampling a new batch of data we're evaluating the loss uh we're we're evaluating the loss uh we're we're evaluating the loss uh we're zeroing out all the gradients from the zeroing out all the gradients from the zeroing out all the gradients from the previous step getting the gradients for previous step getting the gradients for previous step getting the gradients for all the parameters and then using those all the parameters and then using those all the parameters and then using those gradients to up update our parameters so gradients to up update our parameters so gradients to up update our parameters so typical training loop as we saw in the typical training loop as we saw in the typical training loop as we saw in the make more series",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 440,
      "text": "so let me now uh run make more series",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 441,
      "text": "so let me now uh run make more series",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 442,
      "text": "so let me now uh run this for say 100 iterations and let's this for say 100 iterations and let's this for say 100 iterations and let's see what kind of losses we're going to see what kind of losses we're going to see what kind of losses we're going to get so we started around get so we started around get so we started around 4.7 and now we're getting to down to 4.7 and now we're getting to down to 4.7 and now we're getting to down to like 4.6 4.5 Etc so the optimization is like 4.6 4.5 Etc so the optimization is like 4.6 4.5 Etc so the optimization is definitely happening but um let's uh definitely happening but um let's uh definitely happening but um let's uh sort of try to increase number of sort of try to increase number of sort of try to increase number of iterations and only print at the iterations and only print at the iterations and only print at the end because we probably want train for end because we probably want train for end because we probably want train for longer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 443,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 444,
      "text": "so we're down to 3.6 optimization",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 445,
      "text": "okay it's working let's optimization",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 446,
      "text": "okay it's working let's just do just do just do 10,000 and then from here we want to 10,000 and then from here we want to 10,000 and then from here we want to copy this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 447,
      "text": "and hopefully that we're going copy this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 448,
      "text": "and hopefully that we're going copy this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 449,
      "text": "and hopefully that we're going to get something reason and of course to get something reason and of course to get something reason and of course it's not going to be Shakespeare from a it's not going to be Shakespeare from a it's not going to be Shakespeare from a byr model but at least we see",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 450,
      "text": "that the byr model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 451,
      "text": "but at least we see that the byr model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 452,
      "text": "but at least we see that the loss is improving and uh hopefully we're loss is improving and uh hopefully we're loss is improving",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 453,
      "text": "and uh hopefully we're expecting something a bit more expecting something a bit more expecting something a bit more reasonable",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 454,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 455,
      "text": "so we're down at about reasonable",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 456,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 457,
      "text": "so we're down at about reasonable",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 458,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 459,
      "text": "so we're down at about 2.5 is let's see what we get okay 2.5 is let's see what we get okay 2.5 is let's see what we get okay dramatic improvements certainly on what dramatic improvements certainly on what dramatic improvements certainly on what we had here so let me just increase the we had here so let me just increase the we had here so let me just increase the number of tokens",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 460,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 461,
      "text": "so we see that number of tokens",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 462,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 463,
      "text": "so we see that number of tokens",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 464,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 465,
      "text": "so we see that we're starting to get something at least we're starting to get something at least we're starting to get something at least like reasonable is like reasonable is like reasonable is um certainly not shakes spear but uh the um certainly not shakes spear but uh the um certainly not shakes spear but uh the model is making progress so that is the model is making progress so that is the model is making progress so that is the simplest possible simplest possible simplest possible model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 466,
      "text": "so now what I'd like to do model so now what I'd like to do model so now what I'd like to do is obviously this is a very simple model is obviously this is a very simple model is obviously this is a very simple model because the tokens are not talking to because the tokens are not talking to because the tokens are not talking to each other so given the previous context each other",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 467,
      "text": "so given the previous context each other so given the previous context of whatever was generated we're only of whatever was generated we're only of whatever was generated we're only looking at the very last character to looking at the very last character to looking at the very last character to make the predictions about what comes make the predictions about what comes make the predictions about what comes next so now these uh now these tokens next so now these uh now these tokens next so now these uh now these tokens have to start talking to each other and have to start talking to each other and have to start talking to each other and figuring out what is in the context so figuring out what is in the context so figuring out what is in the context so that they can make better predictions that they can make better predictions that they can make better predictions for what comes next and this is how for what comes next and this is how for what comes next and this is how we're going to kick off the uh we're going to kick off the uh we're going to kick off the uh Transformer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 468,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 469,
      "text": "so next I took the code Transformer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 470,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 471,
      "text": "so next I took the code Transformer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 472,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 473,
      "text": "so next I took the code that we developed in this juper notebook that we developed in this juper notebook that we developed in this juper notebook",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 474,
      "text": "and I converted it to be a script",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 475,
      "text": "and",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 476,
      "text": "and I converted it to be a script",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 477,
      "text": "and",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 478,
      "text": "and I converted it to be a script",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 479,
      "text": "and I'm doing this because I just want to I'm doing this because I just want to I'm doing this because I just want to simplify our intermediate work into just simplify our intermediate work into just simplify our intermediate work into just the final product that we have at this the final product that we have at this the final product that we have at this point so in the top here I put all the point so in the top here I put all the point so in the top here I put all the hyp parameters that we to find I hyp parameters that we to find I hyp parameters that we to find I introduced a few",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 480,
      "text": "and I'm going to speak introduced a few",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 481,
      "text": "and I'm going to speak introduced a few",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 482,
      "text": "and I'm going to speak to that in a little bit otherwise a lot to that in a little bit otherwise a lot to that in a little bit otherwise a lot of this should be recognizable uh of this should be recognizable uh of this should be recognizable uh reproducibility read data get the reproducibility read data get the reproducibility read data get the encoder and the decoder create the train encoder and the decoder create the train encoder and the decoder create the train into splits uh use the uh kind of like into splits uh use the uh kind of like into splits uh use the uh kind of like data loader um that gets a batch of the data loader um that gets a batch of the data loader um that gets a batch of the inputs and Targets this is new",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 483,
      "text": "and I'll inputs and Targets this is new",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 484,
      "text": "and I'll inputs and Targets this is new",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 485,
      "text": "and I'll talk about it in a second now this is talk about it in a second now this is talk about it in a second now this is the Byram language model that we the Byram language model that we the Byram language model that we developed and it can forward and give us developed",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 486,
      "text": "and it can forward and give us developed",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 487,
      "text": "and it can forward and give us a logits and loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 488,
      "text": "and it can a logits and loss and it can a logits and loss and it can generate",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 489,
      "text": "and then here we are creating generate",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 490,
      "text": "and then here we are creating generate",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 491,
      "text": "and then here we are creating the optimizer and this is the training the optimizer and this is the training the optimizer and this is the training Loop so everything here should look Loop so everything here should look Loop so everything here should look pretty familiar now some of the small pretty familiar now some of the small pretty familiar now some of the small things that I added number one I added things that I added number one I added things that I added number one I added the ability to run on a GPU if you have the ability to run on a GPU if you have the ability to run on a GPU if you have it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 492,
      "text": "so if you have a GPU then you can it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 493,
      "text": "so if you have a GPU then you can it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 494,
      "text": "so if you have a GPU then you can this will use Cuda instead of just CPU this will use Cuda instead of just CPU this will use Cuda instead of just CPU and everything will be a lot more faster and everything will be a lot more faster and everything will be a lot more faster now when device becomes Cuda",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 495,
      "text": "then we now when device becomes Cuda then we now when device becomes Cuda then we need to make sure that when we load the need to make sure that when we load the need to make sure that when we load the data we move it to data we move it to data we move it to device when we create the model we want device when we create the model we want device when we create the model we want to move uh the model parameters to to move uh the model parameters to to move uh the model parameters to device so as an example here we have the device so as an example here we have the device so as an example here we have the N an embedding table",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 496,
      "text": "and it's got a N an embedding table",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 497,
      "text": "and it's got a N an embedding table",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 498,
      "text": "and it's got a weight inside it which stores the uh weight inside it which stores the uh weight inside it which stores the uh sort of lookup table",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 499,
      "text": "so so that would be sort of lookup table",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 500,
      "text": "so so that would be sort of lookup table",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 501,
      "text": "so so that would be moved to the GPU so that all the moved to the GPU so that all the moved to the GPU so that all the calculations here happen on the GPU and calculations here happen on the GPU and calculations here happen on the GPU and they can be a lot faster",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 502,
      "text": "and then they can be a lot faster",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 503,
      "text": "and then they can be a lot faster and then finally here when I'm creating the finally here when I'm creating the finally here when I'm creating the context that feeds in to generate I have context that feeds in to generate I have context that feeds in to generate I have to make sure that I create it on the to make sure that I create it on the to make sure that I create it on the device number two what I introduced is device number two what I introduced is device number two what I introduced is uh the fact that here in the training uh the fact that here in the training uh the fact that here in the training Loop here I was just printing the um l. Loop here I was just printing the um l. Loop here I was just printing the um l. item inside the training Loop but this item inside the training Loop but this item inside the training Loop but this is a very noisy measurement of the is a very noisy measurement of the is a very noisy measurement of the current loss because every batch will be current loss because every batch will be current loss because every batch will be more or less lucky",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 504,
      "text": "and so what I want to more or less lucky",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 505,
      "text": "and so what I want to more or less lucky",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 506,
      "text": "and so what I want to do usually um is uh I have an estimate do usually um is uh I have an estimate do usually um is uh I have an estimate loss function and the estimate loss loss function and the estimate loss loss function and the estimate loss basically then um goes up here and it basically then um goes up here and it basically then um goes up here and it averages up the loss over multiple averages up the loss over multiple averages up the loss over multiple batches so in particular we're going to batches so in particular we're going to batches so in particular we're going to iterate eval iter times and we're going iterate eval iter times and we're going iterate eval iter times and we're going to basically get our loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 507,
      "text": "and then we're to basically get our loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 508,
      "text": "and then we're to basically get our loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 509,
      "text": "and then we're going to get the average loss for both going to get the average loss for both going to get the average loss for both splits",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 510,
      "text": "and so this will be a lot less splits",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 511,
      "text": "and so this will be a lot less splits",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 512,
      "text": "and so this will be a lot less noisy so here when we call the estimate noisy",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 513,
      "text": "so here when we call the estimate noisy so here when we call the estimate loss we're we're going to report the uh loss we're we're going to report the uh loss we're we're going to report the uh pretty accurate train and validation pretty accurate train and validation pretty accurate train and validation loss now when we come back up you'll loss now when we come back up you'll loss now when we come back up you'll notice a few things here I'm setting the notice a few things here I'm setting the notice a few things here I'm setting the model to evaluation phase and down here model to evaluation phase and down here model to evaluation phase and down here I'm resetting it back to training phase I'm resetting it back to training phase I'm resetting it back to training phase now right now for our model as is this now right now for our model as is this now right now for our model as is this doesn't actually do anything because the doesn't actually do anything because the doesn't actually do anything because the only thing inside this model is this uh only thing inside this model is this uh only thing inside this model is this uh nn.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 514,
      "text": "embedding and um this this um nn. embedding and um this this um nn. embedding and um this this um Network would behave both would behave Network would behave both would behave Network would behave both would behave the same in both evaluation mode and the same in both evaluation mode and the same in both evaluation mode and training mode we have no drop off layers training mode we have no drop off layers training mode we have no drop off layers we have no batm layers Etc",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 515,
      "text": "but it is a we have no batm layers",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 516,
      "text": "Etc",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 517,
      "text": "but it is a we have no batm layers",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 518,
      "text": "Etc",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 519,
      "text": "but it is a good practice to Think Through what mode good practice to Think Through what mode good practice to Think Through what mode your neural network is in because some your neural network is in because some your neural network is in because some layers will have different Behavior Uh layers will have different Behavior Uh layers will have different Behavior Uh at inference time or training time and at inference time or training time and at inference time or training time and there's also this context manager torch there's also this context manager torch there's also this context manager torch up nograd",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 520,
      "text": "and this is just telling up nograd and this is just telling up nograd and this is just telling pytorch that everything that happens pytorch that everything that happens pytorch that everything that happens inside this function we will not call do inside this function we will not call do inside this function we will not call do backward on and so pytorch can be a lot backward on and so pytorch can be a lot backward on and so pytorch can be a lot more efficient with its memory use more efficient with its memory use more efficient with its memory use because it doesn't have to store all the because it doesn't have to store all the because it doesn't have to store all the intermediate variables uh because we're intermediate variables uh because we're intermediate variables uh because we're never going to call backward",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 521,
      "text": "and so it never going to call backward",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 522,
      "text": "and so it never going to call backward",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 523,
      "text": "and so it can it can be a lot more memory can it can be a lot more memory can it can be a lot more memory efficient in that way so also a good efficient in that way so also a good efficient in that way so also a good practice to tpy torch when we don't practice to tpy torch when we don't practice to tpy torch when we don't intend to do back intend to do back intend to do back propagation",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 524,
      "text": "so right now this script is propagation",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 525,
      "text": "so right now this script is propagation",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 526,
      "text": "so right now this script is about 120 lines of code of and that's about 120 lines of code of and that's about 120 lines of code of and that's kind of our starter code I'm calling it kind of our starter code I'm calling it kind of our starter code I'm calling it b.p",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 527,
      "text": "and I'm going to release it later b.p",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 528,
      "text": "and I'm going to release it later b.p",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 529,
      "text": "and I'm going to release it later now running this now running this now running this script gives us output in the terminal script gives us output in the terminal script gives us output in the terminal and it looks something like this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 530,
      "text": "it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 531,
      "text": "and it looks something like this it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 532,
      "text": "and it looks something like this it basically as I ran this code uh it was basically as I ran this code uh it was basically as I ran this code",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 533,
      "text": "uh it was giving me the train loss and Val loss giving me the train loss and Val loss giving me the train loss and Val loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 534,
      "text": "and we see that we convert to somewhere",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 535,
      "text": "and we see that we convert to somewhere",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 536,
      "text": "and we see that we convert to somewhere around around around 2.5 with the pyr model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 537,
      "text": "and then here's 2.5 with the pyr model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 538,
      "text": "and then here's 2.5 with the pyr model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 539,
      "text": "and then here's the sample that we produced at the the sample that we produced at the the sample that we produced at the end",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 540,
      "text": "and so we have everything packaged end",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 541,
      "text": "and so we have everything packaged end",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 542,
      "text": "and so we have everything packaged up in the script and we're in a good up in the script and we're in a good up in the script and we're in a good position now to iterate on this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 543,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 544,
      "text": "so position now to iterate on this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 545,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 546,
      "text": "so position now to iterate on this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 547,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 548,
      "text": "so we are almost ready to start writing our we are almost ready to start writing our we are almost ready to start writing our very first self attention block for very first self attention block for very first self attention block for processing these uh tokens now before we processing these uh tokens now before we processing these uh tokens now before we actually get there I want to get you actually get there I want to get you actually get there I want to get you used to a mathematical trick that is used to a mathematical trick that is used to a mathematical trick that is used in the self attention inside a used in the self attention inside a used in the self attention inside a Transformer and is really just like at Transformer and is really just like at Transformer and is really just like at the heart of an an efficient the heart of an an efficient the heart of an an efficient implementation of self attention and so implementation of self attention and so implementation of self attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 549,
      "text": "and so I want to work with this toy example to I want to work with this toy example to I want to work with this toy example to just get you used to this operation and just get you used to this operation and just get you used to this operation",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 550,
      "text": "and then it's going to make it much more then it's going to make it much more then it's going to make it much more clear once we actually get to um to it clear once we actually get to um to it clear once we actually get to um to it uh in the script uh in the script uh in the script again",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 551,
      "text": "so let's create a b BYT by C where again so let's create a b BYT by C where again so let's create a b BYT by C where BT and C are just 48 and two in the toy BT and C are just 48 and two in the toy BT and C are just 48 and two in the toy example and these are basically channels example and these are basically channels example and these are basically channels and we have uh batches and we have the",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 552,
      "text": "and we have uh batches and we have the",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 553,
      "text": "and we have uh batches and we have the time component and we have information time component and we have information time component and we have information at each point in the sequence so at each point in the sequence so at each point in the sequence so see now what we would like to do is we see now what we would like to do is we see now what we would like to do is we would like these um tokens so we have up would like these um tokens",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 554,
      "text": "so we have up would like these um tokens so we have up to eight tokens here in a batch and to eight tokens here in a batch and to eight tokens here in a batch and these eight tokens are currently not these eight tokens are currently not these eight tokens are currently not talking to each other and we would like talking to each other and we would like talking to each other and we would like them to talk to each other we'd like to them to talk to each other we'd like to them to talk to each other we'd like to couple them and in particular we don't couple them and in particular we don't couple them and in particular we don't we we want to couple them in a very we we want to couple them in a very",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 555,
      "text": "we we want to couple them in a very specific way",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 556,
      "text": "so the token for example at specific way so the token for example at specific way so the token for example at the fifth location it should not the fifth location it should not the fifth location it should not communicate with tokens in the sixth communicate with tokens in the sixth communicate with tokens in the sixth seventh and eighth location seventh and eighth location seventh and eighth location because uh those are future tokens in because uh those are future tokens in because uh those are future tokens in the sequence the token on the fifth the sequence the token on the fifth the sequence the token on the fifth location should only talk to the one in location should only talk to the one in location should only talk to the one in the fourth third second and first so the fourth third second and first so the fourth third second and first",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 557,
      "text": "so it's only so information only flows from it's only so information only flows from it's only so information only flows from previous context to the current time previous context to the current time previous context to the current time step",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 558,
      "text": "and we cannot get any information step",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 559,
      "text": "and we cannot get any information step",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 560,
      "text": "and we cannot get any information from the future because we are about to from the future because we are about to from the future because we are about to try to predict the try to predict the try to predict the future",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 561,
      "text": "so what is the easiest way for future",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 562,
      "text": "so what is the easiest way for future",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 563,
      "text": "so what is the easiest way for tokens to communicate okay the easiest tokens to communicate okay the easiest tokens to communicate okay the easiest way I would say is okay if we're up to way I would say is okay if we're up to way I would say is okay if we're up to if we're a fifth token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 564,
      "text": "and I'd like to if we're a fifth token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 565,
      "text": "and I'd like to if we're a fifth token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 566,
      "text": "and I'd like to communicate with my past the simplest communicate with my past the simplest communicate with my past the simplest way we can do that is to just do a way we can do that is to just do a way we can do that is to just do a weight is to just do an average of all weight is to just do an average of all weight is to just do an average of all the um of all the preceding elements",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 567,
      "text": "so the um of all the preceding elements so the um of all the preceding elements so for example if I'm the fif token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 568,
      "text": "I would for example if I'm the fif token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 569,
      "text": "I would for example if I'm the fif token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 570,
      "text": "I would like to take the channels uh that make like to take the channels uh that make like to take the channels uh that make up that are information at my step but up that are information at my step but up that are information at my step",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 571,
      "text": "but then also the channels from the fourth then also the channels from the fourth then also the channels from the fourth step third step second step and the step third step second step and the step third step second step and the first step I'd like to average those up first step I'd like to average those up first step I'd like to average those up",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 572,
      "text": "and then that would become sort of like",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 573,
      "text": "and then that would become sort of like",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 574,
      "text": "and then that would become sort of like a feature Vector that summarizes me in a feature Vector that summarizes me in a feature Vector that summarizes me in the context of my history now of course the context of my history now of course the context of my history now of course just doing a sum or like an average is just doing a sum or like an average is just doing a sum or like an average is an extremely weak form of interaction an extremely weak form of interaction an extremely weak form of interaction like this communication is uh extremely like this communication is uh extremely like this communication is uh extremely lossy we've lost a ton of information lossy we've lost a ton of information",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 575,
      "text": "lossy we've lost a ton of information about the spatial Arrangements of all about the spatial Arrangements of all about the spatial Arrangements of all those tokens uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 576,
      "text": "but that's okay for now those tokens uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 577,
      "text": "but that's okay for now those tokens uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 578,
      "text": "but that's okay for now we'll see how we can bring that we'll see how we can bring that we'll see how we can bring that information back later for now what we information back later for now what we information back later for now what we would like to do is for every single would like to do is for every single would like to do is for every single batch element independently for every batch element independently for every batch element independently for every teeth token in that sequence we'd like teeth token in that sequence we'd like teeth token in that sequence we'd like to now calculate the average of all the to now calculate the average of all the to now calculate the average of all the vectors in all the previous tokens and vectors in all the previous tokens and vectors in all the previous tokens and also at this token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 579,
      "text": "so let's write that also at this token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 580,
      "text": "so let's write that also at this token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 581,
      "text": "so let's write that out um I have a small snippet here and out um I have a small snippet here and out um I have a small snippet here and instead of just fumbling around let me instead of just fumbling around let me instead of just fumbling around let me just copy paste it and talk to just copy paste it and talk to just copy paste it and talk to it so in other words we're going to it so in other words we're going to it so in other words we're going to create X and B is short for bag of words create X and B is short for bag of words create X and B is short for bag of words because bag of words is um is kind of because bag of words is um is kind of because bag of words is um is kind of like um a term that people use when you like um a term that people use when you like um a term that people use when you are just averaging up things so this is are just averaging up things so this is are just averaging up things so this is just a bag of words basically there's a just a bag of words basically there's a just a bag of words basically there's a word stored on every one of these eight word stored on every one of these eight word stored on every one of these eight locations and we're doing a bag of words locations and we're doing a bag of words locations and we're doing a bag of words we're just averaging we're just averaging we're just averaging so in the beginning we're going to say so in the beginning we're going to say so in the beginning we're going to say that it's just initialized at Zero and that it's just initialized at Zero and that it's just initialized at Zero",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 582,
      "text": "and then I'm doing a for Loop here so we're then I'm doing a for Loop here so we're then I'm doing a for Loop here so we're not being efficient yet that's coming not being efficient yet that's coming not being efficient yet that's coming but for now we're just iterating over but for now we're just iterating over but for now we're just iterating over all the batch Dimensions independently all the batch Dimensions independently all the batch Dimensions independently iterating over time and then the iterating over time and then the iterating over time and then the previous uh tokens are at this uh batch previous uh tokens are at this uh batch previous uh tokens are at this uh batch Dimension and then everything up to and Dimension and then everything up to and Dimension and then everything up to and including the teeth token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 583,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 584,
      "text": "so when including the teeth token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 585,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 586,
      "text": "so when including the teeth token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 587,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 588,
      "text": "so when we slice out X in this way X prev we slice out X in this way X prev we slice out X in this way X prev Becomes of shape um how many T elements Becomes of shape um how many T elements Becomes of shape um how many T elements there were in the past and then of there were in the past",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 589,
      "text": "and then of there were in the past and then of course C so all the two-dimensional course C so all the two-dimensional course C so all the two-dimensional information from these little tokens so information from these little tokens so information from these little tokens so that's the previous uh sort of chunk of that's the previous uh sort of chunk of that's the previous uh sort of chunk of um tokens from my current sequence and um tokens from my current sequence and um tokens from my current sequence",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 590,
      "text": "and then I'm just doing the average or the",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 591,
      "text": "then I'm just doing the average or the then I'm just doing the average or the mean over the zero Dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 592,
      "text": "so I'm mean over the zero Dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 593,
      "text": "so I'm mean over the zero Dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 594,
      "text": "so I'm averaging out the time here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 595,
      "text": "and I'm just averaging out the time here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 596,
      "text": "and I'm just averaging out the time here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 597,
      "text": "and I'm just going to get a little c one dimensional going to get a little c one dimensional going to get a little c one dimensional Vector which I'm going to store in X bag Vector which I'm going to store in X bag Vector which I'm going to store in X bag of words so I can run this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 598,
      "text": "and and uh of words so I can run this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 599,
      "text": "and and uh of words so I can run this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 600,
      "text": "and and uh this is not going to be very informative this is not going to be very informative this is not going to be very informative because let's see so this is X of Zer so because let's see so this is X of Zer so because let's see so this is X of Zer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 601,
      "text": "so this is the zeroth batch element and this is the zeroth batch element and this is the zeroth batch element and then expo at zero now you see how the at then expo at zero now you see how the at then expo at zero now you see how the at the first location here you see that the the first location here you see that the the first location here you see that the two are equal and that's because it's two are equal and that's because it's two are equal and that's because it's we're just doing an average of this one we're just doing an average of this one we're just doing an average of this one token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 602,
      "text": "but here this one is now an token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 603,
      "text": "but here this one is now an token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 604,
      "text": "but here this one is now an average of these two and now this one is average of these two and now this one is average of these two and now this one is an average of these an average of these an average of these three and so on three and so on three and so on so",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 605,
      "text": "uh and this last one is the average",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 606,
      "text": "so uh and this last one is the average",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 607,
      "text": "so uh and this last one is the average of all of these elements so vertical of all of these elements so vertical of all of these elements so vertical average just averaging up all the tokens average just averaging up all the tokens average just averaging up all the tokens now gives this outcome now gives this outcome now gives this outcome here so this is all well and good",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 608,
      "text": "uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 609,
      "text": "but here so this is all well and good uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 610,
      "text": "but here so this is all well and good uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 611,
      "text": "but this is very inefficient now the trick this is very inefficient now the trick this is very inefficient now the trick is that we can be very very efficient",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 612,
      "text": "is that we can be very very efficient is that we can be very very efficient about doing this using matrix about doing this using matrix about doing this using matrix multiplication so that's the multiplication so that's the multiplication so that's the mathematical trick and let me show you mathematical trick and let me show you mathematical trick and let me show you what I mean let's work with the toy what I mean let's work with the toy what I mean let's work with the toy example here let me run it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 613,
      "text": "and I'll example here let me run it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 614,
      "text": "and I'll example here let me run it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 615,
      "text": "and I'll explain I have a simple Matrix here that explain I have a simple Matrix here that explain I have a simple Matrix here that is a 3X3 of all ones a matrix B of just is a 3X3 of all ones a matrix B of just is a 3X3 of all ones a matrix B of just random numbers",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 616,
      "text": "and it's a 3x2 and a random numbers and it's a 3x2 and a random numbers and it's a 3x2 and a matrix C which will be 3x3 multip 3x2 matrix C which will be 3x3 multip 3x2 matrix C which will be 3x3 multip 3x2 which will give out a 3x2",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 617,
      "text": "so here we're which will give out a 3x2",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 618,
      "text": "so here we're which will give out a 3x2",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 619,
      "text": "so here we're just using um matrix multiplication so a just using um matrix multiplication so a just using um matrix multiplication so a multiply B gives us multiply B gives us multiply B gives us C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 620,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 621,
      "text": "so how are these numbers in C um C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 622,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 623,
      "text": "so how are these numbers in C um C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 624,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 625,
      "text": "so how are these numbers in C um achieved right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 626,
      "text": "so this number in the top achieved right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 627,
      "text": "so this number in the top achieved right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 628,
      "text": "so this number in the top left is the first row of a dot product left is the first row of a dot product left is the first row of a dot product with the First Column of B and since all with the First Column of B and since all with the First Column of B and since all the the row of a right now is all just the the row of a right now is all just the the row of a right now is all just ones then the do product here with with ones then the do product here with with ones then the do product here with with this column of B is just going to do a this column of B is just going to do a this column of B is just going to do a sum of these of this column so 2 + 6 + 6 sum of these of this column so 2 + 6 + 6 sum of these of this column so 2 + 6 + 6 is is is 14 the element here in the output of C 14 the element here in the output of C 14 the element here in the output of C is also the first column here the first is also the first column here the first is also the first column here the first row of a multiplied now with the second row of a multiplied now with the second row of a multiplied now with the second column of B so 7 + 4 + 5 is 16 now you column of B",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 629,
      "text": "so 7 + 4 + 5 is 16 now you column of B so 7 + 4 + 5 is 16 now you see that there's repeating elements here see that there's repeating elements here see that there's repeating elements here so this 14 again is because this row is so this 14 again is because this row is so this 14 again is because this row is again all ones and it's multiplying the again all ones and it's multiplying the again all ones and it's multiplying the First Column of B",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 630,
      "text": "so we get 14 and this First Column of B",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 631,
      "text": "so we get 14 and this First Column of B",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 632,
      "text": "so we get 14 and this one is and so on so this last number one is and so on so this last number one is and so on so this last number here is the last row",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 633,
      "text": "do product last here is the last row do product last here is the last row do product last column now the trick here is uh the column now the trick here is uh the column now the trick here is uh the following this is just a boring number following this is just a boring number following this is just a boring number of um it's just a boring array of all of um it's just a boring array of all of um it's just a boring array of all ones but torch has this function called ones but torch has this function called ones but torch has this function called Trail which is short for a Trail which is short for a Trail which is short for a triangular uh something like that and triangular uh something like that and triangular uh something like that and you can wrap it in torch up once and it you can wrap it in torch up once and it you can wrap it in torch up once and it will just return the lower triangular will just return the lower triangular will just return the lower triangular portion of this portion of this portion of this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 634,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 635,
      "text": "so now it will basically zero out",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 636,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 637,
      "text": "so now it will basically zero out",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 638,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 639,
      "text": "so now it will basically zero out uh these guys here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 640,
      "text": "so we just get the uh these guys here so we just get the uh these guys here so we just get the lower triangular part",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 641,
      "text": "well what happens lower triangular part",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 642,
      "text": "well what happens lower triangular part",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 643,
      "text": "well what happens if we do that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 644,
      "text": "so now we'll have a like this and B that so now we'll have a like this and B like this and now what are we getting like this and now what are we getting like this and now what are we getting here in C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 645,
      "text": "well what is this number well here in C well what is this number well here in C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 646,
      "text": "well what is this number",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 647,
      "text": "well this is the first row times the First this is the first row times the First this is the first row times the First Column and because this is zeros Column and because this is zeros Column and because this is zeros uh these elements here are now ignored uh these elements here are now ignored uh these elements here are now ignored so we just get a two",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 648,
      "text": "and then this so we just get a two",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 649,
      "text": "and then this so we just get a two",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 650,
      "text": "and then this number here is the first row times the number here is the first row times the number here is the first row times the second column and because these are second column and because these are second column and because these are zeros they get ignored and it's just zeros they get ignored and it's just zeros they get ignored and it's just seven this seven multiplies this one but seven this seven multiplies this one but seven this seven multiplies this one but look what happened here because this is look what happened here because this is look what happened here because this is one and then zeros we what ended up one and then zeros we what ended up one and then zeros we what ended up happening is we're just plucking out the happening is we're just plucking out the happening is we're just plucking out the row of this row of B",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 651,
      "text": "and that's what we row of this row of B and that's what we row of this row of B and that's what we got now here we have one 1 Z so here 110 got now here we have one 1 Z so here 110 got now here we have one 1 Z so here 110 do product with these two columns will do product with these two columns will do product with these two columns will now give us 2 + 6 which is 8 and 7 + 4 now give us 2 + 6 which is 8 and 7 + 4 now give us 2 + 6 which is 8 and 7 + 4 which is 11 and because this is 111 we which is 11 and because this is 111 we which is 11 and because this is 111 we ended up with the addition of all of ended up with the addition of all of ended up with the addition of all of them and so basically depending on how them",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 652,
      "text": "and so basically depending on how them and so basically depending on how many ones and zeros we have here we are many ones and zeros we have here we are many ones and zeros we have here we are basically doing a sum currently of a basically doing a sum currently of a basically doing a sum currently of a variable number of these rows and that variable number of these rows and that variable number of these rows and that gets deposited into gets deposited into gets deposited into C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 653,
      "text": "So currently we're doing sums because C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 654,
      "text": "So currently we're doing sums because C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 655,
      "text": "So currently we're doing sums because these are ones but we can also do these are ones but we can also do these are ones but we can also do average",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 656,
      "text": "right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 657,
      "text": "and you can start to see average right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 658,
      "text": "and you can start to see average right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 659,
      "text": "and you can start to see how we could do average uh of the rows how we could do average uh of the rows how we could do average uh of the rows of B uh sort of in an incremental of B uh sort of in an incremental of B uh sort of in an incremental fashion because we don't have to we can fashion because we don't have to we can fashion because we don't have to we can basically normalize these rows so that basically normalize these rows so that basically normalize these rows so that they sum to one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 660,
      "text": "and then we're going to they sum to one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 661,
      "text": "and then we're going to they sum to one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 662,
      "text": "and then we're going to get an average",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 663,
      "text": "so if we took a and then get an average",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 664,
      "text": "so if we took a and then get an average",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 665,
      "text": "so if we took a",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 666,
      "text": "and then we did aals we did aals we did aals aide torch.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 667,
      "text": "sum in the um of a in the um aide torch.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 668,
      "text": "sum in the um of a in the um aide torch.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 669,
      "text": "sum in the um of a in the um oneth Dimension and then let's keep them oneth Dimension and then let's keep them oneth Dimension and then let's keep them as true",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 670,
      "text": "so so therefore the broadcasting as true",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 671,
      "text": "so so therefore the broadcasting as true",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 672,
      "text": "so so therefore the broadcasting will work out so if I rerun this you see will work out so if I rerun this you see will work out so if I rerun this you see now that these rows now sum to one so now that these rows now sum to one so now that these rows now sum to one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 673,
      "text": "so this row is one this row is 0. 5.5 Z and this row is one this row is 0. 5.5 Z and this row is one this row is 0. 5.5 Z",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 674,
      "text": "and here we get 1/3 and now when we do a here we get 1/3 and now when we do a here we get 1/3 and now when we do a multiply B what are we getting here we multiply",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 675,
      "text": "B",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 676,
      "text": "what are we getting here we multiply B",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 677,
      "text": "what are we getting here we are just getting the first row first row are just getting the first row first row are just getting the first row first row here now we are getting the average of here now we are getting the average of here now we are getting the average of the first two",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 678,
      "text": "the first two the first two rows",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 679,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 680,
      "text": "so 2 and six average is four rows",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 681,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 682,
      "text": "so 2 and six average is four rows",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 683,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 684,
      "text": "so 2 and six average is four and four and seven average is and four and seven average is and four and seven average is 5.5 and on the bottom here we are now 5.5 and on the bottom here we are now 5.5 and on the bottom here we are now getting the average of these three rows getting the average of these three rows getting the average of these three rows",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 685,
      "text": "so the average of all of elements of B so the average of all of elements of B so the average of all of elements of B are now deposited here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 686,
      "text": "and so you can are now deposited here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 687,
      "text": "and so you can are now deposited here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 688,
      "text": "and so you can see that by manipulating these uh see that by manipulating these uh see that by manipulating these uh elements of this multiplying Matrix and elements of this multiplying Matrix and elements of this multiplying Matrix and then multiplying it with any given then multiplying it with any given then multiplying it with any given Matrix we can do these averages in this Matrix we can do these averages in this Matrix we can do these averages in this incremental fashion because we just get incremental fashion because we just get incremental fashion because we just get um and we can manipulate that based on um and we can manipulate that based on um and we can manipulate that based on the elements of a",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 689,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 690,
      "text": "so that's very the elements of",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 691,
      "text": "a okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 692,
      "text": "so that's very the elements of",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 693,
      "text": "a okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 694,
      "text": "so that's very convenient",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 695,
      "text": "so let's let's swing back up convenient",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 696,
      "text": "so let's let's swing back up convenient",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 697,
      "text": "so let's let's swing back up here and see how we can vectorize this here and see how we can vectorize this here and see how we can vectorize this and make it much more efficient using and make it much more efficient using and make it much more efficient using what we've learned so in what we've learned so in what we've learned so in particular we are going to produce an particular we are going to produce an particular we are going to produce an array a",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 698,
      "text": "but here I'm going to call it we array a",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 699,
      "text": "but here I'm going to call it we array a",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 700,
      "text": "but here I'm going to call it we short for weights but this is our short for weights but this is our short for weights but this is our a and this is how much of every row we a",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 701,
      "text": "and this is how much of every row we a",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 702,
      "text": "and this is how much of every row we want to average up and it's going to be want to average up",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 703,
      "text": "and it's going to be want to average up",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 704,
      "text": "and it's going to be an average because you can see that an average because you can see that an average because you can see that these rows sum to these rows sum to these rows sum to one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 705,
      "text": "so this is our a and then our B in one so this is our a and then our B in one so this is our a and then our B in this example of course is X this example of course is X this example of course is X",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 706,
      "text": "so what's going to happen here now is so what's going to happen here now is so what's going to happen here now is that we are going to have an expo that we are going to have an expo that we are going to have an expo 2 and this Expo 2 is going to be way 2 and this Expo 2 is going to be way 2 and this Expo 2 is going to be way multiplying multiplying multiplying RX",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 707,
      "text": "so let's think this true way is T BYT RX",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 708,
      "text": "so let's think this true way is T BYT RX",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 709,
      "text": "so let's think this true way is T BYT and this is Matrix multiplying in and this is Matrix multiplying in and this is Matrix multiplying in pytorch a b by T by pytorch a b by T by pytorch a b by T by C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 710,
      "text": "and it's giving us uh different what C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 711,
      "text": "and it's giving us uh different what C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 712,
      "text": "and it's giving us uh different what shape so pytorch will come here and it shape so pytorch will come here and it shape so pytorch will come here and it will see that these shapes are not the will see that these shapes are not the will see that these shapes are not the same so it will create a batch Dimension same",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 713,
      "text": "so it will create a batch Dimension same",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 714,
      "text": "so it will create a batch Dimension here and this is a batched matrix here and this is a batched matrix here and this is a batched matrix multiply",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 715,
      "text": "and so it will apply this multiply",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 716,
      "text": "and so it will apply this multiply",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 717,
      "text": "and so it will apply this matrix multiplication in all the batch matrix multiplication in all the batch matrix multiplication in all the batch elements um in parallel and individually elements um in parallel and individually elements um in parallel and individually and then for each batch element there and then for each batch element there and then for each batch element there will be a t BYT multiplying T by C will be a t BYT multiplying T by C will be a t BYT multiplying T by C exactly as we had exactly as we had exactly as we had below so this will now create B by T by below so this will now create B by T by below so this will now create B by T by C and Expo 2 will now become identical C and Expo 2 will now become identical C and Expo 2 will now become identical to Expo so we can see that torch.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 718,
      "text": "all close of so we can see that torch.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 719,
      "text": "all close of xbo and xbo 2 should be true xbo and xbo 2 should be true xbo and xbo 2 should be true now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 720,
      "text": "so this kind of like convinces us now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 721,
      "text": "so this kind of like convinces us now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 722,
      "text": "so this kind of like convinces us that uh these are in fact um the same so that uh these are in fact um the same so that uh these are in fact um the same",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 723,
      "text": "so xbo and xbo 2 if I just print xbo and xbo 2 if I just print xbo and xbo 2 if I just print them uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 724,
      "text": "okay we're not going to be able",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 725,
      "text": "them uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 726,
      "text": "okay we're not going to be able",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 727,
      "text": "them uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 728,
      "text": "okay we're not going to be able to okay we're not going to be able to to",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 729,
      "text": "okay we're not going to be able to to",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 730,
      "text": "okay we're not going to be able to just stare it down but just stare it down but just stare it down but um well let me try Expo basically just um well let me try Expo basically just um well let me try Expo basically just at the zeroth element and Expo two at at the zeroth element and Expo two at at the zeroth element and Expo two at the zeroth element",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 731,
      "text": "so just the first the zeroth element so just the first the zeroth element",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 732,
      "text": "so just the first batch",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 733,
      "text": "and we should see that this and batch",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 734,
      "text": "and we should see that this and batch",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 735,
      "text": "and we should see that this and that should be identical which they that should be identical which they that should be identical which they are right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 736,
      "text": "so what happened here the are right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 737,
      "text": "so what happened here the are right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 738,
      "text": "so what happened here the trick is we were able to use batched trick is we were able to use batched trick is we were able to use batched Matrix multiply to do this uh Matrix multiply to do this uh Matrix multiply to do this uh aggregation",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 739,
      "text": "really",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 740,
      "text": "and it's a weighted aggregation really",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 741,
      "text": "and it's a weighted aggregation really",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 742,
      "text": "and it's a weighted aggregation and the weights are aggregation and the weights are aggregation and the weights are specified in this um T BYT array and specified in this um T BYT array and specified in this um T BYT array",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 743,
      "text": "and we're basically doing weighted sums and we're basically doing weighted sums and we're basically doing weighted sums and uh these weighted sums are are",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 744,
      "text": "U uh these weighted sums are are",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 745,
      "text": "U uh these weighted sums are are U according to uh the weights inside here according to uh the weights inside here according to uh the weights inside here they take on sort of this triangular they take on sort of this triangular they take on sort of this triangular form",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 746,
      "text": "and so that means that a token at form and so that means that a token at form and so that means that a token at the teth dimension will only get uh sort the teth dimension will only get uh sort the teth dimension will only get uh sort of um information from the um tokens of um information from the um tokens of um information from the um tokens perceiving it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 747,
      "text": "so that's exactly what we perceiving it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 748,
      "text": "so that's exactly what we perceiving it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 749,
      "text": "so that's exactly what we want",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 750,
      "text": "and finally I would like to rewrite want and finally I would like to rewrite want and finally I would like to rewrite it in one more way and we're going to it in one more way and we're going to it in one more way and we're going to see why that's useful",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 751,
      "text": "so this is the see why that's useful",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 752,
      "text": "so this is the see why that's useful",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 753,
      "text": "so this is the third version and it's also identical to third version",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 754,
      "text": "and it's also identical to third version",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 755,
      "text": "and it's also identical to the first and second but let me talk the first and second but let me talk the first and second but let me talk through it it uses through it it uses through it it uses softmax",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 756,
      "text": "so Trill here is this Matrix softmax",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 757,
      "text": "so Trill here is this Matrix softmax",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 758,
      "text": "so Trill here is this Matrix lower triangular lower triangular lower triangular ones way begins as all ones way begins as all ones way begins as all zero",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 759,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 760,
      "text": "so if I just print way in the zero okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 761,
      "text": "so if I just print way in the zero okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 762,
      "text": "so if I just print way in the beginning it's all zero then I beginning it's all zero",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 763,
      "text": "then I beginning it's all zero",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 764,
      "text": "then I used masked fill",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 765,
      "text": "so what this is doing used masked fill",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 766,
      "text": "so what this is doing used masked fill",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 767,
      "text": "so what this is doing is we.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 768,
      "text": "masked fill it's all zeros and is we.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 769,
      "text": "masked fill it's all zeros and is we.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 770,
      "text": "masked fill it's all zeros",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 771,
      "text": "and I'm saying for all the elements where I'm saying for all the elements where I'm saying for all the elements where Trill is equal equal Z make them be Trill is equal equal Z make them be Trill is equal equal Z make them be negative Infinity so all the elements negative Infinity so all the elements negative Infinity so all the elements where Trill is zero will become negative where Trill is zero will become negative where Trill is zero will become negative Infinity now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 772,
      "text": "so this is what we get and Infinity now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 773,
      "text": "so this is what we get and Infinity now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 774,
      "text": "so this is what we get and then the final line here is then the final line here is then the final line here is softmax",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 775,
      "text": "so if I take a softmax along softmax so if I take a softmax along softmax so if I take a softmax along every single so dim is negative one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 776,
      "text": "so every single so dim is negative one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 777,
      "text": "so every single so dim is negative one so along every single row if I do softmax along every single row if I do softmax along every single row if I do softmax what is that going to what is that going to what is that going to do well softmax is um is also like a do well softmax is um is also like a do well softmax is um is also like a normalization operation",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 778,
      "text": "right and so normalization operation right and so normalization operation",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 779,
      "text": "right and so spoiler alert you get the exact same spoiler alert you get the exact same spoiler alert you get the exact same Matrix let me bring back to Matrix let me bring back to Matrix let me bring back to softmax and recall that in softmax we're softmax and recall that in softmax we're softmax and recall that in softmax we're going to exponentiate every single one going to exponentiate every single one going to exponentiate every single one of these",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 780,
      "text": "and then we're going to divide of these",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 781,
      "text": "and then we're going to divide of these",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 782,
      "text": "and then we're going to divide by the sum",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 783,
      "text": "and so if we exponentiate by the sum",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 784,
      "text": "and so if we exponentiate by the sum",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 785,
      "text": "and so if we exponentiate every single element here we're going to every single element here we're going to every single element here we're going to get a one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 786,
      "text": "and here we're going to get uh get a one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 787,
      "text": "and here we're going to get uh get a one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 788,
      "text": "and here we're going to get uh basically zero 0",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 789,
      "text": "z0 Z everywhere else basically zero 0",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 790,
      "text": "z0 Z everywhere else basically zero 0",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 791,
      "text": "z0 Z everywhere else and then when we normalize we just get",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 792,
      "text": "and then when we normalize we just get",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 793,
      "text": "and then when we normalize we just get one here we're going to get one one and one here we're going to get one one and one here we're going to get one one and then zeros and then softmax will again then zeros and then softmax will again then zeros and then softmax will again divide and this will give us 5.5 and so divide and this will give us 5.5 and so divide and this will give us 5.5 and so on and so this is also the uh the same on and so this is also the uh the same on and so this is also the uh the same way to produce uh this mask now the way to produce uh this mask now the way to produce uh this mask now the reason that this is a bit more reason that this is a bit more reason that this is a bit more interesting and the reason we're going interesting and the reason we're going interesting and the reason we're going to end up using it in self to end up using it in self to end up using it in self attention is that these weights here attention is that these weights here attention is that these weights here begin uh with zero and you can think of begin uh with zero and you can think of begin uh with zero and you can think of this as like an interaction strength or this as like an interaction strength or this as like an interaction strength or like an affinity",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 794,
      "text": "so basically it's like an affinity",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 795,
      "text": "so basically it's like an affinity",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 796,
      "text": "so basically it's telling us how much of each uh token telling us how much of each uh token telling us how much of each uh token from the past do we want to Aggregate from the past do we want to Aggregate from the past do we want to Aggregate and average up and average up and average up",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 797,
      "text": "and then this line is saying tokens from and then this line is saying tokens from and then this line is saying tokens from the past cannot communicate by setting the past cannot communicate by setting the past cannot communicate by setting them to negative Infinity we're saying them to negative Infinity we're saying them to negative Infinity we're saying that we will not aggregate anything from that we will not aggregate anything from that we will not aggregate anything from those those those tokens and so basically this then goes tokens and so basically this then goes tokens and so basically this then goes through softmax and through the weighted through softmax and through the weighted through softmax and through the weighted and this is the aggregation through and this is the aggregation through and this is the aggregation through matrix matrix matrix multiplication and so what this is now multiplication",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 798,
      "text": "and so what this is now multiplication",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 799,
      "text": "and so what this is now is you can think of these as um these is you can think of these as um these is you can think of these as um these zeros are currently just set by us to be zeros are currently just set by us to be zeros are currently just set by us to be zero but a quick preview is that these zero but a quick preview is that these zero but a quick preview is that these affinities between the tokens are not affinities between the tokens are not affinities between the tokens are not going to be just constant at zero going to be just constant at zero going to be just constant at zero they're going to be data dependent these they're going to be data dependent these they're going to be data dependent these tokens are going to start looking at tokens are going to start looking at tokens are going to start looking at each other and some tokens will find each other and some tokens will find each other and some tokens will find other tokens more or less interesting other tokens more or less interesting other tokens more or less interesting and depending on what their values are and depending on what their values are and depending on what their values are they're going to find each other they're going to find each other they're going to find each other interesting to different amounts and I'm interesting to different amounts and I'm interesting to different amounts and I'm going to call those affinities I think going to call those affinities I think going to call those affinities I think",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 800,
      "text": "and then here we are saying the future",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 801,
      "text": "and then here we are saying the future",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 802,
      "text": "and then here we are saying the future cannot communicate with the past we're cannot communicate with the past we're cannot communicate with the past we're we're going to clamp them",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 803,
      "text": "and then when we're going to clamp them and then when we're going to clamp them",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 804,
      "text": "and then when we normalize and sum we're going to we normalize and sum we're going to we normalize and sum we're going to aggregate uh sort of their values aggregate uh sort of their values aggregate uh sort of their values depending on how interesting they find depending on how interesting they find depending on how interesting they find each other",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 805,
      "text": "and so that's the preview for each other and so that's the preview for each other",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 806,
      "text": "and so that's the preview for self attention and basically long story self attention and basically long story self attention and basically long story short from this entire section is that short from this entire section is that short from this entire section is that you can do weighted aggregations of your you can do weighted aggregations of your you can do weighted aggregations of your past past past Elements by having by using matrix Elements by having by using matrix Elements by having by using matrix multiplication of a lower triangular multiplication of a lower triangular multiplication of a lower triangular fashion and then the elements here in fashion and then the elements here in fashion and then the elements here in the lower triangular part are telling the lower triangular part are telling the lower triangular part are telling you how much of each element uh fuses you how much of each element uh fuses you how much of each element uh fuses into this position so we're going to use into this position so we're going to use into this position so we're going to use this trick now to develop the self this trick now to develop the self this trick now to develop the self attention block block so first let's get attention block block so first let's get attention block block so first let's get some quick preliminaries out of the way some quick preliminaries out of the way some quick preliminaries out of the way first the thing",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 807,
      "text": "I'm kind of bothered by first the thing I'm kind of bothered by first the thing I'm kind of bothered by is that you see how we're passing in",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 808,
      "text": "is that you see how we're passing in is that you see how we're passing in vocap size into the Constructor there's vocap size into the Constructor there's vocap size into the Constructor there's no need to do that because vocap size is no need to do that because vocap size is no need to do that because vocap size is already defined uh up top as a global already defined uh up top as a global already defined uh up top as a global variable so there's no need to pass this variable so there's no need to pass this variable so there's no need to pass this stuff stuff stuff around next what I want to do is I don't around next what I want to do is I don't around next what I want to do is I don't want to actually create I want to create want to actually create I want to create want to actually create I want to create like a level of indirection here where like a level of indirection here where like a level of indirection here where we don't directly go to the embedding we don't directly go to the embedding we don't directly go to the embedding for the um logits",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 809,
      "text": "but instead we go for the um logits",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 810,
      "text": "but instead we go for the um logits",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 811,
      "text": "but instead we go through this intermediate phase because through this intermediate phase because through this intermediate phase because we're going to start making that bigger we're going to start making that bigger we're going to start making that bigger so let me introduce a new variable n",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 812,
      "text": "so let me introduce a new variable n",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 813,
      "text": "so let me introduce a new variable n embed it shorted for number of embedding embed it shorted for number of embedding embed it shorted for number of embedding Dimensions so Dimensions so Dimensions so nbed here will be say 32 that was a nbed here will be say 32 that was a nbed here will be say 32 that was a suggestion from GitHub co-pilot by the suggestion from GitHub co-pilot by the suggestion from GitHub co-pilot by the way um it also suest 32 which is a good way um it also suest 32 which is a good way um it also suest 32 which is a good number",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 814,
      "text": "so this is an embedding table and number so this is an embedding table and number so this is an embedding table and only 32 dimensional only 32 dimensional only 32 dimensional embeddings",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 815,
      "text": "so then here this is not embeddings so then here this is not embeddings",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 816,
      "text": "so then here this is not going to give us logits directly instead going to give us logits directly instead going to give us logits directly instead this is going to give us token this is going to give us token this is going to give us token embeddings that's I'm going to call it embeddings that's I'm going to call it embeddings that's I'm going to call it and then to go from the token Tings to and then to go from the token Tings to and then to go from the token Tings to the logits we're going to need a linear the logits we're going to need a linear the logits we're going to need a linear layer so self.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 817,
      "text": "LM head let's call it layer so self.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 818,
      "text": "LM head let's call it layer so self.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 819,
      "text": "LM head let's call it short for language modeling head is n short for language modeling head is n short for language modeling head is n and linear from n ined up to vocap size and linear from n ined up to vocap size and linear from n ined up to vocap size",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 820,
      "text": "and then when we swing over here we're",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 821,
      "text": "and then when we swing over here we're",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 822,
      "text": "and then when we swing over here we're actually going to get the loits by actually going to get the loits by actually going to get the loits by exactly what the co-pilot says now we exactly what the co-pilot says now we exactly what the co-pilot says now we have to be careful here because this C have to be careful here because this C have to be careful here because this C and this C are not equal um this is nmed and this C are not equal um this is nmed and this C are not equal um this is nmed C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 823,
      "text": "and this is vocap size so let's just C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 824,
      "text": "and this is vocap size so let's just C",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 825,
      "text": "and this is vocap size so let's just say that n ined is equal to say that n ined is equal to say that n ined is equal to C and then this just creates one spous C and then this just creates one spous C and then this just creates one spous layer of interaction through a linear layer of interaction through a linear layer of interaction through a linear layer but uh this should basically run so we see that this runs and uh this run so we see that this runs and uh this currently looks kind of spous",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 826,
      "text": "but uh currently looks kind of spous",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 827,
      "text": "but uh currently looks kind of spous",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 828,
      "text": "but uh we're going to build on top of this now we're going to build on top of this now we're going to build on top of this now next up so far we've taken these indices next up so far we've taken these indices next up so far we've taken these indices and we've encoded them based on the",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 829,
      "text": "and we've encoded them based on the and we've encoded them based on the identity of the uh tokens in inside idx identity of the uh tokens in inside idx identity of the uh tokens in inside idx the next thing that people very often do the next thing that people very often do the next thing that people very often do is that we're not just encoding the is that we're not just encoding the is that we're not just encoding the identity of these tokens but also their identity of these tokens but also their identity of these tokens but also their position so we're going to have a second position so we're going to have a second position so we're going to have a second position uh embedding table here so position uh embedding table here so position uh embedding table here so self.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 830,
      "text": "position embedding table is an an self.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 831,
      "text": "position embedding table is an an self.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 832,
      "text": "position embedding table is an an embedding of block size by an embed and embedding of block size by an embed and embedding of block size by an embed and so each position from zero to block size so each position from zero to block size so each position from zero to block size minus one will also get its own minus one will also get its own minus one will also get its own embedding vector and then here first let embedding vector and then here first let embedding vector and then here first let me decode B BYT from idx do me decode B BYT from idx do me decode B BYT from idx do shape and then here we're also going to shape and then here we're also going to shape and then here we're also going to have a pause embedding which is the have a pause embedding which is the have a pause embedding which is the positional embedding and these are this positional embedding and these are this positional embedding and these are this is to arrange so this will be basically is to arrange so this will be basically is to arrange so this will be basically just integers from Z to T minus one and just integers from Z to T minus one and just integers from Z to T minus one and all of those integers from 0 to T minus all of those integers from 0 to T minus all of those integers from 0 to T minus one get embedded through the table to one get embedded through the table to one get embedded through the table to create a t by create a t by create a t by C and then here this gets renamed to C and then here this gets renamed to C and then here this gets renamed to just say x and x will be the addition of just say x and x will be the addition of just say x and x will be the addition of the token embeddings with the positional the token embeddings with the positional the token embeddings with the positional embeddings and here the broadcasting embeddings and here the broadcasting embeddings and here the broadcasting note will work out so B by T by C plus T note will work out so B by T by C plus T note will work out so B by T by C plus T by C by C by C this gets right aligned a new dimension this gets right aligned a new dimension this gets right aligned a new dimension of one gets added and it gets of one gets added and it gets of one gets added and it gets broadcasted across broadcasted across broadcasted across batch so at this point x holds not just batch so at this point x holds not just batch so at this point x holds not just the token identities but the positions the token identities but the positions the token identities but the positions at which these tokens occur and this is at which these tokens occur and this is at which these tokens occur and this is currently not that useful because of currently not that useful because of currently not that useful because of course we just have a simple byr model course we just have a simple byr model course we just have a simple byr model so it doesn't matter if you're in",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 833,
      "text": "the so it doesn't matter if you're in the so it doesn't matter if you're in the fifth position the second position or fifth position the second position or fifth position the second position or wherever it's all translation invariant wherever it's all translation invariant wherever it's all translation invariant at this stage uh so this information at this stage uh so this information at this stage uh so this information currently wouldn't help uh but as we currently wouldn't help uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 834,
      "text": "but as we currently wouldn't help uh but as we work on the self attention block we'll work on the self attention block we'll work on the self attention block we'll see that this starts to matter",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 835,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 836,
      "text": "so now we get the Crux of self",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 837,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 838,
      "text": "so now we get the Crux of self attention so this is probably the most attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 839,
      "text": "so this is probably the most attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 840,
      "text": "so this is probably the most important part of this video to important part of this video to important part of this video to understand we're going to implement a understand we're going to implement a understand we're going to implement a small self attention for a single small self attention for a single small self attention for a single individual head as they're called so we individual head as they're called so we individual head as they're called so we start off with where we were so all of start off with where we were so all of start off with where we were so all of this code is familiar",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 841,
      "text": "so right now I'm this code is familiar",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 842,
      "text": "so right now I'm this code is familiar",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 843,
      "text": "so right now I'm working with an example where I Chang working with an example where I Chang working with an example where I Chang the number of channels from 2 to 32 so the number of channels from 2 to 32 so the number of channels from 2 to 32",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 844,
      "text": "so we have a 4x8 arrangement of tokens and we have a 4x8 arrangement of tokens and we have a 4x8 arrangement of tokens and each to and the information each token each to and the information each token each to and the information each token is currently 32 dimensional",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 845,
      "text": "but we just is currently 32 dimensional",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 846,
      "text": "but we just is currently 32 dimensional",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 847,
      "text": "but we just are working with random are working with random are working with random numbers now we saw here that the code as numbers now we saw here that the code as numbers now we saw here that the code as we had it before does a uh simple weight we had it before does a uh simple weight we had it before does a uh simple weight simple average of all the past tokens simple average of all the past tokens simple average of all the past tokens and the current token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 848,
      "text": "so it's just the and the current token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 849,
      "text": "so it's just the and the current token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 850,
      "text": "so it's just the previous information and current previous information and current previous information and current information is just being mixed together information is just being mixed together information is just being mixed together in an average",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 851,
      "text": "and that's what this code in an average",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 852,
      "text": "and that's what this code in an average",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 853,
      "text": "and that's what this code currently achieves and it Doo by currently achieves and it Doo by currently achieves and it Doo by creating this lower triangular structure creating this lower triangular structure creating this lower triangular structure which allows us to mask out this uh we which allows us to mask out this uh we which allows us to mask out this uh we uh Matrix that we create so we mask it uh Matrix that we create so we mask it uh Matrix that we create so we mask it out",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 854,
      "text": "and then we normalize it and out",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 855,
      "text": "and then we normalize it and out",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 856,
      "text": "and then we normalize it and currently when we initialize the currently when we initialize the currently when we initialize the affinities between all the different affinities between all the different affinities between all the different sort of tokens or nodes I'm going to use sort of tokens or nodes I'm going to use sort of tokens or nodes I'm going to use those terms those terms those terms interchangeably so when we initialize interchangeably so when we initialize interchangeably so when we initialize the affinities between all the different the affinities between all the different the affinities between all the different tokens to be zero then we see that way tokens to be zero",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 857,
      "text": "then we see that way tokens to be zero then we see that way gives us this um structure where every gives us this um structure where every gives us this um structure where every single row has these um uniform numbers single row has these um uniform numbers single row has these um uniform numbers",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 858,
      "text": "and so that's what that's what then",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 859,
      "text": "uh and so that's what that's what then uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 860,
      "text": "and so that's what that's what then uh in this Matrix multiply makes it so that in this Matrix multiply makes it so that in this Matrix multiply makes it so that we're doing a simple we're doing a simple we're doing a simple average now we don't actually want this average now we don't actually want this average now we don't actually want this to be all uniform because different uh to be all uniform because different uh to be all uniform because different uh tokens will find different other tokens tokens will find different other tokens tokens will find different other tokens more or less interesting and we want more or less interesting and we want more or less interesting and we want that to be data dependent so for example that to be data dependent so for example that to be data dependent so for example if I'm a vowel",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 861,
      "text": "then maybe I'm looking if I'm a vowel",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 862,
      "text": "then maybe I'm looking if I'm a vowel",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 863,
      "text": "then maybe I'm looking for consonants in my past and maybe I for consonants in my past and maybe I for consonants in my past",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 864,
      "text": "and maybe I want to know what those consonants are want to know what those consonants are want to know what those consonants are and I want that information to flow to",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 865,
      "text": "and I want that information to flow to and I want that information to flow to me",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 866,
      "text": "and so I want to now gather me",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 867,
      "text": "and so I want to now gather me",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 868,
      "text": "and so I want to now gather information from the past",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 869,
      "text": "but I want to information from the past",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 870,
      "text": "but I want to information from the past",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 871,
      "text": "but I want to do it in the data dependent way and this do it in the data dependent way and this do it in the data dependent way",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 872,
      "text": "and this is the problem that self attention is the problem that self attention is the problem that self attention solves now the way self attention solves solves now the way self attention solves solves now the way self attention solves this is the following every single node this is the following every single node this is the following every single node or every single token at each position or every single token at each position or every single token at each position will emit two vectors it will emit a will emit two vectors it will emit a will emit two vectors it will emit a query and it will emit a query and it will emit a query and it will emit a key now the query Vector roughly key now the query Vector roughly key now the query Vector roughly speaking is what am I looking for and speaking is what am I looking for and speaking is what am I looking for and the key Vector roughly speaking is what the key Vector roughly speaking is what the key Vector roughly speaking is what do I do",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 873,
      "text": "I do I contain",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 874,
      "text": "and then the way we get contain",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 875,
      "text": "and then the way we get contain",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 876,
      "text": "and then the way we get affinities between these uh tokens now affinities between these uh tokens now affinities between these uh tokens now in a sequence is we basically just do a in a sequence is we basically just do a in a sequence is we basically just do a do product between the keys and the do product between the keys and the do product between the keys and the queries so my query dot products with queries",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 877,
      "text": "so my query dot products with queries so my query dot products with all the keys of all the other tokens and all the keys of all the other tokens and all the keys of all the other tokens and that dot product now becomes that dot product now becomes that dot product now becomes wayy and so um",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 878,
      "text": "if the key and the query wayy",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 879,
      "text": "and so um if the key and the query wayy",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 880,
      "text": "and so um if the key and the query are sort of aligned they will interact are sort of aligned they will interact are sort of aligned they will interact to a very high amount",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 881,
      "text": "and then I will to a very high amount",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 882,
      "text": "and then I will to a very high amount",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 883,
      "text": "and then I will get to learn more about that specific get to learn more about that specific get to learn more about that specific token as opposed to any other token in token as opposed to any other token in token as opposed to any other token in the sequence the sequence the sequence",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 884,
      "text": "so let's implement this now we're going to implement a now we're going to implement a single what's called head of self single what's called head of self single what's called head of self attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 885,
      "text": "so this is just one head attention so this is just one head attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 886,
      "text": "so this is just one head there's a hyper parameter involved with there's a hyper parameter involved with there's a hyper parameter involved with these heads which is the head size and these heads which is the head size and these heads which is the head size",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 887,
      "text": "and then here I'm initializing linear",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 888,
      "text": "then here I'm initializing linear",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 889,
      "text": "then here I'm initializing linear modules and I'm using bias equals false modules and I'm using bias equals false modules and I'm using bias equals false so these are just going to apply a so these are just going to apply a so these are just going to apply a matrix multiply with some fixed matrix multiply with some fixed matrix multiply with some fixed weights and now let me produce a key and weights and now let me produce a key and weights and now let me produce a key and q k and Q by forwarding these modules on q k and Q by forwarding these modules on q k and Q by forwarding these modules on X",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 890,
      "text": "so the size of this will now X so the size of this will now X",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 891,
      "text": "so the size of this will now become B by T by 16 because that is the become B by T by 16 because that is the become B by T by 16 because that is the head size and the same here B by T by 16",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 892,
      "text": "so this being the head size",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 893,
      "text": "so you 16 so this being the head size so you see here that when I forward this linear see here that when I forward this linear see here that when I forward this linear on top of my X all the tokens in all the on top of my X all the tokens in all the on top of my X all the tokens in all the positions in the B BYT Arrangement all positions in the B BYT Arrangement all positions in the B BYT Arrangement all of them them in parallel and of them them in parallel and of them them in parallel and independently produce a key and a query independently produce a key and a query independently produce a key and a query",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 894,
      "text": "so no communication has happened so no communication has happened so no communication has happened yet but the communication comes now all yet but the communication comes now all yet but the communication comes now all the queries will do product with all the the queries will do product with all the the queries will do product with all the keys so basically what we want is we keys so basically what we want is we keys so basically what we want is we want way now or the affinities between want way now or the affinities between want way now or the affinities between these to be query multiplying key",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 895,
      "text": "but we these to be query multiplying key",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 896,
      "text": "but we these to be query multiplying key",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 897,
      "text": "but we have to be careful with uh we can't have to be careful with uh we can't have to be careful with uh we can't Matrix multiply this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 898,
      "text": "we actually need to Matrix multiply this we actually need to Matrix multiply this we actually need to transpose uh K",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 899,
      "text": "but we have to be also transpose uh K",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 900,
      "text": "but we have to be also transpose uh K",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 901,
      "text": "but we have to be also careful because these are when you have careful because these are when you have careful because these are when you have The Bash Dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 902,
      "text": "so in particular we The Bash Dimension so in particular we The Bash Dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 903,
      "text": "so in particular we want to transpose uh the last two want to transpose uh the last two want to transpose uh the last two dimensions dimension1 and dimension -2",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 904,
      "text": "dimensions dimension1 and dimension -2 dimensions dimension1 and dimension -2",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 905,
      "text": "so -21",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 906,
      "text": "and so this Matrix multiply now will -21",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 907,
      "text": "and so this Matrix multiply now will basically do the following B by T by basically do the following B by T by basically do the following B by T by 16 Matrix multiplies B by 16 by T to 16 Matrix multiplies B by 16 by T to 16 Matrix multiplies B by 16 by T to give us B by T by give us B by T by give us B by T by T right T",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 908,
      "text": "right T",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 909,
      "text": "right so for every row of B we're now going to so for every row of B",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 910,
      "text": "we're now going to so for every row of B we're now going to have a t Square Matrix giving us the have a t Square Matrix giving us the have a t Square Matrix giving us the affinities and these are now the way so affinities and these are now the way so affinities and these are now the way so they're not zeros they are now coming they're not zeros they are now coming they're not zeros they are now coming from this dot product between the keys from this dot product between the keys from this dot product between the keys and the queries so this can now run I and the queries so this can now run I and the queries so this can now run I can I can run this and the weighted can I can run this and the weighted can I can run this and the weighted aggregation now is a function in a data aggregation now is a function in a data aggregation now is a function in a data Bandon manner between the keys and Bandon manner between the keys and Bandon manner between the keys and queries of these nodes so just queries of these nodes so just queries of these nodes so just inspecting what happened inspecting what happened inspecting what happened here the way takes on this form here the way takes on this form here the way takes on this form and you see that before way was uh just and you see that before way was uh just and you see that before way was uh just a constant",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 911,
      "text": "so it was applied in the same a constant so it was applied in the same a constant",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 912,
      "text": "so it was applied in the same way to all the batch elements but now way to all the batch elements but now way to all the batch elements but now every single batch elements will have every single batch elements will have every single batch elements will have different sort of we because uh every different sort of we because uh every different sort of we because uh every single batch element contains different single batch element contains different single batch element contains different uh tokens at different positions and so uh tokens at different positions and so uh tokens at different positions and so this is not data dependent",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 913,
      "text": "so when we this is not data dependent so when we this is not data dependent",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 914,
      "text": "so when we look at just the zeroth uh Row for look at just the zeroth uh Row for look at just the zeroth uh Row for example in the input these are the example in the input these are the example in the input these are the weights that came out and so you can see weights that came out and so you can see weights that came out and so you can see now that they're not just exactly now that they're not just exactly now that they're not just exactly uniform um and in particular as an uniform um and in particular as an uniform um and in particular as an example here for the last row this was example here for the last row this was example here for the last row this was the eighth token and the eighth token the eighth token and the eighth token the eighth token and the eighth token knows what content it has and it knows knows what content it has and it knows knows what content it has and it knows at what position it's in and now the E at what position it's in and now the E at what position it's in and now the E token based on that uh creates a query token based on that uh creates a query token based on that uh creates a query",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 915,
      "text": "hey I'm looking for this kind of stuff",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 916,
      "text": "hey I'm looking for this kind of stuff",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 917,
      "text": "hey I'm looking for this kind of stuff um I'm a vowel I'm on the E position I'm um I'm a vowel I'm on the E position I'm um I'm a vowel I'm on the E position I'm looking for any consonant at positions looking for any consonant at positions looking for any consonant at positions up to four",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 918,
      "text": "and then all the nodes get to up to four and then all the nodes get to up to four and then all the nodes get to emit keys and maybe one of the channels emit keys and maybe one of the channels emit keys and maybe one of the channels could be I am a I am a consonant and I could be I am a I am a consonant and I could be I am a I am a consonant and I am in a position up to four and that am in a position up to four and that am in a position up to four and that that key would have a high number in that key would have a high number in that key would have a high number in that specific Channel and that's how the that specific Channel",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 919,
      "text": "and that's how the that specific Channel",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 920,
      "text": "and that's how the query and the key when they do product query and the key when they do product query and the key when they do product they can find each other and create a they can find each other and create a they can find each other and create a high affinity and when they have a high high affinity and when they have a high high affinity and when they have a high Affinity like say uh this token was Affinity like say uh this token was Affinity like say uh this token was pretty interesting to uh to this eighth pretty interesting to uh to this eighth pretty interesting to uh to this eighth token when they have a high Affinity token when they have a high Affinity token when they have a high Affinity then through the softmax I will end up then through the softmax I will end up then through the softmax I will end up aggregating a lot of its information aggregating a lot of its information aggregating a lot of its information into my position",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 921,
      "text": "and so I'll get to into my position",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 922,
      "text": "and so I'll get to into my position",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 923,
      "text": "and so I'll get to learn a lot about learn a lot about learn a lot about it now just this we're looking at way it now just this we're looking at way it now just this we're looking at way after this has already happened um let after this has already happened um let after this has already happened um let me erase this operation as well so let me erase this operation as well so let me erase this operation as well so let me erase the masking and the softmax me erase the masking and the softmax me erase the masking and the softmax just to show you the under the hood just to show you the under the hood just to show you the under the hood internals and how that works so without internals and how that works so without internals and how that works so without the masking in the softmax Whey comes the masking in the softmax Whey comes the masking in the softmax Whey comes out like this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 924,
      "text": "right this is the outputs out like this right this is the outputs out like this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 925,
      "text": "right this is the outputs of the do products um and these are the of the do products um and these are the of the do products um and these are the raw outputs",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 926,
      "text": "and they take on values from raw outputs and they take on values from raw outputs and they take on values from negative you know two to positive two negative",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 927,
      "text": "you know two to positive two negative",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 928,
      "text": "you know two to positive two Etc so that's the raw interactions and Etc so that's the raw interactions and Etc so that's the raw interactions and raw affinities between all the nodes but raw affinities between all the nodes but raw affinities between all the nodes",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 929,
      "text": "but now if I'm going if I'm a fifth node",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 930,
      "text": "I now if I'm going if I'm a fifth node",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 931,
      "text": "I now if I'm going if I'm a fifth node",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 932,
      "text": "I will not want to aggregate anything from will not want to aggregate anything from will not want to aggregate anything from the sixth node seventh node and the the sixth node seventh node and the the sixth node seventh node and the eighth node",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 933,
      "text": "so actually we use the upper eighth node",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 934,
      "text": "so actually we use the upper eighth node",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 935,
      "text": "so actually we use the upper triangular masking so those are not triangular masking so those are not triangular masking so those are not allowed to allowed to allowed to communicate",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 936,
      "text": "and now we actually want to communicate and now we actually want to communicate and now we actually want to have a nice uh distribution",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 937,
      "text": "uh so we have a nice uh distribution uh so we have a nice uh distribution uh so we don't want to aggregate negative .11 of don't want to aggregate negative .11 of don't want to aggregate negative .11 of this node that's crazy",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 938,
      "text": "so instead we this node that's crazy",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 939,
      "text": "so instead we this node that's crazy",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 940,
      "text": "so instead we exponentiate and normalize and now we exponentiate and normalize and now we exponentiate and normalize and now we get a nice distribution that sums to one get a nice distribution that sums to one get a nice distribution that sums to one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 941,
      "text": "and this is telling us now in the data and this is telling us now in the data and this is telling us now in the data dependent manner how much of information dependent manner how much of information dependent manner how much of information to aggregate from any of these tokens in to aggregate from any of these tokens in to aggregate from any of these tokens in the the the past",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 942,
      "text": "so that's way",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 943,
      "text": "and it's not zeros past",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 944,
      "text": "so that's way",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 945,
      "text": "and it's not zeros past",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 946,
      "text": "so that's way",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 947,
      "text": "and it's not zeros anymore",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 948,
      "text": "but but it's calculated in this anymore",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 949,
      "text": "but but it's calculated in this anymore",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 950,
      "text": "but but it's calculated in this way now there's one more uh part to a way now there's one more uh part to a way now there's one more uh part to a single self attention head and that is single self attention head and that is single self attention head and that is that when we do the aggregation we don't that when we do the aggregation we don't that when we do the aggregation we don't actually aggregate the tokens exactly we actually aggregate the tokens exactly we actually aggregate the tokens exactly we aggregate we produce one more value here aggregate we produce one more value here aggregate we produce one more value here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 951,
      "text": "and we call that the and we call that the and we call that the value so in the same way that we value so in the same way that we value so in the same way that we produced p and query we're also going to produced p and query we're also going to produced p and query we're also going to create a value create a value create a value",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 952,
      "text": "and",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 953,
      "text": "and and then here we don't then here we don't then here we don't aggregate X we calculate a v which is aggregate X we calculate a v which is aggregate X we calculate a v which is just achieved by uh propagating this just achieved by uh propagating this just achieved by uh propagating this linear on top of X again",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 954,
      "text": "and then we linear on top of X again",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 955,
      "text": "and then we linear on top of X again",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 956,
      "text": "and then we output way multiplied by V",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 957,
      "text": "so V is the output way multiplied by V",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 958,
      "text": "so V is the output way multiplied by V",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 959,
      "text": "so V is the elements that we aggregate or the the elements that we aggregate or the the elements that we aggregate or the the vectors that we aggregate instead of the vectors that we aggregate instead of the vectors that we aggregate instead of the raw raw raw X and now of course uh this will make it X and now of course uh this will make it X and now of course uh this will make it so that the output here of this single so that the output here of this single so that the output here of this single head will be 16 dimensional because that head will be 16 dimensional because that head will be 16 dimensional because that is the head is the head is the head size so you can think of X as kind of size so you can think of X as kind of size so you can think of X as kind of like private information to this token like private information to this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 960,
      "text": "token like private information to this token if you if you think about it that way",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 961,
      "text": "so if you if you think about it that way",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 962,
      "text": "so if you if you think about it that way so X is kind of private to this token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 963,
      "text": "so X is kind of private to this token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 964,
      "text": "so X is kind of private to this token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 965,
      "text": "so I'm a fifth token at some",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 966,
      "text": "and I have I'm a fifth token at some",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 967,
      "text": "and I have I'm a fifth token at some",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 968,
      "text": "and I have some identity and uh my information is some identity and uh my information is some identity and uh my information is kept in Vector X and now for the kept in Vector X and now for the kept in Vector X and now for the purposes of the single head here's what purposes of the single head here's what purposes of the single head here's what I'm interested in here's what I have",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 969,
      "text": "and I'm interested in here's what I have",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 970,
      "text": "and I'm interested in here's what I have",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 971,
      "text": "and if you find me interesting here's what I if you find me interesting here's what I if you find me interesting here's what I will communicate to you and that's will communicate to you and that's will communicate to you and that's stored in v and so V is the thing that stored in v and so V is the thing that stored in v and so V is the thing that gets aggregated for the purposes of this gets aggregated for the purposes of this gets aggregated for the purposes of this single head between the different single head between the different single head between the different notes and that's uh basically the self notes and that's uh basically the self notes and that's uh basically the self attention mechanism this is this is what attention mechanism this is this is what attention mechanism this is this is what it does there are a few notes",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 972,
      "text": "that I",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 973,
      "text": "it does there are a few notes",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 974,
      "text": "that I",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 975,
      "text": "it does there are a few notes that I would make like to make about attention would make like to make about attention would make like to make about attention number one attention is a communication number one attention is a communication number one attention is a communication mechanism you can really think about it mechanism you can really think about it mechanism you can really think about it as a communication mechanism where you as a communication mechanism where you as a communication mechanism where you have a number of nodes in a directed have a number of nodes in a directed have a number of nodes in a directed graph where basically you have edges graph where basically you have edges graph where basically you have edges pointed between noes like pointed between noes like pointed between noes like this and what happens is every node has this and what happens is every node has this and what happens is every node has some Vector of information and it gets some Vector of information and it gets some Vector of information and it gets to aggregate information via a weighted to aggregate information via a weighted to aggregate information via a weighted sum from all of the nodes that point to sum from all of the nodes that point to sum from all of the nodes that point to it and this is done in a data dependent it and this is done in a data dependent it and this is done in a data dependent manner",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 976,
      "text": "so depending on whatever data is manner",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 977,
      "text": "so depending on whatever data is manner so depending on whatever data is actually stored that you should not at actually stored that you should not at actually stored that you should not at any point in time now our graph doesn't any point in time now our graph doesn't any point in time now our graph doesn't look like this our graph has a different look like this our graph has a different look like this our graph has a different structure we have eight nodes because structure we have eight nodes because structure we have eight nodes because the block size is eight and there's the block size is eight and there's the block size is eight and there's always eight to always eight to always eight to tokens and uh the first node is only tokens and uh the first node is only tokens and uh the first node is only pointed to by itself the second node is pointed to by itself the second node is pointed to by itself the second node is pointed to by the first node and itself pointed to by the first node and itself pointed to by the first node and itself all the way up to the eighth node which all the way up to the eighth node which all the way up to the eighth node which is pointed to by all the previous nodes is pointed to by all the previous nodes is pointed to by all the previous nodes and itself",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 978,
      "text": "and so that's the structure and itself",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 979,
      "text": "and so that's the structure and itself",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 980,
      "text": "and so that's the structure that our directed graph has or happens that our directed graph has or happens that our directed graph has or happens happens to have in Auto regressive sort happens to have in Auto regressive sort happens to have in Auto regressive sort of scenario like language modeling but of scenario like language modeling but of scenario like language modeling but in principle attention can be applied to in principle attention can be applied to in principle attention can be applied to any arbitrary directed graph and it's any arbitrary directed graph and it's any arbitrary directed graph and it's just a communication mechanism between just a communication mechanism between just a communication mechanism between the nodes the second note is that notice the nodes the second note is that notice the nodes the second note is that notice that there is no notion of space so that there is no notion of space so that there is no notion of space so attention simply acts over like a set of attention simply acts over like a set of attention simply acts over like a set of vectors in this graph",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 981,
      "text": "and so by default vectors in this graph and so by default vectors in this graph and so by default these nodes have no idea where they are these nodes have no idea where they are these nodes have no idea where they are positioned in the space and that's why positioned in the space and that's why positioned in the space and that's why we need to encode them positionally and we need to encode them positionally and we need to encode them positionally and sort of give them some information that sort of give them some information that sort of give them some information that is anchored to a specific position so is anchored to a specific position so is anchored to a specific position so that they sort of know where they are that they sort of know where they are that they sort of know where they are and this is different than for example",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 982,
      "text": "and this is different than for example and this is different than for example from convolution because if you're run from convolution because if you're run from convolution because if you're run for example a convolution operation over for example a convolution operation over for example a convolution operation over some input there's a very specific sort some input there's a very specific sort some input there's a very specific sort of layout of the information in space of layout of the information in space of layout of the information in space and the convolutional filters sort of and the convolutional filters sort of and the convolutional filters sort of act in space and so it's it's not like act in space",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 983,
      "text": "and so it's it's not like act in space",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 984,
      "text": "and so it's it's not like an attention in ATT ention is just a set an attention in ATT ention is just a set an attention in ATT ention is just a set of vectors out there in space they of vectors out there in space they of vectors out there in space they communicate and if you want them to have communicate and if you want them to have communicate and if you want them to have a notion of space you need to a notion of space you need to a notion of space you need to specifically add it which is what we've specifically add it which is what we've specifically add it which is what we've done when we calculated the um relative done when we calculated the um relative done when we calculated the um relative the positional encode encodings and the positional encode encodings and the positional encode encodings and added that information to the vectors added that information to the vectors added that information to the vectors the next thing that I hope is very clear the next thing that I hope is very clear the next thing that I hope is very clear is that the elements across the batch is that the elements across the batch is that the elements across the batch Dimension which are independent examples Dimension which are independent examples Dimension which are independent examples never talk to each other they're always never talk to each other they're always never talk to each other they're always processed independently and this is a processed independently and this is a processed independently and this is a batched matrix multiply that applies batched matrix multiply that applies batched matrix multiply that applies basically a matrix multiplication uh basically a matrix multiplication uh basically a matrix multiplication uh kind of in parallel across the batch kind of in parallel across the batch kind of in parallel across the batch dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 985,
      "text": "so maybe it would be more dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 986,
      "text": "so maybe it would be more dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 987,
      "text": "so maybe it would be more accurate to say that in this analogy of accurate to say that in this analogy of accurate to say that in this analogy of a directed graph we really have because a directed graph we really have because a directed graph we really have because the back size is four we really have the back size is four we really have the back size is four we really have four separate pools of eight nodes and four separate pools of eight nodes and four separate pools of eight nodes and those eight nodes only talk to each those eight nodes only talk to each those eight nodes only talk to each other but in total there's like 32 nodes other but in total there's like 32 nodes other but in total there's like 32 nodes that are being processed uh but there's that are being processed uh but there's that are being processed uh but there's um sort of four separate pools of eight um sort of four separate pools of eight um sort of four separate pools of eight you can look at it that way the next you can look at it that way the next you can look at it that way the next note is that here in the case of note is that here in the case of note is that here in the case of language modeling uh we have this language modeling uh we have this language modeling uh we have this specific uh structure of directed graph specific uh structure of directed graph specific uh structure of directed graph where the future tokens will not where the future tokens will not where the future tokens will not communicate to the Past tokens but this communicate to the Past tokens but this communicate to the Past tokens but this doesn't necessarily have to be the doesn't necessarily have to be the doesn't necessarily have to be the constraint in the general case and in constraint in the general case and in constraint in the general case and in fact in many cases you may want to have fact in many cases you may want to have fact in many cases you may want to have all of the uh noes talk to each other uh all of the uh noes talk to each other uh all of the uh noes talk to each other uh fully so as an example if you're doing fully so as an example if you're doing fully so as an example if you're doing sentiment analysis or something like sentiment analysis or something like sentiment analysis or something like that with a Transformer you might have a that with a Transformer you might have a that with a Transformer you might have a number of tokens and you may want to number of tokens and you may want to number of tokens and you may want to have them all talk to each other fully have them all talk to each other fully have them all talk to each other fully because later you are predicting for because later you are predicting for because later you are predicting for example the sentiment of the sentence example the sentiment of the sentence example the sentiment of the sentence and so it's okay for these NOS to talk",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 988,
      "text": "and so it's okay for these NOS to talk",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 989,
      "text": "and so it's okay for these NOS to talk to each other and so in those cases you to each other and so in those cases you to each other and so in those cases you will use an encoder block of self will use an encoder block of self will use an encoder block of self attention and uh all it means that it's attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 990,
      "text": "and uh all it means that it's attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 991,
      "text": "and uh all it means that it's an encoder block is that you will delete an encoder block is that you will delete an encoder block is that you will delete this line of code allowing all the noes this line of code allowing all the noes this line of code allowing all the noes to completely talk to each other what to completely talk to each other what to completely talk to each other what we're implementing here is sometimes we're implementing here is sometimes we're implementing here is sometimes called a decoder block",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 992,
      "text": "and it's called a called a decoder block",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 993,
      "text": "and it's called a called a decoder block",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 994,
      "text": "and it's called a decoder because it is sort of like a decoder because it is sort of like a decoder because it is sort of like a decoding language",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 995,
      "text": "and it's got this decoding language",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 996,
      "text": "and it's got this decoding language",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 997,
      "text": "and it's got this autor regressive format where you have autor regressive format where you have autor regressive format where you have to mask with the Triangular Matrix so to mask with the Triangular Matrix so to mask with the Triangular Matrix so that uh nodes from the future never talk that uh nodes from the future never talk that uh nodes from the future never talk to the Past because they would give away to the Past because they would give away to the Past because they would give away the answer the answer the answer and so basically in encoder blocks you and so basically in encoder blocks you",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 998,
      "text": "and so basically in encoder blocks you would delete this allow all the noes to would delete this allow all the noes to would delete this allow all the noes to talk in decoder blocks this will always talk in decoder blocks this will always talk in decoder blocks this will always be present so that you have this be present so that you have this be present so that you have this triangular structure",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 999,
      "text": "uh but both are triangular structure uh but both are triangular structure",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1000,
      "text": "uh but both are allowed and attention doesn't care allowed and attention doesn't care allowed and attention doesn't care attention supports arbitrary attention supports arbitrary attention supports arbitrary connectivity between nodes the next connectivity between nodes the next connectivity between nodes the next thing I wanted to comment on is you keep thing I wanted to comment on is you keep thing I wanted to comment on is you keep me you keep hearing me say attention me",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1001,
      "text": "you keep hearing me say attention me",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1002,
      "text": "you keep hearing me say attention self attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1003,
      "text": "Etc there's actually also self attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1004,
      "text": "Etc there's actually also self attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1005,
      "text": "Etc there's actually also something called cross attention what is something called cross attention what is something called cross attention what is the the difference difference difference",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1006,
      "text": "so basically the reason this attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1007,
      "text": "so basically the reason this attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1008,
      "text": "so basically the reason this attention is self attention is because because the is self attention is because because the is self attention is because because the keys queries and the values are all keys queries and the values are all keys queries and the values are all coming from the same Source from X so coming from the same Source from X so coming from the same Source from X",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1009,
      "text": "so the same Source X produces Keys queries the same Source X produces Keys queries the same Source X produces Keys queries and values so these nodes are self and values so these nodes are self and values so these nodes are self attending but in principle attention is attending but in principle attention is attending but in principle attention is much more General than that so for much more General than that so for much more General than that so for example an encoder decoder Transformers example an encoder decoder Transformers example an encoder decoder Transformers uh you can have a case where the queries uh you can have a case where the queries uh you can have a case where the queries are produced from X but the keys and the are produced from X but the keys and the are produced from X but the keys and the values come from a whole separate values come from a whole separate values come from a whole separate external source and sometimes from uh external source and sometimes from uh external source and sometimes from uh encoder blocks that encode some context encoder blocks that encode some context encoder blocks that encode some context that we'd like to condition on that we'd like to condition on that we'd like to condition on and so the keys and the values will and so the keys and the values will and so the keys and the values will actually come from a whole separate actually come from a whole separate actually come from a whole separate Source those are nodes on the side and Source those are nodes on the side and Source those are nodes on the side",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1010,
      "text": "and here we're just producing queries",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1011,
      "text": "and here we're just producing queries",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1012,
      "text": "and here we're just producing queries and we're reading off information from the we're reading off information from the we're reading off information from the side so cross attention is used when side so cross attention is used when side so cross attention is used when there's a separate source of nodes we'd there's a separate source of nodes we'd there's a separate source of nodes we'd like to pull information from into our like to pull information from into our like to pull information from into our nodes and it's self attention if we just nodes and it's self attention if we just nodes and it's self attention if we just have nodes that would like to look at have nodes that would like to look at have nodes that would like to look at each other and talk to each other so each other and talk to each other so each other and talk to each other so this attention here happens to be self this attention here happens to be self this attention here happens to be self attention but in principle um attention attention but in principle um attention attention but in principle um attention is a lot more General",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1013,
      "text": "okay and the last is a lot more General",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1014,
      "text": "okay and the last is a lot more General",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1015,
      "text": "okay and the last note at this stage is if we come to the note at this stage is if we come to the note at this stage is if we come to the attention is all need paper here we've attention is all need paper here we've attention is all need paper here we've already implemented attention so given already implemented attention so given already implemented attention so given query key and value we've U multiplied query key and value we've U multiplied query key and value we've U multiplied the query and a key we've soft maxed it the query and a key we've soft maxed it the query and a key we've soft maxed it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1016,
      "text": "and then we are aggregating the values and then we are aggregating the values and then we are aggregating the values there's one more thing that we're there's one more thing that we're there's one more thing that we're missing here which is the dividing by missing here which is the dividing by missing here which is the dividing by one / square root of the head size the one / square root of the head size the one / square root of the head size the DK here is the head size why are they DK here is the head size why are they DK here is the head size why are they doing this finds this important",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1017,
      "text": "so they doing this finds this important so they doing this finds this important",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1018,
      "text": "so they call it the scaled attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1019,
      "text": "and it's call it the scaled attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1020,
      "text": "and it's call it the scaled attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1021,
      "text": "and it's kind of like an important normalization kind of like an important normalization kind of like an important normalization to basically to basically to basically have the problem is if you have unit gsh have the problem is if you have unit gsh have the problem is if you have unit gsh and inputs so zero mean unit variance K and inputs so zero mean unit variance K and inputs so zero mean unit variance K and Q are unit gashin then if you just and Q are unit gashin then if you just and Q are unit gashin",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1022,
      "text": "then if you just do we naively then you see that your",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1023,
      "text": "we do we naively then you see that your",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1024,
      "text": "we do we naively then you see that your we actually will be uh the variance will be actually will be uh the variance will be actually will be uh the variance will be on the order of head size which in our on the order of head size which in our on the order of head size which in our case is 16",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1025,
      "text": "but if you multiply by one case is 16",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1026,
      "text": "but if you multiply by one case is 16",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1027,
      "text": "but if you multiply by one over head size square root so this is over head size square root so this is over head size square root",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1028,
      "text": "so this is square root and this is one square root and this is one square root and this is one over then the variance of we will be one over then the variance of we will be one over then the variance of we will be one so it will be so it will be so it will be preserved now why is this important preserved now why is this important preserved now why is this important you'll not notice that way you'll not notice that way you'll not notice that way here will feed into here will feed into here will feed into softmax",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1029,
      "text": "and so it's really important softmax",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1030,
      "text": "and so it's really important softmax",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1031,
      "text": "and so it's really important especially at initialization that we be especially at initialization that we be especially at initialization that we be fairly diffuse so in our case here we fairly diffuse so in our case here we fairly diffuse so in our case here we sort of locked out here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1032,
      "text": "and we had a sort of locked out here and we had a sort of locked out here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1033,
      "text": "and we had a fairly diffuse numbers here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1034,
      "text": "so um like fairly diffuse numbers here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1035,
      "text": "so um like fairly diffuse numbers here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1036,
      "text": "so um like this now the problem is that because of this now the problem is that because of this now the problem is that because of softmax if weight takes on very positive softmax if weight takes on very positive softmax if weight takes on very positive and very negative numbers inside it and very negative numbers inside it and very negative numbers inside it softmax will actually converge towards softmax will actually converge towards softmax will actually converge towards one hot vectors and so I can illustrate one hot vectors and so I can illustrate one hot vectors and so I can illustrate that here um say we are applying softmax that here um say we are applying softmax that here um say we are applying softmax to a tensor of values that are very to a tensor of values that are very to a tensor of values that are very close to zero then we're going to get a close to zero then we're going to get a close to zero then we're going to get a diffuse thing out of diffuse thing out of diffuse thing out of softmax",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1037,
      "text": "but the moment I take the exact softmax but the moment I take the exact softmax but the moment I take the exact same thing",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1038,
      "text": "and I start sharpening it same thing",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1039,
      "text": "and I start sharpening it same thing",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1040,
      "text": "and I start sharpening it making it bigger by multiplying these making it bigger by multiplying these making it bigger by multiplying these numbers by eight for example you'll see numbers by eight for example you'll see numbers by eight for example you'll see that the softmax will start to sharpen that the softmax will start to sharpen that the softmax will start to sharpen and in fact it will sharpen towards the and in fact it will sharpen towards the and in fact it will sharpen towards the max so it will sharpen towards whatever max so it will sharpen towards whatever max so it will sharpen towards whatever number here is the highest and so um number here is the highest and so um number here is the highest and so um basically we don't want these values to basically we don't want these values to basically we don't want these values to be too extreme especially at be too extreme especially at be too extreme especially at initialization otherwise softmax will be initialization otherwise softmax will be initialization otherwise softmax will be way too peaky and um you're basically way too peaky and um you're basically way too peaky and um you're basically aggregating um information from like a aggregating um information from like a aggregating um information from like a single node every node just agregates single node every node just agregates single node every node just agregates information from a single other node information from a single other node information from a single other node that's not what we want especially at that's not what we want especially at that's not what we want especially at initialization and so the scaling is initialization and so the scaling is initialization and so the scaling is used just to control the variance at used just to control the variance at used just to control the variance at initialization",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1041,
      "text": "okay so having said all initialization",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1042,
      "text": "okay so having said all initialization",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1043,
      "text": "okay so having said all that let's now take our self attention that let's now take our self attention that let's now take our self attention knowledge and let's uh take it for a knowledge and let's uh take it for a knowledge and let's uh take it for a spin so here in the code I created this spin so here in the code I created this spin so here in the code I created this head module and it implements a single head module and it implements a single head module and it implements a single head of self attention so you give it a head of self attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1044,
      "text": "so you give it a head of self attention so you give it a head size",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1045,
      "text": "and then here it creates the head size",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1046,
      "text": "and then here it creates the head size",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1047,
      "text": "and then here it creates the key query and the value linear layers key query and the value linear layers key query and the value linear layers typically people don't use biases in typically people don't use biases in typically people don't use biases in these",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1048,
      "text": "uh so those are the linear these uh so those are the linear these uh so those are the linear projections that we're going to apply to projections that we're going to apply to projections that we're going to apply to all of our nodes now here I'm creating all of our nodes now here I'm creating all of our nodes now here I'm creating this Trill variable Trill is not a this Trill variable Trill is not a this Trill variable Trill is not a parameter of the module so in sort of parameter of the module",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1049,
      "text": "so in sort of parameter of the module so in sort of pytorch naming conventions uh this is pytorch naming conventions uh this is pytorch naming conventions uh this is called a buffer it's not a parameter and called a buffer it's not a parameter and called a buffer it's not a parameter and you have to call it you have to assign you have to call it you have to assign you have to call it you have to assign it to the module using a register buffer it to the module using a register buffer it to the module using a register buffer so that creates the trill uh the triang so that creates the trill uh the triang so that creates the trill uh the triang lower triangular Matrix and we're given lower triangular Matrix and we're given lower triangular Matrix and we're given the input X this should look very the input X",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1050,
      "text": "this should look very the input X this should look very familiar now we calculate the keys the familiar now we calculate the keys the familiar now we calculate the keys the queries we C calculate the attention queries we C calculate the attention queries we C calculate the attention scores inside way uh we normalize it so scores inside way uh we normalize it so scores inside way uh we normalize it so we're using scaled attention here then we're using scaled attention here then we're using scaled attention here then we make sure that uh future doesn't we make sure that uh future doesn't we make sure that uh future doesn't communicate with the past so this makes communicate with the past so this makes communicate with the past",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1051,
      "text": "so this makes it a decoder block and then softmax and it a decoder block and then softmax and it a decoder block and then softmax and then aggregate the value and then aggregate the value and then aggregate the value and output then here in the language model output then here in the language model output then here in the language model I'm creating a head in the Constructor",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1052,
      "text": "I'm creating a head in the Constructor",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1053,
      "text": "I'm creating a head in the Constructor",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1054,
      "text": "and I'm calling it self attention head",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1055,
      "text": "and I'm calling it self attention head",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1056,
      "text": "and I'm calling it self attention head and the head size I'm going to keep as and the head size I'm going to keep as and the head size I'm going to keep as the same and embed just for the same and embed just for the same and embed just for now and then here once we've encoded the now and then here once we've encoded the now and then here once we've encoded the information with the token embeddings information with the token embeddings information with the token embeddings and the position embeddings we're simply and the position embeddings we're simply and the position embeddings we're simply going to feed it into the self attention going to feed it into the self attention going to feed it into the self attention head and then the output of that is head and then the output of that is head and then the output of that is going to go into uh the decoder language going to go into uh the decoder language going to go into uh the decoder language modeling head and create the logits so modeling head and create the logits so modeling head and create the logits so this the sort of the simplest way to this the sort of the simplest way to this the sort of the simplest way to plug in a self attention component uh plug in a self attention component uh plug in a self attention component uh into our Network right now I had to make into our Network right now I had to make into our Network right now I had to make one more change which is that here in one more change which is that here in one more change which is that here in the generate uh we have to make sure the generate uh we have to make sure the generate uh we have to make sure that our idx that we feed into the model that our idx that we feed into the model that our idx that we feed into the model because now we're using positional because now we're using positional because now we're using positional embeddings we can never have more than embeddings we can never have more than embeddings we can never have more than block size coming in because if idx is block size coming in because if idx is block size coming in because if idx is more than block size then our position more than block size then our position more than block size then our position embedding table is going to run out of embedding table is going to run out of embedding table is going to run out of scope because it only has embeddings for scope because it only has embeddings for scope because it only has embeddings for up to block size",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1057,
      "text": "and so therefore I up to block size",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1058,
      "text": "and so therefore I up to block size",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1059,
      "text": "and so therefore I added some uh code here to crop the added some uh code here to crop the added some uh code here to crop the context that we're going to feed into context that we're going to feed into context that we're going to feed into self um so that uh we never pass in more self um so that uh we never pass in more self um so that uh we never pass in more than block siiz elements than block siiz elements than block siiz elements so those are the changes and let's Now so those are the changes and let's Now so those are the changes and let's Now train the network",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1060,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1061,
      "text": "so I also came up train the network",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1062,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1063,
      "text": "so I also came up train the network",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1064,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1065,
      "text": "so I also came up to the script here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1066,
      "text": "and I decreased the to the script here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1067,
      "text": "and I decreased the to the script here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1068,
      "text": "and I decreased the learning rate because uh the self learning rate because uh the self learning rate because uh the self attention can't tolerate very very high attention can't tolerate very very high attention can't tolerate very very high learning rates",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1069,
      "text": "and then I also increased learning rates",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1070,
      "text": "and then I also increased learning rates",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1071,
      "text": "and then I also increased number of iterations because the number of iterations because the number of iterations because the learning rate is lower",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1072,
      "text": "and then I learning rate is lower",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1073,
      "text": "and then I learning rate is lower",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1074,
      "text": "and then I trained it and previously we were only trained it and previously we were only trained it and previously we were only able to get to up to 2.5 and now we are able to get to up to 2.5 and now we are able to get to up to 2.5 and now we are down to 2.4 so we definitely see a down to 2.4 so we definitely see a down to 2.4 so we definitely see a little bit of an improvement from 2.5 to little bit of an improvement from 2.5 to little bit of an improvement from 2.5 to 2.4 roughly uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1075,
      "text": "but the text is still not 2.4 roughly uh but the text is still not 2.4 roughly",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1076,
      "text": "uh but the text is still not amazing so clearly the self attention amazing so clearly the self attention amazing so clearly the self attention head is doing some useful communication head is doing some useful communication head is doing some useful communication",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1077,
      "text": "but um we still have a long way to go",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1078,
      "text": "but um we still have a long way to go",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1079,
      "text": "but um we still have a long way to go okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1080,
      "text": "so now we've implemented the scale.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1081,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1082,
      "text": "so now we've implemented the scale.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1083,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1084,
      "text": "so now we've implemented the scale.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1085,
      "text": "product attention now next up and the product attention now next up and the product attention now next up and the attention is all you need paper there's attention is all you need paper there's attention is all you need paper there's something called multi-head attention something called multi-head attention something called multi-head attention and what is multi-head attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1086,
      "text": "it's and what is multi-head attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1087,
      "text": "it's and what is multi-head attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1088,
      "text": "it's just applying multiple attentions in just applying multiple attentions in just applying multiple attentions in parallel and concatenating their results parallel and concatenating their results parallel and concatenating their results so they have a little bit of diagram",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1089,
      "text": "so they have a little bit of diagram",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1090,
      "text": "so they have a little bit of diagram here I don't know if this is super clear here I don't know if this is super clear here I don't know if this is super clear it's really just multiple attentions in it's really just multiple attentions in it's really just multiple attentions in parallel",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1091,
      "text": "so let's Implement that fairly parallel",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1092,
      "text": "so let's Implement that fairly parallel",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1093,
      "text": "so let's Implement that fairly straightforward straightforward straightforward if we want a multi-head attention then if we want a multi-head attention then if we want a multi-head attention then we want multiple heads of self attention we want multiple heads of self attention we want multiple heads of self attention running in parallel so in pytorch we can running in parallel so in pytorch we can running in parallel so in pytorch we can do this by simply creating multiple do this by simply creating multiple do this by simply creating multiple heads so however heads how however many heads so however heads how however many heads so however heads how however many heads you want and then what is the head heads you want and then what is the head heads you want and then what is the head size of each",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1094,
      "text": "and then we run all of them size of each",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1095,
      "text": "and then we run all of them size of each",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1096,
      "text": "and then we run all of them in parallel into a list and simply in parallel into a list and simply in parallel into a list and simply concatenate all of the outputs and we're concatenate all of the outputs and we're concatenate all of the outputs",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1097,
      "text": "and we're concatenating over the channel concatenating over the channel concatenating over the channel Dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1098,
      "text": "so the way this looks now is Dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1099,
      "text": "so the way this looks now is Dimension",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1100,
      "text": "so the way this looks now is we don't have just a single ATT we don't have just a single ATT we don't have just a single ATT that uh has a hit size of 32 because that uh has a hit size of 32 because that uh has a hit size of 32 because remember n Ed is remember n Ed is remember n Ed is 32 instead of having one Communication 32 instead of having one Communication 32 instead of having one Communication channel we now have four communication channel we now have four communication channel we now have four communication channels in parallel and each one of channels in parallel and each one of channels in parallel and each one of these communication channels typically these communication channels typically these communication channels typically will be uh smaller uh correspondingly so will be uh smaller uh correspondingly so will be uh smaller uh correspondingly so because we have four communication because we have four communication because we have four communication channels we want eight dimensional self channels we want eight dimensional self channels we want eight dimensional self attention and so from each Communication attention and so from each Communication attention and so from each Communication channel we're going to together eight channel we're going to together eight channel we're going to together eight dimensional vectors and then we have dimensional vectors and then we have dimensional vectors and then we have four of them and that concatenates to four of them and that concatenates to four of them and that concatenates to give us 32 which is the original and give us 32 which is the original and give us 32 which is the original and embed and so this is kind of similar to embed",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1101,
      "text": "and so this is kind of similar to embed",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1102,
      "text": "and so this is kind of similar to um if you're familiar with convolutions um if you're familiar with convolutions um if you're familiar with convolutions this is kind of like a group convolution this is kind of like a group convolution this is kind of like a group convolution uh because basically instead of having uh because basically instead of having uh because basically instead of having one large convolution we do convolution one large convolution we do convolution one large convolution we do convolution in groups and uh that's multi-headed in groups and uh that's multi-headed in groups and uh that's multi-headed self self self attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1103,
      "text": "and so then here we just use attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1104,
      "text": "and so then here we just use attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1105,
      "text": "and so then here we just use essay heads self attention heads instead essay heads self attention heads instead essay heads self attention heads instead now I actually ran it and uh scrolling now I actually ran it and uh scrolling now I actually ran it and uh scrolling down I ran the same thing",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1106,
      "text": "and then we down I ran the same thing",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1107,
      "text": "and then we down I ran the same thing",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1108,
      "text": "and then we now get this down to 2.28 roughly and now get this down to 2.28 roughly and now get this down to 2.28 roughly and the output is still the generation is the output is still the generation is the output is still the generation is still not amazing",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1109,
      "text": "but clearly the still not amazing",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1110,
      "text": "but clearly the still not amazing",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1111,
      "text": "but clearly the validation loss is improving because we validation loss is improving because we validation loss is improving because we were at 2.4 just now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1112,
      "text": "and so it helps to were at 2.4 just now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1113,
      "text": "and so it helps to were at 2.4 just now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1114,
      "text": "and so it helps to have multiple communication channels have multiple communication channels have multiple communication channels because obviously these tokens have a because obviously these tokens have a because obviously these tokens have a lot to talk about they want to find the lot to talk about they want to find the lot to talk about they want to find the consonants the vowels they want to find consonants the vowels they want to find consonants the vowels they want to find the vowels just from certain positions the vowels just from certain positions the vowels just from certain positions uh they want to find any kinds of uh they want to find any kinds of uh they want to find any kinds of different things",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1115,
      "text": "and so it helps to different things",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1116,
      "text": "and so it helps to different things",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1117,
      "text": "and so it helps to create multiple independent channels of create multiple independent channels of create multiple independent channels of communication gather lots of different communication gather lots of different communication gather lots of different types of data and then uh decode the types of data and then uh decode the types of data and then uh decode the output now going back to the paper for a output now going back to the paper for a output now going back to the paper for a second of course",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1118,
      "text": "I didn't explain this second of course",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1119,
      "text": "I didn't explain this second of course",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1120,
      "text": "I didn't explain this figure in full detail but we are figure in full detail but we are figure in full detail but we are starting to see some components of what starting to see some components of what starting to see some components of what we've already implemented we have the we've already implemented we have the we've already implemented we have the positional encodings the token encodings positional encodings the token encodings positional encodings the token encodings that add we have the masked multi-headed that add we have the masked multi-headed that add we have the masked multi-headed attention implemented now here's another attention implemented now here's another attention implemented now here's another multi-headed attention which is a cross multi-headed attention which is a cross multi-headed attention which is a cross attention to an encoder which we haven't attention to an encoder which we haven't attention to an encoder which we haven't we're not going to implement in this we're not going to implement in this we're not going to implement in this case I'm going to come back to that case I'm going to come back to that case I'm going to come back to that later but I want you to notice that later",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1121,
      "text": "but I want you to notice that later",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1122,
      "text": "but I want you to notice that there's a feed forward part here and there's a feed forward part here and there's a feed forward part here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1123,
      "text": "and then this is grouped into a block that then this is grouped into a block that then this is grouped into a block that gets repeat it again and again now the gets repeat it again and again now the gets repeat it again and again now the feedforward part here is just a simple feedforward part here is just a simple feedforward part here is just a simple uh multi-layer perceptron uh multi-layer perceptron uh multi-layer perceptron",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1124,
      "text": "um so the multi-headed so here position um so the multi-headed so here position um so the multi-headed so here position wise feed forward networks is just a wise feed forward networks is just a wise feed forward networks is just a simple little MLP",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1125,
      "text": "so I want to start simple little MLP",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1126,
      "text": "so I want to start simple little MLP",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1127,
      "text": "so I want to start basically in a similar fashion also basically in a similar fashion also basically in a similar fashion also adding computation into the network and adding computation into the network and adding computation into the network and this computation is on a per node level this computation is on a per node level this computation is on a per node level",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1128,
      "text": "so I've already implemented it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1129,
      "text": "and you",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1130,
      "text": "so I've already implemented it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1131,
      "text": "and you",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1132,
      "text": "so I've already implemented it and you can see the diff highlighted on the left can see the diff highlighted on the left can see the diff highlighted on the left here when I've added or changed things here when I've added or changed things here when I've added or changed things now before we had the self multi-headed now before we had the self multi-headed now before we had the self multi-headed self attention that did the self attention that did the self attention that did the communication",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1133,
      "text": "but we went way too fast communication",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1134,
      "text": "but we went way too fast communication",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1135,
      "text": "but we went way too fast to calculate the logits so the tokens to calculate the logits so the tokens to calculate the logits so the tokens looked at each other but didn't really looked at each other but didn't really looked at each other but didn't really have a lot of time to think on what they have a lot of time to think on what they have a lot of time to think on what they found from the other tokens and so what found from the other tokens and so what found from the other tokens and so what I've implemented here is a little feet I've implemented here is a little feet I've implemented here is a little feet forward single layer and this little forward single layer and this little forward single layer and this little layer is just a linear followed by a Rel layer is just a linear followed by a Rel layer is just a linear followed by a Rel nonlinearity and that's that's it so nonlinearity and that's that's it so nonlinearity",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1136,
      "text": "and that's that's it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1137,
      "text": "so it's just a little layer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1138,
      "text": "and then I call it's just a little layer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1139,
      "text": "and then I call it's just a little layer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1140,
      "text": "and then I call it feed it feed it feed forward um and embed forward um and embed forward um and embed",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1141,
      "text": "and then this feed forward is just",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1142,
      "text": "and then this feed forward is just",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1143,
      "text": "and then this feed forward is just called sequentially right after the self called sequentially right after the self called sequentially right after the self attention so we self attend then we feed attention so we self attend then we feed attention so we self attend then we feed forward and you'll notice that the feet forward",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1144,
      "text": "and you'll notice that the feet forward",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1145,
      "text": "and you'll notice that the feet forward here when it's applying linear forward here when it's applying linear forward here when it's applying linear",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1146,
      "text": "this is on a per token level all the this is on a per token level all the this is on a per token level all the tokens do this independently so the self tokens do this independently so the self tokens do this independently so the self attention is the communication and then attention is the communication and then attention is the communication and then once they've gathered all the data now once they've gathered all the data now once they've gathered all the data now they need to think on that data they need to think on that data they need to think on that data individually and so that's what feed individually and so that's what feed individually and so that's what feed forward is doing",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1147,
      "text": "and that's why I've forward is doing",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1148,
      "text": "and that's why I've forward is doing",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1149,
      "text": "and that's why I've added it here now when I train this the added it here now when I train this the added it here now when I train this the validation LW actually continues to go validation LW actually continues to go validation LW actually continues to go down now to 2. 24 which is down from down now to 2. 24 which is down from down now to 2. 24 which is down from 2.28 uh the output still look kind of 2.28 uh the output still look kind of 2.28 uh the output still look kind of terrible but at least we've improved the terrible but at least we've improved the terrible but at least we've improved the situation and so as a preview we're situation",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1150,
      "text": "and so as a preview we're situation",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1151,
      "text": "and so as a preview we're going to now start to intersperse the going to now start to intersperse the going to now start to intersperse the communication with the computation and communication with the computation and communication with the computation",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1152,
      "text": "and that's also what the Transformer does that's also what the Transformer does that's also what the Transformer does when it has blocks that communicate and when it has blocks that communicate and when it has blocks that communicate and then compute and it groups them and then compute and it groups them and then compute and it groups them and replicates them",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1153,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1154,
      "text": "so let me show you replicates them",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1155,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1156,
      "text": "so let me show you replicates them",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1157,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1158,
      "text": "so let me show you what we'd like to do we'd like to do what we'd like to do we'd like to do what we'd like to do we'd like to do something like this we have a block and something like this we have a block and something like this we have a block",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1159,
      "text": "and this block is is basically this part this block is is basically this part this block is is basically this part here except for the cross here except for the cross here except for the cross attention now the block basically attention now the block basically attention now the block basically intersperses communication and then intersperses communication and then intersperses communication and then computation the computation the computation the computation the computation the computation the communication is done using multi-headed communication is done using multi-headed communication is done using multi-headed selfelf attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1160,
      "text": "and then the selfelf attention and then the selfelf attention and then the computation is done using a feed forward computation is done using a feed forward computation is done using a feed forward Network on all the tokens Network on all the tokens Network on all the tokens independently now what I've added here independently now what I've added here independently now what I've added here also is you'll also is you'll also is you'll notice this takes the number of notice this takes the number of notice this takes the number of embeddings in the embedding Dimension embeddings in the embedding Dimension embeddings in the embedding Dimension and number of heads that we would like and number of heads that we would like and number of heads that we would like which is kind of like group size in which is kind of like group size in which is kind of like group size in group convolution and and I'm saying group convolution and and I'm saying group convolution and and I'm saying that number of heads we'd like is four that number of heads we'd like is four that number of heads we'd like is four and so because this is 32 we calculate and so because this is 32 we calculate and so because this is 32 we calculate that because this is 32 the number of that because this is 32 the number of that because this is 32 the number of heads should be four um the head size heads should be four um the head size heads should be four um the head size should be eight so that everything sort should be eight so that everything sort should be eight so that everything sort of works out Channel wise um so this is of works out Channel wise um so this is of works out Channel wise um so this is how the Transformer structures uh sort how the Transformer structures uh sort how the Transformer structures uh sort of the uh the sizes typically so the of the uh the sizes typically so the of the uh the sizes typically so the head size will become eight and then head size will become eight and then head size will become eight and then this is how we want to intersperse them this is how we want to intersperse them this is how we want to intersperse them and then here I'm trying to create and then here I'm trying to create and then here I'm trying to create blocks which is just a sequential blocks which is just a sequential blocks which is just a sequential application of block block block so that application of block block block so that application of block block block so that we're interspersing communication feed we're interspersing communication feed we're interspersing communication feed forward many many times and then finally forward many many times and then finally forward many many times and then finally we decode now I actually tried to run we decode now I actually tried to run we decode now I actually tried to run this and the problem is this doesn't this and the problem is this doesn't this and the problem is this doesn't actually give a very good uh answer and actually give a very good uh answer and actually give a very good uh answer and very good result and the reason for that very good result and the reason for that very good result and the reason for that is we're start starting to actually get is we're start starting to actually get is we're start starting to actually get like a pretty deep neural net and deep like a pretty deep neural net and deep like a pretty deep neural net and deep neural Nets uh suffer from optimization neural Nets uh suffer from optimization neural Nets uh suffer from optimization issues and I think that's what we're issues",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1161,
      "text": "and I think that's what we're issues",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1162,
      "text": "and I think that's what we're kind of like slightly starting to run kind of like slightly starting to run kind of like slightly starting to run into so we need one more idea that we into so we need one more idea that we into so we need one more idea that we can borrow from the um Transformer paper can borrow from the um Transformer paper can borrow from the um Transformer paper to resolve those difficulties now there to resolve those difficulties now there to resolve those difficulties now there are two optimizations that dramatically are two optimizations that dramatically are two optimizations that dramatically help with the depth of these networks help with the depth of these networks help with the depth of these networks and make sure that the networks remain and make sure that the networks remain and make sure that the networks remain optimizable let's talk about the first optimizable let's talk about the first optimizable let's talk about the first one the first one in this diagram is you one the first one in this diagram is you one the first one in this diagram is you see this Arrow here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1163,
      "text": "and then this arrow see this Arrow here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1164,
      "text": "and then this arrow see this Arrow here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1165,
      "text": "and then this arrow and this Arrow those are skip and this Arrow those are skip and this Arrow those are skip connections or sometimes called residual connections or sometimes called residual connections or sometimes called residual connections they come from this paper",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1166,
      "text": "uh connections they come from this paper uh connections they come from this paper uh the presidual learning for image the presidual learning for image the presidual learning for image recognition from about recognition from about recognition from about 2015 uh that introduced the concept now 2015 uh that introduced the concept now 2015 uh that introduced the concept now these are basically what it means is you these are basically what it means is you these are basically what it means is you transform data",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1167,
      "text": "but then you have a skip transform data",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1168,
      "text": "but then you have a skip transform data",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1169,
      "text": "but then you have a skip connection with addition from the connection with addition from the connection with addition from the previous features now the way I like to previous features now the way I like to previous features now the way I like to visualize it uh that I prefer is the visualize it uh that I prefer is the visualize it uh that I prefer is the following here the computation happens following here the computation happens following here the computation happens from the top to bottom and basically you from the top to bottom and basically you from the top to bottom",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1170,
      "text": "and basically you have this uh residual pathway and you have this uh residual pathway and you have this uh residual pathway and you are free to Fork off from the residual are free to Fork off from the residual are free to Fork off from the residual pathway perform some computation and pathway perform some computation and pathway perform some computation and then project back to the residual then project back to the residual then project back to the residual pathway via addition",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1171,
      "text": "and so you go from pathway via addition and so you go from pathway via addition",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1172,
      "text": "and so you go from the the uh inputs to the targets only the the uh inputs to the targets only the the uh inputs to the targets only via plus and plus plus and the reason via plus and plus plus and the reason via plus and plus plus and the reason this is useful is because during back this is useful is because during back this is useful is because during back propagation remember from our microG propagation remember from our microG propagation remember from our microG grad video earlier addition distributes grad video earlier addition distributes grad video earlier addition distributes gradients equally to both of its gradients equally to both of its gradients equally to both of its branches that that fed as the input and branches that that fed as the input and branches that that fed as the input",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1173,
      "text": "and so the supervision or the gradients from so the supervision or the gradients from so the supervision or the gradients from the loss basically hop through every the loss basically hop through every the loss basically hop through every addition node all the way to the input addition node all the way to the input addition node all the way to the input and then also Fork off into the residual and then also Fork off into the residual and then also Fork off into the residual blocks but basically you have this blocks",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1174,
      "text": "but basically you have this blocks",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1175,
      "text": "but basically you have this gradient Super Highway that goes gradient Super Highway that goes gradient Super Highway that goes directly from the supervision all the directly from the supervision all the directly from the supervision all the way to the input unimpeded and then way to the input unimpeded and then way to the input unimpeded",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1176,
      "text": "and then these viral blocks are usually these viral blocks are usually these viral blocks are usually initialized in the beginning so they initialized in the beginning so they initialized in the beginning so they contribute very very little if anything contribute very very little if anything contribute very very little if anything to the residual pathway they they are to the residual pathway they they are to the residual pathway they they are initialized that way so in the beginning initialized that way so in the beginning initialized that way so in the beginning they are sort of almost kind of like not they are sort of almost kind of like not they are sort of almost kind of like not there",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1177,
      "text": "but then during the optimization there but then during the optimization there but then during the optimization they come online over time and they uh they come online over time",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1178,
      "text": "and they uh they come online over time and they uh start to contribute but at least at the start to contribute but at least at the start to contribute but at least at the initialization you can go from directly initialization you can go from directly initialization you can go from directly supervision to the input gradient is supervision to the input gradient is supervision to the input gradient is unimpeded and just flows and then the unimpeded and just flows and then the unimpeded and just flows and then the blocks over time blocks over time blocks over time kick in and so that dramatically helps kick in",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1179,
      "text": "and so that dramatically helps kick in and so that dramatically helps with the optimization so let's implement with the optimization so let's implement with the optimization so let's implement this so coming back to our block here this so coming back to our block here this so coming back to our block here basically what we want to do is we want basically what we want to do is we want basically what we want to do is we want to do xal to do xal to do xal X+ self attention and xal X+ self.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1180,
      "text": "feed X+ self attention and xal X+ self.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1181,
      "text": "feed X+ self attention and xal X+ self.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1182,
      "text": "feed forward",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1183,
      "text": "so this is X",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1184,
      "text": "and then we Fork forward so this is X",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1185,
      "text": "and then we Fork forward so this is X",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1186,
      "text": "and then we Fork off and do some communication and come off and do some communication and come off and do some communication and come back and we Fork off and we do some back and we Fork off and we do some back and we Fork off and we do some computation and come back so those are computation and come back so those are computation and come back so those are residual connections and then swinging residual connections and then swinging residual connections and then swinging back up here we also have to introd use back up here we also have to introd use back up here we also have to introd use this projection so nn.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1187,
      "text": "this projection so nn.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1188,
      "text": "this projection so nn. linear",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1189,
      "text": "and uh this is going to be linear",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1190,
      "text": "and uh this is going to be linear",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1191,
      "text": "and uh this is going to be from after we concatenate this this is from after we concatenate this this is from after we concatenate this this is the prze and embed so this is the output the prze and embed so this is the output the prze and embed so this is the output of the self tension itself",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1192,
      "text": "but then we of the self tension itself",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1193,
      "text": "but then we of the self tension itself",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1194,
      "text": "but then we actually want the uh to apply the actually want the uh to apply the actually want the uh to apply the projection and that's the projection and that's the projection and that's the result so the projection is just a result so the projection is just a result so the projection is just a linear transformation of the outcome of linear transformation of the outcome of linear transformation of the outcome of this this this layer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1195,
      "text": "so that's the projection back into layer so that's the projection back into layer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1196,
      "text": "so that's the projection back into the virual pathway and then here in a the virual pathway and then here in a the virual pathway",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1197,
      "text": "and then here in a feet forward it's going to be the same feet forward it's going to be the same feet forward it's going to be the same same thing I could have a a self doot same thing I could have a a self doot same thing I could have a a self doot projection here as well but let me just projection here as well but let me just projection here as well but let me just simplify it and let me uh couple it simplify it and let me uh couple it simplify it and let me uh couple it inside the same sequential container and inside the same sequential container and inside the same sequential container",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1198,
      "text": "and so this is the projection layer going so this is the projection layer going so this is the projection layer going back into the residual back into the residual back into the residual pathway and pathway and pathway and so that's uh well that's it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1199,
      "text": "so now we",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1200,
      "text": "so that's uh well that's it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1201,
      "text": "so now we",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1202,
      "text": "so that's uh well that's it so now we can train this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1203,
      "text": "so I implemented one more can train this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1204,
      "text": "so I implemented one more can train this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1205,
      "text": "so I implemented one more small change when you look into the small change when you look into the small change when you look into the paper again you see that the paper again you see that the paper again you see that the dimensionality of input and output is dimensionality of input and output is dimensionality of input and output is 512 for them",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1206,
      "text": "and they're saying that the 512 for them and they're saying that the 512 for them and they're saying that the inner layer here in the feet forward has inner layer here in the feet forward has inner layer here in the feet forward has dimensionality of 248 so there's a dimensionality of 248 so there's a dimensionality of 248 so there's a multiplier of four and so the inner multiplier of four and so the inner multiplier of four and so the inner layer of the feet forward Network should layer of the feet forward Network should layer of the feet forward Network should be multiplied by four in terms of be multiplied by four in terms of be multiplied by four in terms of Channel sizes so I came here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1207,
      "text": "and I Channel sizes so I came here and I Channel sizes",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1208,
      "text": "so I came here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1209,
      "text": "and I multiplied four times embed here for the multiplied four times embed here for the multiplied four times embed here for the feed forward and then from four times feed forward and then from four times feed forward and then from four times nmed coming back down to nmed when we go nmed coming back down to nmed when we go nmed coming back down to nmed when we go back to the pro uh to the projection so back to the pro uh to the projection so back to the pro uh to the projection so adding a bit of computation here and adding a bit of computation here and adding a bit of computation here and growing that layer that is in the growing that layer that is in the growing that layer that is in the residual block on the side of the residual block on the side of the residual block on the side of the residual residual residual pathway",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1210,
      "text": "and then I train this and we pathway",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1211,
      "text": "and then I train this and we pathway",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1212,
      "text": "and then I train this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1213,
      "text": "and we actually get down all the way to uh 2.08 actually get down all the way to uh 2.08 actually get down all the way to uh 2.08 validation loss and we also see that validation loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1214,
      "text": "and we also see that validation loss",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1215,
      "text": "and we also see that network is starting to get big enough network is starting to get big enough network is starting to get big enough that our train loss is getting ahead of that our train loss is getting ahead of that our train loss is getting ahead of validation loss so we're starting to see validation loss so we're starting to see validation loss so we're starting to see like a little bit of like a little bit of like a little bit of overfitting and um our our overfitting and um our our overfitting and um our our um uh Generations here are still not um uh Generations here are still not um uh Generations here are still not amazing but at least you see that we can amazing but at least you see that we can amazing but at least you see that we can see like is here this now grief syn like see like is here this now grief syn like see like is here this now grief syn like this starts to almost look like English this starts to almost look like English this starts to almost look like English",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1216,
      "text": "so um",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1217,
      "text": "yeah we're starting to really get",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1218,
      "text": "so",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1219,
      "text": "um yeah we're starting to really get",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1220,
      "text": "so",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1221,
      "text": "um yeah we're starting to really get there",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1222,
      "text": "okay and the second Innovation there",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1223,
      "text": "okay and the second Innovation there",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1224,
      "text": "okay and the second Innovation that is very helpful for optimizing very that is very helpful for optimizing very that is very helpful for optimizing very deep neural networks is right here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1225,
      "text": "so we deep neural networks is right here so we deep neural networks is right here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1226,
      "text": "so we have this addition now that's the have this addition now that's the have this addition now that's the residual part",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1227,
      "text": "but this Norm is referring residual part but this Norm is referring residual part but this Norm is referring to something called layer Norm so layer to something called layer Norm so layer to something called layer Norm so layer Norm is implemented in pytorch it's a Norm is implemented in pytorch it's a Norm is implemented in pytorch it's a paper that came out a while back here paper that came out a while back here paper that came out a while back here um and layer Norm is very very similar um and layer Norm is very very similar um and layer Norm is very very similar to bash Norm so remember back to our to bash Norm so remember back to our to bash Norm so remember back to our make more series part three we make more series part three we make more series part three we implemented bash implemented bash implemented bash normalization and uh bash normalization normalization and uh bash normalization normalization and uh bash normalization basically just made sure that um Across basically just made sure that um Across basically just made sure that um Across The Bash dimension any individual neuron The Bash dimension any individual neuron The Bash dimension any individual neuron had unit uh Gan um distribution",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1228,
      "text": "so it had unit uh Gan um distribution so it had unit uh Gan um distribution",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1229,
      "text": "so it was zero mean and unit standard was zero mean and unit standard was zero mean and unit standard deviation one standard deviation output deviation one standard deviation output deviation one standard deviation output",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1230,
      "text": "so what I did here is I'm copy pasting",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1231,
      "text": "so what I did here is I'm copy pasting",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1232,
      "text": "so what I did here is I'm copy pasting the bashor 1D that we developed in our the bashor 1D that we developed in our the bashor 1D that we developed in our make more series and see here we can make more series and see here we can make more series and see here we can initialize for example this module and initialize for example this module and initialize for example this module and we can have a batch of 32 100 we can have a batch of 32 100 we can have a batch of 32 100 dimensional vectors feeding through the dimensional vectors feeding through the dimensional vectors feeding through the bachor layer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1233,
      "text": "so what this does is it bachor layer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1234,
      "text": "so what this does is it bachor layer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1235,
      "text": "so what this does is it guarantees that when we look at just the guarantees that when we look at just the guarantees that when we look at just the zeroth column it's a zero mean one zeroth column it's a zero mean one zeroth column it's a zero mean one standard deviation so it's normalizing standard deviation so it's normalizing standard deviation so it's normalizing every single column of this uh input now every single column of this uh input now every single column of this uh input now the rows are not uh going to be the rows are not uh going to be the rows are not uh going to be normalized by default because we're just normalized by default because we're just normalized by default because we're just normalizing columns so let's now normalizing columns so let's now normalizing columns so let's now Implement layer Norm uh it's",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1236,
      "text": "very Implement layer Norm",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1237,
      "text": "uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1238,
      "text": "it's very Implement layer Norm uh it's very complicated look we come here we change complicated",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1239,
      "text": "look we come here we change complicated look we come here we change this from zero to one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1240,
      "text": "so we don't this from zero to one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1241,
      "text": "so we don't this from zero to one",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1242,
      "text": "so we don't normalize The Columns we normalize the normalize The Columns we normalize the normalize The Columns we normalize the rows and now we've implemented layer rows and now we've implemented layer rows and now we've implemented layer Norm Norm Norm so now the columns are not going to be so now the columns are not going to be so now the columns are not going to be normalized",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1243,
      "text": "um but the rows are going to normalized",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1244,
      "text": "um but the rows are going to normalized",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1245,
      "text": "um but the rows are going to be normalized for every individual be normalized for every individual be normalized for every individual example it's 100 dimensional Vector is example",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1246,
      "text": "it's 100 dimensional Vector is example",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1247,
      "text": "it's 100 dimensional Vector is normalized uh in this way and because normalized uh in this way and because normalized uh in this way and because our computation Now does not span across our computation Now does not span across our computation Now does not span across examples we can delete all of this examples we can delete all of this examples we can delete all of this buffers stuff uh because uh we can buffers stuff uh because uh we can buffers stuff uh because uh we can always apply this operation and don't always apply this operation and don't always apply this operation and don't need to maintain any running buffers so need to maintain any running buffers so need to maintain any running buffers so we don't need the we don't need the we don't need the buffers uh we buffers uh we buffers uh we don't There's no distinction between don't There's no distinction between don't There's no distinction between training and test training and test training and test time",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1248,
      "text": "uh and we don't need these running time",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1249,
      "text": "uh and we don't need these running time",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1250,
      "text": "uh and we don't need these running buffers we do keep gamma and beta we buffers we do keep gamma and beta we buffers we do keep gamma and beta we don't need the momentum we don't care if don't need the momentum we don't care if don't need the momentum we don't care if it's training or not and this is now a it's training or not",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1251,
      "text": "and this is now a it's training or not",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1252,
      "text": "and this is now a layer layer layer norm and it normalizes the rows instead norm and it normalizes the rows instead norm",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1253,
      "text": "and it normalizes the rows instead of the columns and this here is of the columns and this here is of the columns and this here is identical to basically this here so identical to basically this here so identical to basically this here so let's now Implement layer Norm in our let's now Implement layer Norm in our let's now Implement layer Norm in our Transformer before I incorporate the Transformer before I incorporate the Transformer before I incorporate the layer Norm",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1254,
      "text": "I just wanted to note that as layer Norm I just wanted to note that as layer Norm I just wanted to note that as I said very few details about the I said very few details about the I said very few details about the Transformer have changed in the last 5 Transformer have changed in the last 5 Transformer have changed in the last 5 years",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1255,
      "text": "but this is actually something years",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1256,
      "text": "but this is actually something years",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1257,
      "text": "but this is actually something that slightly departs from the original that slightly departs from the original that slightly departs from the original paper you see that the ADD and Norm is paper you see that the ADD and Norm is paper you see that the ADD and Norm is applied after the applied after the applied after the transformation but um in now it is a bit transformation",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1258,
      "text": "but um in now it is a bit transformation",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1259,
      "text": "but um in now it is a bit more uh basically common to apply the more uh basically common to apply the more uh basically common to apply the layer Norm before the transformation",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1260,
      "text": "so layer Norm before the transformation so layer Norm before the transformation so there's a reshuffling of the layer Norms there's a reshuffling of the layer Norms there's a reshuffling of the layer Norms uh so this is called the prorm uh so this is called the prorm uh so this is called the prorm formulation and that's the one that formulation and that's the one that formulation and that's the one that we're going to implement as well",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1261,
      "text": "so we're going to implement as well",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1262,
      "text": "so we're going to implement as well so select deviation from the original paper select deviation from the original paper select deviation from the original paper basically we need two layer Norms layer basically we need two layer Norms layer basically we need two layer Norms layer Norm one is uh NN do layer norm",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1263,
      "text": "and we Norm one is uh NN do layer norm and we Norm one is uh NN do layer norm and we tell it how many um what is the tell it how many um what is the tell it how many um what is the embedding Dimension and we need the embedding Dimension and we need the embedding Dimension and we need the second layer norm",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1264,
      "text": "and then here the second layer norm and then here the second layer norm",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1265,
      "text": "and then here the layer Norms are applied immediately on X layer Norms are applied immediately on X layer Norms are applied immediately on X so self.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1266,
      "text": "layer Norm one applied on X and so self.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1267,
      "text": "layer Norm one applied on X and so self.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1268,
      "text": "layer Norm one applied on X and self.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1269,
      "text": "layer Norm two applied on X before self.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1270,
      "text": "layer Norm two applied on X before self.",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1271,
      "text": "layer Norm two applied on X before it goes into self attention and feed it goes into self attention and feed it goes into self attention and feed forward and uh the size of the layer forward and uh the size of the layer forward and uh the size of the layer Norm here is an ed so 32 so when the Norm here is an ed so 32 so when the Norm here is an ed so 32 so when the layer Norm is normalizing our features layer Norm is normalizing our features layer Norm is normalizing our features it is uh the normalization here uh it is uh the normalization here uh it is uh the normalization here uh happens the mean and the variance are happens the mean and the variance are happens the mean and the variance are taken over 32 numbers so the batch and taken over 32 numbers so the batch and taken over 32 numbers so the batch and the time act as batch Dimensions both of the time act as batch Dimensions both of the time act as batch Dimensions both of them so this is kind of like a per token them",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1272,
      "text": "so this is kind of like a per token them",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1273,
      "text": "so this is kind of like a per token um transformation that just normalizes um transformation that just normalizes um transformation that just normalizes the features and makes them a unit mean the features and makes them a unit mean the features and makes them a unit mean uh unit Gan at uh unit Gan at uh unit Gan at initialization",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1274,
      "text": "but of course because initialization",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1275,
      "text": "but of course because initialization but of course because these layer Norms inside it have these these layer Norms inside it have these these layer Norms inside it have these gamma and beta training gamma and beta training gamma and beta training parameters uh the layer Norm will U parameters uh the layer Norm will U parameters uh the layer Norm will U eventually create outputs that might not eventually create outputs that might not eventually create outputs that might not be unit gion but the optimization will be unit gion but the optimization will be unit gion but the optimization will determine that so for now this is the uh determine that so for now this is the uh determine that so for now this is the uh this is incorporating the layer norms this is incorporating the layer norms this is incorporating the layer norms and let's train them on",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1276,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1277,
      "text": "so I let it and let's train them on",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1278,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1279,
      "text": "so I let it and let's train them on",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1280,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1281,
      "text": "so I let it run and we see that we get down to 2.06 run and we see that we get down to 2.06 run and we see that we get down to 2.06 which is better than the previous 2.08 which is better than the previous 2.08 which is better than the previous 2.08 so a slight Improvement by adding the so a slight Improvement by adding the so a slight Improvement by adding the layer norms and I'd expect that they layer norms and I'd expect that they layer norms and I'd expect that they help uh even more if we had bigger and help uh even more if we had bigger and help uh even more if we had bigger and deeper Network one more thing I forgot deeper Network one more thing I forgot deeper Network one more thing I forgot to add is that there should be a layer to add is that there should be a layer to add is that there should be a layer Norm here also typically as at the end Norm here also typically as at the end Norm here also typically as at the end of the Transformer and right before the of the Transformer and right before the of the Transformer and right before the final uh linear layer that decodes into final uh linear layer that decodes into final uh linear layer that decodes into vocabulary",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1282,
      "text": "so I added that as well so at vocabulary",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1283,
      "text": "so I added that as well so at vocabulary",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1284,
      "text": "so I added that as well so at this stage we actually have a pretty this stage we actually have a pretty this stage we actually have a pretty complete uh Transformer according to the complete uh Transformer according to the complete uh Transformer according to the original paper and it's a decoder only original paper and it's a decoder only original paper",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1285,
      "text": "and it's a decoder only Transformer I'll I'll talk about that in Transformer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1286,
      "text": "I'll I'll talk about that in Transformer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1287,
      "text": "I'll I'll talk about that in a second uh but at this stage uh the a second uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1288,
      "text": "but at this stage uh the a second",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1289,
      "text": "uh but at this stage uh the major pieces are in place so we can try major pieces are in place so we can try major pieces are in place so we can try to scale this up and see how well we can to scale this up and see how well we can to scale this up and see how well we can push this number now in order to scale push this number now in order to scale push this number now in order to scale out the model I had to perform some out the model I had to perform some out the model I had to perform some cosmetic changes here to make it nicer cosmetic changes here to make it nicer cosmetic changes here to make it nicer so I introduced this variable called",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1290,
      "text": "n so I introduced this variable called n",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1291,
      "text": "so I introduced this variable called n layer which just specifies how many layer which just specifies how many layer which just specifies how many layers of the blocks we're going to have layers of the blocks we're going to have layers of the blocks we're going to have I created a bunch of blocks and we have I created a bunch of blocks",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1292,
      "text": "and we have I created a bunch of blocks and we have a new variable number of heads as well I a new variable number of heads as well I a new variable number of heads as well I pulled out the layer Norm here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1293,
      "text": "and uh so pulled out the layer Norm here and uh so pulled out the layer Norm here and uh so this is identical now one thing that I this is identical now one thing that I this is identical now one thing that I did briefly change is I added a Dropout did briefly change is I added a Dropout did briefly change is I added a Dropout",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1294,
      "text": "so Dropout is something that you can add so Dropout is something that you can add so Dropout is something that you can add right before the residual connection right before the residual connection right before the residual connection back right before the connection back back right before the connection back back right before the connection back into the residual pathway so we can drop into the residual pathway so we can drop into the residual pathway so we can drop out that as l layer here we can drop out out that as l layer here we can drop out out that as l layer here we can drop out uh here at the end of the multi-headed uh here at the end of the multi-headed uh here at the end of the multi-headed exension as well and we can also drop exension as well and we can also drop exension as well and we can also drop out here uh when we calculate the um out here uh when we calculate the um out here uh when we calculate the um basically affinities and after the basically affinities and after the basically affinities and after the softmax we can drop out some of those so softmax we can drop out some of those so softmax we can drop out some of those so we can randomly prevent some of the we can randomly prevent some of the we can randomly prevent some of the nodes from nodes from nodes from communicating and so Dropout uh comes communicating and so Dropout uh comes communicating and so Dropout uh comes from this paper from 2014 or so and from this paper from 2014 or so and from this paper from 2014 or so and basically it takes your neural basically it takes your neural basically it takes your neural nut and it randomly every forward nut",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1295,
      "text": "and it randomly every forward nut",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1296,
      "text": "and it randomly every forward backward pass shuts off some subset of backward pass shuts off some subset of backward pass shuts off some subset of uh neurons so randomly drops them to uh neurons so randomly drops them to uh neurons so randomly drops them to zero and trains without them and what zero and trains without them and what zero and trains without them and what this does effectively is because the this does effectively is because the this does effectively is because the mask of what's being dropped out is mask of what's being dropped out is mask of what's being dropped out is changed every single forward backward changed every single forward backward changed every single forward backward pass it ends up kind of uh training an pass it ends up kind of uh training an pass it ends up kind of uh training an ensemble of sub networks and then at ensemble of sub networks and then at ensemble of sub networks and then at test time everything is fully enabled test time everything is fully enabled test time everything is fully enabled and kind of all of those sub networks and kind of all of those sub networks and kind of all of those sub networks are merged into a single Ensemble if you are merged into a single Ensemble if you are merged into a single Ensemble if you can if you want to think about it that can if you want to think about it that can if you want to think about it that way",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1297,
      "text": "so I would read the paper to get the way so I would read the paper to get the way so I would read the paper to get the full detail for now we're just going to full detail for now we're just going to full detail for now we're just going to stay on the level of this is a stay on the level of this is a stay on the level of this is a regularization technique and I added it regularization technique",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1298,
      "text": "and I added it regularization technique",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1299,
      "text": "and I added it because I'm about to scale up the model because I'm about to scale up the model because I'm about to scale up the model quite a bit",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1300,
      "text": "and I was concerned about quite a bit",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1301,
      "text": "and I was concerned about quite a bit",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1302,
      "text": "and I was concerned about overfitting so now when we scroll up to overfitting so now when we scroll up to overfitting so now when we scroll up to the top uh we'll see that I changed a the top",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1303,
      "text": "uh we'll see that I changed a the top uh we'll see that I changed a number of hyper parameters here about number of hyper parameters here about number of hyper parameters here about our neural nut",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1304,
      "text": "so I made the batch size our neural nut",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1305,
      "text": "so I made the batch size our neural nut",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1306,
      "text": "so I made the batch size be much larger now it's 64 I changed the be much larger now it's 64 I changed the be much larger now it's 64 I changed the block size to be 256 so previously it block size to be 256 so previously it block size to be 256 so previously it was just eight eight characters of was just eight eight characters of was just eight eight characters of context",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1307,
      "text": "now it is 256 characters of context now it is 256 characters of context",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1308,
      "text": "now it is 256 characters of context to predict the 257th context to predict the 257th context to predict the 257th uh I brought down the learning rate",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1309,
      "text": "a uh I brought down the learning rate a uh I brought down the learning rate a little bit because the neural net is now little bit because the neural net is now little bit because the neural net is now much bigger so I brought down the much bigger",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1310,
      "text": "so I brought down the much bigger",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1311,
      "text": "so I brought down the learning rate the embedding Dimension is learning rate the embedding Dimension is learning rate the embedding Dimension is now 384 and there are six heads so 384 now 384 and there are six heads so 384 now 384 and there are six heads so 384 divide 6 means that every head is 64 divide 6 means that every head is 64 divide 6 means that every head is 64 dimensional as it as a standard and then dimensional as it as a standard and then dimensional as it as a standard",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1312,
      "text": "and then there's going to be six layers of that there's going to be six layers of that there's going to be six layers of that and the Dropout will be at 02 so every and the Dropout will be at 02 so",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1313,
      "text": "every and the Dropout will be at 02 so every forward backward pass 20% of all of forward backward pass 20% of all of forward backward pass 20% of all of these um intermediate calculations are these um intermediate calculations are these um intermediate calculations are disabled and dropped to zero disabled and dropped to zero disabled and dropped to zero and then I already trained this and I",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1314,
      "text": "and then I already trained this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1315,
      "text": "and I",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1316,
      "text": "and then I already trained this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1317,
      "text": "and I ran it so uh drum roll how well does it ran it so uh drum roll how well does it ran it so uh drum roll how well does it perform so let me just scroll up perform so let me just scroll up perform so let me just scroll up here we get a validation loss of here we get a validation loss of here we get a validation loss of 1.48 which is actually quite a bit of an 1.48 which is actually quite a bit of an 1.48 which is actually quite a bit of an improvement on what we had before which improvement on what we had before which improvement on what we had before which I think was 2.07",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1318,
      "text": "so it went from 2.07 I think was 2.07",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1319,
      "text": "so it went from 2.07 I think was 2.07",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1320,
      "text": "so it went from 2.07 all the way down to 1.48 just by scaling all the way down to 1.48 just by scaling all the way down to 1.48 just by scaling up this neural nut with the code that we up this neural nut with the code that we up this neural nut with the code that we have and this of course ran for a lot have",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1321,
      "text": "and this of course ran for a lot have",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1322,
      "text": "and this of course ran for a lot longer this maybe trained for I want to longer this maybe trained for I want to longer this maybe trained for I want to say about 15 minutes on my a100 GPU so say about 15 minutes on my a100 GPU so say about 15 minutes on my a100 GPU so that's a pretty a GPU and if you don't that's a pretty a GPU and if you don't that's a pretty a GPU and if you don't have a GPU you're not going to be able have a GPU you're not going to be able have a GPU you're not going to be able to reproduce this uh on a CPU this would to reproduce this uh on a CPU this would to reproduce this uh on a CPU this would be um I would not run this on a CPU or be um I would not run this on a CPU or be um I would not run this on a CPU or MacBook or something like that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1323,
      "text": "you'll MacBook or something like that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1324,
      "text": "you'll MacBook or something like that you'll have to Brak down the number of uh have to Brak down the number of uh have to Brak down the number of uh layers and the embedding Dimension and layers and the embedding Dimension and layers and the embedding Dimension and so on uh but in about 15 minutes we can so on uh but in about 15 minutes we can so on uh but in about 15 minutes we can get this kind of a result and um I'm get this kind of a result and um I'm get this kind of a result and um I'm printing some of the Shakespeare here printing some of the Shakespeare here printing some of the Shakespeare here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1325,
      "text": "but what I did also is I printed 10,000 but what I did also is I printed 10,000 but what I did also is I printed 10,000 characters so a lot more",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1326,
      "text": "and I wrote characters so a lot more",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1327,
      "text": "and I wrote characters so a lot more",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1328,
      "text": "and I wrote them to a file",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1329,
      "text": "and so here we see some them to a file",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1330,
      "text": "and so here we see some them to a file",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1331,
      "text": "and so here we see some of the outputs of the outputs of the outputs",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1332,
      "text": "so it's a lot more recognizable as the so it's a lot more recognizable as the",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1333,
      "text": "so it's a lot more recognizable as the input text file so the input text file input text file so the input text file input text file so the input text file just for reference looked like this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1334,
      "text": "so just for reference looked like this so just for reference looked like this so there's always like someone speaking in there's always like someone speaking in there's always like someone speaking in this manner and uh our predictions now this manner and uh our predictions now this manner and uh our predictions now take on that form except of course take on that form except of course take on that form except of course they're they're nonsensical when you they're they're nonsensical when you they're they're nonsensical when you actually read them actually read them actually read them so it is every crimp tap be a house",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1335,
      "text": "oh so it is every crimp tap be a house",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1336,
      "text": "oh so it is every crimp tap be a house",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1337,
      "text": "oh those those prepation we give prepation we give prepation we give heed um you know Lord",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1338,
      "text": "anyway so you can read through this Lord anyway",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1339,
      "text": "so you can read through this um it's nonsensical of course",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1340,
      "text": "but this um it's nonsensical of course",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1341,
      "text": "but this um it's nonsensical of course",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1342,
      "text": "but this is just a Transformer trained on a is just a Transformer trained on a is just a Transformer trained on a character level for 1 million characters character level for 1 million characters character level for 1 million characters that come from Shakespeare so there's that come from Shakespeare so there's that come from Shakespeare so there's sort of like blabbers on in Shakespeare sort of like blabbers on in Shakespeare sort of like blabbers on in Shakespeare like manner but it doesn't of course like manner but it doesn't of course like manner but it doesn't of course make sense at this scale uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1343,
      "text": "but I think make sense at this scale uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1344,
      "text": "but I think make sense at this scale uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1345,
      "text": "but I think I think still a pretty good",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1346,
      "text": "I think still a pretty good",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1347,
      "text": "I think still a pretty good demonstration of what's demonstration of what's demonstration of what's possible so now possible so now possible",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1348,
      "text": "so now I think uh that kind of like concludes I think uh that kind of like concludes I think uh that kind of like concludes the programming section of this video we the programming section of this video we the programming section of this video we basically kind of uh did a pretty good basically kind of uh did a pretty good basically kind of uh did a pretty good job and um of implementing this job and um of implementing this job and um of implementing this Transformer uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1349,
      "text": "but the picture doesn't Transformer uh but the picture doesn't Transformer uh but the picture doesn't exactly match up to what we've done so exactly match up to what we've done so exactly match up to what we've done so what's going on with all these digital",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1350,
      "text": "what's going on with all these digital",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1351,
      "text": "what's going on with all these digital Parts here so let me finish explaining Parts here so let me finish explaining Parts here so let me finish explaining this architecture and why it looks so this architecture",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1352,
      "text": "and why it looks so this architecture",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1353,
      "text": "and why it looks so funky basically what's happening here is funky basically what's happening here is funky basically what's happening here is what we implemented here is a decoder what we implemented here is a decoder what we implemented here is a decoder only Transformer so there's no component only Transformer so there's no component only Transformer so there's no component here this part is called the encoder and here this part is called the encoder and here this part is called the encoder and there's no cross attention block here there's no cross attention block here there's no cross attention block here our block only has a self attention and our block only has a self attention and our block only has a self attention and the feet forward so it is missing this the feet forward so it is missing this the feet forward so it is missing this third in between piece here this piece third in between piece here this piece third in between piece here this piece does cross attention so we don't have it does cross attention so we don't have it does cross attention so we don't have it and we don't have the encoder we just and we don't have the encoder we just",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1354,
      "text": "and we don't have the encoder we just have the decoder and the reason we have have the decoder and the reason we have have the decoder and the reason we have a decoder only uh is because we are just a decoder only uh is because we are just a decoder only uh is because we are just uh generating text",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1355,
      "text": "and it's uh generating text and it's uh generating text",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1356,
      "text": "and it's unconditioned on anything we're just unconditioned on anything we're just unconditioned on anything we're just we're just blabbering on according to a we're just blabbering on according to a we're just blabbering on according to a given data set what makes it a decoder given data set what makes it a decoder given data set what makes it a decoder is that we are using the Triangular mask is that we are using the Triangular mask is that we are using the Triangular mask in our uh trans former so it has this in our uh trans former",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1357,
      "text": "so it has this in our uh trans former so it has this Auto regressive property where we can Auto regressive property where we can Auto regressive property where we can just uh go and sample from it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1358,
      "text": "so the just uh go and sample from it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1359,
      "text": "so the just uh go and sample from it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1360,
      "text": "so the fact that it's using the Triangular fact that it's using the Triangular fact that it's using the Triangular triangular mask to mask out the triangular mask to mask out the triangular mask to mask out the attention makes it a decoder and it can attention makes it a decoder and it can attention makes it a decoder and it can be used for language modeling now the be used for language modeling now the be used for language modeling now the reason that the original paper had an reason that the original paper had an reason that the original paper had an incoder decoder architecture is because incoder decoder architecture is because incoder decoder architecture is because it is a machine translation paper so it it is a machine translation paper",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1361,
      "text": "so it it is a machine translation paper so it is concerned with a different setting in is concerned with a different setting in is concerned with a different setting in particular it expects some uh tokens particular it expects some uh tokens particular it expects some uh tokens that encode say for example French and that encode say for example French and that encode say for example French",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1362,
      "text": "and then it is expecting to decode the then it is expecting to decode the then it is expecting to decode the translation in English",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1363,
      "text": "so so you translation in English",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1364,
      "text": "so so you translation in English",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1365,
      "text": "so so you typically these here are special tokens typically these here are special tokens typically these here are special tokens so you are expected to read in this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1366,
      "text": "and so you are expected to read in this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1367,
      "text": "and so you are expected to read in this and condition on it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1368,
      "text": "and then you start off condition on it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1369,
      "text": "and then you start off condition on it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1370,
      "text": "and then you start off the generation with a special token the generation with a special token the generation with a special token called start so this is a special new called start",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1371,
      "text": "so this is a special new called start so this is a special new",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1372,
      "text": "token",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1373,
      "text": "um that you introduce and always token um that you introduce and always token um that you introduce and always place in the beginning and then the place in the beginning and then the place in the beginning and then the network is expected to Output neural network is expected to Output neural network is expected to Output neural networks are awesome and then a special networks are awesome and then a special networks are awesome and then a special end token to finish the end token to finish the end token to finish the generation",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1374,
      "text": "so this part here will be generation so this part here will be generation",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1375,
      "text": "so this part here will be decoded exactly as we we've done it decoded exactly as we we've done it decoded exactly as we we've done it neural networks are awesome will be neural networks are awesome will be neural networks are awesome will be identical to what we did but unlike what identical to what we did but unlike what identical to what we did but unlike what we did they wanton to condition the we did they wanton to condition the we did they wanton to condition the generation on some additional generation on some additional generation on some additional information and in that case this information and in that case this information and in that case this additional information is the French additional information is the French additional information is the French sentence that they should be sentence that they should be sentence that they should be translating so what they do now is they translating so what they do now is they translating so what they do now is they bring in the encoder now the encoder bring in the encoder now the encoder bring in the encoder now the encoder reads this part here so we're only going reads this part here so we're only going reads this part here so we're only going to take the part of French and we're to take the part of French and we're to take the part of French and we're going to uh create tokens from it going to uh create tokens from it going to uh create tokens from it exactly as we've seen in our video and exactly as we've seen in our video and exactly as we've seen in our video and we're going to put a Transformer on it we're going to put a Transformer on it we're going to put a Transformer on it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1376,
      "text": "but there's going to be no triangular",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1377,
      "text": "but there's going to be no triangular",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1378,
      "text": "but there's going to be no triangular mask",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1379,
      "text": "and so all the tokens are allowed mask",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1380,
      "text": "and so all the tokens are allowed mask",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1381,
      "text": "and so all the tokens are allowed to talk to each other as much as they to talk to each other as much as they to talk to each other as much as they want and they're just encoding want and they're just encoding want and they're just encoding whatever's the content of this French uh whatever's the content of this French uh whatever's the content of this French uh sentence once they've encoded it they sentence once they've encoded it they sentence once they've encoded it they they basically come out in the top here they basically come out in the top here they basically come out in the top here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1382,
      "text": "and then what happens here is in our",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1383,
      "text": "and then what happens here is in our",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1384,
      "text": "and then what happens here is in our decoder which does the uh language decoder which does the uh language decoder which does the uh language modeling there's an additional modeling there's an additional modeling there's an additional connection here to the outputs of the connection here to the outputs of the connection here to the outputs of the encoder encoder encoder and that is brought in through a cross and that is brought in through a cross and that is brought in through a cross attention so the queries are still attention so the queries are still attention so the queries are still generated from X but now the keys and generated from X but now the keys and generated from X but now the keys and the values are coming from the side the the values are coming from the side the the values are coming from the side the keys and the values are coming from the keys and the values are coming from the keys and the values are coming from the top generated by the nodes that came top generated by the nodes that came top generated by the nodes that came outside of the de the encoder and those outside of the de the encoder and those outside of the de the encoder and those tops the keys and the values there the tops the keys and the values there the tops the keys and the values there the top of it feed in on a side into every top of it feed in on a side into every top of it feed in on a side into every single block of the decoder and so single block of the decoder and so single block of the decoder",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1385,
      "text": "and so that's why there's an additional cross that's why there's an additional cross that's why there's an additional cross attention and really what it's doing is attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1386,
      "text": "and really what it's doing is attention and really what it's doing is it's conditioning the decoding it's conditioning the decoding",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1387,
      "text": "it's conditioning the decoding not just on the past of this current not just on the past of this current not just on the past of this current decoding but also on having seen the decoding but also on having seen the decoding but also on having seen the full fully encoded French um prompt sort full fully encoded French um prompt sort full fully encoded French um prompt sort of and so it's an encoder decoder model of",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1388,
      "text": "and so it's an encoder decoder model of",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1389,
      "text": "and so it's an encoder decoder model which is why we have those two which is why we have those two which is why we have those two Transformers an additional block and so Transformers an additional block",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1390,
      "text": "and so Transformers an additional block",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1391,
      "text": "and so on so we did not do this because we have on",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1392,
      "text": "so we did not do this because we have on",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1393,
      "text": "so we did not do this because we have no we have nothing to encode there's no",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1394,
      "text": "no we have nothing to encode there's no",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1395,
      "text": "no we have nothing to encode there's no conditioning we just have a text file conditioning we just have a text file conditioning we just have a text file",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1396,
      "text": "and we just want to imitate it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1397,
      "text": "and and we just want to imitate it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1398,
      "text": "and and we just want to imitate it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1399,
      "text": "and that's why we are using a decoder only that's why we are using a decoder only that's why we are using a decoder only Transformer exactly as done in Transformer exactly as done in Transformer exactly as done in GPT",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1400,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1401,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1402,
      "text": "so now I wanted to do a GPT",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1403,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1404,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1405,
      "text": "so now I wanted to do a GPT",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1406,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1407,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1408,
      "text": "so now I wanted to do a very brief walkthrough of nanog GPT very brief walkthrough of nanog GPT very brief walkthrough of nanog GPT which you can find in my GitHub and uh which you can find in my GitHub and uh which you can find in my GitHub and uh nanog GPT is basically two files of nanog GPT is basically two files of nanog GPT is basically two files of Interest there's train.py and model.py Interest there's train.py and model.py Interest there's train.py and model.py train.py is all the boilerplate code for train.py is all the boilerplate code for train.py is all the boilerplate code for training the network it is basically all training the network it is basically all training the network it is basically all the stuff that we had here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1409,
      "text": "it's the the stuff that we had here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1410,
      "text": "it's the the stuff that we had here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1411,
      "text": "it's the training loop it's just that it's a lot training loop it's just that it's a lot training loop it's just that it's a lot more complicated because we're saving more complicated because we're saving more complicated because we're saving and loading checkpoints and pre-trained and loading checkpoints and pre-trained and loading checkpoints and pre-trained weights and we are uh decaying the weights and we are uh decaying the weights and we are uh decaying the learning rate and compiling the model learning rate and compiling the model learning rate and compiling the model and using distributed training across and using distributed training across and using distributed training across multiple nodes or GP use so the training multiple nodes or GP use so the training multiple nodes or GP use so the training Pi gets a little bit more hairy Pi gets a little bit more hairy Pi gets a little bit more hairy complicated uh there's more options Etc complicated uh there's more options Etc complicated uh there's more options Etc but the model.py should look very very but the model.py should look very very but the model.py should look very very um similar to what we've done here in um similar to what we've done here in um similar to what we've done here in fact the model is is almost identical",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1412,
      "text": "so fact the model is is almost identical",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1413,
      "text": "so fact the model is is almost identical",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1414,
      "text": "so first here we have the causal self first here we have the causal self first here we have the causal self attention block and all of this should attention block and all of this should attention block and all of this should look very very recognizable to you",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1415,
      "text": "we're look very very recognizable to you we're look very very recognizable to you we're producing queries Keys values we're producing queries Keys values we're producing queries Keys values we're doing Dot products we're masking doing Dot products we're masking doing Dot products we're masking applying soft Maxs optionally dropping applying soft Maxs optionally dropping applying soft Maxs optionally dropping out and here we are pulling the wi the out",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1416,
      "text": "and here we are pulling the wi the out",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1417,
      "text": "and here we are pulling the wi the values what is different here is that in values what is different here is that in values what is different here is that in our code I have separated out the our code I have separated out the our code I have separated out the multi-headed detention into just a multi-headed detention into just a multi-headed detention into just a single individual head and then here I single individual head",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1418,
      "text": "and then here I single individual head",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1419,
      "text": "and then here I have multiple heads",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1420,
      "text": "and I explicitly have multiple heads and I explicitly have multiple heads and I explicitly concatenate them whereas here uh all of concatenate them whereas here uh all of concatenate them whereas here uh all of it is implemented in a batched manner it is implemented in a batched manner it is implemented in a batched manner inside a single causal self attention inside a single causal self attention inside a single causal self attention",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1421,
      "text": "and so we don't just have a b and a T",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1422,
      "text": "and so we don't just have a b and a T",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1423,
      "text": "and so we don't just have a b and a T and A C Dimension we also end up with a and A C Dimension we also end up with a and A C Dimension we also end up with a fourth dimension which is the heads and fourth dimension which is the heads and fourth dimension which is the heads",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1424,
      "text": "and so it just gets a lot more sort of hairy",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1425,
      "text": "so it just gets a lot more sort of hairy",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1426,
      "text": "so it just gets a lot more sort of hairy because we have four dimensional array because we have four dimensional array because we have four dimensional array um tensors now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1427,
      "text": "but it is um equivalent um tensors now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1428,
      "text": "but it is um equivalent um tensors now",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1429,
      "text": "but it is um equivalent mathematically so the exact same thing mathematically so the exact same thing mathematically so the exact same thing is happening as what we have it's just is happening as what we have it's just is happening as what we have it's just it's a bit more efficient because all it's a bit more efficient because all it's a bit more efficient because all the heads are now treated as a batch the heads are now treated as a batch the heads are now treated as a batch Dimension as Dimension as Dimension as well then we have the multier perceptron",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1430,
      "text": "well then we have the multier perceptron",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1431,
      "text": "well then we have the multier perceptron it's using the Galu nonlinearity which it's using the Galu nonlinearity which it's using the Galu nonlinearity which is defined here except instead of Ru and is defined here except instead of Ru and is defined here except instead of Ru and this is done just because opening I used this is done just because opening I used this is done just because opening I used it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1432,
      "text": "and I want to be able to load their it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1433,
      "text": "and I want to be able to load their it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1434,
      "text": "and I want to be able to load their checkpoints uh the blocks of the checkpoints uh the blocks of the checkpoints uh the blocks of the Transformer are identical to communicate Transformer are identical to communicate Transformer are identical to communicate in the compute phase as we saw and then in the compute phase as we saw and then in the compute phase as we saw and then the GPT will be identical we have the the GPT will be identical we have the the GPT will be identical we have the position encodings token encodings the position encodings token encodings the position encodings token encodings the blocks the layer Norm at the end uh the blocks the layer Norm at the end uh the blocks the layer Norm at the end uh the final linear layer and this should look final linear layer and this should look final linear layer and this should look all very recognizable and there's a bit all very recognizable and there's a bit all very recognizable and there's a bit more here because I'm loading more here because I'm loading more here because I'm loading checkpoints and stuff like that I'm checkpoints and stuff like that I'm checkpoints and stuff like that I'm separating out the parameters into those separating out the parameters into those separating out the parameters into those that should be weight decayed and those that should be weight decayed and those that should be weight decayed and those that that that shouldn't um but the generate function shouldn't um but the generate function shouldn't um but the generate function should also be very very similar so a should also be very very similar so a should also be very very similar",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1435,
      "text": "so a few details are different but you should few details are different but you should few details are different but you should definitely be able to look at this uh definitely be able to look at this uh definitely be able to look at this uh file and be able to understand little file and be able to understand little file and be able to understand little the pieces now so let's now bring things the pieces now so let's now bring things the pieces now so let's now bring things back to chat GPT what would it look like back to chat GPT what would it look like back to chat GPT what would it look like if we wanted to train chat",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1436,
      "text": "GPT ourselves if we wanted to train chat GPT ourselves if we wanted to train chat GPT ourselves",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1437,
      "text": "and how does it relate to what we and how does it relate to what we and how does it relate to what we learned today well to train in chat GPT learned today well to train in chat GPT learned today well to train in chat GPT there are roughly two stages first is there are roughly two stages first is there are roughly two stages first is the pre-training stage and then the the pre-training stage and then the the pre-training stage and then the fine-tuning stage in the pre-training fine-tuning stage in the pre-training fine-tuning stage in the pre-training stage uh we are training on a large stage uh we are training on a large stage uh we are training on a large chunk of internet and just trying to get chunk of internet and just trying to get chunk of internet and just trying to get a first decoder only Transformer to a first decoder only Transformer to a first decoder only Transformer to babble text",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1438,
      "text": "so it's very very similar to babble text so it's very very similar to babble text",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1439,
      "text": "so it's very very similar to what we've done ourselves except we've what we've done ourselves except we've what we've done ourselves except we've done like a tiny little baby done like a tiny little baby done like a tiny little baby pre-training step um and so in our case pre-training step um and so in our case pre-training step um and so in our case uh this is how you print a number of uh this is how you print a number of uh this is how you print a number of parameters I printed it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1440,
      "text": "and it's about parameters I printed it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1441,
      "text": "and it's about parameters I printed it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1442,
      "text": "and it's about 10 million so this Transformer that I 10 million so this Transformer that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1443,
      "text": "I 10 million so this Transformer that I created here to create little created here to create little created here to create little Shakespeare um Transformer was about 10 Shakespeare um Transformer was about 10 Shakespeare um Transformer was about 10 million parameters our data set is million parameters our data set is million parameters our data set is roughly 1 million uh characters so roughly 1 million uh characters so roughly 1 million uh characters so roughly 1 million tokens but you have to roughly 1 million tokens but you have to roughly 1 million tokens but you have to remember that opening I is different remember that opening I is different remember that opening I is different vocabulary they're not on the Character vocabulary they're not on the Character vocabulary they're not on the Character level they use these um subword chunks level they use these um subword chunks level they use these um subword chunks of words",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1444,
      "text": "and so they have a vocabulary of words",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1445,
      "text": "and so they have a vocabulary of words",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1446,
      "text": "and so they have a vocabulary of 50,000 roughly elements and so their of 50,000 roughly elements and so their of 50,000 roughly elements and so their sequences are a bit more condensed so sequences are a bit more condensed so sequences are a bit more condensed so our data set the Shakespeare data set our data set the Shakespeare data set our data set the Shakespeare data set would be probably around 300,000 uh would be probably around 300,000 uh would be probably around 300,000 uh tokens in the open AI vocabulary roughly tokens in the open AI vocabulary roughly tokens in the open AI vocabulary roughly so we trained about 10 million parameter",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1447,
      "text": "so we trained about 10 million parameter",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1448,
      "text": "so we trained about 10 million parameter model on roughly 300,000 tokens now when model on roughly 300,000 tokens now when model on roughly 300,000 tokens now when you go to the gpt3 you go to the gpt3 you go to the gpt3 paper and you look at the Transformers paper and you look at the Transformers paper and you look at the Transformers that they trained they trained a number that they trained they trained a number that they trained they trained a number of trans Transformers of different sizes of trans Transformers of different sizes of trans Transformers of different sizes but the biggest Transformer here has 175 but the biggest Transformer here has 175 but the biggest Transformer here has 175 billion parameters uh so ours is again billion parameters uh so ours is again billion parameters",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1449,
      "text": "uh so ours is again 10 million they used this number of 10 million they used this number of 10 million they used this number of layers in the Transformer this is the layers in the Transformer this is the layers in the Transformer this is the nmed this is the number of heads and nmed this is the number of heads and nmed this is the number of heads and this is the head size",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1450,
      "text": "and then this is this is the head size",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1451,
      "text": "and then this is this is the head size",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1452,
      "text": "and then this is the batch size uh so ours was the batch size uh so ours was the batch size",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1453,
      "text": "uh so ours was 65 and the learning rate is similar now 65 and the learning rate is similar now 65 and the learning rate is similar now when they train this Transformer they when they train this Transformer they when they train this Transformer they trained on 300 billion tokens so again trained on 300 billion tokens so again trained on 300 billion tokens so again remember ours is about 300,000 remember ours is about 300,000 remember ours is about 300,000 so this is uh about a millionfold",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1454,
      "text": "so this is uh about a millionfold so this is uh about a millionfold increase and this number would not be increase",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1455,
      "text": "and this number would not be increase",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1456,
      "text": "and this number would not be even that large by today's standards even that large by today's standards even that large by today's standards you'd be going up uh 1 trillion",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1457,
      "text": "and you'd be going up uh 1 trillion",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1458,
      "text": "and you'd be going up uh 1 trillion and above so they are training a above",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1459,
      "text": "so they are training a above so they are training a significantly larger significantly larger significantly larger model on uh a good chunk of the internet model on uh a good chunk of the internet model on uh a good chunk of the internet and that is the pre-training stage",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1460,
      "text": "but and that is the pre-training stage",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1461,
      "text": "but and that is the pre-training stage but otherwise these hyper parameters should otherwise these hyper parameters should otherwise these hyper parameters should be fairly recognizable to you and the be fairly recognizable to you and the be fairly recognizable to you and the architecture is actually like nearly architecture is actually like nearly architecture is actually like nearly identical to what we implemented identical to what we implemented identical to what we implemented ourselves",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1462,
      "text": "but of course it's a massive ourselves",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1463,
      "text": "but of course it's a massive ourselves",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1464,
      "text": "but of course it's a massive infrastructure challenge to train this infrastructure challenge to train this infrastructure challenge to train this you're talking about typically thousands you're talking about typically thousands you're talking about typically thousands of gpus having to you know talk to each of gpus having to you know talk to each of gpus having to you know talk to each other to train models of this size so other to train models of this size so other to train models of this size so that's just a pre-training stage now that's just a pre-training stage now that's just a pre-training stage now after you complete the pre-training after you complete the pre-training after you complete the pre-training stage uh you don't get something that stage uh you don't get something that stage uh you don't get something that responds to your questions with answers responds to your questions with answers responds to your questions with answers and is not helpful and Etc you get a and is not helpful and Etc you get a and is not helpful and Etc you get a document document document completer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1465,
      "text": "right so it babbles",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1466,
      "text": "but it completer right",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1467,
      "text": "so it babbles but it completer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1468,
      "text": "right so it babbles but it doesn't Babble Shakespeare it babbles doesn't Babble Shakespeare it babbles doesn't Babble Shakespeare it babbles internet it will create arbitrary news internet it will create arbitrary news internet it will create arbitrary news articles and documents and it will try articles and documents and it will try articles and documents and it will try to complete documents because that's to complete documents because that's to complete documents because that's what it's trained for it's trying to what it's trained for it's trying to what it's trained for it's trying to complete the sequence",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1469,
      "text": "so when you give complete the sequence so when you give complete the sequence",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1470,
      "text": "so when you give it a question it would just uh it a question it would just uh it a question it would just uh potentially just give you more questions potentially just give you more questions potentially just give you more questions it would follow with more questions it it would follow with more questions it it would follow with more questions it will do whatever it looks like the some will do whatever it looks like the some will do whatever it looks like the some close document would do in the training close document would do in the training close document would do in the training data on the internet and so who knows data on the internet and so who knows data on the internet and so who knows you're getting kind of like undefined you're getting kind of like undefined you're getting kind of like undefined Behavior it might basically answer with Behavior it might basically answer with Behavior it might basically answer with to questions with other questions it to questions with other questions it to questions with other questions it might ignore your question it might just might ignore your question it might just might ignore your question it might just try to complete some news article it's try to complete some news article it's try to complete some news article it's totally unineed as we say so the second totally unineed as we say so the second totally unineed as we say so the second fine-tuning stage is to actually align fine-tuning stage is to actually align fine-tuning stage is to actually align it to be an assistant and uh this is the it to be an assistant and uh this is the it to be an assistant and uh this is the second stage and so this chat GPT block second stage",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1471,
      "text": "and so this chat GPT block second stage",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1472,
      "text": "and so this chat GPT block post from openi talks a little bit about post from openi talks a little bit about post from openi talks a little bit about how the stage is achieved we basically how the stage is achieved we basically how the stage is achieved we basically um there's roughly three steps to to um there's roughly three steps to to um there's roughly three steps to to this stage uh so what they do here is this stage uh so what they do here is this stage uh so what they do here is they start to collect training data that they start to collect training data that they start to collect training data that looks specifically like what an looks specifically like what an looks specifically like what an assistant would do so these are assistant would do so these are assistant would do so these are documents that have to format where the documents that have to format where the documents that have to format where the question is on top and then an answer is question is on top and then an answer is question is on top and then an answer is below and they have a large number of below",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1473,
      "text": "and they have a large number of below",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1474,
      "text": "and they have a large number of these but probably not on the order of these but probably not on the order of these but probably not on the order of the internet uh this is probably on the the internet uh this is probably on the the internet uh this is probably on the of maybe thousands of examples",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1475,
      "text": "and so of maybe thousands of examples and so of maybe thousands of examples",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1476,
      "text": "and so they they then fine-tune the model to they they then fine-tune the model to they they then fine-tune the model to basically only focus on documents that basically only focus on documents that basically only focus on documents that look like that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1477,
      "text": "and so you're starting to look like that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1478,
      "text": "and so you're starting to look like that",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1479,
      "text": "and so you're starting to slowly align it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1480,
      "text": "so it's going to expect slowly align it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1481,
      "text": "so it's going to expect slowly align it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1482,
      "text": "so it's going to expect a question at the top",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1483,
      "text": "and it's going to a question at the top",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1484,
      "text": "and it's going to a question at the top",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1485,
      "text": "and it's going to expect to complete the answer and uh expect to complete the answer and uh expect to complete the answer",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1486,
      "text": "and uh these very very large models are very these very very large models are very these very very large models are very sample efficient during their sample efficient during their sample efficient during their fine-tuning",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1487,
      "text": "so this actually somehow fine-tuning so this actually somehow fine-tuning",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1488,
      "text": "so this actually somehow works but that's just step one that's works",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1489,
      "text": "but that's just step one that's works",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1490,
      "text": "but that's just step one that's just fine tuning",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1491,
      "text": "so then they actually just fine tuning so then they actually just fine tuning",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1492,
      "text": "so then they actually have more steps where okay the second have more steps where okay the second have more steps where okay the second step is you let the model respond and step is you let the model respond and step is you let the model respond and then different Raiders look at the then different Raiders look at the then different Raiders look at the different responses and rank them for different responses and rank them for different responses and rank them for their preference as to which one is their preference as to which one is their preference as to which one is better than the other they use that to better than the other they use that to better than the other they use that to train a reward model so they can predict train a reward model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1493,
      "text": "so they can predict train a reward model so they can predict uh basically using a different network uh basically using a different network uh basically using a different network how much of any candidate how much of any candidate how much of any candidate response would be desirable and then response would be desirable and then response would be desirable and then once they have a reward model they run once they have a reward model they run once they have a reward model they run po which is a form of polic policy po which is a form of polic policy po which is a form of polic policy gradient um reinforcement learning gradient um reinforcement learning gradient um reinforcement learning Optimizer to uh fine-tune this sampling Optimizer to uh fine-tune this sampling Optimizer to uh fine-tune this sampling policy uh so that the answers that the policy uh so that the answers that the policy uh so that the answers that the GP chat GPT now generates are expected GP chat GPT now generates are expected GP chat GPT now generates are expected to score a high reward according to the to score a high reward according to the to score a high reward according to the reward model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1494,
      "text": "and so basically there's a reward model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1495,
      "text": "and so basically there's a reward model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1496,
      "text": "and so basically there's a whole aligning stage here or fine-tuning whole aligning stage here or fine-tuning whole aligning stage here or fine-tuning stage",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1497,
      "text": "it's got multiple steps in between stage it's got multiple steps in between stage it's got multiple steps in between there as well and it takes the model there as well and it takes the model there as well and it takes the model from being a document completer to a from being a document completer to a from being a document completer to a question answerer and that's like a question answerer and that's like a question answerer and that's like a whole separate stage a lot of this data whole separate stage a lot of this data whole separate stage a lot of this data is not available publicly it is internal is not available publicly it is internal is not available publicly it is internal to open AI",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1498,
      "text": "and uh it's much harder to to open AI",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1499,
      "text": "and uh it's much harder to to open AI",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1500,
      "text": "and uh it's much harder to replicate this stage um",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1501,
      "text": "and so that's replicate this stage um and so that's replicate this stage um and so that's roughly what would give you a chat GPT roughly what would give you a chat GPT roughly what would give you a chat GPT and nanog GPT focuses on the and nanog GPT focuses on the and nanog GPT focuses on the pre-training stage",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1502,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1503,
      "text": "and that's pre-training stage",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1504,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1505,
      "text": "and that's pre-training stage",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1506,
      "text": "okay",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1507,
      "text": "and that's everything that I wanted to cover today everything that I wanted to cover today everything that I wanted to cover today so we trained to summarize a decoder so we trained to summarize a decoder so we trained to summarize a decoder only Transformer following this famous only Transformer following this famous only Transformer following this famous paper attention is all you need from paper attention is all you need from paper attention is all you need from 2017 and so that's basically a GPT we 2017",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1508,
      "text": "and so that's basically a GPT we 2017",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1509,
      "text": "and so that's basically a GPT we trained it on Tiny Shakespeare and got trained it on Tiny Shakespeare and got trained it on Tiny Shakespeare and got sensible results sensible results sensible results all of the training code is all of the training code is all of the training code is roughly 200 lines of code I will be roughly 200 lines of code I will be roughly 200 lines of code I will be releasing this um code base so also it releasing this um code base so also it releasing this um code base so also it comes with all the git log commits along comes with all the git log commits along comes with all the git log commits along the way as we built it the way as we built it the way as we built it up in addition to this code I'm going to up in addition to this code I'm going to up in addition to this code I'm going to release the um notebook of course the release the um notebook of course the release the um notebook of course the Google collab and I hope that gave you a Google collab and I hope that gave you a Google collab and I hope that gave you a sense for how you can train um these sense for how you can train um these sense for how you can train um these models like say gpt3 that will be um models like say gpt3 that will be um models like say gpt3 that will be um architecturally basically identical to architecturally basically identical to architecturally basically identical to what we have but they are somewhere what we have but they are somewhere what we have but they are somewhere between 10,000 and 1 million times between 10,000 and 1 million times between 10,000 and 1 million times bigger depending on how you count and so bigger depending on how you count and so bigger depending on how you count",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1510,
      "text": "and so uh that's all I have for now uh we did uh that's all I have for now uh we did uh that's all I have for now uh we did not talk about any of the fine-tuning not talk about any of the fine-tuning not talk about any of the fine-tuning stages that would typically go on top of stages that would typically go on top of stages that would typically go on top of this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1511,
      "text": "so if you're interested in this",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1512,
      "text": "so if you're interested in this so if you're interested in something that's not just language something that's not just language something that's not just language modeling",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1513,
      "text": "but you actually want to you modeling",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1514,
      "text": "but you actually want to you modeling",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1515,
      "text": "but you actually want to you know say perform tasks um or you want know say perform tasks um or you want know say perform tasks um or you want them to be aligned in a specific way or them to be aligned in a specific way or them to be aligned in a specific way or you want um to detect sentiment or you want um to detect sentiment or you want um to detect sentiment or anything like that basically anytime you anything like that basically anytime you anything like that basically anytime you don't want something that's just a don't want something that's just a don't want something that's just a document completer you have to complete document completer you have to complete document completer you have to complete further stages of fine tuning which did further stages of fine tuning which did further stages of fine tuning which did not cover uh and that could be simple not cover uh and that could be simple not cover uh and that could be simple supervised fine tuning or it can be supervised fine tuning or it can be supervised fine tuning or it can be something more fancy like we see in chat something more fancy like we see in chat something more fancy like we see in chat jpt where we actually train a reward jpt where we actually train a reward jpt where we actually train a reward model and then do rounds of Po to uh model and then do rounds of Po to uh model and then do rounds of Po to uh align it with respect to the reward align it with respect to the reward align it with respect to the reward model",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1516,
      "text": "so there's a lot more that can be model so there's a lot more that can be model so there's a lot more that can be done on top of it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1517,
      "text": "I think for now we're done on top of it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1518,
      "text": "I think for now we're done on top of it",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1519,
      "text": "I think for now we're starting to get to about two hours Mark starting to get to about two hours Mark starting to get to about two hours Mark",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1520,
      "text": "uh so I'm going to um kind of finish",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1521,
      "text": "uh so I'm going to um kind of finish",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1522,
      "text": "uh so I'm going to um kind of finish here uh I hope you enjoyed the lecture here uh I hope you enjoyed the lecture here",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1523,
      "text": "uh I hope you enjoyed the lecture uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1524,
      "text": "and uh yeah go forth and transform uh and uh yeah go forth and transform uh",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    },
    {
      "id": 1525,
      "text": "and uh yeah go forth and transform see you later",
      "start_time": "00:00:02.510",
      "end_time": "01:56:21.760"
    }
  ]
}