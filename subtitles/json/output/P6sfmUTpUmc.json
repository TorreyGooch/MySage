{
  "video_id": "P6sfmUTpUmc",
  "sentences": [
    {
      "id": 1,
      "text": "hi everyone today we are continuing our hi everyone today we are continuing our implementation of make more now in the implementation of make more now in the implementation of make more now in the last lecture we implemented the multier last lecture we implemented the multier last lecture we implemented the multier perceptron along the lines of benj 2003 perceptron along the lines of benj 2003 perceptron along the lines of benj 2003 for character level language modeling so for character level language modeling so for character level language modeling so we followed this paper took in a few we followed this paper took in a few we followed this paper took in a few characters in the past and used an MLP characters in the past and used an MLP characters in the past and used an MLP to predict the next character in a to predict the next character in a to predict the next character in a sequence so what we'd like to do now is sequence",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 2,
      "text": "so what we'd like to do now is sequence",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 3,
      "text": "so what we'd like to do now is we'd like to move on to more complex",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 4,
      "text": "and we'd like to move on to more complex",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 5,
      "text": "and we'd like to move on to more complex and larger neural networks like recurrent larger neural networks like recurrent larger neural networks like recurrent neural networks and their variations neural networks and their variations neural networks and their variations like the grw lstm and so on now before like the grw lstm and so on now before like the grw lstm and so on now before we do that though we have to stick we do that though we have to stick we do that though we have to stick around the level of malalia perception around the level of malalia perception around the level of malalia perception on for a bit longer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 6,
      "text": "and I'd like to do on for a bit longer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 7,
      "text": "and I'd like to do on for a bit longer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 8,
      "text": "and I'd like to do this because I would like us to have a this because I would like us to have a this because I would like us to have a very good intuitive understanding of the very good intuitive understanding of the very good intuitive understanding of the activations in the neural net during activations in the neural net during activations in the neural net during training and especially the gradients training and especially the gradients training and especially the gradients that are flowing backwards and how they that are flowing backwards and how they that are flowing backwards and how they behave and what they look like and this behave and what they look like and this behave and what they look like and this is going to be very important to is going to be very important to is going to be very important to understand the history of the understand the history of the understand the history of the development of these architectures development of these architectures development of these architectures because we'll see that recurr neural because we'll see that recurr neural because we'll see that recurr neural networks while they are very expressive networks while they are very expressive networks while they are very expressive in that they are a universal in that they are a universal in that they are a universal approximator and can in principle approximator and can in principle approximator and can in principle Implement uh all the algorithms",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 9,
      "text": "uh we'll Implement uh all the algorithms uh we'll Implement uh all the algorithms uh we'll see that they are not very easily see that they are not very easily see that they are not very easily optimizable with the first order optimizable with the first order optimizable with the first order gradient based techniques that we have gradient based techniques that we have gradient based techniques that we have available to us and that we use all the available to us and that we use all the available to us and that we use all the time and the key to understanding why time and the key to understanding why time and the key to understanding why they are not optimizable easily is to they are not optimizable easily is to they are not optimizable easily is to understand the the activations and the understand the the activations and the understand the the activations and the gradients and how they behave during gradients and how they behave during gradients and how they behave during training and we'll see that a lot of the training and we'll see that a lot of the training and we'll see that a lot of the variants since recur neural networks variants since recur neural networks variants since recur neural networks have tried to improve that situation and have tried to improve that situation and have tried to improve that situation and so that's the path that we have to take so that's the path that we have to take so that's the path that we have to take and uh let's get started so the starting and uh let's get started so the starting and uh let's get started so the starting code for this lecture is largely the code for this lecture is largely the code for this lecture is largely the code from before but I've cleaned it up code from before but I've cleaned it up code from before but I've cleaned it up a little bit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 10,
      "text": "so you'll see that we are a little bit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 11,
      "text": "so you'll see that we are a little bit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 12,
      "text": "so you'll see that we are importing importing importing all the torch and math plb utilities all the torch and math plb utilities",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 13,
      "text": "all the torch and math plb utilities",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 14,
      "text": "we're reading in the words just like we're reading in the words just like we're reading in the words just like before these are eight example words before these are eight example words before these are eight example words there's a total of 32,000 of them here's there's a total of 32,000 of them here's there's a total of 32,000 of them here's a vocabulary of all the lowercase a vocabulary of all the lowercase a vocabulary of all the lowercase letters and the special dot token here letters and the special dot token here letters and the special dot token here we are reading the data set",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 15,
      "text": "and we are reading the data set",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 16,
      "text": "and we are reading the data set and processing it and um creating three processing it and um creating three processing it and um creating three splits the train Dev and the test split splits the train Dev and the test split splits the train Dev and the test split now in MLP this is the identical same now in MLP this is the identical same now in MLP this is the identical same MLP except you see that I removed a MLP except you see that I removed a MLP except you see that I removed a bunch of magic numbers that we had here bunch of magic numbers that we had here bunch of magic numbers that we had here and instead we have the dimensionality and instead we have the dimensionality and instead we have the dimensionality of the embedding space of the characters of the embedding space of the characters of the embedding space of the characters and the number of hidden units in the and the number of hidden units in the and the number of hidden units in the hidden layer and so I've pulled them hidden layer and so I've pulled them hidden layer and so I've pulled them outside here uh so that we don't have to outside here uh so that we don't have to outside here uh so that we don't have to go and change all these magic numbers go and change all these magic numbers go and change all these magic numbers all the time we have the same neural net all the time we have the same neural net all the time we have the same neural net with 11,000 parameters that we optimize with 11,000 parameters that we optimize with 11,000 parameters that we optimize now over 200,000 steps with a batch size now over 200,000 steps with a batch size now over 200,000 steps with a batch size of 32 and you'll see that I refactor I of 32 and you'll see that I refactor I of 32 and you'll see that I refactor I refactored the code here a little bit refactored the code here a little bit refactored the code here a little bit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 17,
      "text": "but there are no functional changes",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 18,
      "text": "I",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 19,
      "text": "but there are no functional changes",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 20,
      "text": "I",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 21,
      "text": "but there are no functional changes I just created a few extra variables a few just created a few extra variables a few just created a few extra variables a few more comments",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 22,
      "text": "and I removed all the more comments",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 23,
      "text": "and I removed all the more comments",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 24,
      "text": "and I removed all the magic numbers and otherwise is the exact magic numbers and otherwise is the exact magic numbers and otherwise is the exact same thing then when we optimize we saw same thing then when we optimize we saw same thing then when we optimize we saw that our loss looked something like this that our loss looked something like this that our loss looked something like this we saw that the train and Val loss were we saw that the train and Val loss were we saw that the train and Val loss were about about about 2.16 and so on here I refactored the uh 2.16 and so on here I refactored the uh 2.16 and so on here I refactored the uh code a little bit for the evaluation of code a little bit for the evaluation of code a little bit for the evaluation of arbitary splits so you pass in a string arbitary splits",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 25,
      "text": "so you pass in a string arbitary splits so you pass in a string of which split you'd like to evaluate of which split you'd like to evaluate of which split you'd like to evaluate and then here depending on train Val or and then here depending on train Val or and then here depending on train Val or test I index in",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 26,
      "text": "and I get the correct test I index in",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 27,
      "text": "and I get the correct test I index in",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 28,
      "text": "and I get the correct split",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 29,
      "text": "and then this is the forward pass split and then this is the forward pass split and then this is the forward pass of the network and evaluation of the of the network and evaluation of the of the network and evaluation of the loss and printing it so just making that loss and printing it so just making that loss and printing it so just making that nicer uh one thing that you'll notice nicer uh one thing that you'll notice nicer uh one thing that you'll notice here is I'm using a decorator torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 30,
      "text": "here is I'm using a decorator torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 31,
      "text": "here is I'm using a decorator torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 32,
      "text": "nograd which you can also um look up and nograd which you can also um look up and nograd which you can also um look up and read the documentation of basically what read the documentation of basically what read the documentation of basically what this decorator does on top of a function this decorator does on top of a function this decorator does on top of a function is that whatever happens in this is that whatever happens in this is that whatever happens in this function is assumed by uh torch to never function is assumed by uh torch to never function is assumed by uh torch to never require any gradients so it will not do require any gradients so it will not do require any gradients so it will not do any of the bookkeeping that it does to any of the bookkeeping that it does to any of the bookkeeping that it does to keep track of all the gradients in keep track of all the gradients in keep track of all the gradients in anticipation of an eventual backward anticipation of an eventual backward anticipation of an eventual backward pass it's it's almost as if all the pass",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 33,
      "text": "it's it's almost as if all the pass",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 34,
      "text": "it's it's almost as if all the tensors that get created here have a tensors that get created here have a tensors that get created here have a required grad of false",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 35,
      "text": "and so it just required grad of false",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 36,
      "text": "and so it just required grad of false",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 37,
      "text": "and so it just makes everything much more efficient makes everything much more efficient makes everything much more efficient because you're telling torch that I will because you're telling torch that I will because you're telling torch that I will not call that backward on any of this not call that backward on any of this not call that backward on any of this computation and you don't need to computation and you don't need to computation and you don't need to maintain the graph under the hood so maintain the graph under the hood so maintain the graph under the hood",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 38,
      "text": "so that's what this does and you can also that's what this does and you can also that's what this does and you can also use a context manager uh with torch du use a context manager uh with torch du use a context manager uh with torch du nograd and you can look those nograd and you can look those nograd and you can look those up then here we have the sampling from a up then here we have the sampling from a up then here we have the sampling from a model um just as before just a for model um just as before just a for model um just as before just a for Passive neural nut getting the Passive neural nut getting the Passive neural nut getting the distribution sent from it adjusting the distribution sent from it adjusting the distribution sent from it adjusting the context window and repeating until we context window and repeating until we context window and repeating until we get the special end token",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 39,
      "text": "and we see get the special end token",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 40,
      "text": "and we see get the special end token",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 41,
      "text": "and we see that we are starting to get much nicer that we are starting to get much nicer that we are starting to get much nicer looking words simple from the model it's looking words simple from the model it's looking words simple from the model it's still not amazing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 42,
      "text": "and they're still not still not amazing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 43,
      "text": "and they're still not still not amazing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 44,
      "text": "and they're still not fully name like uh but it's much better fully name like uh but it's much better fully name like uh but it's much better than what we had with the BAM than what we had with the BAM than what we had with the BAM model so that's our starting point now model",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 45,
      "text": "so that's our starting point now model so that's our starting point now the first thing I would like to the first thing I would like to the first thing I would like to scrutinize is the scrutinize is the scrutinize is the initialization I can tell that our initialization I can tell that our initialization I can tell that our network is very improperly configured at network is very improperly configured at network is very improperly configured at initialization and there's multiple initialization and there's multiple initialization and there's multiple things wrong with it but let's just things wrong with it but let's just things wrong with it but let's just start with the first one look here on start with the first one look here on start with the first one look here on the zeroth iteration the very first the zeroth iteration the very first the zeroth iteration the very first iteration we are recording a loss of 27 iteration we are recording a loss of 27 iteration we are recording a loss of 27 and this rapidly comes down to roughly and this rapidly comes down to roughly and this rapidly comes down to roughly one or two or so",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 46,
      "text": "so I can tell that the one or two or so so I can tell that the one or two or so so I can tell that the initialization is all messed up because initialization is all messed up because initialization is all messed up because this is way too high in training of this is way too high in training of this is way too high in training of neural Nets it is almost always the case neural Nets it is almost always the case neural Nets",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 47,
      "text": "it is almost always the case that you will have a rough idea for what that you will have a rough idea for what that you will have a rough idea for what loss to expect at initialization and loss to expect at initialization and loss to expect at initialization and that just depends on the loss function that just depends on the loss function that just depends on the loss function and the problem setup in this case I do and the problem setup in this case I do and the problem setup in this case I do not expect 27 I expect a much lower not expect 27",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 48,
      "text": "I expect a much lower not expect 27 I expect a much lower number and we can calculate it together number and we can calculate it together number and we can calculate it together basically at initialization what we like basically at initialization what we like basically at initialization what we like is that um there's 27 characters that is that um there's 27 characters that is that um there's 27 characters that could come next for any one training could come next for any one training could come next for any one training example at initialization we have no example at initialization we have no example at initialization we have no reason to believe any characters to be reason to believe any characters to be reason to believe any characters to be much more likely than others",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 49,
      "text": "and so we'd much more likely than others",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 50,
      "text": "and so we'd much more likely than others",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 51,
      "text": "and so we'd expect that the propy distribution that expect that the propy distribution that expect that the propy distribution that comes out initially is a uniform comes out initially is a uniform comes out initially is a uniform distribution assigning about equal distribution assigning about equal distribution assigning about equal probability to all the 27 probability to all the 27 probability to all the 27 characters so basically what we' like is characters so basically what we' like is characters so basically what we' like is the probability for any character would the probability for any character would the probability for any character would be roughly 1 over 20 be roughly 1 over 20 be roughly 1 over 20 7 that is the probability we should 7 that is the probability we should 7 that is the probability we should record",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 52,
      "text": "and then the loss is the negative record and then the loss is the negative record",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 53,
      "text": "and then the loss is the negative log probability so let's wrap this in a log probability so let's wrap this in a log probability so let's wrap this in a tensor",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 54,
      "text": "and then then we can take the log tensor",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 55,
      "text": "and then then we can take the log tensor",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 56,
      "text": "and then then we can take the log of it",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 57,
      "text": "and then the negative log of it and then the negative log of it and then the negative log probability is the loss we would expect probability is the loss we would expect probability is the loss we would expect which is 3.29 much much lower than 27 which is 3.29 much much lower than 27 which is 3.29 much much lower than 27 and so what's happening right now is",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 58,
      "text": "and so what's happening right now is",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 59,
      "text": "and so what's happening right now is that at initialization the neural nut is that at initialization the neural nut is that at initialization the neural nut is creating probity distributions that are creating probity distributions that are creating probity distributions that are all messed up some characters are very all messed up some characters are very all messed up some characters are very confident and some characters are very confident and some characters are very confident and some characters are very not confident confident and then not confident confident and then not confident confident and then basically what's happening is that the basically what's happening is that the basically what's happening is that the network is very confidently wrong and uh network is very confidently wrong and uh network is very confidently wrong and uh that that's what makes it um record very that that's what makes it um record very that that's what makes it um record very high loss so here's a smaller high loss so here's a smaller high loss so here's a smaller four-dimensional example of the issue four-dimensional example of the issue four-dimensional example of the issue let's say we only have four characters let's say we only have four characters let's say we only have four characters",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 60,
      "text": "and then we have logits that come out of",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 61,
      "text": "and then we have logits that come out of and then we have logits that come out of the neural net and they are very very the neural net and they are very very the neural net and they are very very close to zero then when we take the close to zero then when we take the close to zero then when we take the softmax of all zeros we get softmax of all zeros we get softmax of all zeros we get probabilities there are a diffused probabilities there are a diffused probabilities there are a diffused distribution so sums to one and is distribution so sums to one and is distribution so sums to one and is exactly exactly exactly uniform and then in this case if the uniform and then in this case if the uniform and then in this case if the label is say two it doesn't actually label is say two it doesn't actually label is say two it doesn't actually matter if this if the label is two or matter if this if the label is two or matter if this if the label is two or three or one or zero because it's a three or one or zero because it's a three or one or zero because it's a uniform distribution we're recording the uniform distribution we're recording the uniform distribution we're recording the exact same loss in this case 1.38 so exact same loss in this case 1.38 so exact same loss in this case 1.38 so this is the loss we would expect for a this is the loss we would expect for a this is the loss we would expect for a four-dimensional example and now you can four-dimensional example and now you can four-dimensional example and now you can see of course that as we start to see of course that as we start to see of course that as we start to manipulate these logits uh we're going manipulate these logits uh we're going manipulate these logits uh we're going to be changing the law here so it could to be changing the law here so it could to be changing the law here so it could be that we lock out and by chance uh be that we lock out and by chance uh be that we lock out and by chance uh this could be a very high number like this could be a very high number like this could be a very high number like you know five or something like that you know five or something like that you know five or something like that then case we'll record a very low loss then case we'll record a very low loss then case we'll record a very low loss because we're assigning the correct because we're assigning the correct because we're assigning the correct probability at initialization by chance probability at initialization by chance probability at initialization by chance to the correct label much more likely it to the correct label much more likely it to the correct label much more likely it is that some other dimension will have a is that some other dimension will have a is that some other dimension will have a high uh logit and then what will happen high uh logit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 62,
      "text": "and then what will happen high uh logit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 63,
      "text": "and then what will happen is we start to record much higher loss is we start to record much higher loss is we start to record much higher loss and what can come what can happen is and what can come what can happen is and what can come what can happen is basically the logits come out like basically the logits come out like basically the logits come out like something like this you know and they something like this you know and they something like this you know",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 64,
      "text": "and they take on Extreme values and we record take on Extreme values",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 65,
      "text": "and we record take on Extreme values",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 66,
      "text": "and we record really high loss really high loss really high loss um for example if we have to 4. random um for example if we have to 4. random um for example if we have to 4. random of four so these are uniform um sorry of four so these are uniform um sorry of four so these are uniform um sorry these are normally distributed um these are normally distributed um these are normally distributed um numbers uh four of numbers uh four of numbers uh four of them then here we can also print the them then here we can also print the them then here we can also print the logits probabilities that come out of it logits probabilities that come out of it logits probabilities that come out of it and the loss",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 67,
      "text": "and so because these logits and the loss and so because these logits and the loss and so because these logits are near zero for the most part the loss are near zero for the most part the loss are near zero for the most part the loss that comes out is is",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 68,
      "text": "okay uh but suppose that comes out is is",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 69,
      "text": "okay uh but suppose that comes out is is",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 70,
      "text": "okay uh but suppose this is like times 10 this is like times 10 this is like times 10 now you see how because these are more now you see how because these are more now you see how because these are more extreme values it's very unlikely that extreme values it's very unlikely that extreme values it's very unlikely that you're going to be guessing the correct you're going to be guessing the correct you're going to be guessing the correct bucket",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 71,
      "text": "and then you're confidently wrong bucket",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 72,
      "text": "and then you're confidently wrong bucket",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 73,
      "text": "and then you're confidently wrong and recording very high loss if your and recording very high loss if your and recording very high loss if your loes are coming out even more loes are coming out even more loes are coming out even more extreme you might get extremely insane extreme you might get extremely insane extreme you might get extremely insane losses like infinity even at losses like infinity even at losses like infinity even at initialization initialization initialization",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 74,
      "text": "um so basically this is not good and we um so basically this is not good and we um so basically this is not good and we want the loges to be roughly zero um want the loges to be roughly zero um want the loges to be roughly zero um when the network is initialized in fact when the network is initialized in fact when the network is initialized in fact the lits can don't have to be just zero the lits can don't have to be just zero the lits can don't have to be just zero they just have to be equal",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 75,
      "text": "so for they just have to be equal",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 76,
      "text": "so for they just have to be equal",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 77,
      "text": "so for example if all the logits are one then example if all the logits are one then example if all the logits are one then because of the normalization inside the because of the normalization inside the because of the normalization inside the softmax this will actually come out okay softmax this will actually come out okay softmax this will actually come out",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 78,
      "text": "okay but by symmetry we don't want it to be but by symmetry we don't want it to be but by symmetry we don't want it to be any arbitrary positive or negative any arbitrary positive or negative any arbitrary positive or negative number we just want it to be all zeros number we just want it to be all zeros number we just want it to be all zeros and record the loss that we expect at and record the loss that we expect at and record the loss that we expect at initialization so let's now concretely initialization so let's now concretely initialization so let's now concretely see where things go wrong in our example see where things go wrong in our example see where things go wrong in our example here we have the initialization let me here we have the initialization let me here we have the initialization let me reinitialize the neuronet and here let reinitialize the neuronet and here let reinitialize the neuronet and here let me break after the very first iteration me break after the very first iteration me break after the very first iteration so we only see the initial loss which is so we only see the initial loss which is so we only see the initial loss which is 27 27 27",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 79,
      "text": "so that's way too high and intuitively so that's way too high and intuitively so that's way too high and intuitively now we can expect the variables involved now we can expect the variables involved now we can expect the variables involved and we see that the logits here if we and we see that the logits here if we and we see that the logits here if we just print some of just print some of just print some of these if we just print the first row",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 80,
      "text": "we these if we just print the first row we these if we just print the first row we see that the Lo just take on quite see that the Lo just take on quite see that the Lo just take on quite extreme values and that's what's extreme values and that's what's extreme values and that's what's creating the fake confidence in creating the fake confidence in creating the fake confidence in incorrect answers and makes the loss um incorrect answers and makes the loss um incorrect answers and makes the loss um get very very high so these loes should get very very high so these loes should get very very high so these loes should be much much closer to zero so now let's be much much closer to zero so now let's be much much closer to zero so now let's think through how we can achieve logits think through how we can achieve logits think through how we can achieve logits coming out of this neur not to be more coming out of this neur not to be more coming out of this neur not to be more closer to zero you see here that loes closer to zero you see here that loes closer to zero you see here that loes are calculated as the hidden states are calculated as the hidden states are calculated as the hidden states multip by W2 plus B2 so first of all multip by W2 plus B2",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 81,
      "text": "so first of all multip by W2 plus B2 so first of all currently we're initializing B2 as currently we're initializing B2 as currently we're initializing B2 as random values uh of the right size but random values uh of the right size but random values uh of the right size but because we want roughly zero we don't because we want roughly zero we don't because we want roughly zero we don't actually want to be adding a bias of actually want to be adding a bias of actually want to be adding a bias of random numbers so in fact I'm going to random numbers so in fact I'm going to random numbers so in fact I'm going to add a times zero here to make sure that add a times zero here to make sure that add a times zero here to make sure that B2 is just um basically zero at B2 is just um basically zero at B2 is just um basically zero at initialization and second this is H initialization and second this is H initialization and second this is H multip by W2",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 82,
      "text": "so if we want logits to be multip by W2",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 83,
      "text": "so if we want logits to be multip by W2 so if we want logits to be very very small then we would be very very small then we would be very very small then we would be multiplying W2 and making that multiplying W2 and making that multiplying W2 and making that smaller so for example if we scale down smaller so for example if we scale down smaller so for example if we scale down W2 by 0.1 all the elements then if I do W2 by 0.1 all the elements then if I do W2 by 0.1 all the elements then if I do again just a very first iteration you again just a very first iteration you again just a very first iteration you see that we are getting much closer to see that we are getting much closer to see that we are getting much closer to what we expect so rough roughly what we what we expect so rough roughly what we what we expect so rough roughly what we want is about want is about want is about 3.29 this is 3.29 this is 3.29 this is 4.2 I can make this maybe even 4.2 I can make this maybe even 4.2 I can make this maybe even smaller 3.32",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 84,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 85,
      "text": "so we're getting smaller 3.32",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 86,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 87,
      "text": "so we're getting smaller 3.32",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 88,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 89,
      "text": "so we're getting closer and closer now you're probably closer and closer now you're probably closer and closer now you're probably wondering can we just set this to zero wondering can we just set this to zero wondering can we just set this to zero then we get of course exactly what we're then we get of course exactly what we're then we get of course exactly what we're looking for um at looking for um at looking for um at initialization and the reason I don't initialization and the reason I don't initialization and the reason I don't usually do this is because I'm I'm",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 90,
      "text": "very usually do this is because I'm I'm very usually do this is because I'm I'm very nervous and I'll show you in a second nervous",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 91,
      "text": "and I'll show you in a second nervous",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 92,
      "text": "and I'll show you in a second why you don't want to be setting W's or why you don't want to be setting W's or why you don't want to be setting W's or weights of a neural nut exactly to zero weights of a neural nut exactly to zero weights of a neural nut exactly to zero um you you usually want it to be small",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 93,
      "text": "um you you usually want it to be small um you you usually want it to be small numbers instead of exactly zero um for numbers instead of exactly zero um for numbers instead of exactly zero um for this output layer in this specific case this output layer in this specific case this output layer in this specific case I think it would be fine",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 94,
      "text": "but I'll show I think it would be fine",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 95,
      "text": "but I'll show I think it would be fine",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 96,
      "text": "but I'll show you in a second where things go wrong you in a second where things go wrong you in a second where things go wrong very quick quickly if you do that so very quick quickly if you do that so very quick quickly if you do that",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 97,
      "text": "so let's just go with let's just go with let's just go with 0.01 in that case our loss is close 0.01 in that case our loss is close 0.01 in that case our loss is close enough but has some entropy it's not enough but has some entropy it's not enough but has some entropy it's not exactly zero",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 98,
      "text": "it's got some little exactly zero",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 99,
      "text": "it's got some little exactly zero",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 100,
      "text": "it's got some little entropy",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 101,
      "text": "and that's used for symmetry entropy and that's used for symmetry entropy and that's used for symmetry breaking as we'll see in a second the breaking as we'll see in a second the breaking as we'll see in a second the logits are now coming out much closer to logits are now coming out much closer to logits are now coming out much closer to zero and everything is well and good so zero and everything is well and good so zero and everything is well and good",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 102,
      "text": "so if I just erase these and I now take if I just erase these and I now take if I just erase these",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 103,
      "text": "and I now take away the break away the break away the break statement we can run the optimization statement we can run the optimization statement we can run the optimization with this new initialization and let's with this new initialization and let's with this new initialization and let's just see just see just see what losses we record",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 104,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 105,
      "text": "so I let it what losses we record",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 106,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 107,
      "text": "so I let it what losses we record",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 108,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 109,
      "text": "so I let it run and you see that we started off good run and you see that we started off good run and you see that we started off good",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 110,
      "text": "and then we came down a",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 111,
      "text": "and then we came down a",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 112,
      "text": "and then we came down a bit the plot of the loss uh now doesn't bit the plot of the loss uh now doesn't bit the plot of the loss uh now doesn't have this hockey shape appearance um have this hockey shape appearance um have this hockey shape appearance um because basically what's happening in because basically what's happening in because basically what's happening in the hockey stick the very first few the hockey stick the very first few the hockey stick the very first few iterations of the loss what's happening iterations of the loss what's happening iterations of the loss what's happening during the optimization is the during the optimization is the during the optimization is the optimization is just squashing down the optimization is just squashing down the optimization is just squashing down the logits",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 113,
      "text": "and then it's rearranging the logits",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 114,
      "text": "and then it's rearranging the logits",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 115,
      "text": "and then it's rearranging the logits so basically we took away this logits",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 116,
      "text": "so basically we took away this logits",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 117,
      "text": "so basically we took away this easy part of the loss function where easy part of the loss function where easy part of the loss function where just the the weights were just being just the the weights were just being just the the weights were just being shrunk down",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 118,
      "text": "and so therefore we're we shrunk down",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 119,
      "text": "and so therefore we're we shrunk down",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 120,
      "text": "and so therefore we're we don't we don't get these easy gains in don't we don't get these easy gains in don't we don't get these easy gains in the beginning and we're just getting the beginning and we're just getting the beginning and we're just getting some of the hard gains of training the some of the hard gains of training the some of the hard gains of training the actual neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 121,
      "text": "and so there's no actual neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 122,
      "text": "and so there's no actual neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 123,
      "text": "and so there's no hockey stick appearance so good things hockey stick appearance so good things",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 124,
      "text": "hockey stick appearance so good things are happening in that both number one are happening in that both number one are happening in that both number one losset initialization is what we expect losset initialization is what we expect losset initialization is what we expect and the the loss doesn't look like a and the the loss doesn't look like a and the the loss doesn't look like a hockey stick and this is true for any hockey stick and this is true for any hockey stick and this is true for any neuron that you might train um and neuron that you might train um and neuron that you might train um and something to look out for and second the something to look out for and second the something to look out for and second the loss that came out is actually quite a loss that came out is actually quite a loss that came out is actually quite a bit improved unfortunately I erased what bit improved unfortunately I erased what bit improved unfortunately I erased what we had here before I believe this was 2.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 125,
      "text": "we had here before I believe this was 2.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 126,
      "text": "we had here before I believe this was 2. um2",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 127,
      "text": "and this was this was 2.16",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 128,
      "text": "so we get um2",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 129,
      "text": "and this was this was 2.16",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 130,
      "text": "so we get um2",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 131,
      "text": "and this was this was 2.16",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 132,
      "text": "so we get a slightly improved result and the a slightly improved result and the a slightly improved result and the reason for that is uh because we're reason for that is uh because we're reason for that is uh because we're spending more Cycles more time spending more Cycles more time spending more Cycles more time optimizing the neuronet actually instead optimizing the neuronet actually instead optimizing the neuronet actually",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 133,
      "text": "instead of just uh spending the first several of just uh spending the first several of just uh spending the first several thousand iterations probably just thousand iterations probably just thousand iterations probably just squashing down the squashing down the squashing down the weights because they are so way too high weights because they are so way too high weights because they are so way too high in the beginning in the initialization in the beginning in the initialization in the beginning in the initialization so something to look out for and uh so something to look out for and uh so something to look out for and uh that's number one now let's look at the that's number one now let's look at the that's number one now let's look at the second problem let me reinitialize our second problem let me reinitialize our second problem let me reinitialize our neural net and let me reintroduce The neural net and let me reintroduce The neural net and let me reintroduce The Brak statement",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 134,
      "text": "so we have a reasonable Brak statement so we have a reasonable Brak statement",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 135,
      "text": "so we have a reasonable initial loss so even though everything initial loss so even though everything initial loss so even though everything is looking good on the level of the loss is looking good on the level of the loss is looking good on the level of the loss and we get something that we expect and we get something that we expect and we get something that we expect there's still a deeper problem looking there's still a deeper problem looking there's still a deeper problem looking inside this neural net and its inside this neural net and its inside this neural net and its initialization so the logits are now initialization so the logits are now initialization so the logits are now",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 136,
      "text": "okay the problem now is with the values",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 137,
      "text": "okay the problem now is with the values",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 138,
      "text": "okay the problem now is with the values of H the activations of the Hidden of H the activations of the Hidden of H the activations of the Hidden States now if we just visualize this States now if we just visualize this States now if we just visualize this Vector sorry this tensor",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 139,
      "text": "h",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 140,
      "text": "it's kind of Vector sorry this tensor",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 141,
      "text": "h",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 142,
      "text": "it's kind of Vector sorry this tensor",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 143,
      "text": "h",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 144,
      "text": "it's kind of hard to see but the problem here roughly hard to see but the problem here roughly hard to see but the problem here roughly speaking is you see how many of the speaking is you see how many of the speaking is you see how many of the elements are one or negative 1 now elements are one or negative 1 now elements are one or negative 1 now recall that torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 145,
      "text": "10 the 10 function is recall that torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 146,
      "text": "10 the 10 function is recall that torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 147,
      "text": "10 the 10 function is a squashing function it takes arbitrary a squashing function it takes arbitrary a squashing function it takes arbitrary numbers and it squashes them into a numbers and it squashes them into a numbers and it squashes them into a range of negative 1 and one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 148,
      "text": "and it does range of negative 1 and one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 149,
      "text": "and it does range of negative 1 and one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 150,
      "text": "and it does so smoothly so let's look at the so smoothly so let's look at the so smoothly so let's look at the histogram of H to get a better idea of histogram of H to get a better idea of histogram of H to get a better idea of the distribution of the values inside the distribution of the values inside the distribution of the values inside this tensor we can do this this tensor we can do this this tensor we can do this",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 151,
      "text": "first well we can see that H is 32 first",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 152,
      "text": "well we can see that H is 32 first",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 153,
      "text": "well we can see that H is 32 examples and 200 activations in each examples and 200 activations in each examples and 200 activations in each example we can view it as1 to stretch it example we can view it as1 to stretch it example we can view it as1 to stretch it out into one large out into one large out into one large vector and we can then call two list to vector and we can then call two list to vector and we can then call two list to convert this into one large python list convert this into one large python list convert this into one large python list of floats and then we can pass this into of floats and then we can pass this into of floats and then we can pass this into PLT doist for histogram and we say we PLT doist for histogram",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 154,
      "text": "and we say we PLT doist for histogram and we say we want 50 bins and a semicolon to suppress want 50 bins and a semicolon to suppress want 50 bins and a semicolon to suppress a bunch of output we don't a bunch of output we don't a bunch of output we don't want",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 155,
      "text": "so we see this histogram",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 156,
      "text": "and we see want",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 157,
      "text": "so we see this histogram",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 158,
      "text": "and we see want",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 159,
      "text": "so we see this histogram",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 160,
      "text": "and we see that most the values by far take on that most the values by far take on that most the values by far take on value of netive one and one so this 10 H value of netive one and one so this 10 H value of netive one and one so this 10 H is very very active and we can also look is very very active",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 161,
      "text": "and we can also look is very very active",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 162,
      "text": "and we can also look at basically why that is we can look at at basically why that is we can look at at basically why that is we can look at the pre activations that feed into the the pre activations that feed into the the pre activations that feed into the 10",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 163,
      "text": "and we can see that the distribution 10 and we can see that the distribution 10 and we can see that the distribution of the pre activations are is very very of the pre activations are is very very of the pre activations are is very very broad these take numbers between -5 and broad these take numbers between -5 and broad these take numbers between -5 and 15 and that's why in a torure 10 15",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 164,
      "text": "and that's why in a torure 10 15",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 165,
      "text": "and that's why in a torure 10 everything is being squashed and capped everything is being squashed and capped everything is being squashed and capped to be in the range of negative 1 and one to be in the range of negative 1 and one to be in the range of negative 1 and one and lots of numbers here take on very and lots of numbers here take on very and lots of numbers here take on very extreme values now if you are new to extreme values now if you are new to extreme values now if you are new to neural networks you might not actually neural networks you might not actually neural networks you might not actually see this as an issue",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 166,
      "text": "but if you're well see this as an issue but if you're well see this as an issue but if you're well vered in the dark arts of back vered in the dark arts of back vered in the dark arts of back propagation and then having an intuitive propagation and then having an intuitive propagation and then having an intuitive sense of how these gradients flow sense of how these gradients flow sense of how these gradients flow through a neural net you are looking at through a neural net you are looking at through a neural net you are looking at your distribution of 10h activations your distribution of 10h activations your distribution of 10h activations here and you are sweating so let me show here and you are sweating so let me show here and you are sweating so let me show you why we have to keep in mind that you why we have to keep in mind that you why we have to keep in mind that during back propagation just like we saw during back propagation just like we saw during back propagation just like we saw in microad we are doing backward passs in microad we are doing backward passs in microad we are doing backward passs starting at the loss and flowing through starting at the loss and flowing through starting at the loss and flowing through the network backwards in particular the network backwards in particular the network backwards in particular we're going to back propagate through we're going to back propagate through we're going to back propagate through this torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 167,
      "text": "this torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 168,
      "text": "this torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 169,
      "text": "10h and this layer here is made up of 10h and this layer here is made up of 10h and this layer here is made up of 200 neurons for each one of these 200 neurons for each one of these 200 neurons for each one of these examples and uh it implements an examples and uh it implements an examples and uh it implements an elementwise 10 so let's look at what elementwise 10 so let's look at what elementwise 10 so let's look at what happens in 10h in the backward pass we happens in 10h in the backward pass we happens in 10h in the backward pass we can actually go back to our previous uh can actually go back to our previous uh can actually go back to our previous uh microgr code in the very first lecture microgr code in the very first lecture microgr code in the very first lecture and see how we implemented 10 AG we saw and see how we implemented 10 AG we saw and see how we implemented 10 AG we saw that the input here was X",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 170,
      "text": "and then we that the input here was X",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 171,
      "text": "and then we that the input here was X",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 172,
      "text": "and then we calculate T which is the 10 age of X so calculate T which is the 10 age of X so calculate T which is the 10 age of X",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 173,
      "text": "so that's T and T is between 1 and 1",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 174,
      "text": "it's that's T and T is between 1 and 1",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 175,
      "text": "it's that's T and T is between 1 and 1",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 176,
      "text": "it's the output of the 10 H and then in the the output of the 10 H and then in the the output of the 10 H and then in the backward pass how do we back propagate backward pass how do we back propagate backward pass how do we back propagate through a 10 H we take out that grad um through a 10 H we take out that grad um through a 10 H we take out that grad",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 177,
      "text": "um",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 178,
      "text": "and then we multiply it this is the",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 179,
      "text": "and then we multiply it this is the",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 180,
      "text": "and then we multiply it this is the chain rule with the local gradient which chain rule with the local gradient which chain rule with the local gradient which took the form of 1 - t ^2",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 181,
      "text": "so what took the form of 1 - t ^2",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 182,
      "text": "so what took the form of 1 - t ^2",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 183,
      "text": "so what happens if the outputs of your t h are happens if the outputs of your t h are happens if the outputs of your t h are very close to1 or 1 if you plug in t one very close to1 or 1 if you plug in t one very close to1 or 1 if you plug in t one here you're going to get a zero here you're going to get a zero here you're going to get a zero multiplying out.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 184,
      "text": "grad no matter what multiplying out.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 185,
      "text": "grad no matter what multiplying out.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 186,
      "text": "grad no matter what out.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 187,
      "text": "grad is we are killing the gradient out.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 188,
      "text": "grad is we are killing the gradient out.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 189,
      "text": "grad is we are killing the gradient and we're stopping effectively the back",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 190,
      "text": "and we're stopping effectively the back",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 191,
      "text": "and we're stopping effectively the back propagation through this 10 unit propagation through this 10 unit propagation through this 10 unit similarly when t is1 this",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 192,
      "text": "will again similarly when t is1 this will again similarly when t is1 this will again become zero and out that grad just stops become zero and out that grad just stops become zero and out that grad just stops and intuitively this makes sense because and intuitively this makes sense because and intuitively this makes sense because this is a 10h this is a 10h this is a 10h neuron and what's happening is if its neuron and what's happening is if its neuron and what's happening is if its output is very close to one then we are output is very close to one then we are output is very close to one then we are in the tail of this in the tail of this in the tail of this 10 and so changing basically the 10 and so changing basically the 10 and so changing basically the input is not going to impact the output input is not going to impact the output input is not going to impact the output of the 10 too much because it's it's so of the 10 too much because it's it's so of the 10 too much because it's it's",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 193,
      "text": "so it's in a flat region of the 10 H",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 194,
      "text": "and so it's in a flat region of the 10 H",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 195,
      "text": "and so it's in a flat region of the 10 H",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 196,
      "text": "and so therefore there's no impact on the loss therefore there's no impact on the loss therefore there's no impact on the loss",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 197,
      "text": "and so so indeed the the weights and the",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 198,
      "text": "and so so indeed the the weights and the and so so indeed the the weights and the biases along with the 10h neuron do not biases along with the 10h neuron do not biases along with the 10h neuron do not impact the loss because the output of impact the loss because the output of impact the loss because the output of the 10 unit is in the flat region of the the 10 unit is in the flat region of the the 10 unit is in the flat region of the 10 and there's no influence we can we 10 and there's no influence we can we 10 and there's no influence we can we can be changing them whatever we want can be changing them whatever we want can be changing them whatever we want however we want and the loss is not however we want and the loss is not however we want and the loss is not impacted that's so that's another way to impacted that's so that's another way to impacted that's so that's another way to justify that indeed the gradient would justify that indeed the gradient would justify that indeed the gradient would be basically zero it be basically zero it be basically zero it vanishes indeed uh when T equals zero we vanishes indeed uh when T equals zero we vanishes indeed uh when T equals zero we get one times out that grad",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 199,
      "text": "so when the get one times out that grad",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 200,
      "text": "so when the get one times out that grad",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 201,
      "text": "so when the 10 h takes on exactly value of zero then 10 h takes on exactly value of zero then 10 h takes on exactly value of zero then out grad is just passed through so out grad is just passed through so out grad is just passed through so basically what this is doing right is if basically what this is doing right is if basically what this is doing right is if T is equal to zero then this the 10 unit T is equal to zero then this the 10 unit T is equal to zero then this the 10 unit is uh sort of inactive and uh gradient is uh sort of inactive and uh gradient is uh sort of inactive and uh gradient just passes through but the more you are just passes through but the more you are just passes through but the more you are in the flat tails the more the gradient in the flat tails the more the gradient in the flat tails the more the gradient is squashed",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 202,
      "text": "so in fact you'll see that is squashed so in fact you'll see that is squashed so in fact you'll see that the the gradient flowing through 10 can the the gradient flowing through 10 can the the gradient flowing through 10 can only ever decrease and the amount that only ever decrease and the amount that only ever decrease and the amount that it decreases is um proportional through it decreases is um proportional through it decreases is um proportional through a square here um depending on how far a square here um depending on how far a square here um depending on how far you are in the flat tail so this 10 H you are in the flat tail so this 10 H you are in the flat tail so this 10 H",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 203,
      "text": "and so that's kind of what's Happening",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 204,
      "text": "and so that's kind of what's Happening",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 205,
      "text": "and so that's kind of what's Happening Here and through this the concern here Here and through this the concern here Here and through this the concern here is that if all of these um outputs H are is that if all of these um outputs H are is that if all of these um outputs H are in the flat regions of negative 1 and in the flat regions of negative 1 and in the flat regions of negative 1 and one then the gradients that are flowing one then the gradients that are flowing one then the gradients that are flowing through the network will just get through the network will just get through the network will just get destroyed at this destroyed at this destroyed at this layer now there is some redeeming layer now there is some redeeming layer now there is some redeeming quality here and that we can actually quality here and that we can actually quality here and that we can actually get a sense of the problem here as get a sense of the problem here as get a sense of the problem here as follows I wrote some code here and follows I wrote some code here and follows I wrote some code here and basically what we want to do here is we basically what we want to do here is we basically what we want to do here is we want to take a look at H take the the want to take a look at H take the the want to take a look at H take the the absolute value and see how often it is absolute value and see how often it is absolute value and see how often it is in the in a flat uh region so say in the in a flat uh region so say in the in a flat uh region so say greater than greater than greater than 099 and what you get is the following 099 and what you get is the following 099 and what you get is the following and this is a Boolean tensor so uh in",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 206,
      "text": "and this is a Boolean tensor",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 207,
      "text": "so uh in and this is a Boolean tensor so uh in the Boolean tensor you get a white if the Boolean tensor you get a white if the Boolean tensor you get a white if this is true and a black if this is this is true and a black if this is this is true and a black if this is false",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 208,
      "text": "and so basically what we have here false",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 209,
      "text": "and so basically what we have here false",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 210,
      "text": "and so basically what we have here is the 32 examples and 200 hidden is the 32 examples and 200 hidden is the 32 examples and 200 hidden neurons and we see that a lot of this is neurons and we see that a lot of this is neurons and we see that a lot of this is white and what that's telling us is that white and what that's telling us is that white",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 211,
      "text": "and what that's telling us is that all these 10h neurons were very very all these 10h neurons were very very all these 10h neurons were very very active and uh they're in a flat tail and active",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 212,
      "text": "and uh they're in a flat tail and active",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 213,
      "text": "and uh they're in a flat tail and so in all these cases uh the back the so in all these cases uh the back the so in all these cases uh the back the backward gradient would get uh backward gradient would get uh backward gradient would get uh destroyed now we would be in a lot of destroyed now we would be in a lot of destroyed now we would be in a lot of trouble if for for any one of these 200 trouble if for for any one of these 200 trouble if for for any one of these 200 neurons if it was the case that the neurons if it was the case that the neurons if it was the case that the entire column is white because in that entire column is white because in that entire column is white because in that case we have what's called a dead neuron case we have what's called a dead neuron case we have what's called a dead neuron and this is could be a 10 neuron where and this is could be a 10 neuron where and this is could be a 10 neuron where the initialization of the weights and the initialization of the weights and the initialization of the weights and the biases could be such that no single the biases could be such that no single the biases could be such that no single example ever activates uh this 10h in example ever activates uh this 10h in example ever activates uh this 10h in the um sort of active part of the 10age the um sort of active part of the 10age the um sort of active part of the 10age if all the examples land in the tail if all the examples land in the tail if all the examples land in the tail then this neuron will never learn it is then this neuron will never learn it is then this neuron will never learn it is a dead neuron and so just scrutinizing a dead neuron and so just scrutinizing a dead neuron and so just scrutinizing this and looking for Columns of this and looking for Columns of this and looking for Columns of completely white uh we see that this is completely white uh we see that this is completely white uh we see that this is not the case",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 214,
      "text": "so uh I don't see a single not the case",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 215,
      "text": "so uh I don't see a single not the case",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 216,
      "text": "so uh I don't see a single neuron that is all of uh you know white neuron that is all of uh you know white neuron that is all of uh you know white",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 217,
      "text": "and so therefore it is the case that for and so therefore it is the case that for and so therefore it is the case that for every one of these 10h neurons uh we do every one of these 10h neurons uh we do every one of these 10h neurons uh we do have some examples that activate them in have some examples that activate them in have some examples that activate them in the uh active part of the 10 and so some the uh active part of the 10 and so some the uh active part of the 10 and so some gradients will flow through and this gradients will flow through and this gradients will flow through and this neuron will learn and the neuron will neuron will learn and the neuron will neuron will learn and the neuron will change and it will move and it will do change and it will move and it will do change and it will move and it will do something",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 218,
      "text": "but you can sometimes get get something",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 219,
      "text": "but you can sometimes get get something",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 220,
      "text": "but you can sometimes get get yourself in cases where you have dead yourself in cases where you have dead yourself in cases where you have dead neurons and the way this manifests is neurons and the way this manifests is neurons and the way this manifests is that um for 10h neuron this would be that um for 10h neuron this would be that um for 10h neuron this would be when no matter what inputs you plug in when no matter what inputs you plug in when no matter what inputs you plug in from your data set this 10h neuron from your data set this 10h neuron from your data set this 10h neuron always fir always fir always fir completely one or completely negative completely one or completely negative completely one or completely negative one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 221,
      "text": "and then it will just not learn one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 222,
      "text": "and then it will just not learn one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 223,
      "text": "and then it will just not learn because all the gradients will be just because all the gradients will be just because all the gradients will be just zeroed out uh this is true not just for zeroed out uh this is true not just for zeroed out uh this is true not just for 10 but for a lot of other nonlinearities 10 but for a lot of other nonlinearities 10 but for a lot of other nonlinearities that people use in neural networks so we that people use in neural networks",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 224,
      "text": "so we that people use in neural networks",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 225,
      "text": "so we certainly used 10 a lot but sigmoid will certainly used 10 a lot but sigmoid will certainly used 10 a lot but sigmoid will have the exact same issue because it is have the exact same issue because it is have the exact same issue because it is a squashing neuron",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 226,
      "text": "and so the same will a squashing neuron",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 227,
      "text": "and so the same will a squashing neuron",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 228,
      "text": "and so the same will be true for sigmoid uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 229,
      "text": "but um but um you be true for sigmoid uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 230,
      "text": "but um but um you be true for sigmoid uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 231,
      "text": "but um but um you know um basically the same will actually know um basically the same will actually know um basically the same will actually apply to sigmoid the same will also apply to sigmoid the same will also apply to sigmoid the same will also apply to reu apply to reu apply to reu",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 232,
      "text": "so reu has a completely flat region here so reu has a completely flat region here so reu has a completely flat region here below zero",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 233,
      "text": "so if you have a reu neuron below zero so if you have a reu neuron below zero",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 234,
      "text": "so if you have a reu neuron then it is a pass through um if it is then it is a pass through um if it is then it is a pass through um if it is positive and if it's if the positive",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 235,
      "text": "and if it's if the positive and if it's if the preactivation is negative it will just preactivation is negative it will just preactivation is negative it will just shut it off since the region here is shut it off since the region here is shut it off since the region here is completely flat then during back completely flat then during back completely flat then during back propagation uh this would be exactly propagation uh this would be exactly propagation uh this would be exactly zeroing out the gradient um like all of zeroing out the gradient um like all of zeroing out the gradient um like all of the gradient would be set exactly to the gradient would be set exactly to the gradient would be set exactly to zero instead of just like a very very zero instead of just like a very very zero instead of just like a very very small number depending on how positive small number depending on how positive small number depending on how positive or negative T is",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 236,
      "text": "and so you can get for or negative T is",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 237,
      "text": "and so you can get for or negative T is",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 238,
      "text": "and so you can get for example a dead reu neuron and a dead reu example a dead reu neuron and a dead reu example a dead reu neuron and a dead reu neuron would basically look like neuron would basically look like neuron would basically look like basically what it is is if a neuron with basically what it is is if a neuron with basically what it is is if a neuron with a reu nonlinearity never activates so a reu nonlinearity never activates so a reu nonlinearity never activates so for any examples that you plug in in the for any examples that you plug in in the for any examples that you plug in in the data set it never turns on it's always data set it never turns on it's always data set it never turns on it's always in this flat region then this re neuron in this flat region then this re neuron in this flat region then this re neuron is a dead neuron its weights and bias is a dead neuron its weights and bias is a dead neuron its weights and bias will never learn they will never get a will never learn they will never get a will never learn they will never get a gradient because the neuron never gradient because the neuron never gradient because the neuron never activated and this can sometimes happen activated and this can sometimes happen activated and this can sometimes happen at initialization uh because the way and at initialization uh because the way and at initialization uh because the way and a biases just make it so that by chance a biases just make it so that by chance a biases just make it so that by chance some neurons are just forever dead but some neurons are just forever dead but some neurons are just forever dead",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 239,
      "text": "but it can also happen during optimization it can also happen during optimization it can also happen during optimization if you have like a too high of learning if you have like a too high of learning if you have like a too high of learning rate for example",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 240,
      "text": "sometimes you have rate for example sometimes you have rate for example sometimes you have these neurons that get too much of a these neurons that get too much of a these neurons that get too much of a gradient and they get knocked out off gradient and they get knocked out off gradient and they get knocked out off the data the data the data manifold and what happens is that from manifold and what happens is that from manifold and what happens is that from then on no example ever activates this then on no example ever activates this then on no example ever activates this neuron so this neuron remains dead neuron",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 241,
      "text": "so this neuron remains dead neuron",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 242,
      "text": "so this neuron remains dead forever so it's kind of like a permanent forever",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 243,
      "text": "so it's kind of like a permanent forever",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 244,
      "text": "so it's kind of like a permanent brain damage in a in a mind of a network brain damage in a in a mind of a network brain damage in a in a mind of a network",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 245,
      "text": "and so sometimes what can happen is if and so sometimes what can happen is if and so sometimes what can happen is if your learning rate is very high for your learning rate is very high for your learning rate is very high for example and you have a neural net with example and you have a neural net with example and you have a neural net with neurons you train the neuron net and you neurons you train the neuron net and you neurons you train the neuron net and you get some last loss",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 246,
      "text": "but then actually get some last loss",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 247,
      "text": "but then actually get some last loss",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 248,
      "text": "but then actually what you do is you go through the entire what you do is you go through the entire what you do is you go through the entire training set",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 249,
      "text": "and you forward um your training set and you forward um your training set and you forward um your examples and you can find neurons that examples and you can find neurons that examples and you can find neurons that never activate they are dead neurons in never activate they are dead neurons in never activate they are dead neurons in your network and so those neurons will your network and so those neurons will your network and so those neurons will will never turn on and usually what will never turn on and usually what will never turn on and usually what happens is that during training these happens is that during training these happens is that during training these Rel neurons are changing moving Etc and Rel neurons are changing moving Etc and Rel neurons are changing moving Etc and then because of a high gradient then because of a high gradient then because of a high gradient somewhere by chance they get knocked off somewhere by chance they get knocked off somewhere by chance they get knocked off and then nothing ever activates them and and then nothing ever activates them and and then nothing ever activates them and from then on they are just dead uh so from then on they are just dead",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 250,
      "text": "uh so from then on they are just dead uh so that's kind of like a permanent brain that's kind of like a permanent brain that's kind of like a permanent brain damage that can happen to some of these damage that can happen to some of these damage that can happen to some of these neurons these other nonlinearities like neurons these other nonlinearities like neurons these other nonlinearities like leyu will not suffer from this issue as leyu will not suffer from this issue as leyu will not suffer from this issue as much because you can see that it doesn't much because you can see that it doesn't much because you can see that it doesn't have flat Tails you'll almost always get have flat Tails you'll almost always get have flat Tails you'll almost always get gradients and uh elu is also fairly uh gradients and uh elu is also fairly uh gradients and uh elu is also fairly uh frequently used um it also might suffer frequently used um it also might suffer frequently used um it also might suffer from this issue because it has flat from this issue because it has flat from this issue because it has flat parts so that's just something to be parts so that's just something to be parts so that's just something to be aware of and something to be concerned aware of and something to be concerned aware of and something to be concerned about and in this case we have way too about and in this case we have way too about and in this case we have way too many um activations AG that take on many um activations AG that take on many um activations AG that take on Extreme values and because there's no Extreme values and because there's no Extreme values and because there's no column of white I think we will be okay column of white",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 251,
      "text": "I think we will be okay column of white",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 252,
      "text": "I think we will be okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 253,
      "text": "and indeed the network optimizes and and indeed the network optimizes and and indeed the network optimizes and gives us a pretty decent loss but it's gives us a pretty decent loss but it's gives us a pretty decent loss but it's just not optimal and this is not just not optimal and this is not just not optimal and this is not something you want especially during something you want especially during something you want especially during initialization and so basically what's initialization and so basically what's initialization and so basically what's happening is that uh this H happening is that uh this H happening is that uh this H preactivation that's floating to 10 H preactivation that's floating to 10 H preactivation that's floating to 10 H",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 254,
      "text": "it's it's too extreme",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 255,
      "text": "it's too large",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 256,
      "text": "it's it's too extreme",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 257,
      "text": "it's too large",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 258,
      "text": "it's it's too extreme",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 259,
      "text": "it's too large it's creating very um it's creating a it's creating very um",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 260,
      "text": "it's creating a it's creating very um it's creating a distribution that is too saturated in distribution that is too saturated in distribution that is too saturated in both sides of the 10 H",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 261,
      "text": "and it's not both sides of the 10 H",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 262,
      "text": "and it's not both sides of the 10 H",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 263,
      "text": "and it's not something you want because it means that something you want because it means that something you want because it means that there's less training uh for these there's less training uh for these there's less training uh for these neurons because they update um less neurons because they update um less neurons because they update um less frequently",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 264,
      "text": "so how do we fix this well H frequently",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 265,
      "text": "so how do we fix this well H frequently",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 266,
      "text": "so how do we fix this well H preactivation is MCAT which comes from C preactivation is MCAT which comes from C preactivation is MCAT which comes from C so these are uniform gsan",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 267,
      "text": "but then it's so these are uniform gsan",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 268,
      "text": "but then it's so these are uniform gsan",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 269,
      "text": "but then it's multiply by W1 plus B1 and",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 270,
      "text": "H preact is multiply by W1 plus B1 and H preact is multiply by W1 plus B1 and H preact is too far off from zero and that's causing too far off from zero and that's causing too far off from zero and that's causing the issue so we want this reactivation the issue",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 271,
      "text": "so we want this reactivation the issue",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 272,
      "text": "so we want this reactivation to be closer to zero very similar to to be closer to zero very similar to to be closer to zero very similar to what we had with what we had with what we had with logits so here logits so here logits",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 273,
      "text": "so here we want actually something very very we want actually something very very we want actually something very very similar now it's okay to set the biases similar now it's okay to set the biases similar now it's okay to set the biases to very small number we can either to very small number we can either to very small number we can either multiply by 0 01 to get like a little multiply by 0 01 to get like a little multiply by 0 01 to get like a little bit of entropy",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 274,
      "text": "um I sometimes like to do bit of entropy um I sometimes like to do bit of entropy",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 275,
      "text": "um I sometimes like to do that um just so that there's like a that um just so that there's like a that um just so that there's like a little bit of variation and diversity in little bit of variation and diversity in little bit of variation and diversity in the original initialization of these 10 the original initialization of these 10 the original initialization of these 10 H neurons and I find in practice that H neurons and I find in practice that H neurons and I find in practice that that can help optimization a little bit that can help optimization a little bit that can help optimization a little bit and then the weights we can also just and then the weights we can also just and then the weights we can also just like squash so let's multiply everything like squash so let's multiply everything like squash so let's multiply everything by 0.1 by 0.1 by 0.1 let's rerun the first batch and now let's rerun the first batch and now let's rerun the first batch and now let's look at this and well first let's let's look at this and well first let's let's look at this and well first let's look look look here you see now because we multiply dou here you see now because we multiply dou here you see now because we multiply dou by 0.1 we have a much better histogram by 0.1 we have a much better histogram by 0.1 we have a much better histogram and that's because the pre activations and that's because the pre activations and that's because the pre activations are now between 1.5 and 1.5 and this we are now between 1.5 and 1.5 and this we are now between 1.5 and 1.5 and this we expect much much less white",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 276,
      "text": "okay there's expect much much less white",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 277,
      "text": "okay there's expect much much less white",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 278,
      "text": "okay there's no white",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 279,
      "text": "so basically that's because no white so basically that's because no white so basically that's because there are no neurons that saturated there are no neurons that saturated there are no neurons that saturated above 99 in either direction",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 280,
      "text": "so this above 99 in either direction so this above 99 in either direction so this actually a pretty decent place to be um actually a pretty decent place to be um actually a pretty decent place to be um maybe we can go up a little maybe we can go up a little maybe we can go up a little bit sorry am I am I changing W1 here so bit sorry am I am I changing W1 here so bit sorry am I am I changing W1 here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 281,
      "text": "so maybe we can go to 0 maybe we can go to 0 maybe we can go to 0 2",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 282,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 283,
      "text": "so maybe something like this is 2",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 284,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 285,
      "text": "so maybe something like this is 2",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 286,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 287,
      "text": "so maybe something like this is is a nice distribution",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 288,
      "text": "so maybe this is is a nice distribution",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 289,
      "text": "so maybe this is is a nice distribution",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 290,
      "text": "so maybe this is what our initialization should be so let what our initialization should be",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 291,
      "text": "so let what our initialization should be so let me now me now me now erase erase erase these and let me starting with these and let me starting with these and let me starting with initialization let me run the full initialization let me run the full initialization let me run the full optimization optimization optimization without the break and uh let's see what without the break and uh let's see what without the break and uh let's see what we get",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 292,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 293,
      "text": "so the optimization finished we get",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 294,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 295,
      "text": "so the optimization finished we get",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 296,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 297,
      "text": "so the optimization finished and I re the loss",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 298,
      "text": "and this is the result and I re the loss",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 299,
      "text": "and this is the result and I re the loss",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 300,
      "text": "and this is the result that we get",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 301,
      "text": "and then just as a reminder that we get and then just as a reminder that we get and then just as a reminder I put down all the losses that we saw I put down all the losses that we saw I put down all the losses that we saw previously in this lecture",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 302,
      "text": "so we see previously in this lecture",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 303,
      "text": "so we see previously in this lecture",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 304,
      "text": "so we see that we actually do get an improvement that we actually do get an improvement that we actually do get an improvement here and just as a reminder we started here and just as a reminder we started here and just as a reminder we started off with a validation loss of 2.17 when off with a validation loss of 2.17 when off with a validation loss of 2.17 when we started by fixing the softmax being we started by fixing the softmax being we started by fixing the softmax being confidently wrong we came down to 2.13 confidently wrong we came down to 2.13 confidently wrong we came down to 2.13 and by fixing the 10h layer being way and by fixing the 10h layer being way and by fixing the 10h layer being way too saturated we came down to 2.10 too saturated we came down to 2.10 too saturated we came down to 2.10 and the reason this is happening of and the reason this is happening of and the reason this is happening of course is because our initialization is course is because our initialization is course is because our initialization is better and so we're spending more time better and so we're spending more time better and so we're spending more time doing productive training instead of um doing productive training instead of um doing productive training instead of um not very productive training because our not very productive training because our not very productive training because our gradients are set to zero and uh we have gradients are set to zero and uh we have gradients are set to zero and uh we have to learn very simple things like uh the to learn very simple things like uh the to learn very simple things like uh the overconfidence of the softmax in the overconfidence of the softmax in the overconfidence of the softmax in the beginning and we're spending Cycles just beginning and we're spending Cycles just beginning and we're spending Cycles just like squashing down the weight Matrix so like squashing down the weight Matrix so like squashing down the weight Matrix",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 305,
      "text": "so this is illustrating um basically this is illustrating um basically this is illustrating um basically initialization and its impacts on initialization and its impacts on initialization and its impacts on performance uh just by being aware of performance uh just by being aware of performance uh just by being aware of the internals of these neural net and the internals of these neural net and the internals of these neural net and their activations their gradients now their activations their gradients now their activations their gradients now we're working with a very small Network we're working with a very small Network we're working with a very small Network this is just one layer multi-layer this is just one layer multi-layer this is just one layer multi-layer perception so because the network is so perception so because the network is so perception so because the network is so shallow the optimization problem is shallow the optimization problem is shallow the optimization problem is actually quite easy and very forgiving actually quite easy and very forgiving actually quite easy and very forgiving so even though our initialization was so even though our initialization was so even though our initialization was terrible the network still learned terrible the network still learned terrible the network still learned eventually it just got a bit worse eventually it just got a bit worse eventually it just got a bit worse result this is not the case in general result this is not the case in general result this is not the case in general though once we actually start um working though once we actually start um working though once we actually start um working with much deeper networks that have say with much deeper networks that have say with much deeper networks that have say 50 layers uh things can get uh much more 50 layers uh things can get uh much more 50 layers uh things can get uh much more complicated and uh these problems stack complicated",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 306,
      "text": "and uh these problems stack complicated",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 307,
      "text": "and uh these problems stack up",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 308,
      "text": "and so you can actually get into a up",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 309,
      "text": "and so you can actually get into a up",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 310,
      "text": "and so you can actually get into a place where the network is basically not place where the network is basically not place where the network is basically not training at all if your initialization training at all if your initialization training at all if your initialization is bad enough and the deeper your is bad enough and the deeper your is bad enough and the deeper your network is and the more complex it is network is and the more complex it is network is and the more complex it is the less forgiving it is to some of the less forgiving it is to some of the less forgiving it is to some of these errors and so um something to these errors and so um something to these errors and so um something to definitely be aware of and uh something definitely be aware of and uh something definitely be aware of and uh something to scrutinize something to plot and to scrutinize something to plot and to scrutinize something to plot and something to be careful with and um yeah something to be careful with and um yeah something to be careful with and um",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 311,
      "text": "yeah",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 312,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 313,
      "text": "so that's great that that worked",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 314,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 315,
      "text": "so that's great that that worked",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 316,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 317,
      "text": "so that's great that that worked for us but what we have here now is all for us but what we have here now is all for us",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 318,
      "text": "but what we have here now is all these magic numbers like0 2 like where these magic numbers like0 2 like where these magic numbers like0 2 like where do I come up with this and how am I do I come up with this and how am I do I come up with this and how am I supposed to set these if I have a large supposed to set these if I have a large supposed to set these if I have a large neural net with lots and lots of layers neural net with lots and lots of layers neural net with lots and lots of layers",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 319,
      "text": "and so obviously no one does this by",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 320,
      "text": "and so obviously no one does this by",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 321,
      "text": "and so obviously no one does this by hand there's actually some relatively hand there's actually some relatively hand there's actually some relatively principled ways of setting these scales principled ways of setting these scales principled ways of setting these scales um that I would like to introduce to you um that I would like to introduce to you um that I would like to introduce to you now so let me paste some code here that now so let me paste some code here that now so let me paste some code here that I prepared just to motivate the I prepared just to motivate the I prepared just to motivate the discussion of discussion of discussion of this",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 322,
      "text": "so what I'm doing here is we have this so what I'm doing here is we have this",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 323,
      "text": "so what I'm doing here is we have some random input here x that is drawn some random input here x that is drawn some random input here x that is drawn from a gan and there's 1,000 examples from a gan and there's 1,000 examples from a gan and there's 1,000 examples that are 10 dimensional that are 10 dimensional that are 10 dimensional",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 324,
      "text": "and then we have a waiting layer here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 325,
      "text": "and then we have a waiting layer here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 326,
      "text": "and then we have a waiting layer here that is also initialized using caution that is also initialized using caution that is also initialized using caution just like we did here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 327,
      "text": "and we these just like we did here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 328,
      "text": "and we these just like we did here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 329,
      "text": "and we these neurons in the hidden layer look at 10 neurons in the hidden layer look at 10 neurons in the hidden layer look at 10 inputs and there are 200 neurons in this inputs and there are 200 neurons in this inputs and there are 200 neurons in this hidden layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 330,
      "text": "and then we have here just hidden layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 331,
      "text": "and then we have here just hidden layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 332,
      "text": "and then we have here just like here um in this case the like here um in this case the like here um in this case the multiplication X multip by W to get the multiplication X multip by W to get the multiplication X multip by W to get the pre activations of these pre activations of these pre activations of these neurons and basically the analysis here neurons and basically the analysis here neurons and basically the analysis here looks at okay suppose these are uniform looks at okay suppose these are uniform looks at okay suppose these are uniform gion and these weights are uniform gion gion and these weights are uniform gion gion and these weights are uniform gion if I do X W and we forget for now the if I do X W and we forget for now the if I do X W and we forget for now the bias and the bias and the bias and the nonlinearity then what is the mean and nonlinearity then what is the mean and nonlinearity then what is the mean and the standard deviation of these gions",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 333,
      "text": "so the standard deviation of these gions so the standard deviation of these gions so in the beginning here the input is uh in the beginning here the input is uh in the beginning here the input is uh just a normal Gan distribution mean zero just a normal Gan distribution mean zero just a normal Gan distribution mean zero and the standard deviation is one and and the standard deviation is one and and the standard deviation is one and the standard deviation again is just the the standard deviation again is just the the standard deviation again is just the measure of a spread of the measure of a spread of the measure of a spread of the gion",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 334,
      "text": "but then once we multiply here and gion",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 335,
      "text": "but then once we multiply here and gion",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 336,
      "text": "but then once we multiply here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 337,
      "text": "and we look at the um histogram of Y we see we look at the um histogram of Y we see we look at the um histogram of Y we see that the mean of course stays the same that the mean of course stays the same that the mean of course stays the same it's about zero because this is a it's about zero because this is a it's about zero because this is a symmetric operation",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 338,
      "text": "but we see here that symmetric operation",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 339,
      "text": "but we see here that symmetric operation",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 340,
      "text": "but we see here that the standard deviation has expanded to the standard deviation has expanded to the standard deviation has expanded to three so the input standard deviation three",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 341,
      "text": "so the input standard deviation three",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 342,
      "text": "so the input standard deviation was one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 343,
      "text": "but now we've grown to three and was one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 344,
      "text": "but now we've grown to three and was one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 345,
      "text": "but now we've grown to three and so what you're seeing in the histogram so what you're seeing in the histogram so what you're seeing in the histogram is that this Gan is is that this Gan is is that this Gan is expanding and so um we're expanding this expanding",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 346,
      "text": "and so um we're expanding this expanding",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 347,
      "text": "and so um we're expanding this Gan um from the input and we don't want Gan um from the input and we don't want Gan um from the input and we don't want that we want most of the neural net to that we want most of the neural net to that we want most of the neural net to have relatively similar activations uh have relatively similar activations uh have relatively similar activations uh so unit gion roughly throughout the so unit gion roughly throughout the so unit gion roughly throughout the neural net and so the question is how do neural net",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 348,
      "text": "and so the question is how do neural net",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 349,
      "text": "and so the question is how do we scale these W's to preserve the uh um we scale these W's to preserve the uh um we scale these W's to preserve the uh um to preserve this distribution to uh to preserve this distribution to uh to preserve this distribution to uh remain remain remain aan and so intuitively if I multiply aan and so intuitively if I multiply aan and so intuitively if I multiply here uh these elements of w by a larger here uh these elements of w by a larger here uh these elements of w by a larger number let's say by number let's say by number let's say by five then this gsan gross and gross in five then this gsan gross and gross in five then this gsan gross and gross in standard deviation so now we're at 15 so standard deviation",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 350,
      "text": "so now we're at 15 so standard deviation so now we're at 15 so basically these numbers here in the basically these numbers here in the basically these numbers here in the output y take on more and more extreme output y take on more and more extreme output y take on more and more extreme values but if we scale it down like .2 values but if we scale it down like .2 values but if we scale it down like .2 then conversely this Gan is getting then conversely this Gan is getting then conversely this Gan is getting smaller and smaller and it's shrinking smaller and smaller and it's shrinking smaller and smaller and it's shrinking and you can see that the standard",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 351,
      "text": "and you can see that the standard",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 352,
      "text": "and you can see that the standard deviation is 6 and so the question is deviation is 6 and so the question is deviation is 6 and so the question is what do I multiply by here to exactly what do I multiply by here to exactly what do I multiply by here to exactly preserve the standard deviation to be preserve the standard deviation to be preserve the standard deviation to be one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 353,
      "text": "and it turns out that the correct one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 354,
      "text": "and it turns out that the correct one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 355,
      "text": "and it turns out that the correct answer mathematically when you work out answer mathematically when you work out answer mathematically when you work out through the variance of uh this through the variance of uh this through the variance of uh this multiplication here is that you are multiplication here is that you are multiplication here is that you are supposed to divide by the square root of supposed to divide by the square root of supposed to divide by the square root of the fan in the fan in is the basically the fan in the fan in is the basically the fan in the fan in is the basically the uh number of input elements here 10 the uh number of input elements here 10 the uh number of input elements here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 356,
      "text": "10 so we are supposed to divide by 10 so we are supposed to divide by 10 so we are supposed to divide by 10 square root and this is one way to do square root and this is one way to do square root and this is one way to do the square root you raise it to a power the square root you raise it to a power the square root you raise it to a power of 0.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 357,
      "text": "five",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 358,
      "text": "that's the same as doing a of 0.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 359,
      "text": "five",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 360,
      "text": "that's the same as doing a of 0.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 361,
      "text": "five",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 362,
      "text": "that's the same as doing a square root so when you divide by the um square root so when you divide by the um square root so when you divide by the um square root of 10 then we see that the square root of 10 then we see that the square root of 10 then we see that the output caution it has exactly standard output caution it has exactly standard output caution it has exactly standard deviation of one now unsurprisingly a deviation of one now unsurprisingly a deviation of one now unsurprisingly a number of papers have looked into how number of papers have looked into how number of papers have looked into how but to best initialized neural networks but to best initialized neural networks but to best initialized neural networks and in the case of multilayer and in the case of multilayer and in the case of multilayer perceptrons we can have fairly deep perceptrons we can have fairly deep perceptrons we can have fairly deep networks that have these nonlinearity in networks that have these nonlinearity in networks that have these nonlinearity in between and we want to make sure that between and we want to make sure that between and we want to make sure that the activations are well behaved and the activations are well behaved and the activations are well behaved and they don't expand to infinity or Shrink they don't expand to infinity or Shrink they don't expand to infinity or Shrink all the way to zero and the question is all the way to zero and the question is all the way to zero and the question is how do we initialize the weights so that how do we initialize the weights so that how do we initialize the weights so that these activations take on reasonable these activations take on reasonable these activations take on reasonable values throughout the network now one values throughout the network now one values throughout the network now one paper that has studied this in quite a paper that has studied this in quite a paper that has studied this in quite a bit of detail that is often referenced bit of detail that is often referenced bit of detail that is often referenced is this paper by King hatal called is this paper by King hatal called is this paper by King hatal called delving deep into rectifiers now in this delving deep into rectifiers now in this delving deep into rectifiers now in this case they actually study convolution case they actually study convolution case they actually study convolution neur neurals and they study especially neur neurals and they study especially neur neurals and they study especially the reu nonlinearity and the p the reu nonlinearity and the p the reu nonlinearity and the p nonlinearity instead of a 10h nonlinearity instead of a 10h nonlinearity instead of a 10h nonlinearity but the analysis is very nonlinearity but the analysis is very nonlinearity but the analysis is very similar and um basically what happens similar and um basically what happens similar and um basically what happens here is for them the the relu here is for them the the relu here is for them the the relu nonlinearity that they care about quite nonlinearity that they care about quite nonlinearity that they care about quite a bit here is a squashing function where a bit here is a squashing function where a bit here is a squashing function where all the negative numbers are simply all the negative numbers are simply all the negative numbers are simply clamped to zero so the positive numbers clamped to zero so the positive numbers clamped to zero so the positive numbers are pass through but everything negative are pass through but everything negative are pass through but everything negative is just set to zero and because uh you is just set to zero and because uh you is just set to zero and because uh you are basically throwing away half of the are basically throwing away half of the are basically throwing away half of the distribution they find in their analysis distribution they find in their analysis distribution they find in their analysis of the forward activations in the neural of the forward activations in the neural of the forward activations in the neural that you have to compensate for that that you have to compensate for that that you have to compensate for that with a with a with a gain",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 363,
      "text": "and so here they find that gain",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 364,
      "text": "and so here they find that gain",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 365,
      "text": "and so here they find that basically when they initialize their basically when they initialize their basically when they initialize their weights they have to do it with a zero weights they have to do it with a zero weights they have to do it with a zero mean Gan whose standard deviation is mean Gan whose standard deviation is mean Gan whose standard deviation is square &lt; TK of 2 over the Fanon what we square &lt; TK of 2 over the Fanon what we square &lt; TK of 2 over the Fanon what we have here is we are initializing gashin have here is we are initializing gashin have here is we are initializing gashin with the square root of Fanon this NL with the square root of Fanon this NL with the square root of Fanon this NL here is the Fanon so what we have is here is the Fanon so what we have is here is the Fanon so what we have is sare root of one over the Fanon because sare root of one over the Fanon because sare root of one over the Fanon because we have the division here we have the division here we have the division here now they have to add this factor of two now they have to add this factor of two now they have to add this factor of two because of the reu which basically because of the reu which basically because of the reu which basically discards half of the distribution and discards half of the distribution and discards half of the distribution and clamps it at zero and so that's where clamps it at zero and so that's where clamps it at zero and so that's where you get an additional Factor now in you get an additional Factor now in you get an additional Factor now in addition to that this paper also studies addition to that this paper also studies addition to that this paper also studies not just the uh sort of behavior of the not just the uh sort of behavior of the not just the uh sort of behavior of the activations in the forward pass of the activations in the forward pass of the activations in the forward pass of the neural net but it also studies the back neural net but it also studies the back neural net but it also studies the back propagation and we have to make sure propagation and we have to make sure propagation and we have to make sure that the gradients also are well behaved that the gradients also are well behaved that the gradients also are well behaved",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 366,
      "text": "and so um because ultimately they end up",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 367,
      "text": "and so um because ultimately they end up",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 368,
      "text": "and so um because ultimately they end up updating our parameters and what they updating our parameters and what they updating our parameters and what they find here through a lot of analysis that find here through a lot of analysis that find here through a lot of analysis that I invite you to read through but it's I invite you to read through",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 369,
      "text": "but it's I invite you to read through",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 370,
      "text": "but it's not exactly approachable what they find not exactly approachable what they find not exactly approachable what they find is basically if you properly initialize is basically if you properly initialize is basically if you properly initialize the forward pass the backward pass is the forward pass the backward pass is the forward pass the backward pass is also approximately initialized up to a also approximately initialized up to a also approximately initialized up to a constant factor that has to do with the constant factor that has to do with the constant factor that has to do with the size of the number of um hidden neurons size of the number of um hidden neurons size of the number of um hidden neurons in an early and a late in an early and a late in an early and a late layer and uh but basically they find layer and uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 371,
      "text": "but basically they find layer and uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 372,
      "text": "but basically they find empirically that this is not a choice empirically that this is not a choice empirically that this is not a choice that matters too much now this timing that matters too much now this timing that matters too much now this timing initialization is also implemented in initialization is also implemented in initialization is also implemented in pytorch so if you go to torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 373,
      "text": "and then.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 374,
      "text": "pytorch so if you go to torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 375,
      "text": "and then.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 376,
      "text": "pytorch so if you go to torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 377,
      "text": "and then.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 378,
      "text": "init documentation you'll find climing init documentation you'll find climing init documentation you'll find climing normal and in my opinion this is normal and in my opinion this is normal and in my opinion this is probably the most common way of probably the most common way of probably the most common way of initializing neural networks now and it initializing neural networks now and it initializing neural networks now",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 379,
      "text": "and it takes a few keyword arguments here so takes a few keyword arguments here so takes a few keyword arguments here so number one it wants to know the mode number one it wants to know the mode number one it wants to know the mode would you like to normalize the would you like to normalize the would you like to normalize the activations or would you like to activations or would you like to activations or would you like to normalize the gradients to to be always normalize the gradients to to be always normalize the gradients to to be always uh gsh in with zero mean and a unit or uh gsh in with zero mean and a unit or uh gsh in with zero mean and a unit or one standard deviation and because they one standard deviation and because they one standard deviation and because they find in the paper that this doesn't find in the paper that this doesn't find in the paper that this doesn't matter too much most of the people just matter too much most of the people just matter too much most of the people just leave it as the default which is Fan in leave it as the default which is Fan in leave it as the default which is Fan in and then second passing the nonlinearity and then second passing the nonlinearity and then second passing the nonlinearity that you are using because depending on that you are using because depending on that you are using because depending on the nonlinearity we need to calculate a the nonlinearity we need to calculate a the nonlinearity we need to calculate a slightly different gain and so if your slightly different gain and so if your slightly different gain and so if your nonlinearity is just um linear",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 380,
      "text": "so nonlinearity is just um linear",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 381,
      "text": "so nonlinearity is just um linear",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 382,
      "text": "so there's no nonlinearity then the gain there's no nonlinearity then the gain there's no nonlinearity then the gain here will be one and we have the exact here will be one and we have the exact here will be one and we have the exact same uh kind of formula that we've come same uh kind of formula that we've come same uh kind of formula that we've come up here but if the nonlinearity is up here but if the nonlinearity is up here but if the nonlinearity is something else we're going to get a something else we're going to get a something else we're going to get a slightly different gain and so if we slightly different gain and so if we slightly different gain and so if we come up here to the top we see that for come up here to the top we see that for come up here to the top we see that for example in the case of reu this gain is example in the case of reu this gain is example in the case of reu this gain is a square root of two and the reason it's a square root of two and the reason it's a square root of two and the reason it's a square root because in this paper you see how the two is inside of paper you see how the two is inside of the square root so the gain is a square the square root so the gain is a square the square root",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 383,
      "text": "so the gain is a square root of two in the case of linear or root of two in the case of linear or root of two in the case of linear or identity we just get a gain of one in a identity we just get a gain of one in a identity we just get a gain of one in a case of 10 H which is what we're using case of 10 H which is what we're using case of 10 H which is what we're using here the advised gain is a 5 over3 and here the advised gain is a 5 over3 and here the advised gain is a 5 over3 and intuitively why do we need a gain on top intuitively why do we need a gain on top intuitively why do we need a gain on top of the initialization is because 10 just of the initialization is because 10 just of the initialization is because 10 just like reu is a contractive uh like reu is a contractive uh like reu is a contractive uh transformation so that means is you're transformation so that means is you're transformation so that means is you're taking the output distribution from this taking the output distribution from this taking the output distribution from this matrix multiplication and then you are matrix multiplication and then you are matrix multiplication and then you are squashing it in some way now reu squashing it in some way now reu squashing it in some way now reu squashes it by taking everything below squashes it by taking everything below squashes it by taking everything below zero and clamping it to zero 10 also zero and clamping it to zero 10 also zero and clamping it to zero 10 also squashes it because it's a contractive squashes it because it's a contractive squashes it because it's a contractive operation it will take the Tails and it operation it will take the Tails and it operation it will take the Tails and it will squeeze them in and so in order to will squeeze them in and so in order to will squeeze them in and so in order to fight the squeezing in we need to boost fight the squeezing in we need to boost fight the squeezing in we need to boost the weights a little bit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 384,
      "text": "so",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 385,
      "text": "that we the weights",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 386,
      "text": "a little bit so that we the weights a little bit so that we renormalize everything back to standard renormalize everything back to standard renormalize everything back to standard unit standard deviation so that's why unit standard deviation",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 387,
      "text": "so that's why unit standard deviation",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 388,
      "text": "so that's why there's a little bit of a gain that there's a little bit of a gain that there's a little bit of a gain that comes out now I'm skipping through this comes out now I'm skipping through this comes out now I'm skipping through this section A little bit quickly",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 389,
      "text": "and I'm section A little bit quickly",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 390,
      "text": "and I'm section A little bit quickly",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 391,
      "text": "and I'm doing that actually intentionally and doing that actually intentionally and doing that actually intentionally and the reason for that is because about 7 the reason for that is because about 7 the reason for that is because about 7 years ago when this paper was written years ago when this paper was written years ago when this paper was written you had to actually be extremely careful you had to actually be extremely careful you had to actually be extremely careful with the activations and ingredients and with the activations and ingredients and with the activations and ingredients and their ranges and their histograms and their ranges and their histograms and their ranges and their histograms",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 392,
      "text": "and you had to be very careful with the you had to be very careful with the you had to be very careful with the precise setting of gains and the precise setting of gains and the precise setting of gains and the scrutinizing of the nonlinearities used scrutinizing of the nonlinearities used scrutinizing of the nonlinearities used and so on and everything was very and so on and everything was",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 393,
      "text": "very and so on and everything was very finicky and very fragile and to be very finicky and very fragile and to be very finicky and very fragile and to be very properly arranged for the neural nut to properly arranged for the neural nut to properly arranged for the neural nut to train especially if your neural nut was train especially if your neural nut was train especially if your neural nut was very deep",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 394,
      "text": "but there are a number of very deep",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 395,
      "text": "but there are a number of very deep",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 396,
      "text": "but there are a number of modern innovations that have made modern innovations that have made modern innovations that have made everything significantly more stable and everything significantly more stable and everything significantly more stable and more well behaved",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 397,
      "text": "and it's become less more well behaved and it's become less more well behaved and it's become less important to initialize these networks important to initialize these networks important to initialize these networks exactly right and some of those modern exactly right and some of those modern exactly right and some of those modern Innovations for example are residual Innovations for example are residual Innovations for example are residual connections which we will cover in the connections which we will cover in the connections which we will cover in the future the use of a number of uh future the use of a number of uh future the use of a number of uh normalization uh layers like for example normalization uh layers like for example normalization uh layers like for example batch normalization layer normalization batch normalization layer normalization batch normalization layer normalization group normalization",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 398,
      "text": "we're going to go group normalization we're going to go group normalization we're going to go into a lot of these as well and number into a lot of these as well and number into a lot of these as well and number three much better optimizers not just three much better optimizers not just three much better optimizers not just stochastic gradient descent the simple stochastic gradient descent the simple stochastic gradient descent the simple Optimizer we're basically using here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 399,
      "text": "but Optimizer we're basically using here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 400,
      "text": "but Optimizer we're basically using here but a slightly more complex optimizers like a slightly more complex optimizers like a slightly more complex optimizers like ARS prop and especially Adam and so all ARS prop and especially Adam and so all ARS prop and especially Adam and so all of these modern Innovations make it less of these modern Innovations make it less of these modern Innovations make it less important for you to precisely calibrate important for you to precisely calibrate important for you to precisely calibrate the neutralization of the neural net all the neutralization of the neural net all the neutralization of the neural net all that being said in practice uh what that being said in practice uh what that being said in practice uh what should we do in practice when I should we do in practice when I should we do in practice when I initialize these neurals I basically initialize these neurals I basically initialize these neurals I basically just uh normalize my weights by the just uh normalize my weights by the just uh normalize my weights by the square root of the Fanon uh so basically square root of the Fanon uh so basically square root of the Fanon uh so basically uh roughly what we did here is what I do uh roughly what we did here is what I do uh roughly what we did here is what I do now if we want to be exactly accurate now if we want to be exactly accurate now if we want to be exactly accurate here we and go by um in it of uh timing here we and go by um in it of uh timing here we and go by um in it of uh timing normal this is how it would implemented normal this is how it would implemented normal this is how it would implemented we want to set the standard deviation to we want to set the standard deviation to we want to set the standard deviation to be gain over the square root of fan in be gain over the square root of fan in be gain over the square root of fan in",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 401,
      "text": "right",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 402,
      "text": "so to set the standard deviation right so to set the standard deviation right so to set the standard deviation of our weights we will proceed as of our weights we will proceed as of our weights we will proceed as follows basically when we have a torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 403,
      "text": "follows basically when we have a torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 404,
      "text": "follows basically when we have a torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 405,
      "text": "Ranon and let's say I just create a th Ranon and let's say I just create a th Ranon and let's say I just create a th numbers we can look at the standard numbers we can look at the standard numbers we can look at the standard deviation of this and of course that's deviation of this and of course that's deviation of this and of course that's one that's the amount of spread let's one that's the amount of spread let's one that's the amount of spread let's make this a bit bigger so it's closer to make this a bit bigger so it's closer to make this a bit bigger so it's closer to one so that's the spread of the Gan of one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 406,
      "text": "so that's the spread of the Gan of one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 407,
      "text": "so that's the spread of the Gan of zero mean and unit standard deviation zero mean and unit standard deviation zero mean and unit standard deviation now basically when you take these and now basically when you take these and now basically when you take these and you multiply by you multiply by you multiply by say2 that basically scales down the Gan say2 that basically scales down the Gan say2 that basically scales down the Gan and that makes it standard deviation 02 and that makes it standard deviation 02 and that makes it standard deviation 02 so basically the number that you so basically the number that you so basically the number that you multiply by here ends up being the multiply by here ends up being the multiply by here ends up being the standard deviation of this caution so standard deviation of this caution so standard deviation of this caution so here this is a um standard deviation here this is a um standard deviation here this is a um standard deviation point2 caution here when we sample our point2 caution here when we sample our point2 caution here when we sample our W1",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 408,
      "text": "but we want to set the standard W1 but we want to set the standard W1",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 409,
      "text": "but we want to set the standard deviation to gain over square root of deviation to gain over square root of deviation to gain over square root of fan mode which is Fanon so in other fan mode which is Fanon so in other fan mode which is Fanon so in other words we want to mul mly by uh gain words we want to mul mly by uh gain words we want to mul mly by uh gain which for 10 H is 5 which for 10 H is 5 which for 10 H is 5 over3 5 over3 is the gain and then over3 5 over3 is the gain and then over3 5 over3 is the gain and then times um or I guess sorry",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 410,
      "text": "um or I guess sorry divide uh square root of the fan in and divide uh square root of the fan in and divide uh square root of the fan in and in this example here the fan in was 10 in this example here the fan in was 10 in this example here the fan in was 10",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 411,
      "text": "and I just noticed that actually here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 412,
      "text": "and I just noticed that actually here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 413,
      "text": "and I just noticed that actually here the fan in for W1 is is actually an the fan in for W1 is is actually an the fan in for W1 is is actually an embed times block size which as you all embed times block size which as you all embed times block size which as you all recall is actually 30 and that's because recall is actually 30 and that's because recall is actually 30 and that's because each character is 10 dimensional but each character is 10 dimensional but each character is 10 dimensional",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 414,
      "text": "but then we have three of them and we can then we have three of them and we can then we have three of them and we can catenate them so actually the fan in catenate them so actually the fan in catenate them so actually the fan in here was 30 and I should have used 30 here was 30 and I should have used 30 here was 30 and I should have used 30 here probably but basically we want 30 here probably but basically we want 30 here probably but basically we want 30 uh square root",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 415,
      "text": "so this is the number uh square root",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 416,
      "text": "so this is the number uh square root",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 417,
      "text": "so this is the number this is what our standard deviation we this is what our standard deviation we this is what our standard deviation we want to be and this number turns out to want to be and this number turns out to want to be and this number turns out to be3 whereas here just by fiddling with be3 whereas here just by fiddling with be3 whereas here just by fiddling with it and looking at the distribution and it and looking at the distribution and it and looking at the distribution and making sure it looks",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 418,
      "text": "okay uh we came up making sure it looks",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 419,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 420,
      "text": "uh we came up making sure it looks",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 421,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 422,
      "text": "uh we came up with 02 and so instead what we want to with 02 and so instead what we want to with 02 and so instead what we want to do here is we want to make the standard do here is we want to make the standard do here is we want to make the standard deviation b deviation b deviation b um 5 over3 which is our gain um 5 over3 which is our gain um 5 over3 which is our gain divide this divide this divide this amount times2 square root and these amount times2 square root and these amount times2 square root and these brackets here are not that uh necessary brackets here are not that uh necessary brackets here are not that uh necessary",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 423,
      "text": "but I'll just put them here for clarity",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 424,
      "text": "but I'll just put them here for clarity",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 425,
      "text": "but I'll just put them here for clarity this is basically what we want this is this is basically what we want this is this is basically what we want this is the timing in it in our case for a 10h the timing in it in our case for a 10h the timing in it in our case for a 10h nonlinearity and this is how we would nonlinearity and this is how we would nonlinearity",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 426,
      "text": "and this is how we would initialize the neural net",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 427,
      "text": "and so we're initialize the neural net",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 428,
      "text": "and so we're initialize the neural net",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 429,
      "text": "and so we're multiplying by .3 instead of multiplying multiplying by .3 instead of multiplying multiplying by .3 instead of multiplying by by by .2",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 430,
      "text": "and so we can we can initialize this .2",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 431,
      "text": "and so we can we can initialize this .2",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 432,
      "text": "and so we can we can initialize this way and then we can train the neural net way",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 433,
      "text": "and then we can train the neural net way",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 434,
      "text": "and then we can train the neural net and see what we get",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 435,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 436,
      "text": "so I trained and see what we get",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 437,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 438,
      "text": "so I trained and see what we get",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 439,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 440,
      "text": "so I trained the neural net and we end up in roughly the neural net and we end up in roughly the neural net",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 441,
      "text": "and we end up in roughly the same spot so looking at the the same spot so looking at the the same spot so looking at the validation loss we now get 2.10 and validation loss we now get 2.10 and validation loss we now get 2.10 and previously we also had 2.10 there's a previously we also had 2.10 there's a previously we also had 2.10 there's a little bit of a difference",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 442,
      "text": "but that's little bit of a difference",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 443,
      "text": "but that's little bit of a difference",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 444,
      "text": "but that's just the randomness of the process I just the randomness of the process I just the randomness of the process I suspect but the big deal of course is we suspect but the big deal of course is we suspect but the big deal of course is we get to the same spot",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 445,
      "text": "but we did not have get to the same spot but we did not have get to the same spot but we did not have to introduce any um magic numbers that to introduce any um magic numbers that to introduce any um magic numbers that we got from just looking at histograms we got from just looking at histograms we got from just looking at histograms and guessing checking we have something and guessing checking we have something and guessing checking we have something that is semi- principled and will scale that is semi- principled and will scale that is semi- principled and will scale us to uh much bigger networks and uh us to uh much bigger networks and uh us to uh much bigger networks and uh something that we can sort of use as a something that we can sort of use as a something that we can sort of use as a guide",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 446,
      "text": "so I mentioned that the precise guide so I mentioned that the precise guide so I mentioned that the precise setting of these initializations is not setting of these initializations is not setting of these initializations is not as important today due to some Modern as important today due to some Modern as important today due to some Modern Innovations",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 447,
      "text": "and I think now is a pretty Innovations",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 448,
      "text": "and I think now is a pretty Innovations",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 449,
      "text": "and I think now is a pretty good time to introduce one of those good time to introduce one of those good time to introduce one of those modern Innovations and that is batch modern Innovations and that is batch modern Innovations and that is batch normalization",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 450,
      "text": "so bat normalization came normalization so bat normalization came normalization so bat normalization came out in uh 2015 from a team at Google and out in uh 2015 from a team at Google and out in uh 2015 from a team at Google",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 451,
      "text": "and it was an extremely impact paper because it was an extremely impact paper because it was an extremely impact paper because it made it possible to train very deep it made it possible to train very deep it made it possible to train very deep neuron Nets quite reliably",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 452,
      "text": "and uh it neuron Nets quite reliably",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 453,
      "text": "and uh it neuron Nets quite reliably",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 454,
      "text": "and uh it basically just worked so here's what basically just worked so here's what basically just worked so here's what bash rization does and let's implement bash rization",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 455,
      "text": "does and let's implement bash rization does and let's implement it it",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 456,
      "text": "it um basically we have these uh hidden um basically we have these uh hidden um basically we have these uh hidden States H preact",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 457,
      "text": "right",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 458,
      "text": "and we were States H preact",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 459,
      "text": "right",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 460,
      "text": "and we were States H preact",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 461,
      "text": "right",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 462,
      "text": "and we were talking about how we don't want these uh talking about how we don't want these uh talking about how we don't want these uh these um preactivation states to be way these um preactivation states to be way these um preactivation states to be way too small because the then the 10 H is too small because the then the 10 H is too small because the then the 10 H is not um doing anything",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 463,
      "text": "but we don't want not um doing anything",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 464,
      "text": "but we don't want not um doing anything",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 465,
      "text": "but we don't want them to be too large because then the 10 them to be too large because then the 10 them to be too large because then the 10 H is saturated in fact we want them to H is saturated in fact we want them to H is saturated in fact we want them to be roughly roughly Gan so zero mean and be roughly roughly Gan so zero mean and be roughly roughly Gan so zero mean and a unit or one standard deviation at a unit or one standard deviation at a unit or one standard deviation at least at least at least at initialization so the Insight from the initialization so the Insight from the initialization so the Insight from the bachor liation paper is",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 466,
      "text": "okay you have bachor liation paper is",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 467,
      "text": "okay you have bachor liation paper is",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 468,
      "text": "okay you have these hidden States and you'd like them these hidden States",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 469,
      "text": "and you'd like them these hidden States",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 470,
      "text": "and you'd like them to be roughly Gan then why not take the to be roughly Gan then why not take the to be roughly Gan then why not take the hidden States and uh just normalize them hidden States and uh just normalize them hidden States and uh just normalize them to be to be to be Gan and it sounds kind of crazy but you Gan",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 471,
      "text": "and it sounds kind of crazy but you Gan and it sounds kind of crazy but you can just do that because uh can just do that because uh can just do that because uh standardizing hidden States so that standardizing hidden States so that standardizing hidden States so that their unit caution is a perfect ly their unit caution is a perfect ly their unit caution is a perfect ly differentiable operation as we'll soon differentiable operation as we'll soon differentiable operation as we'll soon see",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 472,
      "text": "and so that was kind of like the big see",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 473,
      "text": "and so that was kind of like the big see",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 474,
      "text": "and so that was kind of like the big Insight in this paper and when I first Insight in this paper and when I first Insight in this paper and when I first read it my mind was blown because you read it my mind was blown because you read it my mind was blown because you can just normalize these hidden States can just normalize these hidden States can just normalize these hidden States and if you'd like unit Gan States in and if you'd like unit Gan States in and if you'd like unit Gan States in your network uh at least initialization your network uh at least initialization your network uh at least initialization you can just normalize them to be unit you can just normalize them to be unit you can just normalize them to be unit gion",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 475,
      "text": "so uh let's see how that works so gion",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 476,
      "text": "so uh let's see how that works so gion",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 477,
      "text": "so uh let's see how that works so we're going to scroll to our we're going to scroll to our we're going to scroll to our preactivation here just before they preactivation here just before they preactivation here just before they enter into the 10h now the idea again is enter into the 10h now the idea again is enter into the 10h now the idea again is remember we're trying to make these remember we're trying to make these remember we're trying to make these roughly Gan and that's because if these roughly Gan and that's because if these roughly Gan and that's because if these are way too small numbers then the 10 H are way too small numbers then the 10 H are way too small numbers then the 10 H here is kind of inactive but if these here is kind of inactive but if these here is kind of inactive but if these are very large numbers then the 10 H is are very large numbers then the 10 H is are very large numbers then the 10 H is way too saturated and gr is no flow",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 478,
      "text": "so way too saturated and gr is no flow so way too saturated and gr is no flow",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 479,
      "text": "so we'd like this to be roughly goshan",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 480,
      "text": "so we'd like this to be roughly goshan",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 481,
      "text": "so we'd like this to be roughly goshan so the Insight in Bat normalization again the Insight in Bat normalization again the Insight in Bat normalization again is that we can just standardize these",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 482,
      "text": "is that we can just standardize these is that we can just standardize these activations so they are exactly Gan so activations so they are exactly Gan so activations so they are exactly Gan so here H here H here H preact has a shapee of 32 by 200 32 preact has a shapee of 32 by 200 32 preact has a shapee of 32 by 200 32 examples by 200 neurons in the hidden examples by 200 neurons in the hidden examples by 200 neurons in the hidden layer so basically what we can do is we layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 483,
      "text": "so basically what we can do is we layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 484,
      "text": "so basically what we can do is we can take H pract and we can just can take H pract and we can just can take H pract and we can just calculate the mean um and the mean we calculate the mean um and the mean we calculate the mean um and the mean we want to calculate across the zero want to calculate across the zero want to calculate across the zero Dimension and we want to also keep them Dimension and we want to also keep them Dimension and we want to also keep them as true so that we can easily broadcast as true so that we can easily broadcast as true so that we can easily broadcast this so the shape of this is 1 by 200 in this",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 485,
      "text": "so the shape of this is 1 by 200 in this",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 486,
      "text": "so the shape of this is 1 by 200 in other words we are doing the mean over other words we are doing the mean over other words we are doing the mean over all the uh elements in the all the uh elements in the all the uh elements in the batch",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 487,
      "text": "and similarly we can calculate the batch and similarly we can calculate the batch and similarly we can calculate the standard deviation of these standard deviation of these standard deviation of these activations and that will also be 1 by activations and that will also be 1 by activations and that will also be 1 by 200 now in this paper they have 200 now in this paper they have 200 now in this paper they have the uh sort of prescription here and see the uh sort of prescription here and see the uh sort of prescription here and see here we are calculating the mean which here we are calculating the mean which here we are calculating the mean which is just taking uh the average is just taking uh the average is just taking uh the average value of any neurons activation and then value of any neurons activation and then value of any neurons activation and then the standard deviation is basically kind the standard deviation is basically kind the standard deviation is basically kind of like um this the measure of the of like um this the measure of the of like um this the measure of the spread that we've been using which is spread that we've been using which is spread that we've been using which is the distance of every one of these the distance of every one of these the distance of every one of these values away from the mean and that values away from the mean and that values away from the mean and that squared and squared and squared and averaged that's the that's the variance averaged that's the that's the variance averaged that's the that's the variance",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 488,
      "text": "and then if you want to take the",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 489,
      "text": "and then if you want to take the",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 490,
      "text": "and then if you want to take the standard deviation you would square root standard deviation you would square root standard deviation you would square root the variance to get the standard the variance to get the standard the variance to get the standard deviation so these are the two that deviation so these are the two that deviation so these are the two that we're calculating and now we're going to we're calculating and now we're going to we're calculating and now we're going to normalize or standardize these X's by normalize or standardize these X's by normalize or standardize these X's by subtracting the mean and um dividing by subtracting the mean and um dividing by subtracting the mean and um dividing by the standard deviation so basically the standard deviation so basically the standard deviation",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 491,
      "text": "so basically we're taking in pract and we we're taking in pract",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 492,
      "text": "and we we're taking in pract",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 493,
      "text": "and we subtract the mean",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 494,
      "text": "and then we divide by the standard",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 495,
      "text": "and then we divide by the standard deviation this is exactly what these two deviation this is exactly what these two deviation this is exactly what these two STD and mean are calculating STD and mean are calculating STD and mean are calculating oops",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 496,
      "text": "sorry this is the mean and this is oops",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 497,
      "text": "sorry this is the mean and this is oops",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 498,
      "text": "sorry this is the mean and this is the variance you see how the sigma is a the variance you see how the sigma is a the variance you see how the sigma is a standard deviation usually so this is standard deviation usually so this is standard deviation usually",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 499,
      "text": "so this is Sigma Square which the variance is the Sigma Square which the variance is the Sigma Square which the variance is the square of the standard square of the standard square of the standard deviation so this is how you standardize deviation so this is how you standardize deviation",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 500,
      "text": "so this is how you standardize these values and what this will do is these values and what this will do is these values and what this will do is that every single neuron now and its that every single neuron now and its that every single neuron now and its firing rate will be exactly unit Gan on firing rate will be exactly unit Gan on firing rate will be exactly unit Gan on these 32 examples at least of this batch these 32 examples at least of this batch these 32 examples at least of this batch that's why it's called batch that's why it's called batch that's why it's called batch normalization we are normalizing these normalization we are normalizing these normalization we are normalizing these batches",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 501,
      "text": "and then we could in principle batches",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 502,
      "text": "and then we could in principle batches",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 503,
      "text": "and then we could in principle train this notice that calculating the train this notice that calculating the train this notice that calculating the mean and your standard deviation these mean and your standard deviation these mean and your standard deviation these are just mathematical formulas they're are just mathematical formulas they're are just mathematical formulas they're perfectly differentiable all of this is perfectly differentiable all of this is perfectly differentiable all of this is perfectly differentiable and we can just perfectly differentiable and we can just perfectly differentiable and we can just train this the problem is you actually train this the problem is you actually train this the problem is you actually won't achieve a very good result with won't achieve a very good result with won't achieve a very good result with this and the reason for that this and the reason for that this and the reason for that is we want these to be roughly Gan but is we want these to be roughly Gan but is we want these to be roughly Gan but only at initialization",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 504,
      "text": "uh but we don't only at initialization uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 505,
      "text": "but we don't only at initialization uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 506,
      "text": "but we don't want these be to be forced to be Garian want these be to be forced to be Garian want these be to be forced to be Garian",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 507,
      "text": "always we we'd like to allow the neuron",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 508,
      "text": "always we we'd like to allow the neuron",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 509,
      "text": "always we we'd like to allow the neuron net to move this around to potentially net to move this around to potentially net to move this around to potentially make it more diffuse to make it more make it more diffuse to make it more make it more diffuse to make it more sharp to make some 10 neurons maybe be sharp to make some 10 neurons maybe be sharp to make some 10 neurons maybe be more trigger more trigger happy or less more trigger more trigger happy or less more trigger more trigger happy or less trigger happy",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 510,
      "text": "so we'd like this trigger happy",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 511,
      "text": "so we'd like this trigger happy",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 512,
      "text": "so we'd like this distribution to move around and we'd distribution to move around",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 513,
      "text": "and we'd distribution to move around and we'd like the back propagation to tell us how like the back propagation to tell us how like the back propagation to tell us how the distribution should move around and the distribution should move around and the distribution should move around and so in addition to this idea of so in addition to this idea of so in addition to this idea of standardizing the activations that any standardizing the activations that any standardizing the activations that any point in the network uh we have to also point in the network uh we have to also point in the network uh we have to also introduce this additional component in introduce this additional component in introduce this additional component in the paper here described as scale and the paper here described as scale and the paper here described as scale and shift and so basically what we're doing shift and so basically what we're doing shift and so basically what we're doing is we're taking these normalized inputs is we're taking these normalized inputs is we're taking these normalized inputs and we are additionally scaling them by and we are additionally scaling them by and we are additionally scaling them by some gain and offsetting them by some some gain and offsetting them by some some gain and offsetting them by some bias to get our final output from this bias to get our final output from this bias to get our final output from this layer and so what that amounts to is the layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 514,
      "text": "and so what that amounts to is the layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 515,
      "text": "and so what that amounts to is the following we are going to allow a batch following we are going to allow a batch following we are going to allow a batch normalization gain to be initialized at normalization gain to be initialized at normalization gain to be initialized at just uh once just uh once just uh once and the ones will be in the shape of 1 and the ones will be in the shape of 1 and the ones will be in the shape of 1 by n by n by",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 516,
      "text": "n hidden",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 517,
      "text": "and then we also will have a BN hidden",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 518,
      "text": "and then we also will have a BN hidden",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 519,
      "text": "and then we also will have a BN bias which will be torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 520,
      "text": "zeros and it bias which will be torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 521,
      "text": "zeros and it bias which will be torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 522,
      "text": "zeros and it will also be of the shape n by 1 by n will also be of the shape n by 1 by n will also be of the shape n by 1 by n hidden and then here the BN gain will hidden and then here the BN gain will hidden and then here the BN gain will multiply multiply",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 523,
      "text": "multiply this and the BN bias will offset it this and the BN bias will offset it this and the BN bias will offset it here so because this is initialized to here so because this is initialized to here so because this is initialized to one and this to one and this to one and this to zero at initialization each neurons zero at initialization each neurons zero at initialization each neurons firing values in this batch will be firing values in this batch will be firing values in this batch will be exactly unit gion and will have nice exactly unit gion and will have nice exactly unit gion and will have nice numbers no matter what the distribution numbers no matter what the distribution numbers no matter what the distribution of the H pract is coming in coming out of the H pract is coming in coming out of the H pract is coming in coming out it will be un Gan for each neuron and it will be un Gan for each neuron and it will be un Gan for each neuron and that's roughly what we want at least at that's roughly what we want at least at that's roughly what we want at least at initialization um and then during initialization um and then during initialization um and then during optimization we'll be able to back optimization we'll be able to back optimization we'll be able to back propagate into BN gain and BM bias and propagate into BN gain and BM bias and propagate into BN gain and BM bias and change them so the network is given the change them",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 524,
      "text": "so the network is given the change them",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 525,
      "text": "so the network is given the full ability to do with this whatever it full ability to do with this whatever it full ability to do with this whatever it wants uh wants uh wants uh internally here we just have to make internally here we just have to make internally here we just have to make sure sure that we um include these in sure sure that we um include these in sure sure that we um include these in the parameters of the neural nut because the parameters of the neural nut because the parameters of the neural nut because they will be trained with back they will be trained with back they will be trained with back propagation so let's initialize this and propagation",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 526,
      "text": "so let's initialize this and propagation",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 527,
      "text": "so let's initialize this",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 528,
      "text": "and then we should be able to train and then we're going to also copy train",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 529,
      "text": "and then we're going to also copy this line which is the batch this line which is the batch this line which is the batch normalization layer here on a single normalization layer here on a single normalization layer here on a single line of code and we're going to swing line of code and we're going to swing line of code and we're going to swing down here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 530,
      "text": "and we're also going to do the down here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 531,
      "text": "and we're also going to do the down here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 532,
      "text": "and we're also going to do the exact same thing at test time here so similar to train time we're here so similar to train time we're going to normalize uh and then scale and going to normalize uh and then scale and going to normalize uh and then scale and that's going to give us our train and that's going to give us our train and that's going to give us our train and validation validation validation loss and we'll see in a second that loss",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 533,
      "text": "and we'll see in a second that loss",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 534,
      "text": "and we'll see in a second that we're actually going to change this",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 535,
      "text": "a we're actually going to change this",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 536,
      "text": "a we're actually going to change this a little bit but for now I'm going to keep little bit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 537,
      "text": "but for now I'm going to keep little bit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 538,
      "text": "but for now I'm going to keep it this way",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 539,
      "text": "so I'm just going to wait it this way",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 540,
      "text": "so I'm just going to wait it this way",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 541,
      "text": "so I'm just going to wait for this to converge",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 542,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 543,
      "text": "so I allowed for this to converge",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 544,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 545,
      "text": "so I allowed for this to converge",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 546,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 547,
      "text": "so I allowed the neural nut to converge here and when the neural nut to converge here and when the neural nut to converge here and when we scroll down we see that our we scroll down we see that our we scroll down we see that our validation loss here is 2.10 roughly validation loss here is 2.10 roughly validation loss here is 2.10 roughly which I wrote down here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 548,
      "text": "and we see that which I wrote down here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 549,
      "text": "and we see that which I wrote down here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 550,
      "text": "and we see that this is actually kind of comparable to this is actually kind of comparable to this is actually kind of comparable to some of the results that we've achieved some of the results that we've achieved some of the results that we've achieved uh previously now I'm not actually uh previously now I'm not actually uh previously now I'm not actually expecting an improvement in this case expecting an improvement in this case expecting an improvement in this case and that's because we are dealing with a",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 551,
      "text": "and that's because we are dealing with a",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 552,
      "text": "and that's because we are dealing with a very simple neural nut that has just a very simple neural nut that has just a very simple neural nut that has just a single hidden layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 553,
      "text": "so in fact in this single hidden layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 554,
      "text": "so in fact in this single hidden layer so in fact in this very simple case of just one hidden very simple case of just one hidden very simple case of just one hidden layer we were able to actually calculate layer we were able to actually calculate layer we were able to actually calculate what the scale of w should be to make what the scale of w should be to make what the scale of w should be to make these pre activations already have a these pre activations already have a these pre activations already have a roughly Gan shape",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 555,
      "text": "so the bat roughly Gan shape so the bat roughly Gan shape so the bat normalization is not doing much here but normalization is not doing much here but normalization is not doing much here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 556,
      "text": "but you might imagine that once you have a you might imagine that once you have a you might imagine that once you have a much deeper neural nut that has lots of much deeper neural nut that has lots of much deeper neural nut that has lots of different types of operations and different types of operations and different types of operations and there's also for example residual there's also for example residual there's also for example residual connections which we'll cover and so on connections which we'll cover and so on connections which we'll cover and so on it will become basically very very it will become basically very very it will become basically very very difficult to tune the scales of your difficult to tune the scales of your difficult to tune the scales of your weight matrices such that all the weight matrices such that all the weight matrices such that all the activations throughout the neural nut activations throughout the neural nut activations throughout the neural nut are roughly gsen",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 557,
      "text": "and so that's going to are roughly gsen",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 558,
      "text": "and so that's going to are roughly gsen",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 559,
      "text": "and so that's going to become very quickly intractable but become very quickly intractable but become very quickly intractable but compared to that it's going to be much compared to that it's going to be much compared to that it's going to be much much easier to sprinkle batch much easier to sprinkle batch much easier to sprinkle batch normalization layers throughout the normalization layers throughout the normalization layers throughout the neural net so in particular it's common neural net so in particular it's common neural net so in particular it's common to to look at every single linear layer to to look at every single linear layer to to look at every single linear layer like this one one this is a linear layer like this one one this is a linear layer like this one one this is a linear layer multiplying by a weight Matrix and multiplying by a weight Matrix and multiplying by a weight Matrix and adding a bias or for example adding a bias or for example adding a bias or for example convolutions which we'll cover later and convolutions which we'll cover later and convolutions which we'll cover later and also perform basically a multiplication also perform basically a multiplication also perform basically a multiplication with a weight Matrix",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 560,
      "text": "but in a more with a weight Matrix but in a more with a weight Matrix but in a more spatially structured format it's custom spatially structured format it's custom spatially structured format it's custom it's customary to take these linear",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 561,
      "text": "it's customary to take these linear",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 562,
      "text": "it's customary to take these linear layer or convolutional layer and append layer or convolutional layer and append layer or convolutional layer and append a b rization layer right after it to a b rization layer right after it to a b rization layer right after it to control the scale of these activations control the scale of these activations control the scale of these activations at every point in the neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 563,
      "text": "so we'd at every point in the neural nut so we'd at every point in the neural nut so we'd be adding these bom layers throughout be adding these bom layers throughout be adding these bom layers throughout the neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 564,
      "text": "and then this controls the neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 565,
      "text": "and then this controls the neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 566,
      "text": "and then this controls the scale of these AC ations throughout the scale of these AC ations throughout the scale of these AC ations throughout the neural net it doesn't require us to the neural net it doesn't require us to the neural net it doesn't require us to do uh perfect mathematics and care about do uh perfect mathematics and care about do uh perfect mathematics and care about the activation distributions uh for all the activation distributions uh for all the activation distributions uh for all these different types of neural network these different types of neural network these different types of neural network uh Lego building blocks that you might uh Lego building blocks that you might uh Lego building blocks that you might want to introduce into your neural net want to introduce into your neural net want to introduce into your neural net and it significantly stabilizes uh the",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 567,
      "text": "and it significantly stabilizes uh the",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 568,
      "text": "and it significantly stabilizes uh the training and that's why these uh layers training",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 569,
      "text": "and that's why these uh layers training",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 570,
      "text": "and that's why these uh layers are quite popular now the stability are quite popular now the stability are quite popular now the stability offered by bash normalization actually offered by bash normalization actually offered by bash normalization actually comes at a terrible cost and that cost comes at a terrible cost and that cost comes at a terrible cost and that cost is that if you think about what's is that if you think about what's is that if you think about what's Happening Here something something Happening Here something something Happening Here something something terribly strange and unnatural is terribly strange and unnatural is terribly strange and unnatural is happening it used to be that we have a happening it used to be that we have a happening it used to be that we have a single example feeding into a neural nut single example feeding into a neural nut single example feeding into a neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 571,
      "text": "and then uh we calculate its activations and then uh we calculate its activations and then uh we calculate its activations and its loits and this is a and its loits and this is a and its loits and this is a deterministic sort of process so you deterministic sort of process so you deterministic sort of process so you arrive at some logits for this example arrive at some logits for this example arrive at some logits for this example and then because of efficiency of and then because of efficiency of and then because of efficiency of training we suddenly started to use training we suddenly started to use training we suddenly started to use batches of examples",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 572,
      "text": "but those batches of batches of examples but those batches of batches of examples but those batches of examples were processed independently examples were processed independently examples were processed independently and it was just an efficiency thing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 573,
      "text": "but and it was just an efficiency thing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 574,
      "text": "but and it was just an efficiency thing but now suddenly in batch normalization now suddenly in batch normalization now suddenly in batch normalization because of the normalization through the because of the normalization through the because of the normalization through the batch we are coupling these examples batch we are coupling these examples batch we are coupling these examples mathematically and in the forward pass mathematically and in the forward pass mathematically and in the forward pass and the backward pass of a neural l",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 575,
      "text": "so and the backward pass of a neural l",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 576,
      "text": "so and the backward pass of a neural l so now the hidden State activations H pract now the hidden State activations H pract now the hidden State activations H pract in your log jits for any one input in your log jits for any one input in your log jits for any one input example are not just a function of that example are not just a function of that example are not just a function of that example and its input but they're also a example and its input but they're also a example and its input but they're also a function of all the other examples that function of all the other examples that function of all the other examples that happen to come for a ride in that batch happen to come for a ride in that batch happen to come for a ride in that batch and these examples are sampled randomly and these examples are sampled randomly and these examples are sampled randomly and so what's happening is for example",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 577,
      "text": "and so what's happening is for example",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 578,
      "text": "and so what's happening is for example when you look at H pract that's going to when you look at H pract that's going to when you look at H pract that's going to feed into H the hidden State activations feed into H the hidden State activations feed into H the hidden State activations for for example for for any one of these for for example for for any one of these for for example for for any one of these input examples is going to actually input examples is going to actually input examples is going to actually change slightly depending on what other change slightly depending on what other change slightly depending on what other examples there are in a batch and and examples there are in a batch and and examples there are in a batch and and depending on what other examples happen depending on what other examples happen depending on what other examples happen to come for a ride H is going to change to come for a ride H is going to change to come for a ride H is going to change subtly",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 579,
      "text": "and it's going to like Jitter if subtly",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 580,
      "text": "and it's going to like Jitter if subtly",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 581,
      "text": "and it's going to like Jitter if you imagine sampling different examples you imagine sampling different examples you imagine sampling different examples because the the statistics of the mean because the the statistics of the mean because the the statistics of the mean and the standard deviation are going to and the standard deviation are going to and the standard deviation are going to be impacted",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 582,
      "text": "and so you'll get a Jitter be impacted",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 583,
      "text": "and so you'll get a Jitter be impacted",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 584,
      "text": "and so you'll get a Jitter for H",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 585,
      "text": "and you'll get a Jitter for for H and you'll get a Jitter for for H and you'll get a Jitter for loits",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 586,
      "text": "and you think that this would be a loits",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 587,
      "text": "and you think that this would be a loits",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 588,
      "text": "and you think that this would be a bug uh or something undesirable but in a bug uh or something undesirable but in a bug uh or something undesirable but in a very strange way this actually turns out very strange way this actually turns out very strange way this actually turns out to be good in your Network training and to be good in your Network training and to be good in your Network training and as a side effect and the reason for that as a side effect and the reason for that as a side effect and the reason for that is that you can think of this as kind of is that you can think of this as kind of is that you can think of this as kind of like a regularizer because what's like a regularizer because what's like a regularizer because what's happening is you have your input and you happening is you have your input and you happening is you have your input",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 589,
      "text": "and you get your age and then depending on the get your age",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 590,
      "text": "and then depending on the get your age",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 591,
      "text": "and then depending on the other examples this is jittering a bit other examples this is jittering a bit other examples this is jittering a bit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 592,
      "text": "and so what that does is that it's",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 593,
      "text": "and so what that does is that it's",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 594,
      "text": "and so what that does is that it's effectively padding out any one of these effectively padding out any one of these effectively padding out any one of these input examples and it's introducing a input examples and it's introducing a input examples and it's introducing a little bit of entropy and um because of little bit of entropy and um because of little bit of entropy and um because of the padding out it's actually kind of the padding out it's actually kind of the padding out it's actually kind of like a form of a data augmentation which like a form of a data augmentation which like a form of a data augmentation which we'll cover in the future",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 595,
      "text": "and it's kind we'll cover in the future",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 596,
      "text": "and it's kind we'll cover in the future",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 597,
      "text": "and it's kind of like augmenting the input a little of like augmenting the input a little of like augmenting the input a little bit and jittering it and that makes it bit and jittering it and that makes it bit and jittering it and that makes it harder for the neural nut to overfit to harder for the neural nut to overfit to harder for the neural nut to overfit to these concrete specific examples so by these concrete specific examples so by these concrete specific examples so by introducing all this noise it actually introducing all this noise it actually introducing all this noise it actually like Pats out the examples",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 598,
      "text": "and it like Pats out the examples",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 599,
      "text": "and it like Pats out the examples",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 600,
      "text": "and it regularizes the neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 601,
      "text": "and that's regularizes the neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 602,
      "text": "and that's regularizes the neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 603,
      "text": "and that's one of the reasons why uh deceivingly as one of the reasons why uh deceivingly as one of the reasons why uh deceivingly as a second order effect uh this is a second order effect uh this is a second order effect uh this is actually a regularizer and that has made actually a regularizer and that has made actually a regularizer and that has made it harder uh for us to remove the use of it harder uh for us to remove the use of it harder uh for us to remove the use of batch batch batch normalization uh because basically no normalization uh because basically no normalization uh because basically no one likes this property that the the one likes this property that the the one likes this property that the the examples in the batch are coupled examples in the batch are coupled examples in the batch are coupled mathematically and in the forward pass mathematically and in the forward pass and at least all kinds of like strange and at least all kinds of like strange and at least all kinds of like strange uh results uh we'll go into some of that uh results uh we'll go into some of that uh results uh we'll go into some of that in a second as well um and it leads to a in a second",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 604,
      "text": "as well um and it leads to a in a second as well um and it leads to a lot of bugs and um and so on and so no lot of bugs and um and so on and so no lot of bugs and um and so on",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 605,
      "text": "and so no one likes this property uh and so people one likes this property uh and so people one likes this property uh and so people have tried to um deprecate the use of have tried to um deprecate the use of have tried to um deprecate the use of bat normalization and move to other bat normalization and move to other bat normalization and move to other normalization techniques that do not normalization techniques that do not normalization techniques that do not couple the examples of a batch examples couple the examples of a batch examples couple the examples of a batch examples are ler normalization instance are ler normalization instance are ler normalization instance normalization group normalization and so normalization group normalization and so normalization group normalization and so on and we'll come we'll come some these on and we'll come we'll come some these on",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 606,
      "text": "and we'll come we'll come some these uh later um but basically long story uh later um but basically long story uh later um but basically long story short bat normalization was the first short bat normalization was the first short bat normalization was the first kind of normalization layer to be kind of normalization layer to be kind of normalization layer to be introduced it worked extremely well it introduced it worked extremely well it introduced it worked extremely well it happened to have this regularizing happened to have this regularizing happened to have this regularizing effect it stabilized training and people effect it stabilized training and people effect it stabilized training and people have been trying to remove it and move have been trying to remove it and move have been trying to remove it and move to some of the other normalization to some of the other normalization to some of the other normalization techniques uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 607,
      "text": "but it's been hard because techniques uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 608,
      "text": "but it's been hard because techniques uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 609,
      "text": "but it's been hard because it it just works quite well and some of it it just works quite well and some of it it just works quite well and some of the reason that it works quite well is the reason that it works quite well is the reason that it works quite well is again because of this regular rizing again because of this regular rizing again because of this regular rizing effect and because of the because it is effect and because of the because it is effect and because of the because it is quite effective at um controlling the quite effective at um controlling the quite effective at um controlling the activations and their activations and their activations and their distributions",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 610,
      "text": "uh so that's kind of like distributions uh so that's kind of like distributions uh so that's kind of like the brief story of Bash normalization the brief story of Bash normalization the brief story of Bash normalization",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 611,
      "text": "and I'd like to show you one of the",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 612,
      "text": "and I'd like to show you one of the",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 613,
      "text": "and I'd like to show you one of the other weird sort of outcomes of this other weird sort of outcomes of this other weird sort of outcomes of this coupling so here's one of the strange coupling so here's one of the strange coupling so here's one of the strange outcomes that I only glossed over outcomes that I only glossed over outcomes that I only glossed over previously when I was evaluating the previously when I was evaluating the previously when I was evaluating the loss on the validation set basically loss on the validation set basically loss on the validation set basically once we've trained a neural net we'd once we've trained a neural net we'd once we've trained a neural net we'd like to deploy it in some kind of a like to deploy it in some kind of a like to deploy it in some kind of a setting and we'd like to be able to feed setting and we'd like to be able to feed setting and we'd like to be able to feed in a single individual example and get a in a single individual example and get a in a single individual example and get a prediction out from our neural net but prediction out from our neural net but prediction out from our neural net but how do we do that when our neural net how do we do that when our neural net how do we do that when our neural net now in a forward pass estimates the now in a forward pass estimates the now in a forward pass estimates the statistics of the mean understand statistics of the mean understand statistics of the mean understand deviation of a batch the neur lot deviation of a batch the neur lot deviation of a batch the neur lot expects batches as an input now",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 614,
      "text": "so how expects batches as an input now so how expects batches as an input now so how do we feed in a single example and get do we feed in a single example and get do we feed in a single example and get sensible results out and so the proposal sensible results out and so the proposal sensible results out and so the proposal in the batch normalization paper is the in the batch normalization paper is the in the batch normalization paper is the following what we would like to do here following what we would like to do here following what we would like to do here is we would like to basically have a is we would like to basically have a is we would like to basically have a step after training that uh calculates step after training that uh calculates step after training that uh calculates and sets the bach uh mean and standard and sets the bach uh mean and standard and sets the bach uh mean and standard deviation a single time over the deviation a single time over the deviation a single time over the training set",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 615,
      "text": "and so I wrote this code training set",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 616,
      "text": "and so I wrote this code training set",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 617,
      "text": "and so I wrote this code here in interest of time and we're going here in interest of time",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 618,
      "text": "and we're going here in interest of time",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 619,
      "text": "and we're going to call what's called calibrate the to call what's called calibrate the to call what's called calibrate the bachor statistics and basically what we bachor statistics and basically what we bachor statistics and basically what we do is torch torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 620,
      "text": "nograd telling do is torch torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 621,
      "text": "nograd telling do is torch torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 622,
      "text": "nograd telling pytorch that none of this we will call pytorch that none of this we will call pytorch that none of this we will call Dot backward on and it's going to be a Dot backward on and it's going to be a Dot backward on and it's going to be a bit more efficient we're going to take bit more efficient we're going to take bit more efficient we're going to take the training set get the pre activations the training set get the pre activations the training set get the pre activations for every single training example and for every single training example and for every single training example and then one single time estimate the mean then one single time estimate the mean",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 623,
      "text": "then one single time estimate the mean and standard deviation over the entire and standard deviation over the entire and standard deviation over the entire training set and then we're going to get training set",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 624,
      "text": "and then we're going to get training set",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 625,
      "text": "and then we're going to get B and mean and B and standard deviation B and mean and B and standard deviation B and mean and B and standard deviation and now these are fixed numbers and now these are fixed numbers and now these are fixed numbers estimating over the entire training set estimating over the entire training set estimating over the entire training set and",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 626,
      "text": "here instead of estimating it and here instead of estimating it and here instead of estimating it dynamically we are going to instead here dynamically we are going to instead here dynamically we are going to instead here use B and use B and use B and mean",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 627,
      "text": "and here we're just going to use B mean",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 628,
      "text": "and here we're just going to use B mean",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 629,
      "text": "and here we're just going to use B and standard and standard and standard deviation and so at test time we are deviation",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 630,
      "text": "and so at test time we are deviation and so at test time we are going to fix these clamp them and use going to fix these clamp them and use going to fix these clamp them and use them during inference and them during inference and them during inference and now you see that we get basically now you see that we get basically now you see that we get basically identical result uh but the benefit that identical result uh but the benefit that identical result uh but the benefit that we've gained is that we can now also we've gained is that we can now also we've gained is that we can now also forward a single example because the forward a single example because the forward a single example because the mean and standard deviation are now mean and standard deviation are now mean and standard deviation are now fixed uh sort of tensor fixed uh sort of tensor fixed uh sort of tensor that said nobody actually wants to that said nobody actually wants to that said nobody actually wants to estimate this mean and standard estimate this mean and standard estimate this mean and standard deviation as a second stage after neural deviation as a second stage after neural deviation as a second stage after neural network training because everyone is network training because everyone is network training because everyone is lazy and so this batch normalization lazy and so this batch normalization lazy and so this batch normalization paper actually introduced one more idea paper actually introduced one more idea paper actually introduced one more idea which is that we are can we can estimate which is that we are can we can estimate which is that we are can we can estimate the mean and standard deviation in a the mean and standard deviation in a the mean and standard deviation in a running man running manner during running man running manner during running man running manner during training of the neuron nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 631,
      "text": "and then we training of the neuron nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 632,
      "text": "and then we training of the neuron nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 633,
      "text": "and then we can uh simply just have a single stage can uh simply just have a single stage can uh simply just have a single stage of training and on the side of that of training and on the side of that of training and on the side of that training we are estimating the running training we are estimating the running training we are estimating the running mean and standard deviation so let's see mean and standard deviation so let's see mean and standard deviation so let's see what that would look like let me what that would look like let me what that would look like let me basically take the mean here that we are basically take the mean here that we are basically take the mean here that we are estimating on the batch and let me call estimating on the batch and let me call estimating on the batch and let me call this B and mean on the I this B and mean on the I this B and mean on the I iteration um and then here this is BN iteration um",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 634,
      "text": "and then here this is BN iteration um",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 635,
      "text": "and then here this is BN sdd um bnsd at I",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 636,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 637,
      "text": "uh and the mean comes here and",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 638,
      "text": "the okay uh and the mean comes here and the STD comes here so so far I've done STD comes here so so far I've done STD comes here so so far I've done nothing I've just uh moved around",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 639,
      "text": "and I nothing I've just uh moved around and I nothing I've just uh moved around and I created these EXT extra variables for created these EXT extra variables for created these EXT extra variables for the mean and standard deviation and I've the mean and standard deviation and I've the mean and standard deviation and I've put them here so so far nothing has put them here so so far nothing has put them here so so far nothing has changed but what we're going to do now changed but what we're going to do now changed but what we're going to do now is we're going to keep running mean of is we're going to keep running mean of is we're going to keep running mean of both of these values during training",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 640,
      "text": "so both of these values during training",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 641,
      "text": "so both of these values during training so let me swing up here and let me create a let me swing up here and let me create a let me swing up here and let me create a BN meanor running",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 642,
      "text": "and I'm going to BN meanor running",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 643,
      "text": "and I'm going to BN meanor running",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 644,
      "text": "and I'm going to initialize it at uh initialize it at uh initialize it at uh zeros and then BN STD running which",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 645,
      "text": "I'll zeros and then BN STD running which",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 646,
      "text": "I'll zeros and then BN STD running which I'll initialize at initialize at initialize at once because um in the beginning because once because um in the beginning because once because um in the beginning because of the way we ized W1 uh and B1 H pract of the way we ized W1 uh and B1 H pract of the way we ized W1 uh and B1 H pract will be roughly unit gion so the mean will be roughly unit gion",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 647,
      "text": "so the mean will be roughly unit gion so the mean will be roughly zero and a standard will be roughly zero and a standard will be roughly zero and a standard deviation roughly one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 648,
      "text": "so I'm going to deviation roughly one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 649,
      "text": "so I'm going to deviation roughly one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 650,
      "text": "so I'm going to initialize these that way",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 651,
      "text": "but then here initialize these that way",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 652,
      "text": "but then here initialize these that way",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 653,
      "text": "but then here I'm going to update these and in pytorch I'm going to update these and in pytorch I'm going to update these and in pytorch um uh these uh mean and standard um uh these uh mean and standard um uh these uh mean and standard deviation that are running uh they're deviation that are running uh they're deviation that are running uh they're not actually part of the gradient based not actually part of the gradient based not actually part of the gradient based optimization we're never going to derive optimization we're never going to derive optimization we're never going to derive gradients with respect to them",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 654,
      "text": "they're gradients with respect to them they're gradients with respect to them they're they're updated on the side of training they're updated on the side of training they're updated on the side of training and so what we're going to do here is",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 655,
      "text": "and so what we're going to do here is",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 656,
      "text": "and so what we're going to do here is we're going to say with torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 657,
      "text": "nograd we're going to say with torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 658,
      "text": "nograd we're going to say with torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 659,
      "text": "nograd telling pytorch that the update here is telling pytorch that the update here is telling pytorch that the update here is not supposed to be building out a graph not supposed to be building out a graph not supposed to be building out a graph because there will be no dot because there will be no dot because there will be no dot backward but this running is basically backward but this running is basically backward but this running is basically going to be going to be going to be 0.99 uh9 times the current 0.99 uh9 times the current 0.99 uh9 times the current Value Plus 0.001 times the um this value Value Plus 0.001 times the um this value Value Plus 0.001 times the um this value this new this new this new mean",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 660,
      "text": "and in the same way bnsd running mean and in the same way bnsd running mean and in the same way bnsd running will be mostly what it used to be but it will receive a small update in but it will receive a small update in the direction of what the current the direction of what the current the direction of what the current standard deviation standard deviation standard deviation is and as you're seeing here this update is and as you're seeing here this update is and as you're seeing here this update is outside and on the side of the is outside and on the side of the is outside and on the side of the gradient based optimization and it's gradient based optimization",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 661,
      "text": "and it's gradient based optimization",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 662,
      "text": "and it's simply being updated not using gradient simply being updated not using gradient simply being updated not using gradient scent it's just being updated using U scent it's just being updated using U scent it's just being updated using U janky like Smooth um sort of uh running janky like Smooth um sort of uh running janky like Smooth um sort of uh running mean mean mean Manner and so while the network is Manner and",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 663,
      "text": "so while the network is Manner",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 664,
      "text": "and so while the network is training and these pre activations are training and these pre activations are training and these pre activations are sort of changing and shifting around sort of changing and shifting around sort of changing and shifting around during during back propagation we are during during back propagation we are during during back propagation we are keeping track of the typical mean and keeping track of the typical mean and keeping track of the typical mean and standard deviation and we're estimating standard deviation and we're estimating standard deviation and we're estimating them once and when I run them once and when I run them once and when I run this now I'm keeping track of this in this now I'm keeping track of this in this now I'm keeping track of this in the running Manner and what we're hoping the running Manner and what we're hoping the running Manner and what we're hoping for of course is that the me BN meore for of course is that the me BN meore for of course is that the me BN meore running and BN meore STD are going to be running and BN meore STD are going to be running and BN meore STD are going to be very similar to the ones that we very similar to the ones that we very similar to the ones that we calculated here before and that way we calculated here before and that way we calculated here before and that way we don't need a second stage because we've don't need a second stage because we've don't need a second stage because we've sort of combined the two stages and sort of combined the two stages and sort of combined the two stages and we've put them on the side of each other we've put them on the side of each other we've put them on the side of each other if you want to look at it that way and if you want to look at it that way and if you want to look at it that way and this is how this is also implemented in this is how this is also implemented in this is how this is also implemented in The Bash normalization uh layer impi The Bash normalization uh layer impi The Bash normalization uh layer impi torch so during training um the exact torch so during training um the exact torch so during training um the exact same thing will happen and then later same thing will happen and then later same thing will happen and then later when you're using inference it will use when you're using inference it will use when you're using inference it will use the estimated running mean of both the the estimated running mean of both the the estimated running mean of both the mean and standard deviation of those mean and standard deviation of those mean and standard deviation of those hidden States so let's wait for the hidden States so let's wait for the hidden States so let's wait for the optimization to converge and hopefully optimization to converge and hopefully optimization to converge and hopefully the running mean and standard deviation the running mean and standard deviation the running mean and standard deviation are roughly equal to these two and then are roughly equal to these two and then are roughly equal to these two",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 665,
      "text": "and then we can simply use it here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 666,
      "text": "and we don't we can simply use it here and we don't we can simply use it here and we don't need this stage of explicit calibration need this stage of explicit calibration need this stage of explicit calibration at the end",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 667,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 668,
      "text": "so the optimization at the end",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 669,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 670,
      "text": "so the optimization at the end",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 671,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 672,
      "text": "so the optimization finished I'll rerun the explicit finished I'll rerun the explicit finished I'll rerun the explicit estimation and then the B and mean from estimation and then the B and mean from estimation and then the B and mean from the explicit estimation is here and B",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 673,
      "text": "the explicit estimation is here and B the explicit estimation is here and B and mean from the running estimation and mean from the running estimation and mean from the running estimation during the during the optimization you during the during the optimization you during the during the optimization you can see is very very similar it's not can see is very very similar it's not can see is very very similar it's not identical",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 674,
      "text": "but it's pretty identical",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 675,
      "text": "but it's pretty identical",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 676,
      "text": "but it's pretty close and the same way BN STD is this close and the same way BN STD is this close and the same way BN STD is this and BN STD running is this",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 677,
      "text": "and so you and BN STD running is this",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 678,
      "text": "and so you and BN STD running is this",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 679,
      "text": "and so you can see that once again they are fairly can see that once again they are fairly can see that once again they are fairly similar values not identical but pretty similar values not identical but pretty similar values not identical but pretty close",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 680,
      "text": "and so then here instead of being close and so then here instead of being close and so then here instead of being mean we can use the BN mean running mean we can use the BN mean running mean we can use the BN mean running instead of bnsd we can use bnsd instead of bnsd we can use bnsd instead of bnsd we can use bnsd running and uh hopefully the validation running and uh hopefully the validation running and uh hopefully the validation loss will not be impacted too loss will not be impacted too loss will not be impacted too much okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 681,
      "text": "so it's basically identical",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 682,
      "text": "much",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 683,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 684,
      "text": "so it's basically identical",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 685,
      "text": "much",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 686,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 687,
      "text": "so it's basically identical and this way we've eliminated the need and this way we've eliminated the need and this way we've eliminated the need for this explicit stage of calibration for this explicit stage of calibration for this explicit stage of calibration because we are doing it in line over because we are doing it in line over because we are doing it in line over here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 688,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 689,
      "text": "so we're almost done with here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 690,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 691,
      "text": "so we're almost done with here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 692,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 693,
      "text": "so we're almost done with batch normalization there are only two batch normalization there are only two batch normalization there are only two more notes that I'd like to make number more notes that I'd like to make number more notes that I'd like to make number one I've skipped a discussion over what one I've skipped a discussion over what one I've skipped a discussion over what is this plus Epsilon doing here this is this plus Epsilon doing here this is this plus Epsilon doing here this Epsilon is usually like some small fixed Epsilon is usually like some small fixed Epsilon is usually like some small fixed number for example one5 by default and number for example one5 by default and number for example one5 by default and what it's doing is that it's basically what it's doing is that it's basically what it's doing is that it's basically preventing a division by zero in the preventing a division by zero in the preventing a division by zero in the case that the variance over your batch case that the variance over your batch case that the variance over your batch is exactly zero in that case uh here we is exactly zero in that case uh here we is exactly zero in that case uh here we normally have a division by zero but normally have a division by zero but normally have a division by zero but because of the plus Epsilon uh this is because of the plus Epsilon uh this is because of the plus Epsilon uh this is going to become a small number in the going to become a small number in the going to become a small number in the denominator instead and things will be denominator instead and things will be denominator instead and things will be more well behaved so feel free to also more well behaved so feel free to also more well behaved so feel free to also add a plus Epsilon here of a very small add a plus Epsilon here of a very small add a plus Epsilon here of a very small number it doesn't actually substantially number it doesn't actually substantially number",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 694,
      "text": "it doesn't actually substantially change the result I'm going to skip it change the result I'm going to skip it change the result I'm going to skip it in our case just because uh this is in our case just because uh this is in our case just because uh this is unlikely to happen in our very simple unlikely to happen in our very simple unlikely to happen in our very simple example here and the second thing I want example here and the second thing I want example here and the second thing I want you to notice is that we're being you to notice is that we're being you to notice is that we're being wasteful here and it's very subtle but wasteful here and it's very subtle but wasteful here and it's very subtle",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 695,
      "text": "but right here where we are adding the bias right here where we are adding the bias right here where we are adding the bias into H preact these biases now are into H preact these biases now are into H preact these biases now are actually useless because we're adding actually useless because we're adding actually useless because we're adding them to the H preact",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 696,
      "text": "but then we are them to the H preact",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 697,
      "text": "but then we are them to the H preact",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 698,
      "text": "but then we are calculating the mean for every one of calculating the mean for every one of calculating the mean for every one of these neurons and subtracting it",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 699,
      "text": "so these neurons and subtracting it so these neurons and subtracting it so whatever bias you add here is going to whatever bias you add here is going to whatever bias you add here is going to get subtracted right here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 700,
      "text": "and so these get subtracted right here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 701,
      "text": "and so these get subtracted right here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 702,
      "text": "and so these biases are not doing anything in fact biases are not doing anything in fact biases are not doing anything in fact they're being subtracted out and they they're being subtracted out and they they're being subtracted out and they don't impact the rest of the calculation don't impact the rest of the calculation don't impact the rest of the calculation so if you look at b1.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 703,
      "text": "grad it's actually",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 704,
      "text": "so if you look at b1.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 705,
      "text": "grad it's actually",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 706,
      "text": "so if you look at b1.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 707,
      "text": "grad it's actually going to be zero because it's being going to be zero because it's being going to be zero because it's being subtracted out and doesn't actually have subtracted out and doesn't actually have subtracted out and doesn't actually have any effect",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 708,
      "text": "and so whenever you're using any effect",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 709,
      "text": "and so whenever you're using any effect",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 710,
      "text": "and",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 711,
      "text": "so whenever you're using bash normalization layers then if you bash normalization layers then if you bash normalization layers then if you have any weight layers before like a have any weight layers before like a have any weight layers before like a linear or a c or something like that linear or a c or something like that linear or a c or something like that you're better off coming here and just you're better off coming here and just you're better off coming here and just like not using bias",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 712,
      "text": "so you don't want to like not using bias so you don't want to like not using bias so you don't want to use bias",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 713,
      "text": "and then here you don't want to use bias",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 714,
      "text": "and then here you don't want to use bias",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 715,
      "text": "and then here you don't want to add it because it's that spirous instead add it because it's that spirous instead add it because it's that spirous instead we have this B normalization bias here we have this B normalization bias here we have this B normalization bias here and that b rization bias is now in and that b rization bias is now in and that b rization bias is now in charge of the biasing of this charge of the biasing of this charge of the biasing of this distribution instead of this B1 that we distribution instead of this B1 that we distribution instead of this B1 that we had here originally and so uh basically had here originally and so uh basically had here originally and so uh basically bash normalization layer has its own bash normalization layer has its own bash normalization layer has its own bias and there's no need to have a bias bias and there's no need to have a bias bias and there's no need to have a bias in the layer before it because that bias in the layer before it because that bias in the layer before it because that bias is going to be subtracted out anyway so is going to be subtracted out",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 716,
      "text": "anyway so is going to be subtracted out anyway",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 717,
      "text": "so that's the other small detail to be that's the other small detail to be that's the other small detail to be careful with sometimes it's not going to careful with sometimes it's not going to careful with sometimes it's not going to do anything catastrophic this B1 will do anything catastrophic this B1 will do anything catastrophic this B1 will just be useless it will never get any just be useless it will never get any just be useless it will never get any gradient uh it will not learn it will gradient uh it will not learn it will gradient uh it will not learn it will stay constant and it's just wasteful but stay constant and it's just wasteful but stay constant and it's just wasteful but it doesn't actually really uh impact it doesn't actually really uh impact it doesn't actually really uh impact anything",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 718,
      "text": "otherwise okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 719,
      "text": "so I rearranged anything otherwise",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 720,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 721,
      "text": "so I rearranged anything otherwise",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 722,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 723,
      "text": "so I rearranged the code a little bit with comments and the code a little bit with comments and the code a little bit with comments",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 724,
      "text": "and I just wanted to give a very quick I just wanted to give a very quick I just wanted to give a very quick summary of The Bash normalization layer summary of The Bash normalization layer summary of The Bash normalization layer we are using bash normalization to we are using bash normalization to we are using bash normalization to control the statistics of activations in control the statistics of activations in control the statistics of activations in the neural net it is common to sprinkle the neural net it is common to sprinkle the neural net it is common to sprinkle bash normalization layer across the bash normalization layer across the bash normalization layer across the neural net and usually we will place it neural net and usually we will place it neural net and usually we will place it after layer that have multiplications after layer that have multiplications after layer that have multiplications like for example a linear layer or like for example a linear layer or like for example a linear layer or convolutional layer which we may cover convolutional layer which we may cover convolutional layer which we may cover in the in the in the future now the bat normalization future now the bat normalization future now the bat normalization internally has parameters for the gain internally has parameters for the gain internally has parameters for the gain and the bias and these are trained using and the bias and these are trained using and the bias and these are trained using back propagation it also has two buffers back propagation it also has two buffers back propagation it also has two buffers the buffers are the mean and the the buffers are the mean and the the buffers are the mean and the standard deviation the running mean and standard deviation the running mean and standard deviation the running mean and the running mean of the standard the running mean of the standard the running mean of the standard deviation and these are not trained deviation and these are not trained deviation and these are not trained using back propagation these are trained using back propagation these are trained using back propagation these are trained using this uh janky update of kind of using this uh janky update of kind of using this uh janky update of kind of like a running mean like a running mean like a running mean update so update so update",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 725,
      "text": "so um these are sort of the parameters and um these are sort of the parameters and um these are sort of the parameters and the buffers of Bator layer and then the buffers of Bator layer and then the buffers of Bator layer and then really what it's doing is it's really what it's doing is it's really what it's doing is it's calculating the mean and a standard calculating the mean and a standard calculating the mean and a standard deviation of the activations uh that are deviation of the activations uh that are deviation of the activations uh that are feeding into the Bator layer over that feeding into the Bator layer over that feeding into the Bator layer over that batch",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 726,
      "text": "then it's centering that batch to batch then it's centering that batch to batch then it's centering that batch to be unit gion and then it's offsetting be unit gion",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 727,
      "text": "and then it's offsetting be unit gion",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 728,
      "text": "and then it's offsetting and scaling it by the Learned bias and and scaling it by the Learned bias and and scaling it by the Learned bias and gain and then on top of that it's gain and then on top of that it's gain and then on top of that it's keeping track of the mean and standard keeping track of the mean and standard keeping track of the mean and standard deviation of the inputs and it's deviation of the inputs and it's deviation of the inputs",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 729,
      "text": "and it's maintaining this running mean and maintaining this running mean and maintaining this running mean and standard deviation and this will later standard deviation and this will later standard deviation and this will later be used at inference so that we don't be used at inference so that we don't be used at inference so that we don't have to reestimate the mean stand have to reestimate the mean stand have to reestimate the mean stand deviation all the time and in addition deviation all the time and in addition deviation all the time and in addition that allows us to basically forward that allows us to basically forward that allows us to basically forward individual examples at test time so individual examples at test time so individual examples at test time so that's the bash normalization layer it's that's the bash normalization layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 730,
      "text": "it's that's the bash normalization layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 731,
      "text": "it's a fairly complicated layer um but this a fairly complicated layer um but this a fairly complicated layer um but this is what it's doing internally now I is what it's doing internally now I is what it's doing internally now I wanted to show you a little bit of a wanted to show you a little bit of a wanted to show you a little bit of a real example so you can search resnet real example so you can search resnet real example",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 732,
      "text": "so you can search resnet which is a residual neural network and which is a residual neural network and which is a residual neural network and these are common types of neural these are common types of neural these are common types of neural networks used for image networks used for image networks used for image classification and of course we haven't classification and of course we haven't classification and of course we haven't come resnets in detail",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 733,
      "text": "so I'm not going come resnets in detail",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 734,
      "text": "so I'm not going come resnets in detail",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 735,
      "text": "so I'm not going to explain all the pieces of it but for to explain all the pieces of it but for to explain all the pieces of it but for now just note that the image feeds into now just note that the image feeds into now just note that the image feeds into a reset on the top here and there's many a reset on the top here and there's many a reset on the top here and there's many many layers with repeating structure all many layers with repeating structure all many layers with repeating structure all the way to predictions of what's inside the way to predictions of what's inside the way to predictions of what's inside that image this repeating structure is that image this repeating structure is that image this repeating structure is made up of these blocks and these blocks made up of these blocks and these blocks made up of these blocks and these blocks are just sequentially stacked up in this are just sequentially stacked up in this are just sequentially stacked up in this deep neural network now the code for deep neural network now the code for deep neural network now the code for this uh the block basically that's used this uh the block basically that's used this uh the block basically that's used and repeated sequentially in series is and repeated sequentially in series is and repeated sequentially in series is called this bottleneck block bottleneck called this bottleneck block bottleneck called this bottleneck block bottleneck block and there's a lot here this is all block",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 736,
      "text": "and there's a lot here this is all block",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 737,
      "text": "and there's a lot here this is all pych and of course we haven't covered pych and of course we haven't covered pych and of course we haven't covered all of it",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 738,
      "text": "but I want to point out some all of it",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 739,
      "text": "but I want to point out some all of it",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 740,
      "text": "but I want to point out some small pieces of it here in the init is small pieces of it here in the init is small pieces of it here in the init is where we initialize the neuronet",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 741,
      "text": "so this where we initialize the neuronet so this where we initialize the neuronet so this code of block here is basically the kind code of block here is basically the kind code of block here is basically the kind of stuff we're doing here we're of stuff we're doing here we're of stuff we're doing here we're initializing all the layers and in the initializing all the layers",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 742,
      "text": "and in the initializing all the layers and in the forward we are specifying how the neuron forward we are specifying how the neuron forward we are specifying how the neuron lot acts once you actually have the lot acts once you actually have the lot acts once you actually have the input so this code here is along the input so this code here is along the input so this code here is along the lines of what we're doing lines of what we're doing lines of what we're doing here and now these blocks are replicated here and now these blocks are replicated here and now these blocks are replicated and stacked up serially and that's what and stacked up serially and that's what and stacked up serially and that's what a residual Network would be and so a residual Network would be and so a residual Network would be and so notice What's Happening Here com one um notice What's Happening Here com one um notice What's Happening Here com one um these are convolution layers and these these are convolution layers and these these are convolution layers and these convolution layers basically they're the convolution layers basically they're the convolution layers basically they're the same thing as a linear layer except same thing as a linear layer except same thing as a linear layer except convolutional layers don't apply um convolutional layers don't apply um convolutional layers don't apply um convolutional layers are used for images convolutional layers are used for images convolutional layers are used for images and so they have SP structure and and so they have SP structure and and so they have SP structure and basically this linear multiplication and basically this linear multiplication and basically this linear multiplication and bias offset are done on patches instead bias offset are done on patches instead bias offset are done on patches instead of math instead of the full input so of math instead of the full input so of math instead of the full input so because these images have structure because these images have structure because these images have structure spatial structure convolutions just spatial structure convolutions just spatial structure convolutions just basically do WX plus b",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 743,
      "text": "but they do it on basically do WX",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 744,
      "text": "plus",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 745,
      "text": "b",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 746,
      "text": "but they do it on basically do WX",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 747,
      "text": "plus",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 748,
      "text": "b",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 749,
      "text": "but they do it on overlapping patches of the input but overlapping patches of the input but overlapping patches of the input",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 750,
      "text": "but otherwise it's WX",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 751,
      "text": "plus otherwise it's WX",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 752,
      "text": "plus otherwise it's WX plus P",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 753,
      "text": "then we have the norm layer which by P then we have the norm layer which by P",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 754,
      "text": "then we have the norm layer which by default here is initialized to be a bash default here is initialized to be a bash default here is initialized to be a bash Norm in 2D so two- dimensional bash Norm in 2D so two- dimensional bash Norm in 2D so two- dimensional bash normalization layer and then we have a normalization layer and then we have a normalization layer and then we have a nonlinearity like reu so instead of uh nonlinearity like reu",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 755,
      "text": "so instead of uh nonlinearity like reu",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 756,
      "text": "so instead of uh here they use reu we are using 10 in here they use reu we are using 10 in here they use reu we are using 10 in this case but both both are just this case but both both are just this case but both both are just nonlinearities and you can just use them nonlinearities and you can just use them nonlinearities and you can just use them relatively interchangeably for very deep relatively interchangeably for very deep relatively interchangeably for very deep networks re typically empirically work a networks re typically empirically work a networks re typically empirically work a bit better so see the motif that's being bit better so see the motif that's being bit better so see the motif that's being repeated here we have convolution bat repeated here we have convolution bat repeated here we have convolution bat normalization reu convolution bat normalization reu convolution bat normalization reu convolution bat normalization re Etc",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 757,
      "text": "and then here this normalization re Etc",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 758,
      "text": "and then here this normalization re Etc",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 759,
      "text": "and then here this is residual connection that we haven't is residual connection that we haven't is residual connection that we haven't covered yet",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 760,
      "text": "but basically that's the covered yet",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 761,
      "text": "but basically that's the covered yet",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 762,
      "text": "but basically that's the exact same pattern we have here with we exact same pattern we have here with we exact same pattern we have here with we have a weight layer like a convolution have a weight layer like a convolution have a weight layer like a convolution or like a linear layer bash or like a linear layer bash or like a linear layer bash normalization and then 10h which is normalization and then 10h which is normalization and then 10h which is nonlinearity but basically a weight nonlinearity but basically a weight nonlinearity but basically a weight layer a normalization layer and layer a normalization layer and layer a normalization layer and nonlinearity and that's the motif that nonlinearity and that's the motif that nonlinearity and that's the motif that you would be stacking up when you create you would be stacking up when you create you would be stacking up when you create these deep neural networks exactly as these deep neural networks exactly as these deep neural networks exactly as it's done here and one more thing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 763,
      "text": "I'd it's done here and one more thing I'd it's done here and one more thing I'd like you to notice is that here when like you to notice is that here when like you to notice is that here when they are initializing the com layers they are initializing the com layers they are initializing the com layers like com 1 by one the depth for that is like com 1 by one the depth for that is like com 1 by one the depth for that is right here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 764,
      "text": "and so it's initializing an right here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 765,
      "text": "and so it's initializing an right here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 766,
      "text": "and so it's initializing an nn.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 767,
      "text": "Tod which is a convolution layer in nn.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 768,
      "text": "Tod which is a convolution layer in nn.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 769,
      "text": "Tod which is a convolution layer in pytorch and there's a bunch of keyword pytorch and there's a bunch of keyword pytorch and there's a bunch of keyword arguments here that I'm not going to arguments here that I'm not going to arguments here that I'm not going to explain yet",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 770,
      "text": "but you see how there's bias explain yet",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 771,
      "text": "but you see how there's bias explain yet",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 772,
      "text": "but you see how there's bias equals false the bias equals false is equals false the bias equals false is equals false the bias equals false is exactly for the same reason as bias is exactly for the same reason as bias is exactly for the same reason as bias is not used in our case you see how I eras not used in our case you see how I eras not used in our case you see how I eras the use of bias and the use of bias is the use of bias and the use of bias is the use of bias and the use of bias is spous because after this weight layer spous because after this weight layer spous because after this weight layer there's a bash normalization and The there's a bash normalization and The there's a bash normalization and The Bash normalization subtracts that bias Bash normalization subtracts that bias Bash normalization subtracts that bias and then has its own bias so there's no and then has its own bias so there's no",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 773,
      "text": "and then has its own bias so there's no need to introduce these spous parameters need to introduce these spous parameters need to introduce these spous parameters it wouldn't hurt performance it's just it wouldn't hurt performance it's just it wouldn't hurt performance it's just useless and so because they have this useless and so because they have this useless and so because they have this motif of C Bast umbrell they don't need motif of C Bast umbrell they don't need motif of C Bast umbrell they don't need a bias here because there's a bias a bias here because there's a bias a bias here because there's a bias inside here so by the way this example inside here so by the way this example inside here so by the way this example here is very easy to find just do here is very easy to find just do here is very easy to find just do resonet pie resonet pie resonet pie torch and uh it's this example here so torch",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 774,
      "text": "and uh it's this example here so torch",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 775,
      "text": "and uh it's this example here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 776,
      "text": "so this is kind of like the stock this is kind of like the stock this is kind of like the stock implementation of a residual neural implementation of a residual neural implementation of a residual neural network in pytorch and uh you can find network in pytorch and uh you can find network in pytorch and uh you can find that here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 777,
      "text": "but of course I haven't that here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 778,
      "text": "but of course I haven't that here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 779,
      "text": "but of course I haven't covered many of these parts yet",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 780,
      "text": "and I covered many of these parts yet",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 781,
      "text": "and I covered many of these parts yet",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 782,
      "text": "and I would also like to briefly descend into would also like to briefly descend into would also like to briefly descend into the definitions of these pytorch layers the definitions of these pytorch layers the definitions of these pytorch layers and the the parameters that they take and the the parameters that they take and the the parameters that they take now instead of a convolutional layer now instead of a convolutional layer now instead of a convolutional layer we're going to look at a linear layer uh we're going to look at a linear layer uh we're going to look at a linear layer uh because that's the one that we're using because that's the one that we're using because that's the one that we're using here this is a linear layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 783,
      "text": "and I here this is a linear layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 784,
      "text": "and I here this is a linear layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 785,
      "text": "and I haven't cover covered convolutions yet haven't cover covered convolutions yet haven't cover covered convolutions",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 786,
      "text": "yet",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 787,
      "text": "but as I mentioned convolutions are but as I mentioned convolutions are but as I mentioned convolutions are basically linear layers except on basically linear layers except on basically linear layers except on patches so a linear layer performs a WX patches so a linear layer performs a WX patches so a linear layer performs a WX plus b except here they're calling the W plus b except here they're calling the W plus b except here they're calling the W A A A transpose um so to calcul WX plus b very transpose um so to calcul WX plus b very transpose um so to calcul WX plus b very much like we did here to initialize this much like we did here to initialize this much like we did here to initialize this layer you need to know the fan in the layer you need to know the fan in the layer you need to know the fan in the fan out and that's so that they can fan out and that's so that they can fan out and that's so that they can initialize this W this is the fan in and initialize this W this is the fan in and initialize this W this is the fan in and the fan out",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 788,
      "text": "so they know how how big the the fan out",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 789,
      "text": "so they know how how big the the fan out",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 790,
      "text": "so they know how how big the weight Matrix should be you need to also weight Matrix should be you need to also weight Matrix should be you need to also pass in whether you whether or not you pass in whether you whether or not you pass in whether you whether or not you want a bias and if you set it to false want a bias and if you set it to false want a bias and if you set it to false then no bias will be uh inside this then no bias will be uh inside this then no bias will be uh inside this layer um and you may want to do that layer um and you may want to do that layer um and you may want to do that exactly like in our case if your layer exactly like in our case if your layer exactly like in our case if your layer is followed by a normalization layer is followed by a normalization layer is followed by a normalization layer such as batch such as batch such as batch Norm so this allows you to basically Norm so this allows you to basically Norm so this allows you to basically disable a bias now in terms of the disable a bias now in terms of the disable a bias now in terms of the initial ation if we swing down here this initial ation if we swing down here this initial ation if we swing down here this is reporting the variables used inside is reporting the variables used inside is reporting the variables used inside this linear layer and our linear layer this linear layer and our linear layer this linear layer and our linear layer here has two parameters the weight and here has two parameters the weight and here has two parameters the weight and the bias in the same way they have a the bias in the same way they have a the bias in the same way they have a weight and a bias and they're talking weight and a bias and they're talking weight and a bias and they're talking about how they initialize it by default about how they initialize it by default about how they initialize it by default so by default P will initialize your so by default P will initialize your so by default P will initialize your weights by taking the weights by taking the weights by taking the Fanon and then um doing one over fanin Fanon and then um doing one over fanin Fanon and then um doing one over fanin square root and then instead of a normal square root and then instead of a normal square root and then instead of a normal distribution they are using a uniform distribution they are using a uniform distribution they are using a uniform distribution distribution distribution so it's very much the same thing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 791,
      "text": "but so it's very much the same thing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 792,
      "text": "but so it's very much the same thing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 793,
      "text": "but they are using a one instead of 5 over they are using a one instead of 5 over they are using a one instead of 5 over three so there's no gain being three so there's no gain being three so there's no gain being calculated here the gain is just one but calculated here the gain is just one but calculated here the gain is just one but otherwise is exactly one over the square otherwise is exactly one over the square otherwise is exactly one over the square root of fan in exactly as we have root of fan in exactly as we have root of fan in exactly as we have here so one over the square root of K is here so one over the square root of K is here so one over the square root of K is the is the scale of the weights but when the is the scale of the weights but when the is the scale of the weights but when they are drawing the numbers they're not they are drawing the numbers they're not they are drawing the numbers they're not using a gussion by default they're using using a gussion by default they're using using a gussion by default they're using a uniform distribution by default and so a uniform distribution by default and so a uniform distribution by default",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 794,
      "text": "and so they draw uniformly from negative of K they draw uniformly from negative of K they draw uniformly from negative of K to squ of K to squ of K to squ of K",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 795,
      "text": "but it's the exact same thing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 796,
      "text": "and the",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 797,
      "text": "but it's the exact same thing and the",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 798,
      "text": "but it's the exact same thing and the same motivation from for with respect to same motivation from for with respect to same motivation from for with respect to what we've seen in this lecture and the what we've seen in this lecture and the what we've seen in this lecture and the reason they're doing this is if you have reason they're doing this is if you have reason they're doing this is if you have a roughly gsan input this will ensure a roughly gsan input this will ensure a roughly gsan input this will ensure that out of this layer you will have a that out of this layer you will have a that out of this layer you will have a roughly Gan output and you you basically roughly Gan output and you you basically roughly Gan output and you you basically achieve that by scaling the weights by achieve that by scaling the weights by achieve that by scaling the weights by one over the square root of fan in so one over the square root of fan in so one over the square root of fan in",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 799,
      "text": "so that's what this is that's what this is that's what this is doing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 800,
      "text": "and then the second thing is the doing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 801,
      "text": "and then the second thing is the doing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 802,
      "text": "and then the second thing is the bash normalization layer so let's look bash normalization layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 803,
      "text": "so let's look bash normalization layer so let's look at what that looks like in pytorch at what that looks like in pytorch at what that looks like in pytorch so here we have a onedimensional b so here we have a onedimensional b so here we have a onedimensional b normalization layer exactly as we are normalization layer exactly as we are normalization layer exactly as we are using here and there are a number of using here and there are a number of using here and there are a number of keyword arguments going into it as well keyword arguments going into it as well keyword arguments going into it as well",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 804,
      "text": "so we need to know the number of so we need to know the number of so we need to know the number of features uh for us that is 200 and that features uh for us that is 200 and that features uh for us that is 200 and that is needed so that we can initialize is needed so that we can initialize is needed so that we can initialize these parameters here the gain the bias these parameters here the gain the bias these parameters here the gain the bias and the buffers for the running uh mean and the buffers for the running uh mean and the buffers for the running uh mean and standard and standard deviation then they need to know the deviation then they need to know the deviation then they need to know the value of Epsilon here and by default value of Epsilon here and by default value of Epsilon here and by default this is one5 you don't typically change this is one5 you don't typically change this is one5 you don't typically change this too much then they need to know the this too much then they need to know the this too much then they need to know the momentum momentum momentum and the momentum here as they explain is and the momentum here as they explain is and the momentum here as they explain is basically used for these uh running mean basically used for these uh running mean basically used for these uh running mean and running standard deviation so by and running standard deviation so by and running standard deviation so by default the momentum here is 0.1 the default the momentum here is 0.1 the default the momentum here is 0.1 the momentum we are using here in this momentum we are using here in this momentum we are using here in this example is example is example is 0.001 and basically rough you may want 0.001 and basically rough you may want 0.001 and basically rough you may want to change this sometimes and roughly to change this sometimes and roughly to change this sometimes and roughly speaking if you have a very large batch speaking if you have a very large batch speaking if you have a very large batch size then typically what you'll see is size then typically what you'll see is size then typically what you'll see is that when you estimate the mean and the that when you estimate the mean and the that when you estimate the mean and the standard deviation for every single standard deviation for every single standard deviation for every single batch size if it's large enough you're batch size if it's large enough you're batch size if it's large enough you're going to get roughly the same result going to get roughly the same result going to get roughly the same result",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 805,
      "text": "and so therefore you can use slightly",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 806,
      "text": "and so therefore you can use slightly",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 807,
      "text": "and so therefore you can use slightly higher momentum like higher momentum like higher momentum like 0.1 but for a batch size as small as 32 0.1 but for a batch size as small as 32 0.1 but for a batch size as small as 32 the mean and standard deviation here the mean and standard deviation here the mean and standard deviation here might take on slightly different numbers might take on slightly different numbers might take on slightly different numbers because there's only 32 examples we are because there's only 32 examples we are because there's only 32 examples we are using to estimate the mean and standard using to estimate the mean and standard using to estimate the mean and standard deviation so the value is changing deviation so the value is changing deviation so the value is changing around a lot and if your momentum is 0.1 around a lot",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 808,
      "text": "and if your momentum is 0.1 around a lot",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 809,
      "text": "and if your momentum is 0.1 that that might not be good enough for that that might not be good enough for that that might not be good enough for this value to settle and um converge to this value to settle and um converge to this value to settle and um converge to the actual mean and standard deviation the actual mean and standard deviation the actual mean and standard deviation over the entire training set and so over the entire training set and so over the entire training set",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 810,
      "text": "and so basically if your batch size is very basically if your batch size is very basically if your batch size is very small uh momentum of 0.1 is potentially small uh momentum of 0.1 is potentially small uh momentum of 0.1 is potentially dangerous and it might make it so that dangerous",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 811,
      "text": "and it might make it so that dangerous",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 812,
      "text": "and it might make it so that the running uh mean and stand deviation the running uh mean and stand deviation the running uh mean and stand deviation are is thrashing too much during are is thrashing too much during are is thrashing too much during training",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 813,
      "text": "and it's not actually training and it's not actually training",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 814,
      "text": "and it's not actually converging converging converging properly uh aine equals true determines properly uh aine equals true determines properly uh aine equals true determines whether this batch normalization layer whether this batch normalization layer whether this batch normalization layer has these learnable Aline parameters the has these learnable Aline parameters the has these learnable Aline parameters the uh the gain and the bias and this is uh the gain and the bias and this is uh the gain and the bias and this is almost always kept to true",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 815,
      "text": "I'm not almost always kept to true",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 816,
      "text": "I'm not almost always kept to true",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 817,
      "text": "I'm not actually sure why you would want to actually sure why you would want to actually sure why you would want to change this to false um change this to false um change this to false um then track running stats is determining then track running stats is determining then track running stats is determining whether or not B rization layer of whether or not B rization layer of whether or not B rization layer of pytorch will be doing pytorch will be doing pytorch will be doing this and um one reason you may you may this",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 818,
      "text": "and um one reason you may you may this and um one reason you may you may want to skip the running stats is want to skip the running stats is want to skip the running stats is because you may want to for example because you may want to for example because you may want to for example estimate them at the end as a stage two estimate them at the end as a stage two estimate them at the end as a stage two like this and in that case you don't like this and in that case you don't like this and in that case you don't want the bat normalization layer to be want the bat normalization layer to be want the bat normalization layer to be doing all this extra compute that you're doing all this extra compute that you're doing all this extra compute that you're not going to not going to not going to use and uh finally we need to know which use and uh finally we need to know which use and uh finally we need to know which device we're going to run this bash device we're going to run this bash device we're going to run this bash normalization on a CPU or a GPU and what normalization on a CPU or a GPU and what normalization on a CPU or a GPU and what the data type should be uh half the data type should be uh half the data type should be uh half Precision single Precision double Precision single Precision double Precision single Precision double precision and so precision and so precision and so on so that's the bat normalization layer on so that's the bat normalization layer on so that's the bat normalization layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 819,
      "text": "otherwise they link to the paper is the otherwise they link to the paper is the otherwise they link to the paper is the same formula we've implemented and same formula we've implemented and same formula we've implemented and everything is the same exactly as we've everything is the same exactly as we've everything is the same exactly as we've done done done here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 820,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 821,
      "text": "so that's everything",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 822,
      "text": "that I here okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 823,
      "text": "so that's everything",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 824,
      "text": "that I here okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 825,
      "text": "so that's everything that I wanted to cover for this lecture really wanted to cover for this lecture really wanted to cover for this lecture really what I wanted to talk about is the what I wanted to talk about is the what I wanted to talk about is the importance of understanding the importance of understanding the importance of understanding the activations and the gradients and their activations and the gradients and their activations and the gradients and their statistics in neural networks and this statistics in neural networks and this statistics in neural networks and this becomes increasingly important becomes increasingly important becomes increasingly important especially as you make your neural especially as you make your neural especially as you make your neural networks bigger larger and deeper networks bigger larger and deeper networks bigger larger and deeper we looked at the distributions basically we looked at the distributions basically we looked at the distributions basically at the output layer and we saw that if at the output layer and we saw that if at the output layer and we saw that if you have two confident mispredictions you have two confident mispredictions you have two confident mispredictions because the activations are too messed because the activations are too messed because the activations are too messed up at the last layer you can end up with up at the last layer you can end up with up at the last layer you can end up with these hockey stick losses and if you fix these hockey stick losses and if you fix these hockey stick losses and if you fix this you get a better loss at the end of this you get a better loss at the end of this you get a better loss at the end of training because your training is not training because your training is not training because your training is not doing wasteful work then we also saw doing wasteful work then we also saw doing wasteful work then we also saw that we need to control the activations that we need to control the activations that we need to control the activations we don't want them to uh you know squash we don't want them to uh you know squash we don't want them to uh you know squash to zero or explode to infinity and to zero or explode to infinity and to zero or explode to infinity and because that you can run into a lot of because that you can run into a lot of because that you can run into a lot of trouble with all of these uh trouble with all of these uh trouble with all of these uh nonlinearities and these neural Nets and nonlinearities and these neural Nets and nonlinearities and these neural Nets and basically you want everything to be basically you want everything to be basically you want everything to be fairly homogeneous throughout the neural fairly homogeneous throughout the neural fairly homogeneous throughout the neural net you want roughly goshan activations net you want roughly goshan activations net you want roughly goshan activations throughout the neural net let me talked throughout the neural net let me talked throughout the neural net let me talked about",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 826,
      "text": "okay if we want roughly Gan about okay if we want roughly Gan about okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 827,
      "text": "if we want roughly Gan activations how do we scale these weight activations how do we scale these weight activations how do we scale these weight matrices and biases during matrices and biases during matrices and biases during initialization of the neural nut so that initialization of the neural nut so that initialization of the neural nut so that we don't get um you know",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 828,
      "text": "so everything we don't get um you know",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 829,
      "text": "so everything we don't get um you know so everything is as controlled as is as controlled as is as controlled as possible um so that give us a large possible um so that give us a large possible um so that give us a large boost in Improvement and then I talked boost in Improvement and then I talked boost in Improvement and then I talked about how that strategy is not actually about how that strategy is not actually about how that strategy is not actually uh Poss for much much deeper neural nuts uh Poss for much much deeper neural nuts uh Poss for much much deeper neural nuts because um when you have much deeper because um when you have much deeper because um when you have much deeper neural nuts with lots of different types neural nuts with lots of different types neural nuts with lots of different types of layers it becomes really really hard of layers it becomes really really hard of layers it becomes really really hard to precisely set the weights and the to precisely set the weights and the to precisely set the weights and the biases in such a way that the biases in such a way that the biases in such a way that the activations are roughly uniform activations are roughly uniform activations are roughly uniform throughout the neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 830,
      "text": "so then I throughout the neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 831,
      "text": "so then I throughout the neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 832,
      "text": "so then I introduced the notion of a normalization introduced the notion of a normalization introduced the notion of a normalization layer now there are many normalization layer now there are many normalization layer now there are many normalization layers that that people use in practice layers that that people use in practice layers that that people use in practice bat normalization layer normalization bat normalization layer normalization bat normalization layer normalization instance normalization group instance normalization group instance normalization group normalization we haven't covered most of normalization we haven't covered most of normalization we haven't covered most of them",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 833,
      "text": "but I've introduced the first one them",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 834,
      "text": "but I've introduced the first one them",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 835,
      "text": "but I've introduced the first one and also the one that I believe came out and also the one that I believe came out and also the one that I believe came out first and that's called Bat first",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 836,
      "text": "and that's called Bat first",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 837,
      "text": "and that's called Bat normalization",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 838,
      "text": "and we saw how bat normalization",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 839,
      "text": "and we saw how bat normalization",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 840,
      "text": "and we saw how bat normalization Works uh this is a layer normalization Works uh this is a layer normalization Works uh this is a layer that you can sprinkle throughout your that you can sprinkle throughout your that you can sprinkle throughout your deep neural net and the basic idea is if deep neural net and the basic idea is if deep neural net and the basic idea is if you want roughly gsh in activations well you want roughly gsh in activations",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 841,
      "text": "well you want roughly gsh in activations well then take your activations and um take then take your activations and um take then take your activations and um take the mean and the standard deviation and the mean and the standard deviation and the mean and the standard deviation and Center your data and you can do that Center your data and you can do that Center your data and you can do that because the centering operation is because the centering operation is because the centering operation is differentiable but and on top of that we differentiable but and on top of that we differentiable but and on top of that we actually had to add a lot of bells and actually had to add a lot of bells and actually had to add a lot of bells and whistles and that gave you a sense of whistles and that gave you a sense of whistles and that gave you a sense of the complexities of the batch the complexities of the batch the complexities of the batch normalization layer because now we're normalization layer because now we're normalization layer because now we're centering the data that's great but centering the data that's great but centering the data that's great",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 842,
      "text": "but suddenly we need the gain and the bias suddenly we need the gain and the bias suddenly we need the gain and the bias and now those are and now those are and now those are trainable and then because we are trainable and then because we are trainable and then because we are coupling all of the training examples coupling all of the training examples coupling all of the training examples now suddenly the question is how do you now suddenly the question is how do you now suddenly the question is how do you do the inference where to do to do the do the inference where to do to do the do the inference where to do to do the inference we need to now estimate these inference we need to now estimate these inference we need to now estimate these um mean and standard deviation once uh um mean and standard deviation once uh um mean and standard deviation once uh or the entire training set and then use or the entire training set and then use or the entire training set and then use those at inference but then no one likes those at inference but then no one likes those at inference but then no one likes to do stage two",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 843,
      "text": "so instead we fold to do stage two so instead we fold to do stage two",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 844,
      "text": "so instead we fold everything everything into the bat everything everything into the bat everything everything into the bat normalization later during training and normalization later during training and normalization later during training and try to estimate these in the running try to estimate these in the running try to estimate these in the running manner so that everything is a bit manner so that everything is a bit manner so that everything is a bit simpler and that gives us the bat simpler and that gives us the bat simpler and that gives us the bat normalization layer um and as I normalization layer um and as I normalization layer um and as I mentioned no one likes this layer it mentioned no one likes this layer it mentioned no one likes this layer it causes a huge amount of bugs um and causes a huge amount of bugs um and causes a huge amount of bugs um and intuitively it's because it is coupling intuitively it's because it is coupling intuitively it's because it is coupling examples um in the for pass of a neural examples um in the for pass of a neural examples um in the for pass of a neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 845,
      "text": "and uh I've shot myself in the foot nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 846,
      "text": "and uh I've shot myself in the foot nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 847,
      "text": "and uh I've shot myself in the foot with this layer over and over again in with this layer over and over again in with this layer over and over again in my life",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 848,
      "text": "and I don't want you to suffer my life",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 849,
      "text": "and I don't want you to suffer my life",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 850,
      "text": "and I don't want you to suffer the same uh so basically try to avoid it the same",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 851,
      "text": "uh so basically try to avoid it the same uh so basically try to avoid it as much as possible uh some of the other as much as possible uh some of the other as much as possible uh some of the other alternatives to these layers are for alternatives to these layers are for alternatives to these layers are for example group normalization or layer example group normalization or layer example group normalization or layer normalization and those have become more normalization and those have become more normalization and those have become more common uh in more recent deep learning common uh in more recent deep learning common uh in more recent deep learning",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 852,
      "text": "uh but we haven't covered those yet uh uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 853,
      "text": "but we haven't covered those yet uh uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 854,
      "text": "but we haven't covered those yet uh but definitely bash normalization was but definitely bash normalization was but definitely bash normalization was very influential at the time when it very influential at the time when it very influential at the time when it came out in roughly 2015 because it was came out in roughly 2015 because it was came out in roughly 2015 because it was kind of the first time that you could kind of the first time that you could kind of the first time that you could train reliably uh much deeper neural train reliably uh much deeper neural train reliably uh much deeper neural nuts and fundamentally the reason for nuts and fundamentally the reason for nuts and fundamentally the reason for that is because this layer was very that is because this layer was very that is because this layer was very effective at controlling the statistics effective at controlling the statistics effective at controlling the statistics of the activations in the neural nut so of the activations in the neural nut so of the activations in the neural nut so that's the story so far and um that's that's the story so far and um that's that's the story so far and um that's all I wanted to cover and in the future all I wanted to cover and in the future all I wanted to cover and in the future lectures hopefully we can start going lectures hopefully we can start going lectures hopefully we can start going into recurrent R Nets and um recurring into recurrent R Nets and um recurring into recurrent R Nets and um recurring neural Nets as we'll see are just very neural Nets as we'll see are just very neural Nets as we'll see are just very very deep networks because you uh you very deep networks because you uh you very deep networks because you uh you unroll the loop and uh when you actually unroll the loop and uh when you actually unroll the loop and uh when you actually optimize these neurals and that's where optimize these neurals and that's where optimize these neurals and that's where a lot of this a lot of this a lot of this um analysis around the activation um analysis around the activation um analysis around the activation statistics and all these normalization statistics and all these normalization statistics and all these normalization layers will become very very important layers will become very very important layers will become very very important for uh good performance so we'll see for uh good performance so we'll see for uh good performance so we'll see that next time",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 855,
      "text": "bye okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 856,
      "text": "so I lied I that next time",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 857,
      "text": "bye okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 858,
      "text": "so I lied I that next time",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 859,
      "text": "bye okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 860,
      "text": "so I lied I would like us to do one more summary would like us to do one more summary would like us to do one more summary here as a bonus",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 861,
      "text": "and I think it's useful here as a bonus",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 862,
      "text": "and I think it's useful here as a bonus",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 863,
      "text": "and I think it's useful as to have one more summary of as to have one more summary of as to have one more summary of everything I've presented in this everything I've presented in this everything I've presented in this lecture",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 864,
      "text": "but also I would like us to lecture",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 865,
      "text": "but also I would like us to lecture",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 866,
      "text": "but also I would like us to start by torify our code a little bit so start by torify our code a little bit so start by torify our code a little bit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 867,
      "text": "so it looks much more like what you would it looks much more like what you would it looks much more like what you would encounter in PCH",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 868,
      "text": "so you'll see that I encounter in PCH",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 869,
      "text": "so you'll see that I encounter in PCH",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 870,
      "text": "so you'll see that I will structure our code into these will structure our code into these will structure our code into these modules like a link modules like a link modules like a link uh module and a borm module and I'm uh module and a borm module and I'm uh module and a borm module and I'm putting the code inside these modules so putting the code inside these modules so putting the code inside these modules so that we can construct neural networks that we can construct neural networks that we can construct neural networks very much like we would construct them very much like we would construct them very much like we would construct them in pytorch",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 871,
      "text": "and I will go through this in in pytorch and I will go through this in in pytorch",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 872,
      "text": "and I will go through this in detail so we'll create our neural net detail so we'll create our neural net detail so we'll create our neural net then we will do the optimization loop as then we will do the optimization loop as then we will do the optimization loop as we did before",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 873,
      "text": "and then the one more we did before and then the one more we did before and then the one more thing that I want to do here is I want thing that I want to do here is I want thing that I want to do here is I want to look at the activation statistics to look at the activation statistics to look at the activation statistics both in the forward pass and in the both in the forward pass and in the both in the forward pass and in the backward pass and then here we have the backward pass and then here we have the backward pass and then here we have the evaluation and sampling just like before evaluation and sampling just like before evaluation and sampling just like before so let me rewind all the way up here and so let me rewind all the way up here and so let me rewind all the way up here and and go a little bit slower",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 874,
      "text": "so here I and go a little bit slower",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 875,
      "text": "so here I and go a little bit slower so here I creating a linear layer you'll notice creating a linear layer you'll notice creating a linear layer you'll notice that torch.nn has lots of different that torch.nn has lots of different that torch.nn has lots of different types of layers and one of those layers types of layers and one of those layers types of layers and one of those layers is the linear layer torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 876,
      "text": "n. linear is the linear layer torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 877,
      "text": "n. linear is the linear layer torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 878,
      "text": "n. linear takes a number of input features output takes a number of input features output takes a number of input features output features whether or not we should have a features whether or not we should have a features whether or not we should have a bias and then the device that we want to bias and then the device that we want to bias and then the device that we want to place this layer on and the data type so place this layer on and the data type so place this layer on and the data type",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 879,
      "text": "so I will emit these two",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 880,
      "text": "but otherwise we I will emit these two",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 881,
      "text": "but otherwise we I will emit these two",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 882,
      "text": "but otherwise we have the exact same thing we have the have the exact same thing we have the have the exact same thing we have the fan in which is the number of inputs fan fan in which is the number of inputs fan fan in which is the number of inputs fan out the number of outputs and whether or out the number of outputs and whether or out the number of outputs and whether or not we want to use a bias not we want to use a bias not we want to use a bias and internally inside this layer there's and internally inside this layer there's and internally inside this layer there's a weight and a bias if you'd like it it a weight and a bias if you'd like it it a weight and a bias if you'd like it it is typical to initialize the weight is typical to initialize the weight is typical to initialize the weight using um say random numbers drawn from using um say random numbers drawn from using um say random numbers drawn from aashan and then here's the coming aashan and then here's the coming aashan and then here's the coming initialization",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 883,
      "text": "um that we discussed initialization um that we discussed initialization um that we discussed already in this lecture and that's a already in this lecture and that's a already in this lecture",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 884,
      "text": "and that's a good default and also the default that I good default and also the default that I good default and also the default that I believe pytor chooses and by default the believe pytor chooses and by default the believe pytor chooses and by default the bias is usually initialized to zeros now bias is usually initialized to zeros now bias is usually initialized to zeros now when you call this module uh this will when you call this module uh this will when you call this module uh this will basically calculate W * X plus b if you basically calculate W * X plus b if you basically calculate W * X plus b if you have a b",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 885,
      "text": "and then when you also call have a b",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 886,
      "text": "and then when you also call have a b",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 887,
      "text": "and then when you also call that parameters on this module it will that parameters on this module it will that parameters on this module it will return uh the tensors that are the return uh the tensors that are the return uh the tensors that are the parameters of this layer now next we parameters of this layer now next we parameters of this layer now next we have the bash normalization layer so have the bash normalization layer so have the bash normalization layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 888,
      "text": "so I've written that here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 889,
      "text": "and this is very I've written that here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 890,
      "text": "and this is very I've written that here and this is very similar to pytorch nn. bashor 1D layer similar to pytorch nn. bashor 1D layer similar to pytorch nn. bashor 1D layer as shown as shown as shown here so I'm kind of um taking these here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 891,
      "text": "so I'm kind of um taking these here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 892,
      "text": "so I'm kind of um taking these three parameters here the dimensionality three parameters here the dimensionality three parameters here the dimensionality the Epsilon that we will use in the the Epsilon that we will use in the the Epsilon that we will use in the division and the momentum that we will division and the momentum that we will division and the momentum that we will use in keeping track of these running use in keeping track of these running use in keeping track of these running stats the running mean and the running stats the running mean and the running stats the running mean and the running variance um now py actually takes quite variance um now py actually takes quite variance um now py actually takes quite a few more things",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 893,
      "text": "but I'm assuming some a few more things",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 894,
      "text": "but I'm assuming some a few more things",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 895,
      "text": "but I'm assuming some of their settings so for us Aline will of their settings so for us Aline will of their settings so for us Aline will be true that means that we will be using be true that means that we will be using be true that means that we will be using a gamma and beta after the normalization a gamma and beta after the normalization a gamma and beta after the normalization the track running stats will be true",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 896,
      "text": "so the track running stats will be true so the track running stats will be true so we will be keeping track of the running we will be keeping track of the running we will be keeping track of the running mean and the running variance in the in mean and the running variance in the in mean and the running variance in the in the bat Norm our device by default is the bat Norm our device by default is the bat Norm our device by default is the CPU and the data type by default is the CPU and the data type by default is the CPU and the data type by default is uh float float",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 897,
      "text": "uh float float uh float float 32 so those are the defaults otherwise 32 so those are the defaults otherwise 32 so those are the defaults otherwise uh we are taking all the same parameters uh we are taking all the same parameters uh we are taking all the same parameters in this bachom layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 898,
      "text": "so first I'm just in this bachom layer so first I'm just in this bachom layer so first I'm just saving them now here's something new saving them now here's something new saving them now here's something new there's a doc training which by default there's a doc training which by default there's a doc training which by default is true and pytorch andn modules also is true and pytorch andn modules also is true and pytorch andn modules also have this attribute.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 899,
      "text": "training and that's have this attribute.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 900,
      "text": "training and that's have this attribute.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 901,
      "text": "training and that's because many modules in borm is included because many modules in borm is included because many modules in borm is included in that have a different Behavior in that have a different Behavior in that have a different Behavior whether you are training your interet whether you are training your interet whether you are training your interet and or whether you are running it in an and or whether you are running it in an and or whether you are running it in an evaluation mode and calculating your evaluation mode and calculating your evaluation mode and calculating your evaluation loss or using it for evaluation loss or using it for evaluation loss or using it for inference on some test examples and inference on some test examples and inference on some test examples and bashor is an example of this because bashor is an example of this because bashor is an example of this because when we are training we are going to be when we are training we are going to be when we are training we are going to be using the mean and the variance using the mean and the variance using the mean and the variance estimated from the current batch but estimated from the current batch but estimated from the current batch but during inference we are using the during inference we are using the during inference we are using the running mean and running variance and so running mean and running variance and so running mean and running variance and so also if we are training we are updating also if we are training we are updating also if we are training we are updating mean and variance but if we are testing mean and variance",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 902,
      "text": "but if we are testing mean and variance but if we are testing then these are not being updated they're then these are not being updated they're then these are not being updated they're kept fixed and so this flag is necessary kept fixed and so this flag is necessary kept fixed and so this flag is necessary and by default true just like in and by default true just like in and by default true just like in pytorch now the parameters of B 1D are pytorch now the parameters of B 1D are pytorch now the parameters of B 1D are the gamma and the beta the gamma and the beta the gamma and the beta here and then the running mean and here and then the running mean and here and then the running mean and running variance are called buffers in running variance are called buffers in running variance are called buffers in pyto pyto pyto nomenclature and these buffers are nomenclature and these buffers are nomenclature and these buffers are trained using exponential moving average trained using exponential moving average trained using exponential moving average here explicitly and they are not part of here explicitly and they are not part of here explicitly and they are not part of the back propagation and stochastic the back propagation and stochastic the back propagation and stochastic radient descent",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 903,
      "text": "so they are not sort of radient descent so they are not sort of radient descent",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 904,
      "text": "so they are not sort of like parameters of this layer and that's like parameters of this layer and that's like parameters of this layer and that's why when we C when we have a parameters why when we C when we have a parameters why when we C when we have a parameters here we only return gamma and beta we do here we only return gamma and beta we do here we only return gamma and beta we do not return the mean and the variance not return the mean and the variance not return the mean and the variance this is trained sort of like internally this is trained sort of like internally this is trained sort of like internally here um every forward pass using here um every forward pass using here um every forward pass using exponential moving average",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 905,
      "text": "so that's the exponential moving average so that's the exponential moving average",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 906,
      "text": "so that's the initialization now in a forward pass if initialization now in a forward pass if initialization now in a forward pass if we are training then we use the mean",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 907,
      "text": "and we are training then we use the mean",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 908,
      "text": "and we are training then we use the mean and the variance estimated by the batch let the variance estimated by the batch let the variance estimated by the batch let me pull up the paper me pull up the paper me pull up the paper here we calculate the mean and the here we calculate the mean and the here we calculate the mean and the variance now up above I was estimating variance now up above I was estimating variance now up above I was estimating the standard deviation and keeping track the standard deviation and keeping track the standard deviation and keeping track of the standard deviation here in the of the standard deviation here in the of the standard deviation here in the running standard deviation instead of running standard deviation instead of running standard deviation instead of running variance but let's follow the running variance but let's follow the running variance but let's follow the paper exactly here they calculate the paper exactly here they calculate the paper exactly here they calculate the variance which is the standard deviation variance which is the standard deviation variance which is the standard deviation squared and that's what's get track of squared and that's what's get track of squared and that's what's get track of in a running variance instead of a in a running variance instead of a in a running variance instead of a running standard running standard running standard deviation uh but those two would be very deviation uh but those two would be very deviation uh but those two would be very very similar",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 909,
      "text": "I very similar",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 910,
      "text": "I very similar I believe um if we are not training then believe um if we are not training then believe um if we are not training then we use running mean and variance we we use running mean and variance we we use running mean and variance we normalize and then here I am calculating normalize",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 911,
      "text": "and then here I am calculating normalize",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 912,
      "text": "and then here I am calculating the output of this layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 913,
      "text": "and I'm also the output of this layer and I'm also the output of this layer and I'm also assigning it to an attribute called out assigning it to an attribute called out assigning it to an attribute called out now out is something that I'm using in now out is something that I'm using in now out is something that I'm using in our modules here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 914,
      "text": "uh this is not what you our modules here uh this is not what you our modules here uh this is not what you would find in pytorch we are slightly would find in pytorch we are slightly would find in pytorch we are slightly deviating from it I'm creating a DOT out deviating from it I'm creating a DOT out deviating from it I'm creating a DOT out because I would like to very easily um because I would like to very easily um because I would like to very easily um maintain all those variables so that we maintain all those variables so that we maintain all those variables so that we can create statistics of them and plot can create statistics of them and plot can create statistics of them and plot them but pytorch and modules will not them but pytorch and modules will not them but pytorch and modules will not have a do out attribute and finally here have a do out attribute and finally here have a do out attribute and finally here we are updating the buffers using again we are updating the buffers using again we are updating the buffers using again as I mentioned exponential moving as I mentioned exponential moving as I mentioned exponential moving average uh provide given the provided average uh provide given the provided average uh provide given the provided momentum and importantly you'll notice momentum and importantly you'll notice momentum and importantly you'll notice that I'm using the torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 915,
      "text": "nogra context that I'm using the torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 916,
      "text": "nogra context that I'm using the torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 917,
      "text": "nogra context manager and I doing this because if we manager and I doing this because if we manager and I doing this because if we don't use this then pytorch will start don't use this then pytorch will start don't use this then pytorch will start building out an entire computational building out an entire computational building out an entire computational graph out of these tensors because it is graph out of these tensors because it is graph out of these tensors because it is expecting that we will eventually call expecting that we will eventually call expecting that we will eventually call Dot backward but we are never going to Dot backward",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 918,
      "text": "but we are never going to Dot backward",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 919,
      "text": "but we are never going to be calling dot backward on anything that be calling dot backward on anything that be calling dot backward on anything that includes running mean and running includes running mean and running includes running mean and running variance",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 920,
      "text": "so that's why we need to use variance so that's why we need to use variance",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 921,
      "text": "so that's why we need to use this context manager so that we are not this context manager so that we are not this context manager so that we are not um sort of maintaining them using all um sort of maintaining them using all um sort of maintaining them using all this additional memory um so this will this additional memory um so this will this additional memory um so this will make it more efficient and it's just make it more efficient",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 922,
      "text": "and it's just make it more efficient and it's just telling pyour that there will no telling pyour that there will no telling pyour that there will no backward we just have a bunch of tensors",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 923,
      "text": "backward we just have a bunch of tensors backward we just have a bunch of tensors we want to update them that's it",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 924,
      "text": "and we want to update them that's it",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 925,
      "text": "and we want to update them that's it",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 926,
      "text": "and then we then we then we return okay now scrolling down we have return okay now scrolling down we have return okay now scrolling down we have the 10h layer this is very very similar the 10h layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 927,
      "text": "this is very very similar the 10h layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 928,
      "text": "this is very very similar to uh torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 929,
      "text": "10h",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 930,
      "text": "and it doesn't do too to uh torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 931,
      "text": "10h",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 932,
      "text": "and it doesn't do too to uh torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 933,
      "text": "10h",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 934,
      "text": "and it doesn't do too much it just calculates 10 as you might much it just calculates 10 as you might much it just calculates 10 as you might expect so uh that's torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 935,
      "text": "10h and uh expect",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 936,
      "text": "so uh that's torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 937,
      "text": "10h and uh expect",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 938,
      "text": "so uh that's torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 939,
      "text": "10h and uh there's no parameters in this layer but there's no parameters in this layer but there's no parameters in this layer but because these are layers um it now because these are layers um it now because these are layers um it now becomes very easy to sort of like stack becomes very easy to sort of like stack becomes very easy to sort of like stack them up into uh basically just a list um them up into uh basically just a list um them up into uh basically just a list um and uh we can do all the initializations and uh we can do all the initializations and uh we can do all the initializations that we're used to so we have the that we're used to so we have the that we're used to so we have the initial sort of embedding Matrix we have initial sort of embedding Matrix we have initial sort of embedding Matrix we have our layers and we can call them our layers and we can call them our layers and we can call them sequentially and then again with Tor",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 940,
      "text": "no sequentially and then again with Tor no sequentially and then again with Tor no grb",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 941,
      "text": "but there's some initializations grb",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 942,
      "text": "but there's some initializations grb",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 943,
      "text": "but there's some initializations here so we want to make the output here so we want to make the output here so we want to make the output softmax a bit less confident like we saw softmax a bit less confident like we saw softmax a bit less confident like we saw and in addition to that because we are and in addition to that because we are and in addition to that because we are using a six layer multi-layer percep on using a six layer multi-layer percep on using a six layer multi-layer percep on here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 944,
      "text": "so you see how I'm stacking linear here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 945,
      "text": "so you see how I'm stacking linear here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 946,
      "text": "so you see how I'm stacking linear 10age",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 947,
      "text": "linear Tage Etc uh I'm going to be 10age",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 948,
      "text": "linear Tage Etc uh I'm going to be 10age",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 949,
      "text": "linear Tage Etc uh I'm going to be using the gain here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 950,
      "text": "and I'm going to using the gain here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 951,
      "text": "and I'm going to using the gain here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 952,
      "text": "and I'm going to play with this in a second",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 953,
      "text": "so you'll see play with this in a second",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 954,
      "text": "so you'll see play with this in a second",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 955,
      "text": "so you'll see how uh when we change this what happens how uh when we change this what happens how uh when we change this what happens to the to the to the statistics finally the parameters are statistics finally the parameters are statistics finally the parameters are basically the embedding Matrix and all basically the embedding Matrix and all basically the embedding Matrix and all the parameters in all the layers and the parameters in all the layers and the parameters in all the layers and notice here I'm using a double list notice here I'm using a double list notice here I'm using a double list apprehension if you want to call it that apprehension if you want to call it that apprehension if you want to call it that",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 956,
      "text": "but for every layer in layers and for but for every layer in layers and for but for every layer in layers and for every parameter in each of those layers every parameter in each of those layers every parameter in each of those layers we are just stacking up all those piece we are just stacking up all those piece we are just stacking up all those piece uh all those parameters now in total",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 957,
      "text": "we uh all those parameters now in total we uh all those parameters now in total we have 46,000 um have 46,000 um have 46,000 um parameters and I'm telling P that all of parameters and I'm telling P that all of parameters and I'm telling P that all of them require gradient then here uh we have everything gradient then here uh we have everything here we are actually mostly used to uh here we are actually mostly used to uh here we are actually mostly used to uh we are sampling a batch we are doing a we are sampling a batch we are doing a we are sampling a batch we are doing a forward pass the forward pass now is forward pass the forward pass now is forward pass the forward pass now is just the linear application of all the just the linear application of all the just the linear application of all the layers in order followed by the cross layers in order followed by the cross layers in order followed by the cross entropy and then in the backward pass entropy and then in the backward pass entropy and then in the backward pass you'll notice that for every single you'll notice that for every single you'll notice that for every single layer I now iterate over all the outputs layer I now iterate over all the outputs layer I now iterate over all the outputs",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 958,
      "text": "and I'm telling pytorch to retain the",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 959,
      "text": "and I'm telling pytorch to retain the",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 960,
      "text": "and I'm telling pytorch to retain the gradient of them",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 961,
      "text": "and then here we are gradient of them",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 962,
      "text": "and then here we are gradient of them",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 963,
      "text": "and then here we are already used to uh all the all the already used to uh all the all the already used to uh all the all the gradient set To None do the backward to gradient set To None do the backward to gradient set To None do the backward to fill in the gradients uh do an update fill in the gradients uh do an update fill in the gradients uh do an update using stochastic gradient sent and then using stochastic gradient sent and then using stochastic gradient sent and then uh track some statistics",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 964,
      "text": "and then I am uh track some statistics",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 965,
      "text": "and then I am uh track some statistics",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 966,
      "text": "and then I am going to break after a single iteration going to break after a single iteration going to break after a single iteration now here in this cell in this diagram I now here in this cell in this diagram",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 967,
      "text": "I now here in this cell in this diagram I I'm visualizing the histogram the I'm visualizing the histogram the I'm visualizing the histogram the histograms of the for pass activations histograms of the for pass activations histograms of the for pass activations and I'm specifically doing it at the 10",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 968,
      "text": "and I'm specifically doing it at the 10",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 969,
      "text": "and I'm specifically doing it at the 10 each layers so iterating over all the each layers so iterating over all the each layers so iterating over all the layers except for the very last one layers except for the very last one layers except for the very last one which is basically just the U soft Max which is basically just the U soft Max which is basically just the U soft Max layer um if it is a 10h layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 970,
      "text": "and I'm layer um if it is a 10h layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 971,
      "text": "and I'm layer um if it is a 10h layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 972,
      "text": "and I'm using a 10h layer just because they have using a 10h layer just because they have using a 10h layer just because they have a finite output netive 1 to 1 and so a finite output netive 1 to 1 and so a finite output netive 1 to 1 and so it's very easy to visualize here so you",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 973,
      "text": "it's very easy to visualize here so you",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 974,
      "text": "it's very easy to visualize here so you see 1 to one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 975,
      "text": "and it's a finite range and see 1 to one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 976,
      "text": "and it's a finite range and see 1 to one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 977,
      "text": "and it's a finite range and easy to work with I take the out tensor easy to work with I take the out tensor easy to work with I take the out tensor from that layer into T",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 978,
      "text": "and then I'm from that layer into T",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 979,
      "text": "and then I'm from that layer into T",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 980,
      "text": "and then I'm calculating the mean the standard calculating the mean the standard calculating the mean the standard deviation and the percent saturation of deviation and the percent saturation of deviation and the percent saturation of T and the way I Define the percent T and the way I Define the percent T and the way I Define the percent saturation is that t. absolute value is saturation is that t. absolute value is saturation is that t. absolute value is greater than 97 so that means we are greater than 97 so that means we are greater than 97 so that means we are here at the tals of the 10 H and here at the tals of the 10 H and here at the tals of the 10 H and remember that when we are in the tales remember that when we are in the tales remember that when we are in the tales of the 10 H that will actually stop of the 10 H that will actually stop of the 10 H that will actually stop gradients so we don't want this to be gradients so we don't want this to be gradients so we don't want this to be too too too high now here I'm calling torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 981,
      "text": "high now here I'm calling torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 982,
      "text": "high now here I'm calling torch.",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 983,
      "text": "histogram and then I am plotting this histogram and then I am plotting this histogram and then I am plotting this histogram so basically what this is histogram so basically what this is histogram so basically what this is doing is that every different type of doing is that every different type of doing is that every different type of layer and they have a different color we layer and they have a different color we layer and they have a different color we are looking at how many um values in are looking at how many um values in are looking at how many um values in these tensors take on any of the values these tensors take on any of the values these tensors take on any of the values Below on this axis here so the first Below on this axis here so the first Below on this axis here so the first layer is fairly saturated uh here at 20% layer is fairly saturated uh here at 20% layer is fairly saturated uh here at 20% so you can see that it's got Tails here so you can see that it's got Tails here so you can see that it's got Tails here but then everything sort of stabilizes but then everything sort of stabilizes but then everything sort of stabilizes and if we had more layers here it would and if we had more layers here it would and if we had more layers here it would actually just stabilize at around the actually just stabilize at around the actually just stabilize at around the standard deviation of about 65 and the standard deviation of about 65 and the standard deviation of about 65 and the saturation would be roughly 5% and the saturation would be roughly 5% and the saturation would be roughly 5% and the reason that the stabilizes and gives us reason that the stabilizes and gives us reason that the stabilizes and gives us a nice distribution here is because gain a nice distribution here is because gain a nice distribution here is because gain is set to 5 is set to 5 is set to 5 over3 now here this gain you see that by over3 now here this gain you see that by over3 now here this gain you see that by default we initialize with 1 /un of fan default we initialize with 1 /un of fan default we initialize with 1 /un of fan in but then here during initialization I in",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 984,
      "text": "but then here during initialization I in",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 985,
      "text": "but then here during initialization I come in and I erator all the layers and come in and I erator all the layers and come in and I erator all the layers and if it's a linear layer I boost that by if it's a linear layer I boost that by if it's a linear layer I boost that by the gain now we saw that one so the gain now we saw that one so the gain now we saw that one so basically if we just do not use a gain basically if we just do not use a gain basically if we just do not use a gain then what happens if I redraw this you then what happens if I redraw this you then what happens if I redraw this you will see that the standard deviation is will see that the standard deviation is will see that the standard deviation is shrinking and the saturation is coming shrinking and the saturation is coming shrinking and the saturation is coming to zero and basically what's happening to zero and basically what's happening to zero and basically what's happening is the first layer is you know pretty is the first layer is you know pretty is the first layer is you know pretty decent but then further layers are just decent but then further layers are just decent but then further layers are just kind of like shrinking down to zero and kind of like shrinking down to zero and kind of like shrinking down to zero and it's happening slowly but it's shrinking it's happening slowly but it's shrinking it's happening slowly but it's shrinking to zero and the reason for that is when to zero and the reason for that is when to zero and the reason for that is when you just have a sandwich of linear you just have a sandwich of linear you just have a sandwich of linear layers alone then a then initializing layers alone then a then initializing layers alone then a then initializing our weights in this manner we saw our weights in this manner we saw our weights in this manner we saw previously would have conserved the previously would have conserved the previously would have conserved the standard deviation of one but because we standard deviation of one but because we standard deviation of one but because we have this interspersed 10 in layers in have this interspersed 10 in layers in have this interspersed 10 in layers in there these 10h layers are squashing there these 10h layers are squashing there these 10h layers are squashing functions and so they take your functions",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 986,
      "text": "and so they take your functions",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 987,
      "text": "and so they take your distribution and they slightly squash it distribution and they slightly squash it distribution and they slightly squash it",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 988,
      "text": "and so some gain is necessary to keep",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 989,
      "text": "and so some gain is necessary to keep",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 990,
      "text": "and so some gain is necessary to keep expanding it to fight the expanding it to fight the expanding it to fight the squashing so it just turns out that 5 squashing so it just turns out that 5 squashing so it just turns out that 5 over3 is a good value so if we have over3 is a good value",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 991,
      "text": "so if we have over3 is a good value so if we have something too small like one we saw that something too small like one we saw that something too small like one we saw that things will come toward zero but if it's things will come toward zero but if it's things will come toward zero but if it's something too high let's do something too high let's do something too high let's do two then here we see that um well let me do something a bit more well let me do something a bit more extreme because so it's a bit more extreme because so it's a bit more extreme because so it's a bit more visible let's try visible let's try visible let's try three",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 992,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 993,
      "text": "so we see here that the three",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 994,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 995,
      "text": "so we see here that the three",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 996,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 997,
      "text": "so we see here that the saturations are going to be way too saturations are going to be way too saturations are going to be way too large",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 998,
      "text": "okay so three would create way too large",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 999,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1000,
      "text": "so three would create way too large",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1001,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1002,
      "text": "so three would create way too saturated activations so 5 over3 is a saturated activations so 5 over3 is a saturated activations so 5 over3 is a good setting for a sandwich of linear good setting for a sandwich of linear good setting for a sandwich of linear layers with 10h activations and it layers with 10h activations and it layers with 10h activations and it roughly stabilizes the standard roughly stabilizes the standard roughly stabilizes the standard deviation at a reasonable point now deviation at a reasonable point now deviation at a reasonable point now",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1003,
      "text": "honestly I have no idea where 5 over3 honestly I have no idea where 5 over3 honestly I have no idea where 5 over3 came from in pytorch um when we were came from in pytorch um when we were came from in pytorch um when we were looking at the coming initialization um looking at the coming initialization um looking at the coming initialization um I see empirically that it stabilizes I see empirically that it stabilizes I see empirically that it stabilizes this sandwich of linear an 10age and this sandwich of linear an 10age and this sandwich of linear an 10age and that the saturation is in a good range that the saturation is in a good range that the saturation is in a good range um",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1004,
      "text": "but I don't actually know if this",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1005,
      "text": "um",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1006,
      "text": "but I don't actually know if this",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1007,
      "text": "um",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1008,
      "text": "but I don't actually know if this came out of some math formula I tried came out of some math formula I tried came out of some math formula I tried searching briefly for where this comes searching briefly for where this comes searching briefly for where this comes from uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1009,
      "text": "but I wasn't able to find from uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1010,
      "text": "but I wasn't able to find from uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1011,
      "text": "but I wasn't able to find anything uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1012,
      "text": "but certainly we see that anything uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1013,
      "text": "but certainly we see that anything uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1014,
      "text": "but certainly we see that empirically these are very nice ranges empirically these are very nice ranges empirically these are very nice ranges our saturation is roughly 5% which is a our saturation is roughly 5% which is a our saturation is roughly 5% which is a pretty good number and uh this is a good pretty good number",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1015,
      "text": "and uh this is a good pretty good number",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1016,
      "text": "and uh this is a good setting of The gain in this context setting of The gain in this context setting of The gain in this context similarly we can do the exact same thing similarly we can do the exact same thing similarly we can do the exact same thing with the gradients so here is a very with the gradients so here is a very with the gradients so here is a very same Loop if it's a 10h but instead of same Loop if it's a 10h but instead of same Loop if it's a 10h but instead of taking a layer do out",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1017,
      "text": "I'm taking the taking a layer do out",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1018,
      "text": "I'm taking the taking a layer do out",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1019,
      "text": "I'm taking the grad",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1020,
      "text": "and then I'm also showing the mean grad",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1021,
      "text": "and then I'm also showing the mean grad",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1022,
      "text": "and then I'm also showing the mean and the standard deviation",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1023,
      "text": "and I'm and the standard deviation",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1024,
      "text": "and I'm and the standard deviation",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1025,
      "text": "and I'm plotting the histogram of these values plotting the histogram of these values plotting the histogram of these values",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1026,
      "text": "and so you'll see that the gradient",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1027,
      "text": "and so you'll see that the gradient",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1028,
      "text": "and so you'll see that the gradient distribution is uh fairly reasonable and distribution is uh fairly reasonable and distribution is uh fairly reasonable and in particular what we're looking for is in particular what we're looking for is in particular what we're looking for is that all the different layers in this that all the different layers in this that all the different layers in this sandwich has roughly the same gradient sandwich has roughly the same gradient sandwich has roughly the same gradient things are not shrinking or exploding so things are not shrinking or exploding so things are not shrinking or exploding so uh we can for example come here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1029,
      "text": "and we uh we can for example come here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1030,
      "text": "and we uh we can for example come here and we can take a look at what happens if this can take a look at what happens if this can take a look at what happens if this gain was way too small",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1031,
      "text": "so this was gain was way too small",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1032,
      "text": "so this was gain was way too small",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1033,
      "text": "so this was 0.5 then you see the first of all the 0.5 then you see the first of all the 0.5 then you see the first of all the activations are shrinking to zero but activations are shrinking to zero but activations are shrinking to zero but also the gradients are doing something also the gradients are doing something also the gradients are doing something weird the gradients started out here and weird the gradients started out here and weird the gradients started out here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1034,
      "text": "and then now they're like expanding then now they're like expanding then now they're like expanding out and similarly if we for example have out and similarly if we for example have out and similarly if we for example have a too high of a gain so like a too high of a gain so like a too high of a gain",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1035,
      "text": "so like three then we see that also the three then we see that also the three then we see that also the gradients have there's some asymmetry gradients have there's some asymmetry gradients have there's some asymmetry going on where as you go into deeper and going on where as you go into deeper and going on where as you go into deeper and deeper layers the activation CS are deeper layers the activation CS are deeper layers the activation CS are changing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1036,
      "text": "and so that's not what we want changing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1037,
      "text": "and so that's not what we want changing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1038,
      "text": "and so that's not what we want and in this case we saw that without the and in this case we saw that without the and in this case we saw that without the use of batro as we are going through use of batro as we are going through use of batro as we are going through right now we had to very carefully set right now we had to very carefully set right now we had to very carefully set those gains to get nice activations in those gains to get nice activations in those gains to get nice activations in both the forward pass and the backward both the forward pass and the backward both the forward pass and the backward pass now before we move on to bat pass now before we move on to bat pass now before we move on to bat normalization I would also like to take normalization I would also like to take normalization I would also like to take a look at what happens when we have no a look at what happens when we have no a look at what happens when we have no 10h units here so erasing all the 10 10h units here so erasing all the 10 10h units here so erasing all the 10 nonlinearities but keeping the gain at 5 nonlinearities but keeping the gain at 5 nonlinearities but keeping the gain at 5 over3 we now have just a giant linear over3 we now have just a giant linear over3 we now have just a giant linear sandwich",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1039,
      "text": "so let's see what happens to sandwich",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1040,
      "text": "so let's see what happens to sandwich",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1041,
      "text": "so let's see what happens to the activations the activations the activations as we saw before the correct gain here as we saw before the correct gain here as we saw before the correct gain here is one that is the standard deviation is one that is the standard deviation is one that is the standard deviation preserving gain so 1.66 7 is too high preserving gain so 1.66 7 is too high preserving gain so 1.66 7 is too high and so what's going to happen now is the and so what's going to happen now is the and so what's going to happen now is the following uh I have to change this to be following uh I have to change this to be following uh I have to change this to be linear",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1042,
      "text": "so we are because there's no more linear",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1043,
      "text": "so we are because there's no more linear",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1044,
      "text": "so we are because there's no more 10h layers and let me change this to 10h layers and let me change this to 10h layers and let me change this to linear as linear as linear",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1045,
      "text": "as well so what we're seeing is um the well",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1046,
      "text": "so what we're seeing is um the well",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1047,
      "text": "so what we're seeing is um the activations started out on the blue and activations started out on the blue and activations started out on the blue and have by layer four become very diffuse have by layer four become very diffuse have by layer four become very diffuse",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1048,
      "text": "so what's happening to the activations so what's happening to the activations so what's happening to the activations is this and with the gradients on the is this and with the gradients on the is this and with the gradients on the top layer the activation the gradient top layer the activation the gradient top layer the activation the gradient statistics are the purple",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1049,
      "text": "and then they statistics are the purple",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1050,
      "text": "and then they statistics are the purple",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1051,
      "text": "and then they diminish as you go down deeper in the diminish as you go down deeper in the diminish as you go down deeper in the layers",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1052,
      "text": "and so basically you have an layers",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1053,
      "text": "and so basically you have an layers",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1054,
      "text": "and so basically you have an asymmetry like in the neuron net and you asymmetry like in the neuron net and you asymmetry like in the neuron net and you might imagine that if you have very deep might imagine that if you have very deep might imagine that if you have very deep neural networks say like 50 layers or neural networks say like 50 layers or neural networks say like 50 layers or something like that this just uh this is something like that",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1055,
      "text": "this just uh this is something like that this just uh this is not a good place to be uh so that's why not a good place to be uh so that's why not a good place to be uh so that's why before bash normalization this was before bash normalization this was before bash normalization this was incredibly tricky to to set in incredibly tricky to to set in incredibly tricky to to set in particular if this is too large of a particular if this is too large of a particular if this is too large of a gain this happens and if it's too little gain this happens and if it's too little gain this happens and if it's too little of a of a of a gain then this happens so the opposite gain then this happens so the opposite gain then this happens so the opposite of that basically happens here we have a of that basically happens here we have a of that basically happens here we have a um shrinking and a uh diffusion um shrinking and a uh diffusion um shrinking and a uh diffusion depending on which direction you look at depending on which direction you look at depending on which direction you look at it from and so certainly this is not it from",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1056,
      "text": "and so certainly this is not it from",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1057,
      "text": "and so certainly this is not what you want and in this case the what you want and in this case the what you want and in this case the correct setting of The gain is exactly correct setting of The gain is exactly correct setting of The gain is exactly one just like we're doing at one just like we're doing at one just like we're doing at initialization",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1058,
      "text": "and then we see that the initialization",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1059,
      "text": "and then we see that the initialization",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1060,
      "text": "and then we see that the uh statistics for the forward and a uh statistics for the forward and a uh statistics for the forward and a backward pass are well behaved and so backward pass are well behaved and so backward pass are well behaved",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1061,
      "text": "and so the reason I want to show you this is the reason I want to show you this is the reason I want to show you this is that basically like getting neural nness that basically like getting neural nness that basically like getting neural nness to train before these normalization to train before these normalization to train before these normalization layers and before the use of advanced layers and before the use of advanced layers and before the use of advanced optimizers like adom which we still have optimizers like adom which we still have optimizers like adom which we still have to cover and residual connections and so to cover and residual connections and so to cover and residual connections and so on uh training neurs basically looked on uh training neurs basically looked on uh training neurs basically looked like this it's like a total Balancing like this it's like a total Balancing like this it's like a total Balancing Act you have to make sure that Act you have to make sure that Act you have to make sure that everything is precisely orchestrated and everything is precisely orchestrated and everything is precisely orchestrated and you have to care about the activations you have to care about the activations you have to care about the activations and the gradients and their statistics and the gradients and their statistics and the gradients and their statistics",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1062,
      "text": "and then maybe you can train something",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1063,
      "text": "and then maybe you can train something",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1064,
      "text": "and then maybe you can train something uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1065,
      "text": "but it was it was basically",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1066,
      "text": "uh but it was it was basically",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1067,
      "text": "uh but it was it was basically impossible to train very deep networks impossible to train very deep networks impossible to train very deep networks and this is fundamentally the the reason and this is fundamentally the the reason and this is fundamentally the the reason for that you'd have to be very very for that you'd have to be very very for that you'd have to be very very careful with your careful with your careful with your initialization um the other point here initialization um the other point here initialization um the other point here is you might be asking yourself by the is you might be asking yourself by the is you might be asking yourself by the way I'm not sure if I covered this why way I'm not sure if I covered this why way I'm not sure if I covered this why do we need these 10h layers at all uh do we need these 10h layers at all uh do we need these 10h layers at all uh why do we include them and then have to why do we include them and then have to why do we include them and then have to worry about the gain and uh the reason worry about the gain and uh the reason worry about the gain and uh the reason for that of course is that if you just for that of course is that if you just for that of course is that if you just have a stack of linear layers then have a stack of linear layers then have a stack of linear layers then certainly we're getting very easily nice certainly we're getting very easily nice certainly we're getting very easily nice activations and so on uh but this is activations and so on uh but this is activations and so on uh but this is just massive linear sandwich and it just massive linear sandwich and it just massive linear sandwich and it turns out that it collapses to a single turns out that it collapses to a single turns out that it collapses to a single linear layer in terms of its uh linear layer in terms of its uh linear layer in terms of its uh representation power",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1068,
      "text": "so if you were to representation power so if you were to representation power so if you were to plot the output as a function of the plot the output as a function of the plot the output as a function of the input you're just getting a linear input you're just getting a linear input you're just getting a linear function no matter how many linear function no matter how many linear function no matter how many linear layers you stack up you still just end layers you stack up you still just end layers you stack up you still just end up with a linear transformation all the up with a linear transformation all the up with a linear transformation all the WX plus BS just collapse into a large WX WX plus BS just collapse into a large WX WX plus BS just collapse into a large WX plus b with slightly different W's and plus b with slightly different W's and plus b with slightly different W's and slightly different B um but slightly different B",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1069,
      "text": "um but slightly different B um",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1070,
      "text": "but interestingly even though the forward interestingly even though the forward interestingly even though the forward pass collapses to just a linear layer pass collapses to just a linear layer pass collapses to just a linear layer because of back propagation and uh the because of back propagation and uh the because of back propagation and uh the dynamics of the backward pass the dynamics of the backward pass the dynamics of the backward pass the optimization natur is not identical you optimization natur is not identical you optimization natur is not identical you actually end up with uh all kinds of actually end up with uh all kinds of actually end up with uh all kinds of interesting um Dynamics in the backward interesting um Dynamics in the backward interesting um Dynamics in the backward pass uh because of the uh the way the pass uh because of the uh the way the pass uh because of the uh the way the chain Ru is calculating it and so chain Ru is calculating it and so chain Ru is calculating it and so optimizing a linear layer by itself and optimizing a linear layer by itself and optimizing a linear layer by itself and optimizing a sandwich of 10 linear optimizing a sandwich of 10 linear optimizing a sandwich of 10 linear layers in both cases those are just a layers in both cases those are just a layers in both cases those are just a linear transformation in the forward linear transformation in the forward linear transformation in the forward pass but the training Dynamics would be pass but the training Dynamics would be pass but the training Dynamics would be different and there's entire papers that different",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1071,
      "text": "and there's entire papers that different",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1072,
      "text": "and there's entire papers that analyze in fact like infinitely layered analyze in fact like infinitely layered analyze in fact like infinitely layered uh linear layers",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1073,
      "text": "and and so on and so uh linear layers and and so on and so uh linear layers and and so on",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1074,
      "text": "and so there's a lot of things to that you can there's a lot of things to that you can there's a lot of things to that you can play with play with play with there uh but basically the tal there uh but basically the tal there uh but basically the tal linearities allow us to linearities allow us to linearities allow us to um turn this sandwich from just a um turn this sandwich from just a um turn this sandwich from just a linear uh function into uh a neural linear uh function into uh a neural linear uh function into uh a neural network that can in principle um network that can in principle um network that can in principle um approximate any arbitrary function okay approximate any arbitrary function okay approximate any arbitrary function",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1075,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1076,
      "text": "so now I've reset the code to use the so now I've reset the code to use the so",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1077,
      "text": "now I've reset the code to use the linear tanh sandwich like before and I linear tanh sandwich like before",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1078,
      "text": "and I linear tanh sandwich like before",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1079,
      "text": "and I reset everything so the gain is 5 over reset everything so the gain is 5 over reset everything so the gain is 5 over three uh we can run a single step of three uh we can run a single step of three uh we can run a single step of optimization and we can look at the optimization and we can look at the optimization and we can look at the activation statistics of the forward activation statistics of the forward activation statistics of the forward pass and the backward pass",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1080,
      "text": "but I've pass and the backward pass",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1081,
      "text": "but I've pass and the backward pass",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1082,
      "text": "but I've added one more plot here that I think is added one more plot here that I think is added one more plot here that I think is really important to look at when you're really important to look at when you're really important to look at when you're training your neural nuts and",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1083,
      "text": "to training your neural nuts and to training your neural nuts and to consider and ultimately what we're doing consider and ultimately what we're doing consider and ultimately what we're doing is we're updating the parameters of the is we're updating the parameters of the is we're updating the parameters of the neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1084,
      "text": "so we care about the neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1085,
      "text": "so we care about the neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1086,
      "text": "so we care about the parameters and their values and their parameters and their values and their parameters and their values and their gradients so here what I'm doing is I'm gradients",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1087,
      "text": "so here what I'm doing is I'm gradients so here what I'm doing is I'm actually iterating over all the actually iterating over all the actually iterating over all the parameters available",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1088,
      "text": "and then I'm only parameters available",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1089,
      "text": "and then I'm only parameters available",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1090,
      "text": "and then I'm only um restricting it to the two-dimensional um restricting it to the two-dimensional um restricting it to the two-dimensional parameters which are basically the parameters which are basically the parameters which are basically the weights of the linear layers",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1091,
      "text": "and I'm weights of the linear layers and I'm weights of the linear layers and I'm skipping the biases",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1092,
      "text": "and I'm skipping the skipping the biases and I'm skipping the skipping the biases",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1093,
      "text": "and I'm skipping the um gamas and the betas in the bom just um gamas and the betas in the bom just um gamas and the betas in the bom just for Simplicity",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1094,
      "text": "but you can also take a for Simplicity",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1095,
      "text": "but you can also take a for Simplicity",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1096,
      "text": "but you can also take a look at those as well but what's look at those as well but what's look at those as well but what's happening with the weights is um happening with the weights is um happening with the weights is um instructive by instructive by instructive by itself so here we have all the different itself so here we have all the different itself",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1097,
      "text": "so here we have all the different weights their shapes uh so this is the weights their shapes",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1098,
      "text": "uh so this is the weights their shapes",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1099,
      "text": "uh so this is the embedding layer the first linear layer embedding layer the first linear layer embedding layer the first linear layer all the way to the very last linear all the way to the very last linear all the way to the very last linear layer and then we have the mean the layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1100,
      "text": "and then we have the mean the layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1101,
      "text": "and then we have the mean the standard deviation of all these standard deviation of all these standard deviation of all these parameters the histogram",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1102,
      "text": "and you can see parameters the histogram",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1103,
      "text": "and you can see parameters the histogram",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1104,
      "text": "and you can see that actually doesn't look that amazing that actually doesn't look that amazing that actually doesn't look that amazing so there's some trouble in Paradise even so there's some trouble in Paradise even so there's some trouble in Paradise even though these gradients looked okay though these gradients looked okay though these gradients looked okay there's something weird going on here there's something weird going on here there's something weird going on here I'll get to that in a second",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1105,
      "text": "and the I'll get to that in a second and the I'll get to that in a second and the last thing here is the gradient to data last thing here is the gradient to data last thing here is the gradient to data ratio",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1106,
      "text": "so sometimes I like to visualize ratio",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1107,
      "text": "so sometimes I like to visualize ratio",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1108,
      "text": "so sometimes I like to visualize this as well because what this gives you this as well because what this gives you this as well because what this gives you a sense of is what is the scale of the a sense of is what is the scale of the a sense of is what is the scale of the gradient compared to the scale of the gradient compared to the scale of the gradient compared to the scale of the actual values and this is important actual values and this is important actual values and this is important because we're going to end up taking a because we're going to end up taking a because we're going to end up taking a step update um that is the learning rate step update um that is the learning rate step update um that is the learning rate times the gradient onto the data times the gradient onto the data times the gradient onto the data",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1109,
      "text": "and so if the gradient has too large of and so if the gradient has too large of and so if the gradient has too large of magnitude if the numbers in there are magnitude if the numbers in there are magnitude if the numbers in there are too large compared to the numbers in too large compared to the numbers in too large compared to the numbers in data then you'd be in trouble but in data then you'd be in trouble but in data then you'd be in trouble but in this case the gradient to data is our this case the gradient to data is our this case the gradient to data is our low numbers so the values inside grad low numbers",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1110,
      "text": "so the values inside grad low numbers so the values inside grad are 1,000 times smaller than the values are 1,000 times smaller than the values are 1,000 times smaller than the values inside data in these weights most of inside data in these weights most of inside data in these weights most of them now notably that is not true about them now notably that is not true about them now notably that is not true about the last layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1111,
      "text": "and so the last layer the last layer and so the last layer the last layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1112,
      "text": "and so the last layer actually here the output layer is a bit actually here the output layer is a bit actually here the output layer is a bit of a troublemaker in the way",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1113,
      "text": "that this of a troublemaker in the way that this of a troublemaker in the way that this is currently arranged because you can is currently arranged because you can is currently arranged because you can see that the um last layer here in pink see that the um last layer here in pink see that the um last layer here in pink takes on values that are much larger takes on values that are much larger takes on values that are much larger than some of the values inside um inside than some of the values inside um inside than some of the values inside um inside the neural nut so the standard the neural nut so the standard the neural nut so the standard deviations are roughly 1 and3 throughout deviations are roughly 1 and3 throughout deviations are roughly 1 and3 throughout except for the last last uh layer which except for the last last uh layer which except for the last last uh layer which actually has roughly one -2 standard actually has roughly one -2 standard actually has roughly one -2 standard deviation of gradients and so the deviation of gradients and so the deviation of gradients and so the gradients on the last layer are gradients on the last layer are gradients on the last layer are currently about 100 times greater sorry currently about 100 times greater sorry currently about 100 times greater sorry 10 times greater than all the other 10 times greater than all the other 10 times greater than all the other weights inside the neural net and so weights inside the neural net and so weights inside the neural net and so that's problematic because in the simple that's problematic because in the simple that's problematic because in the simple stochastic rting theend setup you would stochastic rting theend setup you would stochastic rting theend setup you would be training this last layer about 10 be training this last layer about 10 be training this last layer about 10 times faster than you would be training times faster than you would be training times faster than you would be training the other layers at the other layers at the other layers at initialization now this actually like initialization now this actually like initialization now this actually like kind of fixes itself a little bit if you kind of fixes itself a little bit if you kind of fixes itself a little bit if you train for a bit longer so for example if train for a bit longer so for example if train for a bit longer so for example if I greater than 1,000 only then do a I greater than 1,000 only then do a I greater than 1,000 only then do a break let me reinitialize and then let break let me reinitialize and then let break let me reinitialize and then let me do it 1,000 steps and after 1,000 me do it 1,000 steps and after 1,000 me do it 1,000 steps and after 1,000 steps we can look at the forward pass steps we can look at the forward pass steps we can look at the forward pass",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1114,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1115,
      "text": "so you see how the neurons are",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1116,
      "text": "a okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1117,
      "text": "so you see how the neurons are",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1118,
      "text": "a okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1119,
      "text": "so you see how the neurons are a bit are saturating a bit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1120,
      "text": "and we can also bit are saturating a bit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1121,
      "text": "and we can also bit are saturating a bit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1122,
      "text": "and we can also look at the backward pass but otherwise look at the backward pass but otherwise look at the backward pass but otherwise they look good they're about equal and they look good they're about equal",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1123,
      "text": "and they look good they're about equal and there's no shrinking to zero or there's no shrinking to zero or there's no shrinking to zero or exploding to Infinities and you can see exploding to Infinities and you can see exploding to Infinities and you can see that here in the weights uh things are that here in the weights uh things are that here in the weights uh things are also stabilizing a little bit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1124,
      "text": "so the also stabilizing a little bit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1125,
      "text": "so the also stabilizing a little bit so the Tails of the last pink layer are Tails of the last pink layer are Tails of the last pink layer are actually coming coming in during the actually coming coming in during the actually coming coming in during the optimization but certainly this is like optimization but certainly this is like optimization but certainly this is like a little bit of troubling especially if a little bit of troubling especially if a little bit of troubling especially if you are using a very simple update rule you are using a very simple update rule you are using a very simple update rule like stochastic gradient descent instead like stochastic gradient descent instead like stochastic gradient descent instead of a modern Optimizer like Adam now I'd of a modern Optimizer like Adam now I'd of a modern Optimizer like Adam now I'd like to show you one more plot that I like to show you one more plot that I like to show you one more plot that I usually look at when I train neural usually look at when I train neural usually look at when I train neural networks and basically the gradient to networks and basically the gradient to networks and basically the gradient to data ratio is not actually that data ratio is not actually that data ratio is not actually that informative because what matters at the informative because what matters at the informative because what matters at the end is not the gradient to data ratio end is not the gradient to data ratio end is not the gradient to data ratio but the update to the data ratio because but the update to the data ratio because but the update to the data ratio because that is the amount by which we will that is the amount by which we will that is the amount by which we will actually change the data in these actually change the data in these actually change the data in these tensors so coming up here what I'd like tensors so coming up here what I'd like tensors so coming up here what I'd like to do is I'd like to introduce a new to do is I'd like to introduce a new to do is I'd like to introduce a new update to data uh ratio it's going to be update to data uh ratio it's going to be update to data uh ratio it's going to be list",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1126,
      "text": "and we're going to build it out list and we're going to build it out list",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1127,
      "text": "and we're going to build it out every single iteration",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1128,
      "text": "and here I'd like every single iteration",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1129,
      "text": "and here I'd like every single iteration",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1130,
      "text": "and here I'd like to keep track of basically the to keep track of basically the to keep track of basically the ratio every single ratio every single ratio every single iteration so without any gradients I'm iteration so without any gradients I'm iteration so without any gradients I'm comparing the update which is learning comparing the update which is learning comparing the update which is learning rate times the times the rate times the times the rate times the times the gradient that is the update that we're gradient that is the update that we're gradient that is the update that we're going to apply to every going to apply to every going to apply to every parameter uh so see I'm iterating over parameter",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1131,
      "text": "uh so see I'm iterating over parameter uh so see I'm iterating over all the parameters",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1132,
      "text": "and then I'm taking all the parameters",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1133,
      "text": "and then I'm taking all the parameters",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1134,
      "text": "and then I'm taking the basically standard deviation of the the basically standard deviation of the the basically standard deviation of the update we're going to apply and divided update",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1135,
      "text": "we're going to apply and divided update we're going to apply and divided by the um actual content the data of of by the um actual content the data of of by the um actual content the data of of that parameter and its standard that parameter and its standard that parameter and its standard deviation so this is the ratio of deviation",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1136,
      "text": "so this is the ratio of deviation so this is the ratio of basically how great are the updates to basically how great are the updates to basically how great are the updates to the values in these tensors then we're the values in these tensors then we're the values in these tensors then we're going to take a log of it and actually going to take a log of it and actually going to take a log of it",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1137,
      "text": "and actually I'd like to take a log I'd like to take a log I'd like to take a log 10 um",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1138,
      "text": "just so it's a nicer 10 um just so it's a nicer 10 um",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1139,
      "text": "just so it's a nicer visualization um so we're going to be visualization um so we're going to be visualization um so we're going to be basically looking at the exponents of uh basically looking at the exponents of uh basically looking at the exponents of uh the of this division here and then that the of this division here and then that the of this division here and then that item to pop out the float",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1140,
      "text": "and we're item to pop out the float",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1141,
      "text": "and we're item to pop out the float and we're going to be keeping track of this for going to be keeping track of this for going to be keeping track of this for all the parameters and adding it to all the parameters and adding it to all the parameters and adding it to these UD answer so now let me these UD answer so now let me these UD answer so now let me reinitialize and run a th iterations we reinitialize and run a th iterations we reinitialize and run a th iterations we can look at the activations the can look at the activations the can look at the activations the gradients and the parameter gradients as gradients and the parameter gradients as gradients and the parameter gradients as we did before",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1142,
      "text": "but now I have one more we did before",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1143,
      "text": "but now I have one more we did before",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1144,
      "text": "but now I have one more plot here to plot here to plot here to introduce and what's Happening Here is introduce and what's Happening Here is introduce and what's Happening Here is we're are interval parameters",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1145,
      "text": "and I'm we're are interval parameters",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1146,
      "text": "and I'm we're are interval parameters and I'm constraining it again like I did here to constraining it again like I did here to constraining it again like I did here to just the just the just the weights",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1147,
      "text": "so the number of dimensions in weights so the number of dimensions in weights so the number of dimensions in these sensors is two",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1148,
      "text": "and then I'm these sensors is two",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1149,
      "text": "and then I'm these sensors is two",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1150,
      "text": "and then I'm basically plotting all of these um basically plotting all of these um basically plotting all of these um update ratios over time update ratios over time update ratios over time",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1151,
      "text": "so when I plot this I plot those ratios so when I plot this I plot those ratios so when I plot this I plot those ratios and you can see that they evolve over and you can see that they evolve over and you can see that they evolve over time during initialization they take on time during initialization they take on time during initialization they take on certain values and then these updates s certain values and then these updates s certain values and then these updates s of like start stabilizing usually during of like start stabilizing usually during of like start stabilizing usually during training then the other thing that I'm training then the other thing that I'm training then the other thing that I'm plotting here is I'm plotting here like plotting here is I'm plotting here like plotting here is I'm plotting here like an approximate value that is a Rough an approximate value that is a Rough an approximate value that is a Rough Guide for what it roughly should be and Guide for what it roughly should be and Guide for what it roughly should be and it should be like roughly it should be like roughly it should be like roughly one3 and so that means that basically one3 and so that means that basically one3 and so that means that basically there's some values in the tensor um and there's some values in the tensor",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1152,
      "text": "um and there's some values in the tensor um",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1153,
      "text": "and they take on certain values and the they take on certain values and the they take on certain values and the updates to them at every iteration are updates to them at every iteration are updates to them at every iteration are no more than roughly 1,000th of the no more than roughly 1,000th of the no more than roughly 1,000th of the actual like magnitude in those tensors actual like magnitude in those tensors actual like magnitude in those tensors uh if this was much larger like for uh if this was much larger like for uh if this was much larger like for example if this was um if the log of example if this was um if the log of example if this was um if the log of this was like say negative 1 this is this was like say negative 1",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1154,
      "text": "this is this was like say negative 1",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1155,
      "text": "this is actually updating those values quite a actually updating those values quite a actually updating those values quite a lot they're undergoing a lot of change lot they're undergoing a lot of change lot they're undergoing a lot of change but the reason that the final rate the but the reason that the final rate the but the reason that the final rate the final uh layer here is an outlier is final uh layer here is an outlier is final uh layer here is an outlier is because this layer was artificially because this layer was artificially because this layer was artificially shrunk down to keep the soft Max um shrunk down to keep the soft Max um shrunk down to keep the soft Max um incom unconfident incom unconfident incom unconfident so here you see how we multiplied The so here you see how we multiplied The so here you see how we multiplied The Weight by Weight by Weight by 0.1 uh in the initialization to make the 0.1 uh in the initialization to make the 0.1 uh in the initialization to make the last layer prediction less confident last layer prediction less confident last layer prediction less confident that made that artificially made the that made that artificially made the that made that artificially made the values inside that tensor way too low values inside that tensor way too low values inside that tensor way too low and that's why we're getting temporarily and that's why we're getting temporarily",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1156,
      "text": "and that's why we're getting temporarily a very high ratio",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1157,
      "text": "but you see that that a very high ratio",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1158,
      "text": "but you see that that a very high ratio",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1159,
      "text": "but you see that that stabilizes over time once uh that weight stabilizes over time once uh that weight stabilizes over time once uh that weight starts to learn starts to learn but starts to learn starts to learn but starts to learn starts to learn",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1160,
      "text": "but basically I like to look at the basically I like to look at the basically I like to look at the evolution of this update ratio for all evolution of this update ratio for all evolution of this update ratio for all my parameters usually",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1161,
      "text": "and I like to make my parameters usually",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1162,
      "text": "and I like to make my parameters usually",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1163,
      "text": "and I like to make sure that it's not too much above onean sure that it's not too much above onean sure that it's not too much above onean neg3 roughly uh so around3 on this log neg3 roughly uh so around3 on this log neg3 roughly uh so around3 on this log plot if it's below -3 usually that means plot if it's below -3 usually that means plot if it's below -3 usually that means that the parameters are not trained fast that the parameters are not trained fast that the parameters are not trained fast enough so if our learning rate was very enough so if our learning rate was very enough so if our learning rate was very low let's do that low let's do that low let's do that experiment uh let's initialize and then experiment uh let's initialize and then experiment uh let's initialize and then let's actually do a learning rate of say let's actually do a learning rate of say let's actually do a learning rate of say one3 here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1164,
      "text": "so one3 here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1165,
      "text": "so one3 here so 0.001 if your learning rate is way too 0.001 if your learning rate is way too 0.001 if your learning rate is way too low this plot will typically reveal it so this plot will typically reveal it",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1166,
      "text": "so you see how all of these updates are way you see how all of these updates are way you see how all of these updates are way too small so the size of the update is too small so the size of the update is too small so the size of the update is uh basically uh 10,000 times um in uh basically uh 10,000 times um in uh basically uh 10,000 times um in magnitude to the size of the numbers in magnitude to the size of the numbers in magnitude to the size of the numbers in that tensor in the first place so this that tensor in the first place so this that tensor in the first place so this is a symptom of training way too is a symptom of training way too is a symptom of training way too slow",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1167,
      "text": "so this is another way to sometimes slow so this is another way to sometimes slow so this is another way to sometimes set the learning rate and to get a sense set the learning rate and to get a sense set the learning rate and to get a sense of what that learning rate should be and of what that learning rate should be and of what that learning rate should be and ultimately this is something that you ultimately this is something that you ultimately this is something that you would uh keep track of if anything the learning rate here is a if anything the learning rate here is a little bit on the higher side uh because little bit on the higher side uh because little bit on the higher side uh because you see that um we're above the black you see that um we're above the black you see that um we're above the black line of3",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1168,
      "text": "we're somewhere around -2.5 line",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1169,
      "text": "of3",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1170,
      "text": "we're somewhere around -2.5 line",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1171,
      "text": "of3",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1172,
      "text": "we're somewhere around -2.5",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1173,
      "text": "it's like",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1174,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1175,
      "text": "and uh but everything is it's like",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1176,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1177,
      "text": "and uh but everything is it's like",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1178,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1179,
      "text": "and uh but everything is like somewhat stabilizing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1180,
      "text": "and so this like somewhat stabilizing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1181,
      "text": "and so this like somewhat stabilizing",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1182,
      "text": "and so this looks like a pretty decent setting of of looks like a pretty decent setting of of looks like a pretty decent setting of of um learning rates and so on but this is um learning rates and so on but this is um learning rates and so on but this is something to look at and when things are something to look at and when things are something to look at and when things are miscalibrated you will you will see very miscalibrated",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1183,
      "text": "you will you will see very miscalibrated",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1184,
      "text": "you will you will see very quickly so for quickly so for quickly so for example everything looks pretty well example everything looks pretty well example everything looks pretty well behaved right",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1185,
      "text": "but just as a comparison behaved right",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1186,
      "text": "but just as a comparison behaved right",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1187,
      "text": "but just as a comparison when things are not properly calibrated when things are not properly calibrated when things are not properly calibrated what does that look like let me come up what does that look like let me come up what does that look like let me come up here and let's say that for example uh here and let's say that for example uh here and let's say that for example uh what do we do let's say that we forgot what do we do let's say that we forgot what do we do let's say that we forgot to apply this a fan in normalization so to apply this a fan in normalization so to apply this a fan in normalization so the weights inside the linear layers are the weights inside the linear layers are the weights inside the linear layers are just sampled from aaan and all the just sampled from aaan and all the just sampled from aaan and all the stages what happens to our how do we stages what happens to our how do we stages what happens to our how do we notice that something's off well the notice that something's off well the notice that something's off well the activation plot will tell you whoa your activation plot will tell you whoa your activation plot will tell you whoa your neurons are way too saturated uh the neurons are way too saturated uh the neurons are way too saturated uh the gradients are going to be all messed up gradients are going to be all messed up gradients are going to be all messed up uh the histogram for these weights are uh the histogram for these weights are uh the histogram for these weights are going to be all messed up as well and going to be all messed up as well and going to be all messed up as well and there's a lot of asymmetry",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1188,
      "text": "and then if there's a lot of asymmetry",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1189,
      "text": "and then if there's a lot of asymmetry and then if we look here I suspect it's all going to we look here I suspect it's all going to we look here I suspect it's all going to be also pretty messed up",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1190,
      "text": "so uh you see be also pretty messed up",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1191,
      "text": "so uh you see be also pretty messed up",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1192,
      "text": "so uh you see there's a lot of uh discrepancy in how there's a lot of uh discrepancy in how there's a lot of uh discrepancy in how fast these layers are learning and some fast these layers are learning and some fast these layers are learning and some of them are learning way too fast so uh1 of them are learning way too fast so uh1 of them are learning way too fast",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1193,
      "text": "so uh1 1.5 those are very large numbers in 1.5 those are very large numbers in 1.5 those are very large numbers in terms of this ratio again you should be terms of this ratio again you should be terms of this ratio again you should be somewhere around3 and not much more somewhere around3 and not much more somewhere around3 and not much more about that um so this is how about that",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1194,
      "text": "um so this is how about that um so this is how miscalibrations of your neuron nuts are miscalibrations of your neuron nuts are miscalibrations of your neuron nuts are going to manifest and these kinds of going to manifest and these kinds of going to manifest and these kinds of plots here are a good way of um sort of plots here are a good way of um sort of plots here are a good way of um sort of bringing um those miscalibrations sort bringing um those miscalibrations sort bringing um those miscalibrations sort of uh to your attention and so you can of uh to your attention",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1195,
      "text": "and so you can of uh to your attention",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1196,
      "text": "and so you can address them",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1197,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1198,
      "text": "so so far we've seen address them",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1199,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1200,
      "text": "so so far we've seen address them",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1201,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1202,
      "text": "so so far we've seen that when we have this linear tanh that when we have this linear tanh that when we have this linear tanh sandwich we can actually precisely sandwich we can actually precisely sandwich we can actually precisely calibrate the gains and make the calibrate the gains and make the calibrate the gains and make the activations the gradients and the activations the gradients and the activations the gradients and the parameters and the updates all look parameters and the updates all look parameters and the updates all look pretty decent",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1203,
      "text": "but it definitely feels a pretty decent",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1204,
      "text": "but it definitely feels a pretty decent",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1205,
      "text": "but it definitely feels a little bit like balancing of a pencil on little bit like balancing of a pencil on little bit like balancing of a pencil on your finger and that's because this gain your finger and that's because this gain your finger and that's because this gain has to be very precisely calibrated so has to be very precisely calibrated so has to be very precisely calibrated so now let's introduce bat normalization now let's introduce bat normalization now let's introduce bat normalization layers into the fix into the mix and layers into the fix into the mix and layers into the fix into the mix and let's let's see how that helps fix the let's let's see how that helps fix the let's let's see how that helps fix the problem",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1206,
      "text": "so problem",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1207,
      "text": "so problem so here I'm going to take the bachom 1D here I'm going to take the bachom 1D here I'm going to take the bachom 1D class",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1208,
      "text": "and I'm going to start placing it class",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1209,
      "text": "and I'm going to start placing it class",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1210,
      "text": "and I'm going to start placing it inside and as I mentioned before the inside and as I mentioned before the inside and as I mentioned before the standard typical place you would place standard typical place you would place standard typical place you would place it is between the linear layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1211,
      "text": "so right it is between the linear layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1212,
      "text": "so right it is between the linear layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1213,
      "text": "so right after it but before the nonlinearity but after it but before the nonlinearity but after it but before the nonlinearity but people have definitely played with that people have definitely played with that people have definitely played with that and uh in fact you can get very similar and uh in fact you can get very similar and uh in fact you can get very similar results even if you place it after the results even if you place it after the results even if you place it after the nonlinearity um and the other thing that nonlinearity um and the other thing that nonlinearity um and the other thing that I wanted to mention is it's totally fine I wanted to mention is it's totally fine",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1214,
      "text": "I wanted to mention is it's totally fine to also place it at the end uh after the to also place it at the end uh after the to also place it at the end uh after the last linear layer and before the L last linear layer and before the L last linear layer and before the L function so this is potentially fine as function so this is potentially fine as function so this is potentially fine as well um and in this case this would be well um and in this case this would be well um and in this case this would be output would be WAP output would be WAP output would be WAP size um now because the last layer is size um now because the last layer is size um now because the last layer is Bash we would not be changing the weight Bash we would not be changing the weight Bash we would not be changing the weight to make the softmax less confident we'd to make the softmax less confident we'd to make the softmax less confident we'd be changing the gamma because gamma be changing the gamma because gamma be changing the gamma because gamma remember in the bathroom is the variable remember in the bathroom is the variable remember in the bathroom is the variable that multiplicatively interacts with the that multiplicatively interacts with the that multiplicatively interacts with the output of that normalization so we can initialize this normalization so we can initialize this sandwich now we can train and we can see sandwich now we can train and we can see sandwich now we can train and we can see that the activations uh are going to of that the activations uh are going to of that the activations uh are going to of course look uh very good and they are course look uh very good and they are course look uh very good and they are going to necessarily look good because going to necessarily look good because going to necessarily look good because now before every single 10h layer there now before every single 10h layer there now before every single 10h layer there is a normalization in the bashor so this is a normalization in the bashor so this is a normalization in the bashor so this is unsurprisingly all uh looks pretty is unsurprisingly all uh looks pretty is unsurprisingly all uh looks pretty good it's going to be standard deviation good it's going to be standard deviation good it's going to be standard deviation of roughly 65 2% and roughly equal of roughly 65 2% and roughly equal of roughly 65 2% and roughly equal standard deviation throughout the entire standard deviation throughout the entire standard deviation throughout the entire layers so everything looks very layers",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1215,
      "text": "so everything looks very layers",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1216,
      "text": "so everything looks very homogeneous the gradients look good the homogeneous the gradients look good the homogeneous the gradients look good the weights look good and their weights look good and their weights look good and their distributions",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1217,
      "text": "and then the distributions and then the distributions",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1218,
      "text": "and then the updates also look um pretty reasonable updates also look um pretty reasonable updates also look um pretty reasonable uh we are going above3 a little bit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1219,
      "text": "but uh we are going above3 a little bit",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1220,
      "text": "but uh we are going above3 a little bit but not by too much so all the parameters not by too much so all the parameters not by too much so all the parameters are training at roughly the same rate um are training at roughly the same rate um are training at roughly the same rate um here here here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1221,
      "text": "but now what we've gained is um we are",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1222,
      "text": "but now what we've gained is um we are",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1223,
      "text": "but now what we've gained is um we are going to be slightly less going to be slightly less going to be slightly less um brittle with respect to the gain of um brittle with respect to the gain of um brittle with respect to the gain of these so for example I can make the gain these so for example I can make the gain these so for example I can make the gain be say2 here um which is much much much be say2 here um which is much much much be say2 here um which is much much much slower than what we had with the tan slower than what we had with the tan slower than what we had with the tan H",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1224,
      "text": "but as we'll see the activations will H",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1225,
      "text": "but as we'll see the activations will H",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1226,
      "text": "but as we'll see the activations will actually be exactly unaffected uh and actually be exactly unaffected uh and actually be exactly unaffected uh and that's because of again this explicit that's because of again this explicit that's because of again this explicit normalization the gradients are going to normalization the gradients are going to normalization the gradients are going to look okay the weight gradients are going look okay the weight gradients are going look okay the weight gradients are going to look",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1227,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1228,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1229,
      "text": "but actually the to look okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1230,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1231,
      "text": "but actually the to look okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1232,
      "text": "okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1233,
      "text": "but actually the updates will updates will updates will change and so even though the forward change",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1234,
      "text": "and so even though the forward change",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1235,
      "text": "and so even though the forward and backward pass to a very large extent and backward pass to a very large extent and backward pass to a very large extent look okay because of the backward pass look okay because of the backward pass look okay because of the backward pass of the Bator and how the scale of the of the Bator and how the scale of the of the Bator and how the scale of the incoming activations interacts in the incoming activations interacts in the incoming activations interacts in the Bator and its uh backward pass this is Bator and its uh backward pass this is Bator and its uh backward pass this is actually changing the um the scale of actually changing the um the scale of actually changing the um the scale of the updates on these parameters so the the updates on these parameters so the the updates on these parameters so the grades on gradients of these weights are grades on gradients of these weights are grades on gradients of these weights are affected so we still don't get it affected",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1236,
      "text": "so we still don't get it affected so we still don't get it completely free pass to pass in arbitral completely free pass to pass in arbitral completely free pass to pass in arbitral um weights here but it everything else um weights here but it everything else um weights here but it everything else is significantly more robust in terms of is significantly more robust in terms of is significantly more robust in terms of the forward backward and the weight the forward backward and the weight the forward backward and the weight gradients it's just that you may have to gradients it's just that you may have to gradients it's just that you may have to retune your learning rate if you are retune your learning rate if you are retune your learning rate if you are changing sufficiently the the scale of changing sufficiently the the scale of changing sufficiently the the scale of the activations that are coming into the the activations that are coming into the the activations that are coming into the batch Norms so here for example this um batch Norms so here for example this um batch Norms so here for example this um we changed the gains of these linear we changed the gains of these linear we changed the gains of these linear layers to be greater and we're seeing layers to be greater and we're seeing layers to be greater and we're seeing that the updates are coming out lower as that the updates are coming out lower as that the updates are coming out lower as a a a result and then finally we can also so result and then finally we can also so result and then finally we can also so if we are using borms we don't actually if we are using borms we don't actually if we are using borms we don't actually need to necessarily let me reset this to need to necessarily let me reset this to need to necessarily let me reset this to one so there's no gain we don't one so there's no gain we don't one so there's no gain we don't necessarily even have to um normalize by necessarily even have to um normalize by necessarily even have to um normalize by fan in sometimes",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1237,
      "text": "so if I take out the fan in sometimes so if I take out the fan in sometimes so if I take out the fan in so these are just now uh random fan in so these are just now uh random fan in so these are just now uh random gsh in we'll see that because of borm gsh in we'll see that because of borm gsh in we'll see that because of borm this will actually be relatively well this will actually be relatively well this will actually be relatively well behaved behaved behaved so the statistic look of course in the so the statistic look of course in the so the statistic look of course in the forward pass look good the gradients forward pass look good the gradients forward pass look good the gradients look good the uh backward uh the weight look good the uh backward uh the weight look good the uh backward uh the weight updates look okay A little bit of fat updates look okay",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1238,
      "text": "A little bit of fat updates look okay A little bit of fat tails on some of the tails on some of the tails on some of the layers and uh this looks okay as well layers",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1239,
      "text": "and uh this looks okay as well layers",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1240,
      "text": "and uh this looks okay as well",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1241,
      "text": "but as you as you can see uh we're",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1242,
      "text": "but as you as you can see uh we're",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1243,
      "text": "but as you as you can see uh we're significantly below ne3 so we'd have to significantly below ne3",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1244,
      "text": "so we'd have to significantly below ne3",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1245,
      "text": "so we'd have to bump up the learning rate of this bachor bump up the learning rate of this bachor bump up the learning rate of this bachor uh so that we are training more properly uh so that we are training more properly uh so that we are training more properly and in particular looking at this and in particular looking at this and in particular looking at this roughly looks like we have to 10x the roughly looks like we have to 10x the roughly looks like we have to 10x the learning rate to get to about learning rate to get to about learning rate to get to about one3",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1246,
      "text": "so we' come here and we would one3",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1247,
      "text": "so we' come here and we would one3",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1248,
      "text": "so we' come here and we would change this to be update of 1.0 and if I change this to be update of 1.0 and if I change this to be update of 1.0 and if I reinitialize then we'll see that everything still of then we'll see that everything still of course looks good and now we are roughly course looks good and now we are roughly course looks good and now we are roughly here and we expect this to be an okay here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1249,
      "text": "and we expect this to be an okay here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1250,
      "text": "and we expect this to be an okay training run so long story short we are training run so long story short we are training run so long story short we are significantly more robust to the gain of significantly more robust to the gain of significantly more robust to the gain of these linear layers whether or not we these linear layers whether or not we these linear layers whether or not we have to apply the fan in",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1251,
      "text": "and then we can have to apply the fan in",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1252,
      "text": "and then we can have to apply the fan in",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1253,
      "text": "and then we can change the gain uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1254,
      "text": "but we actually do change the gain uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1255,
      "text": "but we actually do change the gain uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1256,
      "text": "but we actually do have to worry a little bit about the have to worry a little bit about the have to worry a little bit about the update um scales and making sure that uh update um scales and making sure that uh update um scales and making sure that uh the learning rate is properly calibrated the learning rate is properly calibrated the learning rate is properly calibrated here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1257,
      "text": "but this the activations of the here but this the activations of the here but this the activations of the forward backward pass and the updates forward backward pass and the updates forward backward pass and the updates are are looking significantly more well are are looking significantly more well are are looking significantly more well behaved except for the global scale that behaved except for the global scale that behaved except for the global scale that is potentially being adjusted here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1258,
      "text": "okay is potentially being adjusted here okay is potentially being adjusted here",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1259,
      "text": "okay so now let me summarize there are three so now let me summarize there are three so now let me summarize there are three things I was hoping to achieve with this things I was hoping to achieve with this things I was hoping to achieve with this section number one I wanted to introduce section number one I wanted to introduce section number one I wanted to introduce you to bat normalization which is one of you to bat normalization which is one of you to bat normalization which is one of the first modern innovations that we're the first modern innovations that we're the first modern innovations that we're looking into that helped stabilize very looking into that helped stabilize very looking into that helped stabilize very deep neural networks and their training deep neural networks and their training deep neural networks and their training",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1260,
      "text": "and I hope you understand how the B",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1261,
      "text": "and I hope you understand how the B",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1262,
      "text": "and I hope you understand how the B normalization works and um how it would normalization works and um how it would normalization works and um how it would be used in a neural network number two I be used in a neural network number two I be used in a neural network number two I was hoping to py torify some of our code was hoping to py torify some of our code was hoping to py torify some of our code and wrap it up into these uh modules so and wrap it up into these uh modules so and wrap it up into these uh modules so like linear bash 1D 10h",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1263,
      "text": "Etc these are like linear bash 1D 10h",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1264,
      "text": "Etc these are like linear bash 1D 10h",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1265,
      "text": "Etc these are layers or modules and they can be layers or modules and they can be layers or modules and they can be stacked up into neural nuts like Lego stacked up into neural nuts like Lego stacked up into neural nuts like Lego building blocks and these layers building blocks and these layers building blocks and these layers actually exist in pytorch and if you actually exist in pytorch and if you actually exist in pytorch and if you import torch NN then you can actually import torch NN then you can actually import torch NN then you can actually the way I've constructed it you can the way I've constructed it you can the way I've constructed it you can simply just use pytorch by prepending n simply just use pytorch by prepending n simply just use pytorch by prepending n and Dot to all these different and Dot to all these different and Dot to all these different layers and actually everything will just layers and actually everything will just layers and actually everything will just work because the API that I've developed work because the API that I've developed work because the API that I've developed here is identical to the API that here is identical to the API that here is identical to the API that pytorch uses and the implementation also pytorch uses and the implementation also pytorch uses and the implementation also is basically as far as I'm Weare is basically as far as I'm Weare is basically as far as I'm Weare identical to the one in pytorch and identical to the one in pytorch and identical to the one in pytorch and number three I tried to introduce you to number three I tried to introduce you to number three I tried to introduce you to the diagnostic tools that you would use the diagnostic tools that you would use the diagnostic tools that you would use to understand whether your neural to understand whether your neural to understand whether your neural network is in a good State dynamically network is in a good State dynamically network is in a good State dynamically so we are looking at the statistics",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1266,
      "text": "and so we are looking at the statistics",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1267,
      "text": "and so we are looking at the statistics and histograms and activation of the forward histograms and activation of the forward histograms and activation of the forward pass activ activations the backward pass pass activ activations the backward pass pass activ activations the backward pass gradients and then also we're looking at gradients and then also we're looking at gradients and then also we're looking at the weights that are going to be updated the weights that are going to be updated the weights that are going to be updated as part of stochastic gradi in ascent as part of stochastic gradi in ascent as part of stochastic gradi in ascent and we're looking at their means and we're looking at their means and we're looking at their means standard deviations and also the ratio standard deviations and also the ratio standard deviations and also the ratio of gradients to data or even better the of gradients to data or even better the of gradients to data or even better the updates to data and we saw that updates to data and we saw that updates to data and we saw that typically we don't actually look at it typically we don't actually look at it typically we don't actually look at it as a single snapshot Frozen in time at as a single snapshot Frozen in time at as a single snapshot Frozen in time at some particular iteration typically some particular iteration typically some particular iteration typically people look at this as a over time just people look at this as a over time just people look at this as a over time just like I've done here and they look at like I've done here and they look at like I've done here and they look at these update to data ratios and they these update to data ratios and they these update to data ratios and they make sure everything looks okay and in make sure everything looks okay and in make sure everything looks okay and in particular I said said that um particular",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1268,
      "text": "I said said that um particular I said said that um W3 or basically ne3 on the lock scale is W3 or basically ne3 on the lock scale is W3 or basically ne3 on the lock scale is a good uh rough euristic for what you a good uh rough euristic for what you a good uh rough euristic for what you want this ratio to be and if it's way want this ratio to be and if it's way want this ratio to be and if it's way too high then probably the learning rate too high then probably the learning rate too high then probably the learning rate or the updates are a little too too big or the updates are a little too too big or the updates are a little too too big and if it's way too small that the and if it's way too small that the and if it's way too small that the learning rate is probably too small so learning rate is probably too small so learning rate is probably too small so that's just some of the things",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1269,
      "text": "that you",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1270,
      "text": "that's just some of the things that you",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1271,
      "text": "that's just some of the things that you may want to play with when you try to may want to play with when you try to may want to play with when you try to get your neural network to uh work with get your neural network to uh work with get your neural network to uh work with very very very well",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1272,
      "text": "now there's a number of things I well now there's a number of things I well now there's a number of things I did not try to achieve I did not try to did not try to achieve I did not try to did not try to achieve I did not try to beat our previous performance as an beat our previous performance as an beat our previous performance as an example by introducing using the bash example by introducing using the bash example by introducing using the bash layer actually I did try um and I found layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1273,
      "text": "actually I did try um and I found layer",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1274,
      "text": "actually I did try um and I found the new I used the learning rate finding the new I used the learning rate finding the new I used the learning rate finding mechanism that I've described before I mechanism that I've described before I mechanism that I've described before I tried to train a borm layer a borm tried to train a borm layer a borm tried to train a borm layer a borm neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1275,
      "text": "and uh I actually ended up neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1276,
      "text": "and uh I actually ended up neural nut",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1277,
      "text": "and uh I actually ended up with results that are very very similar with results that are very very similar with results that are very very similar to what we've obtained before and that's to what we've obtained before",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1278,
      "text": "and that's to what we've obtained before",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1279,
      "text": "and that's because our performance now is not because our performance now is not because our performance now is not bottlenecked by the optimization which bottlenecked by the optimization which bottlenecked by the optimization which is what borm is helping with the is what borm is helping with the is what borm is helping with the performance at this stage is bottleneck performance at this stage is bottleneck performance at this stage is bottleneck by what I suspect is the context length by what I suspect is the context length by what I suspect is the context length of our context so currently we are of our context",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1280,
      "text": "so currently we are of our context so currently we are taking three characters to predict the taking three characters to predict the taking three characters to predict the fourth one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1281,
      "text": "and I think we need to go fourth one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1282,
      "text": "and I think we need to go fourth one",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1283,
      "text": "and I think we need to go beyond that and we need to look at more beyond that",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1284,
      "text": "and we need to look at more beyond that and we need to look at more powerful architectures like recurrent powerful architectures like recurrent powerful architectures like recurrent neural networks and Transformers in neural networks and Transformers in neural networks and Transformers in order to further push um the lock order to further push um the lock order to further push um the lock probabilities that we're achieving on probabilities that we're achieving on probabilities that we're achieving on this data this data this data set",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1285,
      "text": "and I also did not try to have a set",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1286,
      "text": "and I also did not try to have a set",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1287,
      "text": "and I also did not try to have a full explanation of all of these full explanation of all of these full explanation of all of these activations the gradients and the activations the gradients and the backward pass and the statistics of all backward pass and the statistics of all backward pass and the statistics of all these gradients",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1288,
      "text": "and so you may have these gradients and so you may have these gradients",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1289,
      "text": "and so you may have found some of the parts here un found some of the parts here un found some of the parts here un intuitive",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1290,
      "text": "and maybe you're slightly intuitive",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1291,
      "text": "and maybe you're slightly intuitive",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1292,
      "text": "and maybe you're slightly confused about",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1293,
      "text": "okay if I change the uh confused about okay if I change the uh confused about okay if I change the uh gain here how come that we need a gain here how come that we need a gain here how come that we need a different learning rate",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1294,
      "text": "and I didn't go different learning rate",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1295,
      "text": "and I didn't go different learning rate",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1296,
      "text": "and I didn't go into the full detail because you'd have into the full detail because you'd have into the full detail because you'd have to actually look at the backward pass of to actually look at the backward pass of to actually look at the backward pass of all these different layers and get an all these different layers and get an all these different layers and get an intuitive understanding of how that intuitive understanding of how that intuitive understanding of how that works",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1297,
      "text": "and I did not go into that in this works and I did not go into that in this works",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1298,
      "text": "and I did not go into that in this lecture the purpose really was just to lecture the purpose really was just to lecture the purpose really was just to introduce you to the diagnostic tools introduce you to the diagnostic tools introduce you to the diagnostic tools and what they look like but there's and what they look like but there's and what they look like but there's still a lot of work remaining on the still a lot of work remaining on the still a lot of work remaining on the intuitive level to understand the intuitive level to understand the intuitive level to understand the initialization the backward pass and how initialization the backward pass and how initialization the backward pass and how all of that interacts uh but you all of that interacts uh but you all of that interacts uh",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1299,
      "text": "but you shouldn't feel too bad because honestly shouldn't feel too bad because honestly shouldn't feel too bad because honestly we are getting to The Cutting Edge of we are getting to The Cutting Edge of we are getting to The Cutting Edge of where the field is where the field is where the field is we certainly haven't I would say soled we certainly haven't I would say soled we certainly haven't I would say soled initialization and we haven't soled back initialization and we haven't soled back initialization and we haven't soled back propagation and these are still very propagation and these are still very propagation and these are still very much an active area of research people much an active area of research people much an active area of research people are still trying to figure out what is are still trying to figure out what is are still trying to figure out what is the best way to initialize these the best way to initialize these the best way to initialize these networks what is the best update rule to networks what is the best update rule to networks what is the best update rule to use um and so on so none of this is use um and so on so none of this is use um and so on so none of this is really solved",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1300,
      "text": "and we don't really have really solved and we don't really have really solved",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1301,
      "text": "and we don't really have all the answers to all the to you know all the answers to all the to you know all the answers to all the to you know all these cases but at least uh you know all these cases but at least uh you know all these cases but at least uh you know we're making progress and at least we we're making progress and at least we we're making progress and at least we have some tools to tell us uh whether or have some tools to tell us uh whether or have some tools to tell us uh whether or not things are on the right track for not things are on the right track for not things are on the right track for now",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1302,
      "text": "so now so now so I think we've made positive progress in I think we've made positive progress in I think we've made positive progress in this lecture",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1303,
      "text": "and I hope you enjoyed that this lecture",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1304,
      "text": "and I hope you enjoyed that this lecture",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1305,
      "text": "and I hope you enjoyed that",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    },
    {
      "id": 1306,
      "text": "and I will see you next time",
      "start_time": "00:00:02.310",
      "end_time": "01:55:59.119"
    }
  ]
}